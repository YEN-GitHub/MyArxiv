{"2023-03-13T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.07001v1","updated":"2023-03-13T10:59:38Z","published":"2023-03-13T10:59:38Z","title":"Neural Group Recommendation Based on a Probabilistic Semantic\n  Aggregation","summary":"  Recommendation to groups of users is a challenging subfield of recommendation\nsystems. Its key concept is how and where to make the aggregation of each set\nof user information into an individual entity, such as a ranked recommendation\nlist, a virtual user, or a multi-hot input vector encoding. This paper proposes\nan innovative strategy where aggregation is made in the multi-hot vector that\nfeeds the neural network model. The aggregation provides a probabilistic\nsemantic, and the resulting input vectors feed a model that is able to\nconveniently generalize the group recommendation from the individual\npredictions. Furthermore, using the proposed architecture, group\nrecommendations can be obtained by simply feedforwarding the pre-trained model\nwith individual ratings; that is, without the need to obtain datasets\ncontaining group of user information, and without the need of running two\nseparate trainings (individual and group). This approach also avoids\nmaintaining two different models to support both individual and group learning.\nExperiments have tested the proposed architecture using three representative\ncollaborative filtering datasets and a series of baselines; results show\nsuitable accuracy improvements compared to the state-of-the-art.\n","authors":["Jorge Dueñas-Lerín","Raúl Lara-Cabrera","Fernando Ortega","Jesús Bobadilla"],"pdf_url":"https://arxiv.org/pdf/2303.07001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04426v2","updated":"2023-03-13T08:43:27Z","published":"2023-03-08T08:08:57Z","title":"NASTyLinker: NIL-Aware Scalable Transformer-based Entity Linker","summary":"  Entity Linking (EL) is the task of detecting mentions of entities in text and\ndisambiguating them to a reference knowledge base. Most prevalent EL approaches\nassume that the reference knowledge base is complete. In practice, however, it\nis necessary to deal with the case of linking to an entity that is not\ncontained in the knowledge base (NIL entity). Recent works have shown that,\ninstead of focusing only on affinities between mentions and entities,\nconsidering inter-mention affinities can be used to represent NIL entities by\nproducing clusters of mentions. At the same time, inter-mention affinities can\nhelp to substantially improve linking performance for known entities. With\nNASTyLinker, we introduce an EL approach that is aware of NIL entities and\nproduces corresponding mention clusters while maintaining high linking\nperformance for known entities. The approach clusters mentions and entities\nbased on dense representations from Transformers and resolves conflicts (if\nmore than one entity is assigned to a cluster) by computing transitive\nmention-entity affinities. We show the effectiveness and scalability of\nNASTyLinker on NILK, a dataset that is explicitly constructed to evaluate EL\nwith respect to NIL entities. Further, we apply the presented approach to an\nactual EL task, namely to knowledge graph population by linking entities in\nWikipedia listings, and provide an analysis of the outcome.\n","authors":["Nicolas Heist","Heiko Paulheim"],"pdf_url":"https://arxiv.org/pdf/2303.04426v2.pdf","comment":"Preprint of a paper in the research track of the 20th Extended\n  Semantic Web Conference (ESWC'23)"},{"id":"http://arxiv.org/abs/2303.06791v1","updated":"2023-03-13T00:39:04Z","published":"2023-03-13T00:39:04Z","title":"Beyond Single Items: Exploring User Preferences in Item Sets with the\n  Conversational Playlist Curation Dataset","summary":"  Users in consumption domains, like music, are often able to more efficiently\nprovide preferences over a set of items (e.g. a playlist or radio) than over\nsingle items (e.g. songs). Unfortunately, this is an underexplored area of\nresearch, with most existing recommendation systems limited to understanding\npreferences over single items. Curating an item set exponentiates the search\nspace that recommender systems must consider (all subsets of items!): this\nmotivates conversational approaches-where users explicitly state or refine\ntheir preferences and systems elicit preferences in natural language-as an\nefficient way to understand user needs. We call this task conversational item\nset curation and present a novel data collection methodology that efficiently\ncollects realistic preferences about item sets in a conversational setting by\nobserving both item-level and set-level feedback. We apply this methodology to\nmusic recommendation to build the Conversational Playlist Curation Dataset\n(CPCD), where we show that it leads raters to express preferences that would\nnot be otherwise expressed. Finally, we propose a wide range of conversational\nretrieval models as baselines for this task and evaluate them on the dataset.\n","authors":["Arun Tejasvi Chaganty","Megan Leszczynski","Shu Zhang","Ravi Ganti","Krisztian Balog","Filip Radlinski"],"pdf_url":"https://arxiv.org/pdf/2303.06791v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2206.00395v2","updated":"2023-03-13T17:59:40Z","published":"2022-06-01T11:02:06Z","title":"Optimization with access to auxiliary information","summary":"  We investigate the fundamental optimization question of minimizing a target\nfunction $f(x)$ whose gradients are expensive to compute or have limited\navailability, given access to some auxiliary side function $h(x)$ whose\ngradients are cheap or more available. This formulation captures many settings\nof practical relevance such as i) re-using batches in SGD, ii) transfer\nlearning, iii) federated learning, iv) training with compressed models/dropout,\netc. We propose two generic new algorithms which are applicable in all these\nsettings and prove using only an assumption on the Hessian similarity between\nthe target and side information that we can benefit from this framework.\n","authors":["El Mahdi Chayti","Sai Praneeth Karimireddy"],"pdf_url":"https://arxiv.org/pdf/2206.00395v2.pdf","comment":"We corrected a mistake that we had in Lemma 9 in the previous version\n  of the paper"},{"id":"http://arxiv.org/abs/2303.07338v1","updated":"2023-03-13T17:59:02Z","published":"2023-03-13T17:59:02Z","title":"Revisiting Class-Incremental Learning with Pre-Trained Models:\n  Generalizability and Adaptivity are All You Need","summary":"  Class-incremental learning (CIL) aims to adapt to emerging new classes\nwithout forgetting old ones. Traditional CIL models are trained from scratch to\ncontinually acquire knowledge as data evolves. Recently, pre-training has\nachieved substantial progress, making vast pre-trained models (PTMs) accessible\nfor CIL. Contrary to traditional methods, PTMs possess generalizable\nembeddings, which can be easily transferred. In this work, we revisit CIL with\nPTMs and argue that the core factors in CIL are adaptivity for model updating\nand generalizability for knowledge transferring. 1) We first reveal that frozen\nPTM can already provide generalizable embeddings for CIL. Surprisingly, a\nsimple baseline (SimpleCIL) which continually sets the classifiers of PTM to\nprototype features can beat state-of-the-art even without training on the\ndownstream task. 2) Due to the distribution gap between pre-trained and\ndownstream datasets, PTM can be further cultivated with adaptivity via model\nadapting. We propose ADapt And Merge (ADAM), which aggregates the embeddings of\nPTM and adapted models for classifier construction. ADAM is a general framework\nthat can be orthogonally combined with any parameter-efficient tuning method,\nwhich holds the advantages of PTM's generalizability and adapted model's\nadaptivity. 3) Additionally, we find previous benchmarks are unsuitable in the\nera of PTM due to data overlapping and propose four new benchmarks for\nassessment, namely ImageNet-A, ObjectNet, OmniBenchmark, and VTAB. Extensive\nexperiments validate the effectiveness of ADAM with a unified and concise\nframework.\n","authors":["Da-Wei Zhou","Han-Jia Ye","De-Chuan Zhan","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2303.07338v1.pdf","comment":"Code is available at: https://github.com/zhoudw-zdw/RevisitingCIL"},{"id":"http://arxiv.org/abs/2202.06854v3","updated":"2023-03-13T17:53:07Z","published":"2022-02-14T16:40:24Z","title":"Random Laplacian Features for Learning with Hyperbolic Space","summary":"  Due to its geometric properties, hyperbolic space can support high-fidelity\nembeddings of tree- and graph-structured data, upon which various hyperbolic\nnetworks have been developed. Existing hyperbolic networks encode geometric\npriors not only for the input, but also at every layer of the network. This\napproach involves repeatedly mapping to and from hyperbolic space, which makes\nthese networks complicated to implement, computationally expensive to scale,\nand numerically unstable to train. In this paper, we propose a simpler\napproach: learn a hyperbolic embedding of the input, then map once from it to\nEuclidean space using a mapping that encodes geometric priors by respecting the\nisometries of hyperbolic space, and finish with a standard Euclidean network.\nThe key insight is to use a random feature mapping via the eigenfunctions of\nthe Laplace operator, which we show can approximate any isometry-invariant\nkernel on hyperbolic space. Our method can be used together with any graph\nneural networks: using even a linear graph model yields significant\nimprovements in both efficiency and performance over other hyperbolic baselines\nin both transductive and inductive tasks.\n","authors":["Tao Yu","Christopher De Sa"],"pdf_url":"https://arxiv.org/pdf/2202.06854v3.pdf","comment":"ICLR 2023"},{"id":"http://arxiv.org/abs/2212.03241v2","updated":"2023-03-13T17:43:16Z","published":"2022-12-06T18:59:58Z","title":"PØDA: Prompt-driven Zero-shot Domain Adaptation","summary":"  Domain adaptation has been vastly investigated in computer vision but still\nrequires access to target images at train time, which might be intractable in\nsome uncommon conditions. In this paper, we propose the task of `Prompt-driven\nZero-shot Domain Adaptation', where we adapt a model trained on a source domain\nusing only a single general textual description of the target domain, i.e., a\nprompt. First, we leverage a pretrained contrastive vision-language model\n(CLIP) to optimize affine transformations of source features, steering them\ntowards target text embeddings, while preserving their content and semantics.\nSecond, we show that augmented features can be used to perform zero-shot domain\nadaptation for semantic segmentation. Experiments demonstrate that our method\nsignificantly outperforms CLIP-based style transfer baselines on several\ndatasets for the downstream task at hand. Our prompt-driven approach even\noutperforms one-shot unsupervised domain adaptation on some datasets, and gives\ncomparable results on others. Our code is available at\nhttps://github.com/astra-vision/PODA.\n","authors":["Mohammad Fahes","Tuan-Hung Vu","Andrei Bursuc","Patrick Pérez","Raoul de Charette"],"pdf_url":"https://arxiv.org/pdf/2212.03241v2.pdf","comment":"Project page: https://astra-vision.github.io/PODA/"},{"id":"http://arxiv.org/abs/2212.02623v3","updated":"2023-03-13T17:42:44Z","published":"2022-12-05T22:14:49Z","title":"Unifying Vision, Text, and Layout for Universal Document Processing","summary":"  We propose Universal Document Processing (UDOP), a foundation Document AI\nmodel which unifies text, image, and layout modalities together with varied\ntask formats, including document understanding and generation. UDOP leverages\nthe spatial correlation between textual content and document image to model\nimage, text, and layout modalities with one uniform representation. With a\nnovel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain\ndownstream tasks into a prompt-based sequence generation scheme. UDOP is\npretrained on both large-scale unlabeled document corpora using innovative\nself-supervised objectives and diverse labeled data. UDOP also learns to\ngenerate document images from text and layout modalities via masked image\nreconstruction. To the best of our knowledge, this is the first time in the\nfield of document AI that one model simultaneously achieves high-quality neural\ndocument editing and content customization. Our method sets the\nstate-of-the-art on 8 Document AI tasks, e.g., document understanding and QA,\nacross diverse data domains like finance reports, academic papers, and\nwebsites. UDOP ranks first on the leaderboard of the Document Understanding\nBenchmark.\n","authors":["Zineng Tang","Ziyi Yang","Guoxin Wang","Yuwei Fang","Yang Liu","Chenguang Zhu","Michael Zeng","Cha Zhang","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2212.02623v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07321v1","updated":"2023-03-13T17:42:11Z","published":"2023-03-13T17:42:11Z","title":"Collision Cross-entropy and EM Algorithm for Self-labeled Classification","summary":"  We propose \"collision cross-entropy\" as a robust alternative to the Shannon's\ncross-entropy in the context of self-labeled classification with posterior\nmodels. Assuming unlabeled data, self-labeling works by estimating latent\npseudo-labels, categorical distributions y, that optimize some discriminative\nclustering criteria, e.g. \"decisiveness\" and \"fairness\". All existing\nself-labeled losses incorporate Shannon's cross-entropy term targeting the\nmodel prediction, softmax, at the estimated distribution y. In fact, softmax is\ntrained to mimic the uncertainty in y exactly. Instead, we propose the negative\nlog-likelihood of \"collision\" to maximize the probability of equality between\ntwo random variables represented by distributions softmax and y. We show that\nour loss satisfies some properties of a generalized cross-entropy.\nInterestingly, it agrees with the Shannon's cross-entropy for one-hot\npseudo-labels y, but the training from softer labels weakens. For example, if y\nis a uniform distribution at some data point, it has zero contribution to the\ntraining. Our self-labeling loss combining collision cross entropy with basic\nclustering criteria is convex w.r.t. pseudo-labels, but non-trivial to optimize\nover the probability simplex. We derive a practical EM algorithm optimizing\npseudo-labels y significantly faster than generic methods, e.g. the projectile\ngradient descent. The collision cross-entropy consistently improves the results\non multiple self-labeled clustering examples using different DNNs.\n","authors":["Zhongwen Zhang","Yuri Boykov"],"pdf_url":"https://arxiv.org/pdf/2303.07321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07320v1","updated":"2023-03-13T17:41:57Z","published":"2023-03-13T17:41:57Z","title":"Model-tuning Via Prompts Makes NLP Models Adversarially Robust","summary":"  In recent years, NLP practitioners have converged on the following practice:\n(i) import an off-the-shelf pretrained (masked) language model; (ii) append a\nmultilayer perceptron atop the CLS token's hidden representation (with randomly\ninitialized weights); and (iii) fine-tune the entire model on a downstream task\n(MLP). This procedure has produced massive gains on standard NLP benchmarks,\nbut these models remain brittle, even to mild adversarial perturbations, such\nas word-level synonym substitutions. In this work, we demonstrate surprising\ngains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an\nalternative method of adapting to downstream tasks. Rather than modifying the\nmodel (by appending an MLP head), MVP instead modifies the input (by appending\na prompt template). Across three classification datasets, MVP improves\nperformance against adversarial word-level synonym substitutions by an average\nof 8% over standard methods and even outperforms adversarial training-based\nstate-of-art defenses by 3.5%. By combining MVP with adversarial training, we\nachieve further improvements in robust accuracy while maintaining clean\naccuracy. Finally, we conduct ablations to investigate the mechanism underlying\nthese gains. Notably, we find that the main causes of vulnerability of MLP can\nbe attributed to the misalignment between pre-training and fine-tuning tasks,\nand the randomly initialized MLP parameters. Code is available at\nhttps://github.com/acmi-lab/mvp\n","authors":["Mrigank Raman","Pratyush Maini","J. Zico Kolter","Zachary C. Lipton","Danish Pruthi"],"pdf_url":"https://arxiv.org/pdf/2303.07320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07310v1","updated":"2023-03-13T17:32:46Z","published":"2023-03-13T17:32:46Z","title":"Learning Reduced-Order Models for Cardiovascular Simulations with Graph\n  Neural Networks","summary":"  Reduced-order models based on physics are a popular choice in cardiovascular\nmodeling due to their efficiency, but they may experience reduced accuracy when\nworking with anatomies that contain numerous junctions or pathological\nconditions. We develop one-dimensional reduced-order models that simulate blood\nflow dynamics using a graph neural network trained on three-dimensional\nhemodynamic simulation data. Given the initial condition of the system, the\nnetwork iteratively predicts the pressure and flow rate at the vessel\ncenterline nodes. Our numerical results demonstrate the accuracy and\ngeneralizability of our method in physiological geometries comprising a variety\nof anatomies and boundary conditions. Our findings demonstrate that our\napproach can achieve errors below 2% and 3% for pressure and flow rate,\nrespectively, provided there is adequate training data. As a result, our method\nexhibits superior performance compared to physics-based one-dimensional models,\nwhile maintaining high efficiency at inference time.\n","authors":["Luca Pegolotti","Martin R. Pfaller","Natalia L. Rubio","Ke Ding","Rita Brugarolas Brufau","Eric Darve","Alison L. Marsden"],"pdf_url":"https://arxiv.org/pdf/2303.07310v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07308v1","updated":"2023-03-13T17:30:43Z","published":"2023-03-13T17:30:43Z","title":"NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial\n  Understanding with Objects","summary":"  We present NeuSE, a novel Neural SE(3)-Equivariant Embedding for objects, and\nillustrate how it supports object SLAM for consistent spatial understanding\nwith long-term scene changes. NeuSE is a set of latent object embeddings\ncreated from partial object observations. It serves as a compact point cloud\nsurrogate for complete object models, encoding full shape information while\ntransforming SE(3)-equivariantly in tandem with the object in the physical\nworld. With NeuSE, relative frame transforms can be directly derived from\ninferred latent codes. Our proposed SLAM paradigm, using NeuSE for object shape\nand pose characterization, can operate independently or in conjunction with\ntypical SLAM systems. It directly infers SE(3) camera pose constraints that are\ncompatible with general SLAM pose graph optimization, while also maintaining a\nlightweight object-centric map that adapts to real-world changes. Our approach\nis evaluated on synthetic and real-world sequences featuring changed objects\nand shows improved localization accuracy and change-aware mapping capability,\nwhen working either standalone or jointly with a common SLAM pipeline.\n","authors":["Jiahui Fu","Yilun Du","Kurran Singh","Joshua B. Tenenbaum","John J. Leonard"],"pdf_url":"https://arxiv.org/pdf/2303.07308v1.pdf","comment":"Project webpage: https://neuse-slam.github.io/neuse/"},{"id":"http://arxiv.org/abs/2303.07305v1","updated":"2023-03-13T17:30:04Z","published":"2023-03-13T17:30:04Z","title":"Transformer Models for Acute Brain Dysfunction Prediction","summary":"  Acute brain dysfunctions (ABD), which include coma and delirium, are\nprevalent in the ICU, especially among older patients. The current approach in\nmanual assessment of ABD by care providers may be sporadic and subjective.\nHence, there exists a need for a data-driven robust system automating the\nassessment and prediction of ABD. In this work, we develop a machine learning\nsystem for real-time prediction of ADB using Electronic Health Record (HER)\ndata. Our data processing pipeline enables integration of static and temporal\ndata, and extraction of features relevant to ABD. We train several\nstate-of-the-art transformer models and baseline machine learning models\nincluding CatBoost and XGB on the data that was collected from patients\nadmitted to the ICU at UF Shands Hospital. We demonstrate the efficacy of our\nsystem for tasks related to acute brain dysfunction including binary\nclassification of brain acuity and multi-class classification (i.e., coma,\ndelirium, death, or normal), achieving a mean AUROC of 0.953 on our Long-former\nimplementation. Our system can then be deployed for real-time prediction of ADB\nin ICUs to reduce the number of incidents caused by ABD. Moreover, the\nreal-time system has the potential to reduce costs, duration of patients stays\nin the ICU, and mortality among those afflicted.\n","authors":["Brandon Silva","Miguel Contreras","Tezcan Ozrazgat Baslanti","Yuanfang Ren","Guan Ziyuan","Kia Khezeli","Azra Bihorac","Parisa Rashidi"],"pdf_url":"https://arxiv.org/pdf/2303.07305v1.pdf","comment":"15 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2202.04110v3","updated":"2023-03-13T17:20:47Z","published":"2022-02-08T19:27:48Z","title":"PGMax: Factor Graphs for Discrete Probabilistic Graphical Models and\n  Loopy Belief Propagation in JAX","summary":"  PGMax is an open-source Python package for (a) easily specifying discrete\nProbabilistic Graphical Models (PGMs) as factor graphs; and (b) automatically\nrunning efficient and scalable loopy belief propagation (LBP) in JAX. PGMax\nsupports general factor graphs with tractable factors, and leverages modern\naccelerators like GPUs for inference. Compared with existing alternatives,\nPGMax obtains higher-quality inference results with up to three\norders-of-magnitude inference time speedups. PGMax additionally interacts\nseamlessly with the rapidly growing JAX ecosystem, opening up new research\npossibilities. Our source code, examples and documentation are available at\nhttps://github.com/deepmind/PGMax.\n","authors":["Guangyao Zhou","Antoine Dedieu","Nishanth Kumar","Miguel Lázaro-Gredilla","Shrinu Kushagra","Dileep George"],"pdf_url":"https://arxiv.org/pdf/2202.04110v3.pdf","comment":"Update authors list"},{"id":"http://arxiv.org/abs/2303.07295v1","updated":"2023-03-13T17:17:11Z","published":"2023-03-13T17:17:11Z","title":"Meet in the Middle: A New Pre-training Paradigm","summary":"  Most language models (LMs) are trained and applied in an autoregressive\nleft-to-right fashion, assuming that the next token only depends on the\npreceding ones. However, this assumption ignores the potential benefits of\nusing the full sequence information during training, and the possibility of\nhaving context from both sides during inference. In this paper, we propose a\nnew pre-training paradigm with techniques that jointly improve the training\ndata efficiency and the capabilities of the LMs in the infilling task. The\nfirst is a training objective that aligns the predictions of a left-to-right LM\nwith those of a right-to-left LM, trained on the same data but in reverse\norder. The second is a bidirectional inference procedure that enables both LMs\nto meet in the middle. We show the effectiveness of our pre-training paradigm\nwith extensive experiments on both programming and natural language models,\noutperforming strong baselines.\n","authors":["Anh Nguyen","Nikos Karampatziakis","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2303.07295v1.pdf","comment":"24 pages, 2 figures"},{"id":"http://arxiv.org/abs/2301.02886v2","updated":"2023-03-13T17:16:37Z","published":"2023-01-07T16:17:48Z","title":"Perceptual-Neural-Physical Sound Matching","summary":"  Sound matching algorithms seek to approximate a target waveform by parametric\naudio synthesis. Deep neural networks have achieved promising results in\nmatching sustained harmonic tones. However, the task is more challenging when\ntargets are nonstationary and inharmonic, e.g., percussion. We attribute this\nproblem to the inadequacy of loss function. On one hand, mean square error in\nthe parametric domain, known as \"P-loss\", is simple and fast but fails to\naccommodate the differing perceptual significance of each parameter. On the\nother hand, mean square error in the spectrotemporal domain, known as \"spectral\nloss\", is perceptually motivated and serves in differentiable digital signal\nprocessing (DDSP). Yet, spectral loss is a poor predictor of pitch intervals\nand its gradient may be computationally expensive; hence a slow convergence.\nAgainst this conundrum, we present Perceptual-Neural-Physical loss (PNP). PNP\nis the optimal quadratic approximation of spectral loss while being as fast as\nP-loss during training. We instantiate PNP with physical modeling synthesis as\ndecoder and joint time-frequency scattering transform (JTFS) as spectral\nrepresentation. We demonstrate its potential on matching synthetic drum sounds\nin comparison with other loss functions.\n","authors":["Han Han","Vincent Lostanlen","Mathieu Lagrange"],"pdf_url":"https://arxiv.org/pdf/2301.02886v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.12276v2","updated":"2023-03-13T17:13:04Z","published":"2022-06-16T23:08:27Z","title":"Multi-Frequency Joint Community Detection and Phase Synchronization","summary":"  This paper studies the joint community detection and phase synchronization\nproblem on the stochastic block model with relative phase, where each node is\nassociated with an unknown phase angle. This problem, with a variety of\nreal-world applications, aims to recover the cluster structure and associated\nphase angles simultaneously. We show this problem exhibits a\n``multi-frequency'' structure by closely examining its maximum likelihood\nestimation (MLE) formulation, whereas existing methods are not originated from\nthis perspective. To this end, two simple yet efficient algorithms that\nleverage the MLE formulation and benefit from the information across multiple\nfrequencies are proposed. The former is a spectral method based on the novel\nmulti-frequency column-pivoted QR factorization. The factorization applied to\nthe top eigenvectors of the observation matrix provides key information about\nthe cluster structure and associated phase angles. The second approach is an\niterative multi-frequency generalized power method, where each iteration\nupdates the estimation in a matrix-multiplication-then-projection manner.\nNumerical experiments show that our proposed algorithms significantly improve\nthe ability of exactly recovering the cluster structure and the accuracy of the\nestimated phase angles, compared to state-of-the-art algorithms.\n","authors":["Lingda Wang","Zhizhen Zhao"],"pdf_url":"https://arxiv.org/pdf/2206.12276v2.pdf","comment":"Accepted by IEEE Transactions on Signal and Information Processing\n  over Networks"},{"id":"http://arxiv.org/abs/2303.07287v1","updated":"2023-03-13T17:03:19Z","published":"2023-03-13T17:03:19Z","title":"Tight Non-asymptotic Inference via Sub-Gaussian Intrinsic Moment Norm","summary":"  In non-asymptotic statistical inferences, variance-type parameters of\nsub-Gaussian distributions play a crucial role. However, direct estimation of\nthese parameters based on the empirical moment generating function (MGF) is\ninfeasible. To this end, we recommend using a sub-Gaussian intrinsic moment\nnorm [Buldygin and Kozachenko (2000), Theorem 1.3] through maximizing a series\nof normalized moments. Importantly, the recommended norm can not only recover\nthe exponential moment bounds for the corresponding MGFs, but also lead to\ntighter Hoeffding's sub-Gaussian concentration inequalities. In practice,\n{\\color{black} we propose an intuitive way of checking sub-Gaussian data with a\nfinite sample size by the sub-Gaussian plot}. Intrinsic moment norm can be\nrobustly estimated via a simple plug-in approach. Our theoretical results are\napplied to non-asymptotic analysis, including the multi-armed bandit.\n","authors":["Huiming Zhang","Haoyu Wei","Guang Cheng"],"pdf_url":"https://arxiv.org/pdf/2303.07287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07280v1","updated":"2023-03-13T16:54:11Z","published":"2023-03-13T16:54:11Z","title":"Vision-Language Models as Success Detectors","summary":"  Detecting successful behaviour is crucial for training intelligent agents. As\nsuch, generalisable reward models are a prerequisite for agents that can learn\nto generalise their behaviour. In this work we focus on developing robust\nsuccess detectors that leverage large, pretrained vision-language models\n(Flamingo, Alayrac et al. (2022)) and human reward annotations. Concretely, we\ntreat success detection as a visual question answering (VQA) problem, denoted\nSuccessVQA. We study success detection across three vastly different domains:\n(i) interactive language-conditioned agents in a simulated household, (ii) real\nworld robotic manipulation, and (iii) \"in-the-wild\" human egocentric videos. We\ninvestigate the generalisation properties of a Flamingo-based success detection\nmodel across unseen language and visual changes in the first two domains, and\nfind that the proposed method is able to outperform bespoke reward models in\nout-of-distribution test scenarios with either variation. In the last domain of\n\"in-the-wild\" human videos, we show that success detection on unseen real\nvideos presents an even more challenging generalisation task warranting future\nwork. We hope our initial results encourage further work in real world success\ndetection and reward modelling.\n","authors":["Yuqing Du","Ksenia Konyushkova","Misha Denil","Akhil Raju","Jessica Landon","Felix Hill","Nando de Freitas","Serkan Cabi"],"pdf_url":"https://arxiv.org/pdf/2303.07280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10999v2","updated":"2023-03-13T16:51:03Z","published":"2022-11-20T15:27:55Z","title":"LA-VocE: Low-SNR Audio-visual Speech Enhancement using Neural Vocoders","summary":"  Audio-visual speech enhancement aims to extract clean speech from a noisy\nenvironment by leveraging not only the audio itself but also the target\nspeaker's lip movements. This approach has been shown to yield improvements\nover audio-only speech enhancement, particularly for the removal of interfering\nspeech. Despite recent advances in speech synthesis, most audio-visual\napproaches continue to use spectral mapping/masking to reproduce the clean\naudio, often resulting in visual backbones added to existing speech enhancement\narchitectures. In this work, we propose LA-VocE, a new two-stage approach that\npredicts mel-spectrograms from noisy audio-visual speech via a\ntransformer-based architecture, and then converts them into waveform audio\nusing a neural vocoder (HiFi-GAN). We train and evaluate our framework on\nthousands of speakers and 11+ different languages, and study our model's\nability to adapt to different levels of background noise and speech\ninterference. Our experiments show that LA-VocE outperforms existing methods\naccording to multiple metrics, particularly under very noisy scenarios.\n","authors":["Rodrigo Mira","Buye Xu","Jacob Donley","Anurag Kumar","Stavros Petridis","Vamsi Krishna Ithapu","Maja Pantic"],"pdf_url":"https://arxiv.org/pdf/2211.10999v2.pdf","comment":"accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07275v1","updated":"2023-03-13T16:49:43Z","published":"2023-03-13T16:49:43Z","title":"A Survey of Graph Prompting Methods: Techniques, Applications, and\n  Challenges","summary":"  While deep learning has achieved great success on various tasks, the\ntask-specific model training notoriously relies on a large volume of labeled\ndata. Recently, a new training paradigm of ``pre-train, prompt, predict'' has\nbeen proposed to improve model generalization ability with limited labeled\ndata. The main idea is that, based on a pre-trained model, the prompting\nfunction uses a template to augment input samples with indicative context and\nreformalizes the target task to one of the pre-training tasks. In this survey,\nwe provide a unique review of prompting methods from the graph perspective.\nGraph data has served as structured knowledge repositories in various systems\nby explicitly modeling the interaction between entities. Compared with\ntraditional methods, graph prompting functions could induce task-related\ncontext and apply templates with structured knowledge. The pre-trained model is\nthen adaptively generalized for future samples. In particular, we introduce the\nbasic concepts of graph prompt learning, organize the existing work of\ndesigning graph prompting functions, and describe their applications and\nchallenges to a variety of machine learning problems. This survey attempts to\nbridge the gap between structured graphs and prompt design to facilitate future\nmethodology development.\n","authors":["Xuansheng Wu","Kaixiong Zhou","Mingchen Sun","Xin Wang","Ninghao Liu"],"pdf_url":"https://arxiv.org/pdf/2303.07275v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2303.07264v1","updated":"2023-03-13T16:44:15Z","published":"2023-03-13T16:44:15Z","title":"A Surface-normal Based Neural Framework for Colonoscopy Reconstruction","summary":"  Reconstructing a 3D surface from colonoscopy video is challenging due to\nillumination and reflectivity variation in the video frame that can cause\ndefective shape predictions. Aiming to overcome this challenge, we utilize the\ncharacteristics of surface normal vectors and develop a two-step neural\nframework that significantly improves the colonoscopy reconstruction quality.\nThe normal-based depth initialization network trained with self-supervised\nnormal consistency loss provides depth map initialization to the normal-depth\nrefinement module, which utilizes the relationship between illumination and\nsurface normals to refine the frame-wise normal and depth predictions\nrecursively. Our framework's depth accuracy performance on phantom colonoscopy\ndata demonstrates the value of exploiting the surface normals in colonoscopy\nreconstruction, especially on en face views. Due to its low depth error, the\nprediction result from our framework will require limited post-processing to be\nclinically applicable for real-time colonoscopy reconstruction.\n","authors":["Shuxian Wang","Yubo Zhang","Sarah K. McGill","Julian G. Rosenman","Jan-Michael Frahm","Soumyadip Sengupta","Stephen M. Pizer"],"pdf_url":"https://arxiv.org/pdf/2303.07264v1.pdf","comment":"Accepted at IPMI 2023; first two authors contributed equally"},{"id":"http://arxiv.org/abs/2201.09635v4","updated":"2023-03-13T16:36:11Z","published":"2022-01-24T12:30:38Z","title":"State-Conditioned Adversarial Subgoal Generation","summary":"  Hierarchical reinforcement learning (HRL) proposes to solve difficult tasks\nby performing decision-making and control at successively higher levels of\ntemporal abstraction. However, off-policy HRL often suffers from the problem of\na non-stationary high-level policy since the low-level policy is constantly\nchanging. In this paper, we propose a novel HRL approach for mitigating the\nnon-stationarity by adversarially enforcing the high-level policy to generate\nsubgoals compatible with the current instantiation of the low-level policy. In\npractice, the adversarial learning is implemented by training a simple\nstate-conditioned discriminator network concurrently with the high-level policy\nwhich determines the compatibility level of subgoals. Comparison to\nstate-of-the-art algorithms shows that our approach improves both learning\nefficiency and performance in challenging continuous control tasks.\n","authors":["Vivienne Huiling Wang","Joni Pajarinen","Tinghuai Wang","Joni-Kristian Kämäräinen"],"pdf_url":"https://arxiv.org/pdf/2201.09635v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07248v1","updated":"2023-03-13T16:22:16Z","published":"2023-03-13T16:22:16Z","title":"Channel Estimation for Underwater Visible Light Communication: A Sparse\n  Learning Perspective","summary":"  The underwater propagation environment for visible light signals is affected\nby complex factors such as absorption, shadowing, and reflection, making it\nvery challengeable to achieve effective underwater visible light communication\n(UVLC) channel estimation. It is difficult for the UVLC channel to be sparse\nrepresented in the time and frequency domains, which limits the chance of using\nsparse signal processing techniques to achieve better performance of channel\nestimation. To this end, a compressed sensing (CS) based framework is\nestablished in this paper by fully exploiting the sparsity of the underwater\nvisible light channel in the distance domain of the propagation links. In order\nto solve the sparse recovery problem and achieve more accurate UVLC channel\nestimation, a sparse learning based underwater visible light channel estimation\n(SL-UVCE) scheme is proposed. Specifically, a deep-unfolding neural network\nmimicking the classical iterative sparse recovery algorithm of approximate\nmessage passing (AMP) is employed, which decomposes the iterations of AMP into\na series of layers with different learnable parameters. Compared with the\nexisting non-CS-based and CS-based schemes, the proposed scheme shows better\nperformance of accuracy in channel estimation, especially in severe conditions\nsuch as insufficient measurement pilots and large number of multipath\ncomponents.\n","authors":["Younan Mou","Sicong Liu"],"pdf_url":"https://arxiv.org/pdf/2303.07248v1.pdf","comment":"This paper has been accepted by and is to appear in Proc. 2023 IEEE\n  International Conference on Communications (ICC)"},{"id":"http://arxiv.org/abs/2210.16192v2","updated":"2023-03-13T16:21:28Z","published":"2022-10-27T12:59:00Z","title":"Learning Audio Features with Metadata and Contrastive Learning","summary":"  Methods based on supervised learning using annotations in an end-to-end\nfashion have been the state-of-the-art for classification problems. However,\nthey may be limited in their generalization capability, especially in the low\ndata regime. In this study, we address this issue using supervised contrastive\nlearning combined with available metadata to solve multiple pretext tasks that\nlearn a good representation of data. We apply our approach on ICBHI, a\nrespiratory sound classification dataset suited for this setting. We show that\nlearning representations using only metadata, without class labels, obtains\nsimilar performance as using cross entropy with those labels only. In addition,\nwe obtain state-of-the-art score when combining class labels with metadata\nusing multiple supervised contrastive learning. This work suggests the\npotential of using multiple metadata sources in supervised contrastive\nsettings, in particular in settings with class imbalance and few data. Our code\nis released at https://github.com/ilyassmoummad/scl_icbhi2017\n","authors":["Ilyass Moummad","Nicolas Farrugia"],"pdf_url":"https://arxiv.org/pdf/2210.16192v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05737v2","updated":"2023-03-13T16:19:43Z","published":"2023-03-10T06:46:23Z","title":"Clinical BERTScore: An Improved Measure of Automatic Speech Recognition\n  Performance in Clinical Settings","summary":"  Automatic Speech Recognition (ASR) in medical contexts has the potential to\nsave time, cut costs, increase report accuracy, and reduce physician burnout.\nHowever, the healthcare industry has been slower to adopt this technology, in\npart due to the importance of avoiding medically-relevant transcription\nmistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR\nmetric that penalizes clinically-relevant mistakes more than others. We\ndemonstrate that this metric more closely aligns with clinician preferences on\nmedical sentences as compared to other metrics (WER, BLUE, METEOR, etc),\nsometimes by wide margins. We collect a benchmark of 13 clinician preferences\non 149 realistic medical sentences called the Clinician Transcript Preference\nbenchmark (CTP), demonstrate that CBERTScore more closely matches what\nclinicians prefer, and release the benchmark for the community to further\ndevelop clinically-aware ASR metrics.\n","authors":["Joel Shor","Ruyue Agnes Bi","Subhashini Venugopalan","Steven Ibara","Roman Goldenberg","Ehud Rivlin"],"pdf_url":"https://arxiv.org/pdf/2303.05737v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.09435v4","updated":"2023-03-13T16:15:46Z","published":"2022-05-19T09:50:25Z","title":"Adversarial random forests for density estimation and generative\n  modeling","summary":"  We propose methods for density estimation and data synthesis using a novel\nform of unsupervised random forests. Inspired by generative adversarial\nnetworks, we implement a recursive procedure in which trees gradually learn\nstructural properties of the data through alternating rounds of generation and\ndiscrimination. The method is provably consistent under minimal assumptions.\nUnlike classic tree-based alternatives, our approach provides smooth\n(un)conditional densities and allows for fully synthetic data generation. We\nachieve comparable or superior performance to state-of-the-art probabilistic\ncircuits and deep learning models on various tabular data benchmarks while\nexecuting about two orders of magnitude faster on average. An accompanying\n$\\texttt{R}$ package, $\\texttt{arf}$, is available on $\\texttt{CRAN}$.\n","authors":["David S. Watson","Kristin Blesch","Jan Kapar","Marvin N. Wright"],"pdf_url":"https://arxiv.org/pdf/2205.09435v4.pdf","comment":"Camera ready version (AISTATS 2023)"},{"id":"http://arxiv.org/abs/2303.07240v1","updated":"2023-03-13T16:13:16Z","published":"2023-03-13T16:13:16Z","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical\n  Documents","summary":"  Foundation models trained on large-scale dataset gain a recent surge in CV\nand NLP. In contrast, development in biomedical domain lags far behind due to\ndata scarcity. To address this issue, we build and release PMC-OA, a biomedical\ndataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess\nsubset, which is 8 times larger than before. PMC-OA covers diverse modalities\nor diseases, with majority of the image-caption samples aligned at\nfiner-grained level, i.e., subfigure and subcaption. While pretraining a\nCLIP-style model on PMC-OA, our model named PMC-CLIP achieves state-of-the-art\nresults on various downstream tasks, including image-text retrieval on ROCO,\nMedMNIST image classification, Medical VQA, i.e. +8.1% R@10 on image-text\nretrieval, +3.9% accuracy on image classification.\n","authors":["Weixiong Lin","Ziheng Zhao","Xiaoman Zhang","Chaoyi Wu","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2303.07240v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2210.10488v4","updated":"2023-03-13T16:05:11Z","published":"2022-10-19T11:53:13Z","title":"Attribution and Obfuscation of Neural Text Authorship: A Data Mining\n  Perspective","summary":"  Two interlocking research questions of growing interest and importance in\nprivacy research are Authorship Attribution (AA) and Authorship Obfuscation\n(AO). Given an artifact, especially a text t in question, an AA solution aims\nto accurately attribute t to its true author out of many candidate authors\nwhile an AO solution aims to modify t to hide its true authorship.\nTraditionally, the notion of authorship and its accompanying privacy concern is\nonly toward human authors. However, in recent years, due to the explosive\nadvancements in Neural Text Generation (NTG) techniques in NLP, capable of\nsynthesizing human-quality open-ended texts (so-called \"neural texts\"), one has\nto now consider authorships by humans, machines, or their combination. Due to\nthe implications and potential threats of neural texts when used maliciously,\nit has become critical to understand the limitations of traditional AA/AO\nsolutions and develop novel AA/AO solutions in dealing with neural texts. In\nthis survey, therefore, we make a comprehensive review of recent literature on\nthe attribution and obfuscation of neural text authorship from a Data Mining\nperspective, and share our view on their limitations and promising research\ndirections.\n","authors":["Adaku Uchendu","Thai Le","Dongwon Lee"],"pdf_url":"https://arxiv.org/pdf/2210.10488v4.pdf","comment":"Accepted at ACM SIGKDD Explorations, Vol. 25, June 2023"},{"id":"http://arxiv.org/abs/2211.02641v2","updated":"2023-03-13T15:57:19Z","published":"2022-10-25T04:48:11Z","title":"Graph Neural Networks on SPD Manifolds for Motor Imagery Classification:\n  A Perspective from the Time-Frequency Analysis","summary":"  The classification of motor imagery (MI) is a highly sought-after research\ntopic in the field of Electroencephalography (EEG)-based brain-computer\ninterfaces (BCIs), with immense commercial value. Over the past two decades,\nthere has been a fundamental shift in the trend of MI-EEG classifiers,\nresulting in a gradual increase in their performance. The emergence of\nTensor-CSPNet, the first geometric deep learning (GDL) framework in BCI\nresearch, is attributed to the imperative of characterizing the non-Euclidean\nnature of signals. Fundamentally, Tensor-CSPNet is a deep learning-based\nclassifier that capitalizes on the second-order statistics of EEGs. In contrast\nto the conventional approach of utilizing first-order statistics for EEG\nsignals, the utilization of these second-order statistics represents the\nclassical treatment. These statistics provide adequate discriminative\ninformation, rendering them suitable for MI-EEG classification. In this study,\nwe introduce another GDL classifier, called Graph-CSPNet, for MI-EEG\nclassification. Graph-CSPNet utilizes graph-based techniques to characterize\nEEG signals in both the time and frequency domains, realizing the fundamental\nperspective of time-frequency analysis. The architecture of Graph-CSPNet is\nfurther simplified, offering greater flexibility to cope with variable\ntime-frequency resolution for signal segmentation and capturing localized\nfluctuations. In contrast to Tensor-CSPNet, this approach enables Graph-CSPNet\nto achieve better results in MI-EEG classification. To evaluate the efficacy of\nGraph-CSPNet, we utilize five commonly-used publicly available MI-EEG datasets,\nand it produces near-optimal classification accuracies, winning nine out of\neleven subject-specific scenarios. The Python implementation of Graph-CSPNet is\navailable on a GitHub repository\nhttps://github.com/GeometricBCI/Tensor-CSPNet-and-Graph-CSPNet.\n","authors":["Ce Ju","Cuntai Guan"],"pdf_url":"https://arxiv.org/pdf/2211.02641v2.pdf","comment":"17 pages, 5 figures, 11 Tables; This work has been submitted to the\n  IEEE for possible publication. Copyright may be transferred without notice,\n  after which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2208.14698v5","updated":"2023-03-13T15:57:00Z","published":"2022-08-31T08:47:02Z","title":"Bayesian Optimization-based Combinatorial Assignment","summary":"  We study the combinatorial assignment domain, which includes combinatorial\nauctions and course allocation. The main challenge in this domain is that the\nbundle space grows exponentially in the number of items. To address this,\nseveral papers have recently proposed machine learning-based preference\nelicitation algorithms that aim to elicit only the most important information\nfrom agents. However, the main shortcoming of this prior work is that it does\nnot model a mechanism's uncertainty over values for not yet elicited bundles.\nIn this paper, we address this shortcoming by presenting a Bayesian\noptimization-based combinatorial assignment (BOCA) mechanism. Our key technical\ncontribution is to integrate a method for capturing model uncertainty into an\niterative combinatorial auction mechanism. Concretely, we design a new method\nfor estimating an upper uncertainty bound that can be used to define an\nacquisition function to determine the next query to the agents. This enables\nthe mechanism to properly explore (and not just exploit) the bundle space\nduring its preference elicitation phase. We run computational experiments in\nseveral spectrum auction domains to evaluate BOCA's performance. Our results\nshow that BOCA achieves higher allocative efficiency than state-of-the-art\napproaches.\n","authors":["Jakob Weissteiner","Jakob Heiss","Julien Siems","Sven Seuken"],"pdf_url":"https://arxiv.org/pdf/2208.14698v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.13939v2","updated":"2023-03-13T15:50:44Z","published":"2022-04-29T08:32:02Z","title":"Short-Term Density Forecasting of Low-Voltage Load using\n  Bernstein-Polynomial Normalizing Flows","summary":"  The transition to a fully renewable energy grid requires better forecasting\nof demand at the low-voltage level to increase efficiency and ensure reliable\ncontrol. However, high fluctuations and increasing electrification cause huge\nforecast variability, not reflected in traditional point estimates.\nProbabilistic load forecasts take future uncertainties into account and thus\nallow more informed decision-making for the planning and operation of\nlow-carbon energy systems. We propose an approach for flexible conditional\ndensity forecasting of short-term load based on Bernstein polynomial\nnormalizing flows, where a neural network controls the parameters of the flow.\nIn an empirical study with 363 smart meter customers, our density predictions\ncompare favorably against Gaussian and Gaussian mixture densities. Also, they\noutperform a non-parametric approach based on the pinball loss for 24h-ahead\nload forecasting for two different neural network architectures.\n","authors":["Marcel Arpogaus","Marcus Voss","Beate Sick","Mark Nigge-Uricher","Oliver Dürr"],"pdf_url":"https://arxiv.org/pdf/2204.13939v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.00047v2","updated":"2023-03-13T15:41:00Z","published":"2023-01-31T19:28:00Z","title":"Probabilistic Point Cloud Modeling via Self-Organizing Gaussian Mixture\n  Models","summary":"  This letter presents a continuous probabilistic modeling methodology for\nspatial point cloud data using finite Gaussian Mixture Models (GMMs) where the\nnumber of components are adapted based on the scene complexity. Few\nhierarchical and adaptive methods have been proposed to address the challenge\nof balancing model fidelity with size. Instead, state-of-the-art mapping\napproaches require tuning parameters for specific use cases, but do not\ngeneralize across diverse environments. To address this gap, we utilize a\nself-organizing principle from information-theoretic learning to automatically\nadapt the complexity of the GMM model based on the relevant information in the\nsensor data. The approach is evaluated against existing point cloud modeling\ntechniques on real-world data with varying degrees of scene complexity.\n","authors":["Kshitij Goel","Nathan Michael","Wennie Tabib"],"pdf_url":"https://arxiv.org/pdf/2302.00047v2.pdf","comment":"8 pages, 6 figures, to appear in IEEE Robotics and Automation Letters"},{"id":"http://arxiv.org/abs/2302.14853v2","updated":"2023-03-13T15:37:44Z","published":"2023-02-28T18:51:55Z","title":"An Efficient Tester-Learner for Halfspaces","summary":"  We give the first efficient algorithm for learning halfspaces in the testable\nlearning model recently defined by Rubinfeld and Vasilyan (2023). In this\nmodel, a learner certifies that the accuracy of its output hypothesis is near\noptimal whenever the training set passes an associated test, and training sets\ndrawn from some target distribution -- e.g., the Gaussian -- must pass the\ntest. This model is more challenging than distribution-specific agnostic or\nMassart noise models where the learner is allowed to fail arbitrarily if the\ndistributional assumption does not hold.\n  We consider the setting where the target distribution is Gaussian (or more\ngenerally any strongly log-concave distribution) in $d$ dimensions and the\nnoise model is either Massart or adversarial (agnostic). For Massart noise, our\ntester-learner runs in polynomial time and outputs a hypothesis with\n(information-theoretically optimal) error $\\mathsf{opt} + \\epsilon$ for any\nstrongly log-concave target distribution. For adversarial noise, our\ntester-learner obtains error $O(\\mathsf{opt}) + \\epsilon$ in polynomial time\nwhen the target distribution is Gaussian; for strongly log-concave\ndistributions, we obtain $\\tilde{O}(\\mathsf{opt}) + \\epsilon$ in\nquasipolynomial time.\n  Prior work on testable learning ignores the labels in the training set and\nchecks that the empirical moments of the covariates are close to the moments of\nthe base distribution. Here we develop new tests of independent interest that\nmake critical use of the labels and combine them with the moment-matching\napproach of Gollakota et al. (2023). This enables us to simulate a variant of\nthe algorithm of Diakonikolas et al. (2020) for learning noisy halfspaces using\nnonconvex SGD but in the testable learning setting.\n","authors":["Aravind Gollakota","Adam R. Klivans","Konstantinos Stavropoulos","Arsen Vasilyan"],"pdf_url":"https://arxiv.org/pdf/2302.14853v2.pdf","comment":"26 pages, 3 figures, Version v2: strengthened the agnostic guarantee"},{"id":"http://arxiv.org/abs/2303.07189v1","updated":"2023-03-13T15:30:28Z","published":"2023-03-13T15:30:28Z","title":"Optimizing Convolutional Neural Networks for Chronic Obstructive\n  Pulmonary Disease Detection in Clinical Computed Tomography Imaging","summary":"  Chronic Obstructive Pulmonary Disease (COPD) is a leading cause of death\nworldwide, yet early detection and treatment can prevent the progression of the\ndisease. In contrast to the conventional method of detecting COPD with\nspirometry tests, X-ray Computed Tomography (CT) scans of the chest provide a\nmeasure of morphological changes in the lung. It has been shown that automated\ndetection of COPD can be performed with deep learning models. However, the\npotential of incorporating optimal window setting selection, typically carried\nout by clinicians during examination of CT scans for COPD, is generally\noverlooked in deep learning approaches. We aim to optimize the binary\nclassification of COPD with densely connected convolutional neural networks\n(DenseNets) through implementation of manual and automated Window-Setting\nOptimization (WSO) steps. Our dataset consisted of 78 CT scans from the\nKlinikum rechts der Isar research hospital. Repeated inference on the test set\nshowed that without WSO, the plain DenseNet resulted in a mean slice-level AUC\nof 0.80$\\pm$0.05. With input images manually adjusted to the emphysema window\nsetting, the plain DenseNet model predicted COPD with a mean AUC of\n0.86$\\pm$0.04. By automating the WSO through addition of a customized layer to\nthe DenseNet, an optimal window setting in the proximity of the emphysema\nwindow setting was learned and a mean AUC of 0.82$\\pm$0.04 was achieved.\nDetection of COPD with DenseNet models was optimized by WSO of CT data to the\nemphysema window setting range, demonstrating the importance of implementing\noptimal window setting selection in the deep learning pipeline.\n","authors":["Tina Dorosti","Manuel Schultheiss","Felix Hofmann","Luisa Kirchner","Theresa Urban","Franz Pfeiffer","Johannes Thalhammer","Florian Schaff","Tobias Lasser","Daniela Pfeiffer"],"pdf_url":"https://arxiv.org/pdf/2303.07189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07184v1","updated":"2023-03-13T15:27:07Z","published":"2023-03-13T15:27:07Z","title":"Traffic Prediction with Transfer Learning: A Mutual Information-based\n  Approach","summary":"  In modern traffic management, one of the most essential yet challenging tasks\nis accurately and timely predicting traffic. It has been well investigated and\nexamined that deep learning-based Spatio-temporal models have an edge when\nexploiting Spatio-temporal relationships in traffic data. Typically,\ndata-driven models require vast volumes of data, but gathering data in small\ncities can be difficult owing to constraints such as equipment deployment and\nmaintenance costs. To resolve this problem, we propose TrafficTL, a cross-city\ntraffic prediction approach that uses big data from other cities to aid\ndata-scarce cities in traffic prediction. Utilizing a periodicity-based\ntransfer paradigm, it identifies data similarity and reduces negative transfer\ncaused by the disparity between two data distributions from distant cities. In\naddition, the suggested method employs graph reconstruction techniques to\nrectify defects in data from small data cities. TrafficTL is evaluated by\ncomprehensive case studies on three real-world datasets and outperforms the\nstate-of-the-art baseline by around 8 to 25 percent.\n","authors":["Yunjie Huang","Xiaozhuang Song","Yuanshao Zhu","Shiyao Zhang","James J. Q. Yu"],"pdf_url":"https://arxiv.org/pdf/2303.07184v1.pdf","comment":"submited to T-ITS, 16 pages, 13 figures in color"},{"id":"http://arxiv.org/abs/2303.07180v1","updated":"2023-03-13T15:22:50Z","published":"2023-03-13T15:22:50Z","title":"Incomplete Multi-View Multi-Label Learning via Label-Guided Masked View-\n  and Category-Aware Transformers","summary":"  As we all know, multi-view data is more expressive than single-view data and\nmulti-label annotation enjoys richer supervision information than single-label,\nwhich makes multi-view multi-label learning widely applicable for various\npattern recognition tasks. In this complex representation learning problem,\nthree main challenges can be characterized as follows: i) How to learn\nconsistent representations of samples across all views? ii) How to exploit and\nutilize category correlations of multi-label to guide inference? iii) How to\navoid the negative impact resulting from the incompleteness of views or labels?\nTo cope with these problems, we propose a general multi-view multi-label\nlearning framework named label-guided masked view- and category-aware\ntransformers in this paper. First, we design two transformer-style based\nmodules for cross-view features aggregation and multi-label classification,\nrespectively. The former aggregates information from different views in the\nprocess of extracting view-specific features, and the latter learns subcategory\nembedding to improve classification performance. Second, considering the\nimbalance of expressive power among views, an adaptively weighted view fusion\nmodule is proposed to obtain view-consistent embedding features. Third, we\nimpose a label manifold constraint in sample-level representation learning to\nmaximize the utilization of supervised information. Last but not least, all the\nmodules are designed under the premise of incomplete views and labels, which\nmakes our method adaptable to arbitrary multi-view and multi-label data.\nExtensive experiments on five datasets confirm that our method has clear\nadvantages over other state-of-the-art methods.\n","authors":["Chengliang Liu","Jie Wen","Xiaoling Luo","Yong Xu"],"pdf_url":"https://arxiv.org/pdf/2303.07180v1.pdf","comment":"Accepted to AAAI-23"},{"id":"http://arxiv.org/abs/2303.07172v1","updated":"2023-03-13T15:14:26Z","published":"2023-03-13T15:14:26Z","title":"Evaluating Visual Number Discrimination in Deep Neural Networks","summary":"  The ability to discriminate between large and small quantities is a core\naspect of basic numerical competence in both humans and animals. In this work,\nwe examine the extent to which the state-of-the-art neural networks designed\nfor vision exhibit this basic ability. Motivated by studies in animal and\ninfant numerical cognition, we use the numerical bisection procedure to test\nnumber discrimination in different families of neural architectures. Our\nresults suggest that vision-specific inductive biases are helpful in numerosity\ndiscrimination, as models with such biases have lowest test errors on the task,\nand often have psychometric curves that qualitatively resemble those of humans\nand animals performing the task. However, even the strongest models, as\nmeasured on standard metrics of performance, fail to discriminate quantities in\ntransfer experiments with differing training and testing conditions, indicating\nthat such inductive biases might not be sufficient.\n","authors":["Ivana Kajić","Aida Nematzadeh"],"pdf_url":"https://arxiv.org/pdf/2303.07172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03984v2","updated":"2023-03-13T15:11:48Z","published":"2023-03-07T15:33:12Z","title":"Enhanced Adaptive Gradient Algorithms for Nonconvex-PL Minimax\n  Optimization","summary":"  In the paper, we study a class of nonconvex nonconcave minimax optimization\nproblems (i.e., $\\min_x\\max_y f(x,y)$), where $f(x,y)$ is possible nonconvex in\n$x$, and it is nonconcave and satisfies the Polyak-Lojasiewicz (PL) condition\nin $y$. Moreover, we propose a class of enhanced momentum-based gradient\ndescent ascent methods (i.e., MSGDA and AdaMSGDA) to solve these stochastic\nNonconvex-PL minimax problems. In particular, our AdaMSGDA algorithm can use\nvarious adaptive learning rates in updating the variables $x$ and $y$ without\nrelying on any global and coordinate-wise adaptive learning rates.\nTheoretically, we present an effective convergence analysis framework for our\nmethods. Specifically, we prove that our MSGDA and AdaMSGDA methods have the\nbest known sample (gradient) complexity of $O(\\epsilon^{-3})$ only requiring\none sample at each loop in finding an $\\epsilon$-stationary solution (i.e.,\n$\\mathbb{E}\\|\\nabla F(x)\\|\\leq \\epsilon$, where $F(x)=\\max_y f(x,y)$). This\nmanuscript commemorates the mathematician Boris Polyak (1935-2023).\n","authors":["Feihu Huang"],"pdf_url":"https://arxiv.org/pdf/2303.03984v2.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2303.07166v1","updated":"2023-03-13T15:09:52Z","published":"2023-03-13T15:09:52Z","title":"Improved Tree Search for Automatic Program Synthesis","summary":"  In the task of automatic program synthesis, one obtains pairs of matching\ninputs and outputs and generates a computer program, in a particular\ndomain-specific language (DSL), which given each sample input returns the\nmatching output. A key element is being able to perform an efficient search in\nthe space of valid programs. Here, we suggest a variant of MCTS that leads to\nstate of the art results on two vastly different DSLs. The exploration method\nwe propose includes multiple contributions: a modified visit count, a\npreprocessing procedure for the training dataset, and encoding the part of the\nprogram that was already executed.\n","authors":["Aran Carmon","Lior Wolf"],"pdf_url":"https://arxiv.org/pdf/2303.07166v1.pdf","comment":"Proceedings of the 2nd Exploration in Reinforcement Learning Workshop\n  at the 36th International Conference on Machine Learning, 2019"},{"id":"http://arxiv.org/abs/2303.07160v1","updated":"2023-03-13T14:35:55Z","published":"2023-03-13T14:35:55Z","title":"Tighter Lower Bounds for Shuffling SGD: Random Permutations and Beyond","summary":"  We study convergence lower bounds of without-replacement stochastic gradient\ndescent (SGD) for solving smooth (strongly-)convex finite-sum minimization\nproblems. Unlike most existing results focusing on final iterate lower bounds\nin terms of the number of components $n$ and the number of epochs $K$, we seek\nbounds for arbitrary weighted average iterates that are tight in all factors\nincluding the condition number $\\kappa$. For SGD with Random Reshuffling, we\npresent lower bounds that have tighter $\\kappa$ dependencies than existing\nbounds. Our results are the first to perfectly close the gap between lower and\nupper bounds for weighted average iterates in both strongly-convex and convex\ncases. We also prove weighted average iterate lower bounds for arbitrary\npermutation-based SGD, which apply to all variants that carefully choose the\nbest permutation. Our bounds improve the existing bounds in factors of $n$ and\n$\\kappa$ and thereby match the upper bounds shown for a recently proposed\nalgorithm called GraB.\n","authors":["Jaeyoung Cha","Jaewook Lee","Chulhee Yun"],"pdf_url":"https://arxiv.org/pdf/2303.07160v1.pdf","comment":"62 pages"},{"id":"http://arxiv.org/abs/2303.07154v1","updated":"2023-03-13T14:28:21Z","published":"2023-03-13T14:28:21Z","title":"Differential Good Arm Identification","summary":"  This paper targets a variant of the stochastic multi-armed bandit problem\ncalled good arm identification (GAI). GAI is a pure-exploration bandit problem\nwith the goal to output as many good arms using as few samples as possible,\nwhere a good arm is defined as an arm whose expected reward is greater than a\ngiven threshold. In this work, we propose DGAI - a differentiable good arm\nidentification algorithm to improve the sample complexity of the\nstate-of-the-art HDoC algorithm in a data-driven fashion. We also showed that\nthe DGAI can further boost the performance of a general multi-arm bandit (MAB)\nproblem given a threshold as a prior knowledge to the arm set. Extensive\nexperiments confirm that our algorithm outperform the baseline algorithms\nsignificantly in both synthetic and real world datasets for both GAI and MAB\ntasks.\n","authors":["Yun-Da Tsai","Tzu-Hsien Tsai","Shou-De Lin"],"pdf_url":"https://arxiv.org/pdf/2303.07154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07153v1","updated":"2023-03-13T14:27:34Z","published":"2023-03-13T14:27:34Z","title":"SA-CNN: Application to text categorization issues using simulated\n  annealing-based convolutional neural network optimization","summary":"  Convolutional neural networks (CNNs) are a representative class of deep\nlearning algorithms including convolutional computation that perform\ntranslation-invariant classification of input data based on their hierarchical\narchitecture. However, classical convolutional neural network learning methods\nuse the steepest descent algorithm for training, and the learning performance\nis greatly influenced by the initial weight settings of the convolutional and\nfully connected layers, requiring re-tuning to achieve better performance under\ndifferent model structures and data. Combining the strengths of the simulated\nannealing algorithm in global search, we propose applying it to the\nhyperparameter search process in order to increase the effectiveness of\nconvolutional neural networks (CNNs). In this paper, we introduce SA-CNN neural\nnetworks for text classification tasks based on Text-CNN neural networks and\nimplement the simulated annealing algorithm for hyperparameter search.\nExperiments demonstrate that we can achieve greater classification accuracy\nthan earlier models with manual tuning, and the improvement in time and space\nfor exploration relative to human tuning is substantial.\n","authors":["Zihao Guo","Yueying Cao"],"pdf_url":"https://arxiv.org/pdf/2303.07153v1.pdf","comment":"ACM EITCE-2022"},{"id":"http://arxiv.org/abs/2303.07152v1","updated":"2023-03-13T14:26:27Z","published":"2023-03-13T14:26:27Z","title":"Score Attack: A Lower Bound Technique for Optimal Differentially Private\n  Learning","summary":"  Achieving optimal statistical performance while ensuring the privacy of\npersonal data is a challenging yet crucial objective in modern data analysis.\nHowever, characterizing the optimality, particularly the minimax lower bound,\nunder privacy constraints is technically difficult.\n  To address this issue, we propose a novel approach called the score attack,\nwhich provides a lower bound on the differential-privacy-constrained minimax\nrisk of parameter estimation. The score attack method is based on the tracing\nattack concept in differential privacy and can be applied to any statistical\nmodel with a well-defined score statistic. It can optimally lower bound the\nminimax risk of estimating unknown model parameters, up to a logarithmic\nfactor, while ensuring differential privacy for a range of statistical\nproblems. We demonstrate the effectiveness and optimality of this general\nmethod in various examples, such as the generalized linear model in both\nclassical and high-dimensional sparse settings, the Bradley-Terry-Luce model\nfor pairwise comparisons, and nonparametric regression over the Sobolev class.\n","authors":["T. Tony Cai","Yichen Wang","Linjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07152v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2011.03900"},{"id":"http://arxiv.org/abs/2303.07150v1","updated":"2023-03-13T14:23:39Z","published":"2023-03-13T14:23:39Z","title":"Multi PILOT: Learned Feasible Multiple Acquisition Trajectories for\n  Dynamic MRI","summary":"  Dynamic Magnetic Resonance Imaging (MRI) is known to be a powerful and\nreliable technique for the dynamic imaging of internal organs and tissues,\nmaking it a leading diagnostic tool. A major difficulty in using MRI in this\nsetting is the relatively long acquisition time (and, hence, increased cost)\nrequired for imaging in high spatio-temporal resolution, leading to the\nappearance of related motion artifacts and decrease in resolution. Compressed\nSensing (CS) techniques have become a common tool to reduce MRI acquisition\ntime by subsampling images in the k-space according to some acquisition\ntrajectory. Several studies have particularly focused on applying deep learning\ntechniques to learn these acquisition trajectories in order to attain better\nimage reconstruction, rather than using some predefined set of trajectories. To\nthe best of our knowledge, learning acquisition trajectories has been only\nexplored in the context of static MRI. In this study, we consider acquisition\ntrajectory learning in the dynamic imaging setting. We design an end-to-end\npipeline for the joint optimization of multiple per-frame acquisition\ntrajectories along with a reconstruction neural network, and demonstrate\nimproved image reconstruction quality in shorter acquisition times. The code\nfor reproducing all experiments is accessible at\nhttps://github.com/tamirshor7/MultiPILOT.\n","authors":["Tamir Shor","Tomer Weiss","Dor Noti","Alex Bronstein"],"pdf_url":"https://arxiv.org/pdf/2303.07150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07143v1","updated":"2023-03-13T14:11:34Z","published":"2023-03-13T14:11:34Z","title":"Multi-Microphone Speaker Separation by Spatial Regions","summary":"  We consider the task of region-based source separation of reverberant\nmulti-microphone recordings. We assume pre-defined spatial regions with a\nsingle active source per region. The objective is to estimate the signals from\nthe individual spatial regions as captured by a reference microphone while\nretaining a correspondence between signals and spatial regions. We propose a\ndata-driven approach using a modified version of a state-of-the-art network,\nwhere different layers model spatial and spectro-temporal information. The\nnetwork is trained to enforce a fixed mapping of regions to network outputs.\nUsing speech from LibriMix, we construct a data set specifically designed to\ncontain the region information. Additionally, we train the network with\npermutation invariant training. We show that both training methods result in a\nfixed mapping of regions to network outputs, achieve comparable performance,\nand that the networks exploit spatial information. The proposed network\noutperforms a baseline network by 1.5 dB in scale-invariant\nsignal-to-distortion ratio.\n","authors":["Julian Wechsler","Srikanth Raj Chetupalli","Wolfgang Mack","Emanuël A. P. Habets"],"pdf_url":"https://arxiv.org/pdf/2303.07143v1.pdf","comment":"Submitted to the 2023 IEEE International Conference on Acoustics,\n  Speech, and Signal Processing"},{"id":"http://arxiv.org/abs/2204.03471v4","updated":"2023-03-13T14:05:43Z","published":"2022-04-07T14:39:38Z","title":"DynLight: Realize dynamic phase duration with multi-level traffic signal\n  control","summary":"  We would like to withdraw this article for the following reasons: 1 this\narticle is not satisfactory for limited language and theoretical description; 2\nwe have enriched and revised this article with the help of other authors; 3 we\nmust update the author contribution information.\n","authors":["Liang Zhang","Shubin Xie","Jianming Deng"],"pdf_url":"https://arxiv.org/pdf/2204.03471v4.pdf","comment":"We would like to withdraw this article for the following reasons: 1\n  this article is not satisfactory for limited language and theoretical\n  description; 2 we have enriched and revised this article with the help of\n  other authors; 3 we must update the author contribution information. PLease\n  see: arXiv:2211.01025"},{"id":"http://arxiv.org/abs/2303.07139v1","updated":"2023-03-13T14:05:19Z","published":"2023-03-13T14:05:19Z","title":"Comparing statistical and machine learning methods for time series\n  forecasting in data-driven logistics -- A simulation study","summary":"  Many planning and decision activities in logistics and supply chain\nmanagement are based on forecasts of multiple time dependent factors.\nTherefore, the quality of planning depends on the quality of the forecasts. We\ncompare various forecasting methods in terms of out of the box forecasting\nperformance on a broad set of simulated time series. We simulate various linear\nand non-linear time series and look at the one step forecast performance of\nstatistical learning methods.\n","authors":["Lena Schmid","Moritz Roidl","Markus Pauly"],"pdf_url":"https://arxiv.org/pdf/2303.07139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07138v1","updated":"2023-03-13T14:05:18Z","published":"2023-03-13T14:05:18Z","title":"Transferable Deep Learning Power System Short-Term Voltage Stability\n  Assessment with Physics-Informed Topological Feature Engineering","summary":"  Deep learning (DL) algorithms have been widely applied to short-term voltage\nstability (STVS) assessment in power systems. However, transferring the\nknowledge learned in one power grid to other power grids with topology changes\nis still a challenging task. This paper proposed a transferable DL-based model\nfor STVS assessment by constructing the topology-aware voltage dynamic features\nfrom raw PMU data. Since the reactive power flow and grid topology are\nessential to voltage stability, the topology-aware and physics-informed voltage\ndynamic features are utilized to effectively represent the topological and\ntemporal patterns from post-disturbance system dynamic trajectories. The\nproposed DL-based STVS assessment model is tested under random operating\nconditions on the New England 39-bus system. It has 99.99\\% classification\naccuracy of the short-term voltage stability status using the topology-aware\nand physics-informed voltage dynamic features. In addition to high accuracy,\nthe experiments show good adaptability to PMU errors. Moreover, The proposed\nSTVS assessment method has outstanding performance on new grid topologies after\nfine-tuning. In particular, the highest accuracy reaches 99.68\\% in evaluation,\nwhich demonstrates a good knowledge transfer ability of the proposed model for\npower grid topology change.\n","authors":["Zijian Feng","Xin Chen","Zijian Lv","Peiyuan Sun","Kai Wu"],"pdf_url":"https://arxiv.org/pdf/2303.07138v1.pdf","comment":"This work has been submitted to the IEEE Transactions on Power\n  Systems for possible publication. Copyright may be transferred without\n  notice, after which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2303.07131v1","updated":"2023-03-13T14:01:37Z","published":"2023-03-13T14:01:37Z","title":"Evolutionary quantum feature selection","summary":"  Effective feature selection is essential for enhancing the performance of\nartificial intelligence models. It involves identifying feature combinations\nthat optimize a given metric, but this is a challenging task due to the\nproblem's exponential time complexity. In this study, we present an innovative\nheuristic called Evolutionary Quantum Feature Selection (EQFS) that employs the\nQuantum Circuit Evolution (QCE) algorithm. Our approach harnesses the unique\ncapabilities of QCE, which utilizes shallow depth circuits to generate sparse\nprobability distributions. Our computational experiments demonstrate that EQFS\ncan identify good feature combinations with quadratic scaling in the number of\nfeatures. To evaluate EQFS's performance, we counted the number of times a\ngiven classical model assesses the cost function for a specific metric, as a\nfunction of the number of generations.\n","authors":["Anton S. Albino","Otto M. Pires","Mauro Q. Nooblath","Erick G. S. Nascimento"],"pdf_url":"https://arxiv.org/pdf/2303.07131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07130v1","updated":"2023-03-13T13:59:47Z","published":"2023-03-13T13:59:47Z","title":"Enhancing COVID-19 Severity Analysis through Ensemble Methods","summary":"  Computed Tomography (CT) scans provide a detailed image of the lungs,\nallowing clinicians to observe the extent of damage caused by COVID-19. The CT\nseverity score (CTSS) of COVID-19 can be categorized based on the extent of\nlung involvement observed on a CT scan. This paper proposes a domain\nknowledge-based pipeline to extract the infection regions using diverse\nimage-processing algorithms and a pre-trained UNET model. An ensemble of three\nmachine-learning models, Random Forest (RF), Extremely Randomized Trees (ERT),\nand Support Vector Machine (SVM), is employed to classify the CT scans into\ndifferent severity classes. The proposed system achieved a macro F1 score of\n57.47% on the validation dataset in the AI-Enabled Medical Image Analysis\nWorkshop and COVID-19 Diagnosis Competition (AI-MIA-COV19D).\n","authors":["Anand Thyagachandran","Hema A Murthy"],"pdf_url":"https://arxiv.org/pdf/2303.07130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07129v1","updated":"2023-03-13T13:59:20Z","published":"2023-03-13T13:59:20Z","title":"AdaptiveNet: Post-deployment Neural Architecture Adaptation for Diverse\n  Edge Environments","summary":"  Deep learning models are increasingly deployed to edge devices for real-time\napplications. To ensure stable service quality across diverse edge\nenvironments, it is highly desirable to generate tailored model architectures\nfor different conditions. However, conventional pre-deployment model generation\napproaches are not satisfactory due to the difficulty of handling the diversity\nof edge environments and the demand for edge information. In this paper, we\npropose to adapt the model architecture after deployment in the target\nenvironment, where the model quality can be precisely measured and private edge\ndata can be retained. To achieve efficient and effective edge model generation,\nwe introduce a pretraining-assisted on-cloud model elastification method and an\nedge-friendly on-device architecture search method. Model elastification\ngenerates a high-quality search space of model architectures with the guidance\nof a developer-specified oracle model. Each subnet in the space is a valid\nmodel with different environment affinity, and each device efficiently finds\nand maintains the most suitable subnet based on a series of edge-tailored\noptimizations. Extensive experiments on various edge devices demonstrate that\nour approach is able to achieve significantly better accuracy-latency tradeoffs\n(e.g. 46.74\\% higher on average accuracy with a 60\\% latency budget) than\nstrong baselines with minimal overhead (13 GPU hours in the cloud and 2 minutes\non the edge server).\n","authors":["Hao Wen","Yuanchun Li","Zunshuai Zhang","Shiqi Jiang","Xiaozhou Ye","Ye Ouyang","Ya-Qin Zhang","Yunxin Liu"],"pdf_url":"https://arxiv.org/pdf/2303.07129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07127v1","updated":"2023-03-13T13:58:03Z","published":"2023-03-13T13:58:03Z","title":"Improving physics-informed neural networks with meta-learned\n  optimization","summary":"  We show that the error achievable using physics-informed neural networks for\nsolving systems of differential equations can be substantially reduced when\nthese networks are trained using meta-learned optimization methods rather than\nto using fixed, hand-crafted optimizers as traditionally done. We choose a\nlearnable optimization method based on a shallow multi-layer perceptron that is\nmeta-trained for specific classes of differential equations. We illustrate\nmeta-trained optimizers for several equations of practical relevance in\nmathematical physics, including the linear advection equation, Poisson's\nequation, the Korteweg--de Vries equation and Burgers' equation. We also\nillustrate that meta-learned optimizers exhibit transfer learning abilities, in\nthat a meta-trained optimizer on one differential equation can also be\nsuccessfully deployed on another differential equation.\n","authors":["Alex Bihlo"],"pdf_url":"https://arxiv.org/pdf/2303.07127v1.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2210.04087v2","updated":"2023-03-13T13:56:31Z","published":"2022-10-08T18:49:58Z","title":"Symmetry Defense Against CNN Adversarial Perturbation Attacks","summary":"  Convolutional neural network classifiers (CNNs) are susceptible to\nadversarial attacks that perturb original samples to fool classifiers such as\nan autonomous vehicle's road sign image classifier. CNNs also lack invariance\nin the classification of symmetric samples because CNNs can classify symmetric\nsamples differently. Considered together, the CNN lack of adversarial\nrobustness and the CNN lack of invariance mean that the classification of\nsymmetric adversarial samples can differ from their incorrect classification.\nCould symmetric adversarial samples revert to their correct classification?\nThis paper answers this question by designing a symmetry defense that inverts\nor horizontally flips adversarial samples before classification against\nadversaries unaware of the defense. Against adversaries aware of the defense,\nthe defense devises a Klein four symmetry subgroup that includes the horizontal\nflip and pixel inversion symmetries. The symmetry defense uses the subgroup\nsymmetries in accuracy evaluation and the subgroup closure property to confine\nthe transformations that an adaptive adversary can apply before or after\ngenerating the adversarial sample. Without changing the preprocessing,\nparameters, or model, the proposed symmetry defense counters the Projected\nGradient Descent (PGD) and AutoAttack attacks with near-default accuracies for\nImageNet. Without using attack knowledge or adversarial samples, the proposed\ndefense exceeds the current best defense, which trains on adversarial samples.\nThe defense maintains and even improves the classification accuracy of\nnon-adversarial samples.\n","authors":["Blerta Lindqvist"],"pdf_url":"https://arxiv.org/pdf/2210.04087v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2303.07125v1","updated":"2023-03-13T13:56:20Z","published":"2023-03-13T13:56:20Z","title":"Don't PANIC: Prototypical Additive Neural Network for Interpretable\n  Classification of Alzheimer's Disease","summary":"  Alzheimer's disease (AD) has a complex and multifactorial etiology, which\nrequires integrating information about neuroanatomy, genetics, and\ncerebrospinal fluid biomarkers for accurate diagnosis. Hence, recent deep\nlearning approaches combined image and tabular information to improve\ndiagnostic performance. However, the black-box nature of such neural networks\nis still a barrier for clinical applications, in which understanding the\ndecision of a heterogeneous model is integral. We propose PANIC, a prototypical\nadditive neural network for interpretable AD classification that integrates 3D\nimage and tabular data. It is interpretable by design and, thus, avoids the\nneed for post-hoc explanations that try to approximate the decision of a\nnetwork. Our results demonstrate that PANIC achieves state-of-the-art\nperformance in AD classification, while directly providing local and global\nexplanations. Finally, we show that PANIC extracts biologically meaningful\nsignatures of AD, and satisfies a set of desirable desiderata for trustworthy\nmachine learning. Our implementation is available at\n\\url{https://github.com/ai-med/PANIC}.\n","authors":["Tom Nuno Wolf","Sebastian Pölster","Christian Wachinger"],"pdf_url":"https://arxiv.org/pdf/2303.07125v1.pdf","comment":"To be published in proceedings of Information Processing In Medical\n  Imaging 2023"},{"id":"http://arxiv.org/abs/2303.07123v1","updated":"2023-03-13T13:56:11Z","published":"2023-03-13T13:56:11Z","title":"Modality-Agnostic Debiasing for Single Domain Generalization","summary":"  Deep neural networks (DNNs) usually fail to generalize well to outside of\ndistribution (OOD) data, especially in the extreme case of single domain\ngeneralization (single-DG) that transfers DNNs from single domain to multiple\nunseen domains. Existing single-DG techniques commonly devise various\ndata-augmentation algorithms, and remould the multi-source domain\ngeneralization methodology to learn domain-generalized (semantic) features.\nNevertheless, these methods are typically modality-specific, thereby being only\napplicable to one single modality (e.g., image). In contrast, we target a\nversatile Modality-Agnostic Debiasing (MAD) framework for single-DG, that\nenables generalization for different modalities. Technically, MAD introduces a\nnovel two-branch classifier: a biased-branch encourages the classifier to\nidentify the domain-specific (superficial) features, and a general-branch\ncaptures domain-generalized features based on the knowledge from biased-branch.\nOur MAD is appealing in view that it is pluggable to most single-DG models. We\nvalidate the superiority of our MAD in a variety of single-DG scenarios with\ndifferent modalities, including recognition on 1D texts, 2D images, 3D point\nclouds, and semantic segmentation on 2D images. More remarkably, for\nrecognition on 3D point clouds and semantic segmentation on 2D images, MAD\nimproves DSU by 2.82\\% and 1.5\\% in accuracy and mIOU.\n","authors":["Sanqing Qu","Yingwei Pan","Guang Chen","Ting Yao","Changjun Jiang","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2303.07123v1.pdf","comment":"To appear in CVPR-2023"},{"id":"http://arxiv.org/abs/2303.07110v1","updated":"2023-03-13T13:44:04Z","published":"2023-03-13T13:44:04Z","title":"Upcycling Models under Domain and Category Shift","summary":"  Deep neural networks (DNNs) often perform poorly in the presence of domain\nshift and category shift. How to upcycle DNNs and adapt them to the target task\nremains an important open problem. Unsupervised Domain Adaptation (UDA),\nespecially recently proposed Source-free Domain Adaptation (SFDA), has become a\npromising technology to address this issue. Nevertheless, existing SFDA methods\nrequire that the source domain and target domain share the same label space,\nconsequently being only applicable to the vanilla closed-set setting. In this\npaper, we take one step further and explore the Source-free Universal Domain\nAdaptation (SF-UniDA). The goal is to identify \"known\" data samples under both\ndomain and category shift, and reject those \"unknown\" data samples (not present\nin source classes), with only the knowledge from standard pre-trained source\nmodel. To this end, we introduce an innovative global and local clustering\nlearning technique (GLC). Specifically, we design a novel, adaptive one-vs-all\nglobal clustering algorithm to achieve the distinction across different target\nclasses and introduce a local k-NN clustering strategy to alleviate negative\ntransfer. We examine the superiority of our GLC on multiple benchmarks with\ndifferent category shift scenarios, including partial-set, open-set, and\nopen-partial-set DA. Remarkably, in the most challenging open-partial-set DA\nscenario, GLC outperforms UMAD by 14.8\\% on the VisDA benchmark. The code is\navailable at https://github.com/ispc-lab/GLC.\n","authors":["Sanqing Qu","Tianpei Zou","Florian Roehrbein","Cewu Lu","Guang Chen","Dacheng Tao","Changjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.07110v1.pdf","comment":"To appear in CVPR 2023. The code has been made public"},{"id":"http://arxiv.org/abs/2303.07109v1","updated":"2023-03-13T13:43:59Z","published":"2023-03-13T13:43:59Z","title":"Transformer-based World Models Are Happy With 100k Interactions","summary":"  Deep neural networks have been successful in many reinforcement learning\nsettings. However, compared to human learners they are overly data hungry. To\nbuild a sample-efficient world model, we apply a transformer to real-world\nepisodes in an autoregressive manner: not only the compact latent states and\nthe taken actions but also the experienced or predicted rewards are fed into\nthe transformer, so that it can attend flexibly to all three modalities at\ndifferent time steps. The transformer allows our world model to access previous\nstates directly, instead of viewing them through a compressed recurrent state.\nBy utilizing the Transformer-XL architecture, it is able to learn long-term\ndependencies while staying computationally efficient. Our transformer-based\nworld model (TWM) generates meaningful, new experience, which is used to train\na policy that outperforms previous model-free and model-based reinforcement\nlearning algorithms on the Atari 100k benchmark.\n","authors":["Jan Robine","Marc Höftmann","Tobias Uelwer","Stefan Harmeling"],"pdf_url":"https://arxiv.org/pdf/2303.07109v1.pdf","comment":"Published as a conference paper at ICLR 2023. Code is available at\n  https://github.com/jrobine/twm"},{"id":"http://arxiv.org/abs/2303.03634v2","updated":"2023-03-13T13:33:15Z","published":"2023-03-07T03:46:53Z","title":"PreFallKD: Pre-Impact Fall Detection via CNN-ViT Knowledge Distillation","summary":"  Fall accidents are critical issues in an aging and aged society. Recently,\nmany researchers developed pre-impact fall detection systems using deep\nlearning to support wearable-based fall protection systems for preventing\nsevere injuries. However, most works only employed simple neural network models\ninstead of complex models considering the usability in resource-constrained\nmobile devices and strict latency requirements. In this work, we propose a\nnovel pre-impact fall detection via CNN-ViT knowledge distillation, namely\nPreFallKD, to strike a balance between detection performance and computational\ncomplexity. The proposed PreFallKD transfers the detection knowledge from the\npre-trained teacher model (vision transformer) to the student model\n(lightweight convolutional neural networks). Additionally, we apply data\naugmentation techniques to tackle issues of data imbalance. We conduct the\nexperiment on the KFall public dataset and compare PreFallKD with other\nstate-of-the-art models. The experiment results show that PreFallKD could boost\nthe student model during the testing phase and achieves reliable F1-score\n(92.66%) and lead time (551.3 ms).\n","authors":["Tin-Han Chi","Kai-Chun Liu","Chia-Yeh Hsieh","Yu Tsao","Chia-Tai Chan"],"pdf_url":"https://arxiv.org/pdf/2303.03634v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.07182v6","updated":"2023-03-13T13:27:02Z","published":"2022-10-13T17:03:36Z","title":"PDEBENCH: An Extensive Benchmark for Scientific Machine Learning","summary":"  Machine learning-based modeling of physical systems has experienced increased\ninterest in recent years. Despite some impressive progress, there is still a\nlack of benchmarks for Scientific ML that are easy to use but still challenging\nand representative of a wide range of problems. We introduce PDEBench, a\nbenchmark suite of time-dependent simulation tasks based on Partial\nDifferential Equations (PDEs). PDEBench comprises both code and data to\nbenchmark the performance of novel machine learning models against both\nclassical numerical simulations and machine learning baselines. Our proposed\nset of benchmark problems contribute the following unique features: (1) A much\nwider range of PDEs compared to existing benchmarks, ranging from relatively\ncommon examples to more realistic and difficult problems; (2) much larger\nready-to-use datasets compared to prior work, comprising multiple simulation\nruns across a larger number of initial and boundary conditions and PDE\nparameters; (3) more extensible source codes with user-friendly APIs for data\ngeneration and baseline results with popular machine learning models (FNO,\nU-Net, PINN, Gradient-Based Inverse Method). PDEBench allows researchers to\nextend the benchmark freely for their own purposes using a standardized API and\nto compare the performance of new models to existing baseline methods. We also\npropose new evaluation metrics with the aim to provide a more holistic\nunderstanding of learning methods in the context of Scientific ML. With those\nmetrics we identify tasks which are challenging for recent ML methods and\npropose these tasks as future challenges for the community. The code is\navailable at https://github.com/pdebench/PDEBench.\n","authors":["Makoto Takamoto","Timothy Praditia","Raphael Leiteritz","Dan MacKinlay","Francesco Alesiani","Dirk Pflüger","Mathias Niepert"],"pdf_url":"https://arxiv.org/pdf/2210.07182v6.pdf","comment":"16 pages (main body) + 34 pages (supplemental material), accepted for\n  publication in NeurIPS 2022 Track Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2209.11173v3","updated":"2023-03-13T13:13:10Z","published":"2022-09-19T15:56:08Z","title":"U-Sleep's resilience to AASM guidelines","summary":"  AASM guidelines are the result of decades of efforts aiming at standardizing\nsleep scoring procedure, with the final goal of sharing a worldwide common\nmethodology. The guidelines cover several aspects from the technical/digital\nspecifications,e.g., recommended EEG derivations, to detailed sleep scoring\nrules accordingly to age. Automated sleep scoring systems have always largely\nexploited the standards as fundamental guidelines. In this context, deep\nlearning has demonstrated better performance compared to classical machine\nlearning. Our present work shows that a deep learning based sleep scoring\nalgorithm may not need to fully exploit the clinical knowledge or to strictly\nadhere to the AASM guidelines. Specifically, we demonstrate that U-Sleep, a\nstate-of-the-art sleep scoring algorithm, can be strong enough to solve the\nscoring task even using clinically non-recommended or non-conventional\nderivations, and with no need to exploit information about the chronological\nage of the subjects. We finally strengthen a well-known finding that using data\nfrom multiple data centers always results in a better performing model compared\nwith training on a single cohort. Indeed, we show that this latter statement is\nstill valid even by increasing the size and the heterogeneity of the single\ndata cohort. In all our experiments we used 28528 polysomnography studies from\n13 different clinical studies.\n","authors":["Luigi Fiorillo","Giuliana Monachino","Julia van der Meer","Marco Pesce","Jan D. Warncke","Markus H. Schmidt","Claudio L. A. Bassetti","Athina Tzovara","Paolo Favaro","Francesca D. Faraci"],"pdf_url":"https://arxiv.org/pdf/2209.11173v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.00815v3","updated":"2023-03-13T12:48:19Z","published":"2023-01-01T12:48:12Z","title":"A attention way in Explainable methods for infant brain","summary":"  Deploying reliable deep learning techniques in interdisciplinary applications\nneeds learned models to output accurate and ({even more importantly})\nexplainable predictions. Existing approaches typically explicate network\noutputs in a post-hoc fashion, under an implicit assumption that faithful\nexplanations come from accurate predictions/classifications. We have an\nopposite claim that explanations boost (or even determine) classification. That\nis, end-to-end learning of explanation factors to augment discriminative\nrepresentation extraction could be a more intuitive strategy to inversely\nassure fine-grained explainability, e.g., in those neuroimaging and\nneuroscience studies with high-dimensional data containing noisy, redundant,\nand task-irrelevant information. In this paper, we propose such an explainable\ngeometric deep network dubbed.\n","authors":["Chenyu Xue"],"pdf_url":"https://arxiv.org/pdf/2301.00815v3.pdf","comment":"Some parts of the thesis are still being revised"},{"id":"http://arxiv.org/abs/2303.07068v1","updated":"2023-03-13T12:44:32Z","published":"2023-03-13T12:44:32Z","title":"n-Step Temporal Difference Learning with Optimal n","summary":"  We consider the problem of finding the optimal value of n in the n-step\ntemporal difference (TD) algorithm. We find the optimal n by resorting to the\nmodel-free optimization technique of simultaneous perturbation stochastic\napproximation (SPSA). We adopt a one-simulation SPSA procedure that is\noriginally for continuous optimization to the discrete optimization framework\nbut incorporates a cyclic perturbation sequence. We prove the convergence of\nour proposed algorithm, SDPSA, and show that it finds the optimal value of n in\nn-step TD. Through experiments, we show that the optimal value of n is achieved\nwith SDPSA for any arbitrary initial value of the same.\n","authors":["Lakshmi Mandal","Shalabh Bhatnagar"],"pdf_url":"https://arxiv.org/pdf/2303.07068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07067v1","updated":"2023-03-13T12:42:02Z","published":"2023-03-13T12:42:02Z","title":"Cross-device Federated Learning for Mobile Health Diagnostics: A First\n  Study on COVID-19 Detection","summary":"  Federated learning (FL) aided health diagnostic models can incorporate data\nfrom a large number of personal edge devices (e.g., mobile phones) while\nkeeping the data local to the originating devices, largely ensuring privacy.\nHowever, such a cross-device FL approach for health diagnostics still imposes\nmany challenges due to both local data imbalance (as extreme as local data\nconsists of a single disease class) and global data imbalance (the disease\nprevalence is generally low in a population). Since the federated server has no\naccess to data distribution information, it is not trivial to solve the\nimbalance issue towards an unbiased model. In this paper, we propose FedLoss, a\nnovel cross-device FL framework for health diagnostics. Here the federated\nserver averages the models trained on edge devices according to the predictive\nloss on the local data, rather than using only the number of samples as\nweights. As the predictive loss better quantifies the data distribution at a\ndevice, FedLoss alleviates the impact of data imbalance. Through a real-world\ndataset on respiratory sound and symptom-based COVID-$19$ detection task, we\nvalidate the superiority of FedLoss. It achieves competitive COVID-$19$\ndetection performance compared to a centralised model with an AUC-ROC of\n$79\\%$. It also outperforms the state-of-the-art FL baselines in sensitivity\nand convergence speed. Our work not only demonstrates the promise of federated\nCOVID-$19$ detection but also paves the way to a plethora of mobile health\nmodel development in a privacy-preserving fashion.\n","authors":["Tong Xia","Jing Han","Abhirup Ghosh","Cecilia Mascolo"],"pdf_url":"https://arxiv.org/pdf/2303.07067v1.pdf","comment":"This paper has been accepted by IEEE ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07062v1","updated":"2023-03-13T12:34:17Z","published":"2023-03-13T12:34:17Z","title":"Quantile Online Learning for Semiconductor Failure Analysis","summary":"  With high device integration density and evolving sophisticated device\nstructures in semiconductor chips, detecting defects becomes elusive and\ncomplex. Conventionally, machine learning (ML)-guided failure analysis is\nperformed with offline batch mode training. However, the occurrence of new\ntypes of failures or changes in the data distribution demands retraining the\nmodel. During the manufacturing process, detecting defects in a single-pass\nonline fashion is more challenging and favoured. This paper focuses on novel\nquantile online learning for semiconductor failure analysis. The proposed\nmethod is applied to semiconductor device-level defects: FinFET bridge defect,\nGAA-FET bridge defect, GAA-FET dislocation defect, and a public database:\nSECOM. From the obtained results, we observed that the proposed method is able\nto perform better than the existing methods. Our proposed method achieved an\noverall accuracy of 86.66% and compared with the second-best existing method it\nimproves 15.50% on the GAA-FET dislocation defect dataset.\n","authors":["Bangjian Zhou","Pan Jieming","Maheswari Sivan","Aaron Voon-Yew Thean","J. Senthilnath"],"pdf_url":"https://arxiv.org/pdf/2303.07062v1.pdf","comment":"5 pages, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2007.14462v4","updated":"2023-03-13T12:23:59Z","published":"2020-07-20T16:39:15Z","title":"Anomaly Awareness","summary":"  We present a new algorithm for anomaly detection called Anomaly Awareness.\nThe algorithm learns about normal events while being made aware of the\nanomalies through a modification of the cost function. We show how this method\nworks in different Particle Physics situations and in standard Computer Vision\ntasks. For example, we apply the method to images from a Fat Jet topology\ngenerated by Standard Model Top and QCD events, and test it against an array of\nnew physics scenarios, including Higgs production with EFT effects and\nresonances decaying into two, three or four subjets. We find that the algorithm\nis effective identifying anomalies not seen before, and becomes robust as we\nmake it aware of a varied-enough set of anomalies.\n","authors":["Charanjit K. Khosa","Veronica Sanz"],"pdf_url":"https://arxiv.org/pdf/2007.14462v4.pdf","comment":"12 pages, 17 figures"},{"id":"http://arxiv.org/abs/2303.07053v1","updated":"2023-03-13T12:22:38Z","published":"2023-03-13T12:22:38Z","title":"Bandit-supported care planning for older people with complex health and\n  care needs","summary":"  Long-term care service for old people is in great demand in most of the aging\nsocieties. The number of nursing homes residents is increasing while the number\nof care providers is limited. Due to the care worker shortage, care to\nvulnerable older residents cannot be fully tailored to the unique needs and\npreference of each individual. This may bring negative impacts on health\noutcomes and quality of life among institutionalized older people. To improve\ncare quality through personalized care planning and delivery with limited care\nworkforce, we propose a new care planning model assisted by artificial\nintelligence. We apply bandit algorithms which optimize the clinical decision\nfor care planning by adapting to the sequential feedback from the past\ndecisions. We evaluate the proposed model on empirical data acquired from the\nSystems for Person-centered Elder Care (SPEC) study, a ICT-enhanced care\nmanagement program.\n","authors":["Gi-Soo Kim","Young Suh Hong","Tae Hoon Lee","Myunghee Cho Paik","Hongsoo Kim"],"pdf_url":"https://arxiv.org/pdf/2303.07053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.08888v2","updated":"2023-03-13T12:14:55Z","published":"2023-02-17T14:17:44Z","title":"Multimodal Federated Learning via Contrastive Representation Ensemble","summary":"  With the increasing amount of multimedia data on modern mobile systems and\nIoT infrastructures, harnessing these rich multimodal data without breaching\nuser privacy becomes a critical issue. Federated learning (FL) serves as a\nprivacy-conscious alternative to centralized machine learning. However,\nexisting FL methods extended to multimodal data all rely on model aggregation\non single modality level, which restrains the server and clients to have\nidentical model architecture for each modality. This limits the global model in\nterms of both model complexity and data capacity, not to mention task\ndiversity. In this work, we propose Contrastive Representation Ensemble and\nAggregation for Multimodal FL (CreamFL), a multimodal federated learning\nframework that enables training larger server models from clients with\nheterogeneous model architectures and data modalities, while only communicating\nknowledge on public dataset. To achieve better multimodal representation\nfusion, we design a global-local cross-modal ensemble strategy to aggregate\nclient representations. To mitigate local model drift caused by two\nunprecedented heterogeneous factors stemming from multimodal discrepancy\n(modality gap and task gap), we further propose two inter-modal and intra-modal\ncontrasts to regularize local training, which complements information of the\nabsent modality for uni-modal clients and regularizes local clients to head\ntowards global consensus. Thorough evaluations and ablation studies on\nimage-text retrieval and visual question answering tasks showcase the\nsuperiority of CreamFL over state-of-the-art FL methods and its practical\nvalue.\n","authors":["Qiying Yu","Yang Liu","Yimu Wang","Ke Xu","Jingjing Liu"],"pdf_url":"https://arxiv.org/pdf/2302.08888v2.pdf","comment":"ICLR 2023. Code is available at https://github.com/FLAIR-THU/CreamFL"},{"id":"http://arxiv.org/abs/2303.07048v1","updated":"2023-03-13T12:13:28Z","published":"2023-03-13T12:13:28Z","title":"Hybrid Variational Autoencoder for Time Series Forecasting","summary":"  Variational autoencoders (VAE) are powerful generative models that learn the\nlatent representations of input data as random variables. Recent studies show\nthat VAE can flexibly learn the complex temporal dynamics of time series and\nachieve more promising forecasting results than deterministic models. However,\na major limitation of existing works is that they fail to jointly learn the\nlocal patterns (e.g., seasonality and trend) and temporal dynamics of time\nseries for forecasting. Accordingly, we propose a novel hybrid variational\nautoencoder (HyVAE) to integrate the learning of local patterns and temporal\ndynamics by variational inference for time series forecasting. Experimental\nresults on four real-world datasets show that the proposed HyVAE achieves\nbetter forecasting results than various counterpart methods, as well as two\nHyVAE variants that only learn the local patterns or temporal dynamics of time\nseries, respectively.\n","authors":["Borui Cai","Shuiqiao Yang","Longxiang Gao","Yong Xiang"],"pdf_url":"https://arxiv.org/pdf/2303.07048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07046v1","updated":"2023-03-13T12:13:16Z","published":"2023-03-13T12:13:16Z","title":"Deploying Offline Reinforcement Learning with Human Feedback","summary":"  Reinforcement learning (RL) has shown promise for decision-making tasks in\nreal-world applications. One practical framework involves training\nparameterized policy models from an offline dataset and subsequently deploying\nthem in an online environment. However, this approach can be risky since the\noffline training may not be perfect, leading to poor performance of the RL\nmodels that may take dangerous actions. To address this issue, we propose an\nalternative framework that involves a human supervising the RL models and\nproviding additional feedback in the online deployment phase. We formalize this\nonline deployment problem and develop two approaches. The first approach uses\nmodel selection and the upper confidence bound algorithm to adaptively select a\nmodel to deploy from a candidate set of trained offline RL models. The second\napproach involves fine-tuning the model in the online deployment phase when a\nsupervision signal arrives. We demonstrate the effectiveness of these\napproaches for robot locomotion control and traffic light control tasks through\nempirical validation.\n","authors":["Ziniu Li","Ke Xu","Liu Liu","Lanqing Li","Deheng Ye","Peilin Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.07046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.05608v3","updated":"2023-03-13T11:57:06Z","published":"2022-06-11T20:16:24Z","title":"Gradient Boosting Performs Gaussian Process Inference","summary":"  This paper shows that gradient boosting based on symmetric decision trees can\nbe equivalently reformulated as a kernel method that converges to the solution\nof a certain Kernel Ridge Regression problem. Thus, we obtain the convergence\nto a Gaussian Process' posterior mean, which, in turn, allows us to easily\ntransform gradient boosting into a sampler from the posterior to provide better\nknowledge uncertainty estimates through Monte-Carlo estimation of the posterior\nvariance. We show that the proposed sampler allows for better knowledge\nuncertainty estimates leading to improved out-of-domain detection.\n","authors":["Aleksei Ustimenko","Artem Beliakov","Liudmila Prokhorenkova"],"pdf_url":"https://arxiv.org/pdf/2206.05608v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07030v1","updated":"2023-03-13T11:45:48Z","published":"2023-03-13T11:45:48Z","title":"$\\nabla$SD: Differentiable Programming for Sparse Tensors","summary":"  Sparse tensors are prevalent in many data-intensive applications, yet\nexisting differentiable programming frameworks are tailored towards dense\ntensors. This presents a significant challenge for efficiently computing\ngradients through sparse tensor operations, as their irregular sparsity\npatterns can result in substantial memory and computational overheads. In this\nwork, we introduce a novel framework that enables the efficient and automatic\ndifferentiation of sparse tensors, addressing this fundamental issue. Our\nexperiments demonstrate the effectiveness of the proposed framework in terms of\nperformance and scalability, outperforming state-of-the-art frameworks across a\nrange of synthetic and real-world datasets. Our approach offers a promising\ndirection for enabling efficient and scalable differentiable programming with\nsparse tensors, which has significant implications for numerous applications in\nmachine learning, natural language processing, and scientific computing.\n","authors":["Amir Shaikhha","Mathieu Huot","Shideh Hashemian"],"pdf_url":"https://arxiv.org/pdf/2303.07030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.02678v2","updated":"2023-03-13T11:43:55Z","published":"2022-10-27T14:24:48Z","title":"Efficient ECG-based Atrial Fibrillation Detection via Parameterised\n  Hypercomplex Neural Networks","summary":"  Atrial fibrillation (AF) is the most common cardiac arrhythmia and associated\nwith a high risk for serious conditions like stroke. The use of wearable\ndevices embedded with automatic and timely AF assessment from\nelectrocardiograms (ECGs) has shown to be promising in preventing\nlife-threatening situations. Although deep neural networks have demonstrated\nsuperiority in model performance, their use on wearable devices is limited by\nthe trade-off between model performance and complexity. In this work, we\npropose to use lightweight convolutional neural networks (CNNs) with\nparameterised hypercomplex (PH) layers for AF detection based on ECGs. The\nproposed approach trains small-scale CNNs, thus overcoming the limited\ncomputing resources on wearable devices. We show comparable performance to\ncorresponding real-valued CNNs on two publicly available ECG datasets using\nsignificantly fewer model parameters. PH models are more flexible than other\nhypercomplex neural networks and can operate on any number of input ECG leads.\n","authors":["Leonie Basso","Zhao Ren","Wolfgang Nejdl"],"pdf_url":"https://arxiv.org/pdf/2211.02678v2.pdf","comment":"Revised paper organisation. Further experiments to emphasise flexible\n  model compression and comparison with other baselines"},{"id":"http://arxiv.org/abs/2211.01704v2","updated":"2023-03-13T11:41:49Z","published":"2022-11-03T10:56:17Z","title":"Cutting Through the Noise: An Empirical Comparison of Psychoacoustic and\n  Envelope-based Features for Machinery Fault Detection","summary":"  Acoustic-based fault detection has a high potential to monitor the health\ncondition of mechanical parts. However, the background noise of an industrial\nenvironment may negatively influence the performance of fault detection.\nLimited attention has been paid to improving the robustness of fault detection\nagainst industrial environmental noise. Therefore, we present the Lenze\nproduction background-noise (LPBN) real-world dataset and an automated and\nnoise-robust auditory inspection (ARAI) system for the end-of-line inspection\nof geared motors. An acoustic array is used to acquire data from motors with a\nminor fault, major fault, or which are healthy. A benchmark is provided to\ncompare the psychoacoustic features with different types of envelope features\nbased on expert knowledge of the gearbox. To the best of our knowledge, we are\nthe first to apply time-varying psychoacoustic features for fault detection. We\ntrain a state-of-the-art one-class-classifier, on samples from healthy motors\nand separate the faulty ones for fault detection using a threshold. The\nbest-performing approaches achieve an area under curve of 0.87 (logarithm\nenvelope), 0.86 (time-varying psychoacoustics), and 0.91 (combination of both).\n","authors":["Peter Wißbrock","Yvonne Richter","David Pelkmann","Zhao Ren","Gregory Palmer"],"pdf_url":"https://arxiv.org/pdf/2211.01704v2.pdf","comment":"the final published version at ICASSP 2023 include small additional\n  content as well as some minor revisions"},{"id":"http://arxiv.org/abs/2303.07009v1","updated":"2023-03-13T11:07:17Z","published":"2023-03-13T11:07:17Z","title":"Symbolic Regression for PDEs using Pruned Differentiable Programs","summary":"  Physics-informed Neural Networks (PINNs) have been widely used to obtain\naccurate neural surrogates for a system of Partial Differential Equations\n(PDE). One of the major limitations of PINNs is that the neural solutions are\nchallenging to interpret, and are often treated as black-box solvers. While\nSymbolic Regression (SR) has been studied extensively, very few works exist\nwhich generate analytical expressions to directly perform SR for a system of\nPDEs. In this work, we introduce an end-to-end framework for obtaining\nmathematical expressions for solutions of PDEs. We use a trained PINN to\ngenerate a dataset, upon which we perform SR. We use a Differentiable Program\nArchitecture (DPA) defined using context-free grammar to describe the space of\nsymbolic expressions. We improve the interpretability by pruning the DPA in a\ndepth-first manner using the magnitude of weights as our heuristic. On average,\nwe observe a 95.3% reduction in parameters of DPA while maintaining accuracy at\npar with PINNs. Furthermore, on an average, pruning improves the accuracy of\nDPA by 7.81% . We demonstrate our framework outperforms the existing\nstate-of-the-art SR solvers on systems of complex PDEs like Navier-Stokes:\nKovasznay flow and Taylor-Green Vortex flow. Furthermore, we produce analytical\nexpressions for a complex industrial use-case of an Air-Preheater, without\nsuffering from performance loss viz-a-viz PINNs.\n","authors":["Ritam Majumdar","Vishal Jadhav","Anirudh Deodhar","Shirish Karande","Lovekesh Vig","Venkataramana Runkana"],"pdf_url":"https://arxiv.org/pdf/2303.07009v1.pdf","comment":"Publication accepted at International Conference for Learning\n  Representations 2023: Physics for Machine Learning"},{"id":"http://arxiv.org/abs/2303.07000v1","updated":"2023-03-13T10:57:35Z","published":"2023-03-13T10:57:35Z","title":"Predicting Density of States via Multi-modal Transformer","summary":"  The density of states (DOS) is a spectral property of materials, which\nprovides fundamental insights on various characteristics of materials. In this\npaper, we propose a model to predict the DOS by reflecting the nature of DOS:\nDOS determines the general distribution of states as a function of energy.\nSpecifically, we integrate the heterogeneous information obtained from the\ncrystal structure and the energies via multi-modal transformer, thereby\nmodeling the complex relationships between the atoms in the crystal structure,\nand various energy levels. Extensive experiments on two types of DOS, i.e.,\nPhonon DOS and Electron DOS, with various real-world scenarios demonstrate the\nsuperiority of DOSTransformer. The source code for DOSTransformer is available\nat https://github.com/HeewoongNoh/DOSTransformer.\n","authors":["Namkyeong Lee","Heewoong Noh","Sungwon Kim","Dongmin Hyun","Gyoung S. Na","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2303.07000v1.pdf","comment":"ICLR 2023 Workshop on Machine Learning for Materials (ML4Materials)"},{"id":"http://arxiv.org/abs/2303.06999v1","updated":"2023-03-13T10:54:52Z","published":"2023-03-13T10:54:52Z","title":"Identifying Label Errors in Object Detection Datasets by Loss Inspection","summary":"  Labeling datasets for supervised object detection is a dull and\ntime-consuming task. Errors can be easily introduced during annotation and\noverlooked during review, yielding inaccurate benchmarks and performance\ndegradation of deep neural networks trained on noisy labels. In this work, we\nfor the first time introduce a benchmark for label error detection methods on\nobject detection datasets as well as a label error detection method and a\nnumber of baselines. We simulate four different types of randomly introduced\nlabel errors on train and test sets of well-labeled object detection datasets.\nFor our label error detection method we assume a two-stage object detector to\nbe given and consider the sum of both stages' classification and regression\nlosses. The losses are computed with respect to the predictions and the noisy\nlabels including simulated label errors, aiming at detecting the latter. We\ncompare our method to three baselines: a naive one without deep learning, the\nobject detector's score and the entropy of the classification softmax\ndistribution. We outperform all baselines and demonstrate that among the\nconsidered methods, ours is the only one that detects label errors of all four\ntypes efficiently. Furthermore, we detect real label errors a) on commonly used\ntest datasets in object detection and b) on a proprietary dataset. In both\ncases we achieve low false positives rates, i.e., when considering 200\nproposals from our method, we detect label errors with a precision for a) of up\nto 71.5% and for b) with 97%.\n","authors":["Marius Schubert","Tobias Riedlinger","Karsten Kahl","Daniel Kröll","Sebastian Schoenen","Siniša Šegvić","Matthias Rottmann"],"pdf_url":"https://arxiv.org/pdf/2303.06999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06992v1","updated":"2023-03-13T10:47:24Z","published":"2023-03-13T10:47:24Z","title":"Improving Mutual Information Estimation with Annealed and Energy-Based\n  Bounds","summary":"  Mutual information (MI) is a fundamental quantity in information theory and\nmachine learning. However, direct estimation of MI is intractable, even if the\ntrue joint probability density for the variables of interest is known, as it\ninvolves estimating a potentially high-dimensional log partition function. In\nthis work, we present a unifying view of existing MI bounds from the\nperspective of importance sampling, and propose three novel bounds based on\nthis approach. Since accurate estimation of MI without density information\nrequires a sample size exponential in the true MI, we assume either a single\nmarginal or the full joint density information is known. In settings where the\nfull joint density is available, we propose Multi-Sample Annealed Importance\nSampling (AIS) bounds on MI, which we demonstrate can tightly estimate large\nvalues of MI in our experiments. In settings where only a single marginal\ndistribution is known, we propose Generalized IWAE (GIWAE) and MINE-AIS bounds.\nOur GIWAE bound unifies variational and contrastive bounds in a single\nframework that generalizes InfoNCE, IWAE, and Barber-Agakov bounds. Our\nMINE-AIS method improves upon existing energy-based methods such as MINE-DV and\nMINE-F by directly optimizing a tighter lower bound on MI. MINE-AIS uses MCMC\nsampling to estimate gradients for training and Multi-Sample AIS for evaluating\nthe bound. Our methods are particularly suitable for evaluating MI in deep\ngenerative models, since explicit forms of the marginal or joint densities are\noften available. We evaluate our bounds on estimating the MI of VAEs and GANs\ntrained on the MNIST and CIFAR datasets, and showcase significant gains over\nexisting bounds in these challenging settings with high ground truth MI.\n","authors":["Rob Brekelmans","Sicong Huang","Marzyeh Ghassemi","Greg Ver Steeg","Roger Grosse","Alireza Makhzani"],"pdf_url":"https://arxiv.org/pdf/2303.06992v1.pdf","comment":"A shorter version appeared in the International Conference on\n  Learning Representations (ICLR) 2022"},{"id":"http://arxiv.org/abs/2303.06980v1","updated":"2023-03-13T10:30:02Z","published":"2023-03-13T10:30:02Z","title":"Self-supervised based general laboratory progress pretrained model for\n  cardiovascular event detection","summary":"  Regular surveillance is an indispensable aspect of managing cardiovascular\ndisorders. Patient recruitment for rare or specific diseases is often limited\ndue to their small patient size and episodic observations, whereas prevalent\ncases accumulate longitudinal data easily due to regular follow-ups. These\ndata, however, are notorious for their irregularity, temporality, sparsity, and\nabsenteeism. In this study, we leveraged self-supervised learning (SSL) and\ntransfer learning to overcome the above-mentioned barriers, transferring\npatient progress trends in cardiovascular laboratory parameters from prevalent\ncases to rare or specific cardiovascular events detection. We pretrained a\ngeneral laboratory progress (GLP) pretrain model using hypertension patients\n(who were yet to be diabetic), and transferred their laboratory progress trend\nto assist in detecting target vessel revascularization (TVR) in percutaneous\ncoronary intervention patients. GLP adopted a two-stage training process that\nutilized interpolated data, enhancing the performance of SSL. After pretraining\nGLP, we fine-tuned it for TVR prediction. The proposed two-stage training\nprocess outperformed SSL. Upon processing by GLP, the classification\ndemonstrated a marked improvement, increasing from 0.63 to 0.90 in averaged\naccuracy. All metrics were significantly superior (p < 0.01) to the performance\nof prior GLP processing. The representation displayed distinct separability\nindependent of algorithmic mechanisms, and diverse data distribution trend. Our\napproach effectively transferred the progression trends of cardiovascular\nlaboratory parameters from prevalent cases to small-numbered cases, thereby\ndemonstrating its efficacy in aiding the risk assessment of cardiovascular\nevents without limiting to episodic observation. The potential for extending\nthis approach to other laboratory tests and diseases is promising.\n","authors":["Li-Chin Chen","Kuo-Hsuan Hung","Yi-Ju Tseng","Hsin-Yao Wang","Tse-Min Lu","Wei-Chieh Huang","Yu Tsao"],"pdf_url":"https://arxiv.org/pdf/2303.06980v1.pdf","comment":"submitted to journal"},{"id":"http://arxiv.org/abs/1904.03445v3","updated":"2023-03-13T10:28:26Z","published":"2019-04-06T13:47:48Z","title":"Feature-Based Interpolation and Geodesics in the Latent Spaces of\n  Generative Models","summary":"  Interpolating between points is a problem connected simultaneously with\nfinding geodesics and study of generative models. In the case of geodesics, we\nsearch for the curves with the shortest length, while in the case of generative\nmodels we typically apply linear interpolation in the latent space. However,\nthis interpolation uses implicitly the fact that Gaussian is unimodal. Thus the\nproblem of interpolating in the case when the latent density is non-Gaussian is\nan open problem.\n  In this paper, we present a general and unified approach to interpolation,\nwhich simultaneously allows us to search for geodesics and interpolating curves\nin latent space in the case of arbitrary density. Our results have a strong\ntheoretical background based on the introduced quality measure of an\ninterpolating curve. In particular, we show that maximising the quality measure\nof the curve can be equivalently understood as a search of geodesic for a\ncertain redefinition of the Riemannian metric on the space.\n  We provide examples in three important cases. First, we show that our\napproach can be easily applied to finding geodesics on manifolds. Next, we\nfocus our attention in finding interpolations in pre-trained generative models.\nWe show that our model effectively works in the case of arbitrary density.\nMoreover, we can interpolate in the subset of the space consisting of data\npossessing a given feature. The last case is focused on finding interpolation\nin the space of chemical compounds.\n","authors":["Łukasz Struski","Michał Sadowski","Tomasz Danel","Jacek Tabor","Igor T. Podolak"],"pdf_url":"https://arxiv.org/pdf/1904.03445v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.03880v3","updated":"2023-03-13T10:20:38Z","published":"2022-09-08T15:35:42Z","title":"Learning Sparse Graphon Mean Field Games","summary":"  Although the field of multi-agent reinforcement learning (MARL) has made\nconsiderable progress in the last years, solving systems with a large number of\nagents remains a hard challenge. Graphon mean field games (GMFGs) enable the\nscalable analysis of MARL problems that are otherwise intractable. By the\nmathematical structure of graphons, this approach is limited to dense graphs\nwhich are insufficient to describe many real-world networks such as power law\ngraphs. Our paper introduces a novel formulation of GMFGs, called LPGMFGs,\nwhich leverages the graph theoretical concept of $L^p$ graphons and provides a\nmachine learning tool to efficiently and accurately approximate solutions for\nsparse network problems. This especially includes power law networks which are\nempirically observed in various application areas and cannot be captured by\nstandard graphons. We derive theoretical existence and convergence guarantees\nand give empirical examples that demonstrate the accuracy of our learning\napproach for systems with many agents. Furthermore, we extend the Online Mirror\nDescent (OMD) learning algorithm to our setup to accelerate learning speed,\nempirically show its capabilities, and conduct a theoretical analysis using the\nnovel concept of smoothed step graphons. In general, we provide a scalable,\nmathematically well-founded machine learning approach to a large class of\notherwise intractable problems of great relevance in numerous research fields.\n","authors":["Christian Fabian","Kai Cui","Heinz Koeppl"],"pdf_url":"https://arxiv.org/pdf/2209.03880v3.pdf","comment":"accepted for publication at the International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2023; code available at:\n  https://github.com/ChrFabian/Learning_sparse_GMFGs"},{"id":"http://arxiv.org/abs/2303.06972v1","updated":"2023-03-13T10:16:19Z","published":"2023-03-13T10:16:19Z","title":"Leveraging Neural Koopman Operators to Learn Continuous Representations\n  of Dynamical Systems from Scarce Data","summary":"  Over the last few years, several works have proposed deep learning\narchitectures to learn dynamical systems from observation data with no or\nlittle knowledge of the underlying physics. A line of work relies on learning\nrepresentations where the dynamics of the underlying phenomenon can be\ndescribed by a linear operator, based on the Koopman operator theory. However,\ndespite being able to provide reliable long-term predictions for some dynamical\nsystems in ideal situations, the methods proposed so far have limitations, such\nas requiring to discretize intrinsically continuous dynamical systems, leading\nto data loss, especially when handling incomplete or sparsely sampled data.\nHere, we propose a new deep Koopman framework that represents dynamics in an\nintrinsically continuous way, leading to better performance on limited training\ndata, as exemplified on several datasets arising from dynamical systems.\n","authors":["Anthony Frion","Lucas Drumetz","Mauro Dalla Mura","Guillaume Tochon","Abdeldjalil Aissa El Bey"],"pdf_url":"https://arxiv.org/pdf/2303.06972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.04788v2","updated":"2023-03-13T10:13:00Z","published":"2022-04-10T22:58:02Z","title":"Representation Learning by Detecting Incorrect Location Embeddings","summary":"  In this paper, we introduce a novel self-supervised learning (SSL) loss for\nimage representation learning. There is a growing belief that generalization in\ndeep neural networks is linked to their ability to discriminate object shapes.\nSince object shape is related to the location of its parts, we propose to\ndetect those that have been artificially misplaced. We represent object parts\nwith image tokens and train a ViT to detect which token has been combined with\nan incorrect positional embedding. We then introduce sparsity in the inputs to\nmake the model more robust to occlusions and to speed up the training. We call\nour method DILEMMA, which stands for Detection of Incorrect Location EMbeddings\nwith MAsked inputs. We apply DILEMMA to MoCoV3, DINO and SimCLR and show an\nimprovement in their performance of respectively 4.41%, 3.97%, and 0.5% under\nthe same training time and with a linear probing transfer on ImageNet-1K. We\nalso show full fine-tuning improvements of MAE combined with our method on\nImageNet-100. We evaluate our method via fine-tuning on common SSL benchmarks.\nMoreover, we show that when downstream tasks are strongly reliant on shape\n(such as in the YOGA-82 pose dataset), our pre-trained features yield a\nsignificant gain over prior work.\n","authors":["Sepehr Sameni","Simon Jenni","Paolo Favaro"],"pdf_url":"https://arxiv.org/pdf/2204.04788v2.pdf","comment":"accepted at AAAI2023, https://github.com/Separius/DILEMMA"},{"id":"http://arxiv.org/abs/2303.06965v1","updated":"2023-03-13T10:06:41Z","published":"2023-03-13T10:06:41Z","title":"Uni-RXN: An Unified Framework that Bridge the Gap between Chemical\n  Reaction Pretraining and Conditional Molecule Generation","summary":"  Chemical reactions are the fundamental building blocks of drug design and\norganic chemistry research. Machine learning for chemistry is a rapidly\nadvancing field with numerous applications. In recent years, there has been a\ngrowing need for a large-scale deep-learning framework that can efficiently\ncapture the basic rules of chemical reactions. In this paper, we have proposed\na unified framework that addresses both the reaction representation learning\nand molecule generation tasks, which allows for a more holistic approach.\nInspired by the organic chemistry mechanism, we develop a novel pretraining\nframework that enables us to incorporate inductive biases into the model. Our\nframework achieves state-of-the-art results on challenging downstream tasks. By\npossessing chemical knowledge, this framework can be applied to reaction-based\ngenerative models, overcoming the limitations of current molecule generation\nmodels that rely on a small number of reaction templates. In the extensive\nexperiments, our model generates synthesizable drug-like structures of high\nquality. Overall, our work presents a significant step toward a large-scale\ndeep-learning framework for a variety of reaction-based applications.\n","authors":["Bo Qiang","Yiran Zhou","Yuheng Ding","Ningfeng Liu","Liangren Zhang","Zhenming Liu"],"pdf_url":"https://arxiv.org/pdf/2303.06965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04788v2","updated":"2023-03-13T10:04:28Z","published":"2023-03-08T18:39:43Z","title":"Enabling Non-Linear Quantum Operations through Variational Quantum\n  Splines","summary":"  The postulates of quantum mechanics impose only unitary transformations on\nquantum states, which is a severe limitation for quantum machine learning\nalgorithms. Quantum Splines (QSplines) have recently been proposed to\napproximate quantum activation functions to introduce non-linearity in quantum\nalgorithms. However, QSplines make use of the HHL as a subroutine and require a\nfault-tolerant quantum computer to be correctly implemented. This work proposes\nthe Generalised QSplines (GQSplines), a novel method for approximating\nnon-linear quantum activation functions using hybrid quantum-classical\ncomputation. The GQSplines overcome the highly demanding requirements of the\noriginal QSplines in terms of quantum hardware and can be implemented using\nnear-term quantum computers. Furthermore, the proposed method relies on a\nflexible problem representation for non-linear approximation and it is suitable\nto be embedded in existing quantum neural network architectures. In addition,\nwe provide a practical implementation of GQSplines using Pennylane and show\nthat our model outperforms the original QSplines in terms of quality of\nfitting.\n","authors":["Matteo Antonio Inajetovic","Filippo Orazi","Antonio Macaluso","Stefano Lodi","Claudio Sartori"],"pdf_url":"https://arxiv.org/pdf/2303.04788v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.03393v2","updated":"2023-03-13T09:57:05Z","published":"2021-06-07T07:43:28Z","title":"Adversarially Regularized Graph Attention Networks for Inductive\n  Learning on Partially Labeled Graphs","summary":"  The high cost of data labeling often results in node label shortage in real\napplications. To improve node classification accuracy, graph-based\nsemi-supervised learning leverages the ample unlabeled nodes to train together\nwith the scarce available labeled nodes. However, most existing methods require\nthe information of all nodes, including those to be predicted, during model\ntraining, which is not practical for dynamic graphs with newly added nodes. To\naddress this issue, an adversarially regularized graph attention model is\nproposed to classify newly added nodes in a partially labeled graph. An\nattention-based aggregator is designed to generate the representation of a node\nby aggregating information from its neighboring nodes, thus naturally\ngeneralizing to previously unseen nodes. In addition, adversarial training is\nemployed to improve the model's robustness and generalization ability by\nenforcing node representations to match a prior distribution. Experiments on\nreal-world datasets demonstrate the effectiveness of the proposed method in\ncomparison with the state-of-the-art methods. The code is available at\nhttps://github.com/JiarenX/AGAIN.\n","authors":["Jiaren Xiao","Quanyu Dai","Xiaochen Xie","James Lam","Ka-Wai Kwok"],"pdf_url":"https://arxiv.org/pdf/2106.03393v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.02731v2","updated":"2023-03-13T09:42:22Z","published":"2022-10-06T07:44:51Z","title":"PSVRF: Learning to restore Pitch-Shifted Voice without reference","summary":"  Pitch scaling algorithms have a significant impact on the security of\nAutomatic Speaker Verification (ASV) systems. Although numerous anti-spoofing\nalgorithms have been proposed to identify the pitch-shifted voice and even\nrestore it to the original version, they either have poor performance or\nrequire the original voice as a reference, limiting the prospects of\napplications. In this paper, we propose a no-reference approach termed\nPSVRF$^1$ for high-quality restoration of pitch-shifted voice. Experiments on\nAISHELL-1 and AISHELL-3 demonstrate that PSVRF can restore the voice disguised\nby various pitch-scaling techniques, which obviously enhances the robustness of\nASV systems to pitch-scaling attacks. Furthermore, the performance of PSVRF\neven surpasses that of the state-of-the-art reference-based approach.\n","authors":["Yangfu Li","Xiaodan Lin","Jiaxin Yang"],"pdf_url":"https://arxiv.org/pdf/2210.02731v2.pdf","comment":"Have some errors"},{"id":"http://arxiv.org/abs/2303.06947v1","updated":"2023-03-13T09:31:20Z","published":"2023-03-13T09:31:20Z","title":"A Multi-Modal Simulation Framework to Enable Digital Twin-based V2X\n  Communications in Dynamic Environments","summary":"  Digital Twins (DTs) for physical wireless environments have been recently\nproposed as accurate virtual representations of the propagation environment\nthat can enable multi-layer decisions at the physical communication equipment.\nAt high frequency bands, DTs can help to overcome the challenges emerging in\nthe high mobility conditions featuring vehicular environments. In this paper,\nwe propose a novel data-driven workflow for the creation of the DT of a\nVehicle-to-Everything (V2X) communication scenario and a multi-modal simulation\nframework for the generation of realistic sensor data and accurate\nmmWave/sub-THz wireless channels. The proposed method leverages an automotive\nsimulation and testing framework based on the Unreal Engine game engine and an\naccurate ray-tracing channel simulator. Simulations over an urban scenario show\nthe achievable realistic sensor and channel modelling both at the\ninfrastructure and at an ego-vehicle.\n","authors":["Lorenzo Cazzella","Francesco Linsalata","Maurizio Magarini","Matteo Matteucci","Umberto Spagnolini"],"pdf_url":"https://arxiv.org/pdf/2303.06947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06946v1","updated":"2023-03-13T09:27:52Z","published":"2023-03-13T09:27:52Z","title":"Context-Aware Selective Label Smoothing for Calibrating Sequence\n  Recognition Model","summary":"  Despite the success of deep neural network (DNN) on sequential data (i.e.,\nscene text and speech) recognition, it suffers from the over-confidence problem\nmainly due to overfitting in training with the cross-entropy loss, which may\nmake the decision-making less reliable. Confidence calibration has been\nrecently proposed as one effective solution to this problem. Nevertheless, the\nmajority of existing confidence calibration methods aims at non-sequential\ndata, which is limited if directly applied to sequential data since the\nintrinsic contextual dependency in sequences or the class-specific statistical\nprior is seldom exploited. To the end, we propose a Context-Aware Selective\nLabel Smoothing (CASLS) method for calibrating sequential data. The proposed\nCASLS fully leverages the contextual dependency in sequences to construct\nconfusion matrices of contextual prediction statistics over different classes.\nClass-specific error rates are then used to adjust the weights of smoothing\nstrength in order to achieve adaptive calibration. Experimental results on\nsequence recognition tasks, including scene text recognition and speech\nrecognition, demonstrate that our method can achieve the state-of-the-art\nperformance.\n","authors":["Shuangping Huang","Yu Luo","Zhenzhou Zhuang","Jin-Gang Yu","Mengchao He","Yongpan Wang"],"pdf_url":"https://arxiv.org/pdf/2303.06946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06945v1","updated":"2023-03-13T09:27:34Z","published":"2023-03-13T09:27:34Z","title":"CoGANPPIS: Coevolution-enhanced Global Attention Neural Network for\n  Protein-Protein Interaction Site Prediction","summary":"  Protein-protein interactions are essential in biochemical processes. Accurate\nprediction of the protein-protein interaction sites (PPIs) deepens our\nunderstanding of biological mechanism and is crucial for new drug design.\nHowever, conventional experimental methods for PPIs prediction are costly and\ntime-consuming so that many computational approaches, especially ML-based\nmethods, have been developed recently. Although these approaches have achieved\ngratifying results, there are still two limitations: (1) Most models have\nexcavated some useful input features, but failed to take coevolutionary\nfeatures into account, which could provide clues for inter-residue\nrelationships; (2) The attention-based models only allocate attention weights\nfor neighboring residues, instead of doing it globally, neglecting that some\nresidues being far away from the target residues might also matter.\n  We propose a coevolution-enhanced global attention neural network, a\nsequence-based deep learning model for PPIs prediction, called CoGANPPIS. It\nutilizes three layers in parallel for feature extraction: (1) Local-level\nrepresentation aggregation layer, which aggregates the neighboring residues'\nfeatures; (2) Global-level representation learning layer, which employs a novel\ncoevolution-enhanced global attention mechanism to allocate attention weights\nto all the residues on the same protein sequences; (3) Coevolutionary\ninformation learning layer, which applies CNN & pooling to coevolutionary\ninformation to obtain the coevolutionary profile representation. Then, the\nthree outputs are concatenated and passed into several fully connected layers\nfor the final prediction. Application on two benchmark datasets demonstrated a\nstate-of-the-art performance of our model. The source code is publicly\navailable at https://github.com/Slam1423/CoGANPPIS_source_code.\n","authors":["Jiaxing Guo","Xuening Zhu","Zixin Hu","Xiaoxi Hu"],"pdf_url":"https://arxiv.org/pdf/2303.06945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.07160v2","updated":"2023-03-13T09:18:52Z","published":"2023-02-14T16:14:39Z","title":"Learning a model is paramount for sample efficiency in reinforcement\n  learning control of PDEs","summary":"  The goal of this paper is to make a strong point for the usage of dynamical\nmodels when using reinforcement learning (RL) for feedback control of dynamical\nsystems governed by partial differential equations (PDEs). To breach the gap\nbetween the immense promises we see in RL and the applicability in complex\nengineering systems, the main challenges are the massive requirements in terms\nof the training data, as well as the lack of performance guarantees. We present\na solution for the first issue using a data-driven surrogate model in the form\nof a convolutional LSTM with actuation. We demonstrate that learning an\nactuated model in parallel to training the RL agent significantly reduces the\ntotal amount of required data sampled from the real system. Furthermore, we\nshow that iteratively updating the model is of major importance to avoid biases\nin the RL training. Detailed ablation studies reveal the most important\ningredients of the modeling process. We use the chaotic Kuramoto-Sivashinsky\nequation do demonstarte our findings.\n","authors":["Stefan Werner","Sebastian Peitz"],"pdf_url":"https://arxiv.org/pdf/2302.07160v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06937v1","updated":"2023-03-13T09:11:54Z","published":"2023-03-13T09:11:54Z","title":"Addressing Catastrophic Forgetting in Federated Class-Continual Learning","summary":"  This paper focuses on an under-explored yet important problem: Federated\nClass-Continual Learning (FCCL), where new classes are dynamically added in\nfederated learning. Existing FCCL works suffer from various limitations, such\nas requiring additional datasets or storing the private data from previous\ntasks. In response, we first demonstrate that non-IID data exacerbates\ncatastrophic forgetting issue in FL. Then we propose a novel method called\nTARGET (federat\\textbf{T}ed cl\\textbf{A}ss-continual lea\\textbf{R}nin\\textbf{G}\nvia \\textbf{E}xemplar-free dis\\textbf{T}illation), which alleviates\ncatastrophic forgetting in FCCL while preserving client data privacy. Our\nproposed method leverages the previously trained global model to transfer\nknowledge of old tasks to the current task at the model level. Moreover, a\ngenerator is trained to produce synthetic data to simulate the global\ndistribution of data on each client at the data level. Compared to previous\nFCCL methods, TARGET does not require any additional datasets or storing real\ndata from previous tasks, which makes it ideal for data-sensitive scenarios.\n","authors":["Jie Zhang","Chen Chen","Weiming Zhuang","Lingjuan Lv"],"pdf_url":"https://arxiv.org/pdf/2303.06937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.12538v2","updated":"2023-03-13T09:04:19Z","published":"2023-02-24T09:49:43Z","title":"UnbiasedNets: A Dataset Diversification Framework for Robustness Bias\n  Alleviation in Neural Networks","summary":"  Performance of trained neural network (NN) models, in terms of testing\naccuracy, has improved remarkably over the past several years, especially with\nthe advent of deep learning. However, even the most accurate NNs can be biased\ntoward a specific output classification due to the inherent bias in the\navailable training datasets, which may propagate to the real-world\nimplementations. This paper deals with the robustness bias, i.e., the bias\nexhibited by the trained NN by having a significantly large robustness to noise\nfor a certain output class, as compared to the remaining output classes. The\nbias is shown to result from imbalanced datasets, i.e., the datasets where all\noutput classes are not equally represented. Towards this, we propose the\nUnbiasedNets framework, which leverages K-means clustering and the NN's noise\ntolerance to diversify the given training dataset, even from relatively smaller\ndatasets. This generates balanced datasets and reduces the bias within the\ndatasets themselves. To the best of our knowledge, this is the first framework\ncatering to the robustness bias problem in NNs. We use real-world datasets to\ndemonstrate the efficacy of the UnbiasedNets for data diversification, in case\nof both binary and multi-label classifiers. The results are compared to\nwell-known tools aimed at generating balanced datasets, and illustrate how\nexisting works have limited success while addressing the robustness bias. In\ncontrast, UnbiasedNets provides a notable improvement over existing works,\nwhile even reducing the robustness bias significantly in some cases, as\nobserved by comparing the NNs trained on the diversified and original datasets.\n","authors":["Mahum Naseer","Bharath Srinivas Prabakaran","Osman Hasan","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2302.12538v2.pdf","comment":"Springer Machine Learning 2023"},{"id":"http://arxiv.org/abs/2303.06931v1","updated":"2023-03-13T08:55:10Z","published":"2023-03-13T08:55:10Z","title":"DeepVigor: Vulnerability Value Ranges and Factors for DNNs' Reliability\n  Assessment","summary":"  Deep Neural Networks (DNNs) and their accelerators are being deployed ever\nmore frequently in safety-critical applications leading to increasing\nreliability concerns. A traditional and accurate method for assessing DNNs'\nreliability has been resorting to fault injection, which, however, suffers from\nprohibitive time complexity. While analytical and hybrid fault\ninjection-/analytical-based methods have been proposed, they are either\ninaccurate or specific to particular accelerator architectures. In this work,\nwe propose a novel accurate, fine-grain, metric-oriented, and\naccelerator-agnostic method called DeepVigor that provides vulnerability value\nranges for DNN neurons' outputs. An outcome of DeepVigor is an analytical model\nrepresenting vulnerable and non-vulnerable ranges for each neuron that can be\nexploited to develop different techniques for improving DNNs' reliability.\nMoreover, DeepVigor provides reliability assessment metrics based on\nvulnerability factors for bits, neurons, and layers using the vulnerability\nranges. The proposed method is not only faster than fault injection but also\nprovides extensive and accurate information about the reliability of DNNs,\nindependent from the accelerator. The experimental evaluations in the paper\nindicate that the proposed vulnerability ranges are 99.9% to 100% accurate even\nwhen evaluated on previously unseen test data. Also, it is shown that the\nobtained vulnerability factors represent the criticality of bits, neurons, and\nlayers proficiently. DeepVigor is implemented in the PyTorch framework and\nvalidated on complex DNN benchmarks.\n","authors":["Mohammad Hasan Ahmadilivani","Mahdi Taheri","Jaan Raik","Masoud Daneshtalab","Maksim Jenihhin"],"pdf_url":"https://arxiv.org/pdf/2303.06931v1.pdf","comment":"6 pages, 6 figures, 2 tables, accepted at ETS 2023\n  (cas.polito.it/ETS23/#/program-conference#tab-accepted)"},{"id":"http://arxiv.org/abs/2210.13867v2","updated":"2023-03-13T08:41:43Z","published":"2022-10-25T09:43:36Z","title":"A Dynamical System View of Langevin-Based Non-Convex Sampling","summary":"  Non-convex sampling is a key challenge in machine learning, central to\nnon-convex optimization in deep learning as well as to approximate\nprobabilistic inference. Despite its significance, theoretically there remain\nmany important challenges: Existing guarantees (1) typically only hold for the\naveraged iterates rather than the more desirable last iterates, (2) lack\nconvergence metrics that capture the scales of the variables such as\nWasserstein distances, and (3) mainly apply to elementary schemes such as\nstochastic gradient Langevin dynamics. In this paper, we develop a new\nframework that lifts the above issues by harnessing several tools from the\ntheory of dynamical systems. Our key result is that, for a large class of\nstate-of-the-art sampling schemes, their last-iterate convergence in\nWasserstein distances can be reduced to the study of their continuous-time\ncounterparts, which is much better understood. Coupled with standard\nassumptions of MCMC sampling, our theory immediately yields the last-iterate\nWasserstein convergence of many advanced sampling schemes such as proximal,\nrandomized mid-point, and Runge-Kutta integrators. Beyond existing methods, our\nframework also motivates more efficient schemes that enjoy the same rigorous\nguarantees.\n","authors":["Mohammad Reza Karimi","Ya-Ping Hsieh","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2210.13867v2.pdf","comment":"typos corrected, references added"},{"id":"http://arxiv.org/abs/2205.12706v2","updated":"2023-03-13T08:33:02Z","published":"2022-05-25T12:02:59Z","title":"Maximum Mean Discrepancy on Exponential Windows for Online Change\n  Detection","summary":"  Detecting changes is of fundamental importance when analyzing data streams\nand has many applications, e.g., predictive maintenance, fraud detection, or\nmedicine. A principled approach to detect changes is to compare the\ndistributions of observations within the stream to each other via hypothesis\ntesting. Maximum mean discrepancy (MMD; also called energy distance) is a\nwell-known (semi-)metric on the space of probability distributions. MMD gives\nrise to powerful non-parametric two-sample tests on kernel-enriched domains\nunder mild conditions, which makes its deployment for change detection\ndesirable. However, the classic MMD estimators suffer quadratic complexity,\nwhich prohibits their application in the online change detection setting. We\npropose a general-purpose change detection algorithm, Maximum Mean Discrepancy\non Exponential Windows (MMDEW), which leverages the MMD two-sample test,\nfacilitates its efficient online computation on any kernel-enriched domain, and\nis able to detect any disparity between distributions. Our experiments and\nanalysis show that (1) MMDEW achieves better detection quality than\nstate-of-the-art competitors and that (2) the algorithm has polylogarithmic\nruntime and logarithmic memory requirements, which allow its deployment to the\nstreaming setting.\n","authors":["Florian Kalinke","Marco Heyden","Edouard Fouché","Klemens Böhm"],"pdf_url":"https://arxiv.org/pdf/2205.12706v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2012.03386v2","updated":"2023-03-13T08:23:37Z","published":"2020-12-06T22:24:28Z","title":"SoK: Training Machine Learning Models over Multiple Sources with Privacy\n  Preservation","summary":"  Nowadays, gathering high-quality training data from multiple data sources\nwith privacy preservation is a crucial challenge to training high-performance\nmachine learning models. The potential solutions could break the barriers among\nisolated data corpus, and consequently enlarge the range of data available for\nprocessing. To this end, both academic researchers and industrial vendors are\nrecently strongly motivated to propose two main-stream folders of solutions\nmainly based on software constructions: 1) Secure Multi-party Learning (MPL for\nshort); and 2) Federated Learning (FL for short). The above two technical\nfolders have their advantages and limitations when we evaluate them according\nto the following five criteria: security, efficiency, data distribution, the\naccuracy of trained models, and application scenarios.\n  Motivated to demonstrate the research progress and discuss the insights on\nthe future directions, we thoroughly investigate these protocols and frameworks\nof both MPL and FL. At first, we define the problem of Training machine\nlearning Models over Multiple data sources with Privacy Preservation (TMMPP for\nshort). Then, we compare the recent studies of TMMPP from the aspects of the\ntechnical routes, the number of parties supported, data partitioning, threat\nmodel, and machine learning models supported, to show their advantages and\nlimitations. Next, we investigate and evaluate five popular FL platforms.\nFinally, we discuss the potential directions to resolve the problem of TMMPP in\nthe future.\n","authors":["Lushan Song","Guopeng Lin","Jiaxuan Wang","Haoqi Wu","Wenqiang Ruan","Weili Han"],"pdf_url":"https://arxiv.org/pdf/2012.03386v2.pdf","comment":"19pages, 4 figures"},{"id":"http://arxiv.org/abs/2203.03179v3","updated":"2023-03-13T08:23:37Z","published":"2022-03-07T07:23:18Z","title":"Detecting data-driven robust statistical arbitrage strategies with deep\n  neural networks","summary":"  We present an approach, based on deep neural networks, that allows\nidentifying robust statistical arbitrage strategies in financial markets.\nRobust statistical arbitrage strategies refer to trading strategies that enable\nprofitable trading under model ambiguity. The presented novel methodology\nallows to consider a large amount of underlying securities simultaneously and\ndoes not depend on the identification of cointegrated pairs of assets, hence it\nis applicable on high-dimensional financial markets or in markets where\nclassical pairs trading approaches fail. Moreover, we provide a method to build\nan ambiguity set of admissible probability measures that can be derived from\nobserved market data. Thus, the approach can be considered as being model-free\nand entirely data-driven. We showcase the applicability of our method by\nproviding empirical investigations with highly profitable trading performances\neven in 50 dimensions, during financial crises, and when the cointegration\nrelationship between asset pairs stops to persist.\n","authors":["Ariel Neufeld","Julian Sester","Daiying Yin"],"pdf_url":"https://arxiv.org/pdf/2203.03179v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.07892v2","updated":"2023-03-13T08:19:14Z","published":"2022-06-16T02:46:26Z","title":"Max-Margin Works while Large Margin Fails: Generalization without\n  Uniform Convergence","summary":"  A major challenge in modern machine learning is theoretically understanding\nthe generalization properties of overparameterized models. Many existing tools\nrely on uniform convergence (UC), a property that, when it holds, guarantees\nthat the test loss will be close to the training loss, uniformly over a class\nof candidate models. Nagarajan and Kolter (2019) show that in certain simple\nlinear and neural-network settings, any uniform convergence bound will be\nvacuous, leaving open the question of how to prove generalization in settings\nwhere UC fails. Our main contribution is proving novel generalization bounds in\ntwo such settings, one linear, and one non-linear. We study the linear\nclassification setting of Nagarajan and Kolter, and a quadratic ground truth\nfunction learned via a two-layer neural network in the non-linear regime. We\nprove a new type of margin bound showing that above a certain signal-to-noise\nthreshold, any near-max-margin classifier will achieve almost no test loss in\nthese two settings. Our results show that near-max-margin is important: while\nany model that achieves at least a $(1 - \\epsilon)$-fraction of the max-margin\ngeneralizes well, a classifier achieving half of the max-margin may fail\nterribly. Building on the impossibility results of Nagarajan and Kolter, under\nslightly stronger assumptions, we show that one-sided UC bounds and classical\nmargin bounds will fail on near-max-margin classifiers. Our analysis provides\ninsight on why memorization can coexist with generalization: we show that in\nthis challenging regime where generalization occurs but UC fails,\nnear-max-margin classifiers simultaneously contain some generalizable\ncomponents and some overfitting components that memorize the data. The presence\nof the overfitting components is enough to preclude UC, but the near-extremal\nmargin guarantees that sufficient generalizable components are present.\n","authors":["Margalit Glasgow","Colin Wei","Mary Wootters","Tengyu Ma"],"pdf_url":"https://arxiv.org/pdf/2206.07892v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05205v2","updated":"2023-03-13T08:06:17Z","published":"2023-03-09T12:19:20Z","title":"Real-time scheduling of renewable power systems through planning-based\n  reinforcement learning","summary":"  The growing renewable energy sources have posed significant challenges to\ntraditional power scheduling. It is difficult for operators to obtain accurate\nday-ahead forecasts of renewable generation, thereby requiring the future\nscheduling system to make real-time scheduling decisions aligning with\nultra-short-term forecasts. Restricted by the computation speed, traditional\noptimization-based methods can not solve this problem. Recent developments in\nreinforcement learning (RL) have demonstrated the potential to solve this\nchallenge. However, the existing RL methods are inadequate in terms of\nconstraint complexity, algorithm performance, and environment fidelity. We are\nthe first to propose a systematic solution based on the state-of-the-art\nreinforcement learning algorithm and the real power grid environment. The\nproposed approach enables planning and finer time resolution adjustments of\npower generators, including unit commitment and economic dispatch, thus\nincreasing the grid's ability to admit more renewable energy. The well-trained\nscheduling agent significantly reduces renewable curtailment and load shedding,\nwhich are issues arising from traditional scheduling's reliance on inaccurate\nday-ahead forecasts. High-frequency control decisions exploit the existing\nunits' flexibility, reducing the power grid's dependence on hardware\ntransformations and saving investment and operating costs, as demonstrated in\nexperimental results. This research exhibits the potential of reinforcement\nlearning in promoting low-carbon and intelligent power systems and represents a\nsolid step toward sustainable electricity generation.\n","authors":["Shaohuai Liu","Jinbo Liu","Weirui Ye","Nan Yang","Guanglun Zhang","Haiwang Zhong","Chongqing Kang","Qirong Jiang","Xuri Song","Fangchun Di","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2303.05205v2.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2204.00497v3","updated":"2023-03-13T07:50:11Z","published":"2022-04-01T14:57:52Z","title":"Separate and conquer heuristic allows robust mining of contrast sets in\n  classification, regression, and survival data","summary":"  Identifying differences between groups is one of the most important knowledge\ndiscovery problems. The procedure, also known as contrast sets mining, is\napplied in a wide range of areas like medicine, industry, or economics.\n  In the paper we present RuleKit-CS, an algorithm for contrast set mining\nbased on separate and conquer - a well established heuristic for decision rule\ninduction. Multiple passes accompanied with an attribute penalization scheme\nprovide contrast sets describing same examples with different attributes,\ndistinguishing presented approach from the standard separate and conquer. The\nalgorithm was also generalized for regression and survival data allowing\nidentification of contrast sets whose label attribute/survival prognosis is\nconsistent with the label/prognosis for the predefined contrast groups. This\nfeature, not provided by the existing approaches, further extends the usability\nof RuleKit-CS.\n  Experiments on over 130 data sets from various areas and detailed analysis of\nselected cases confirmed RuleKit-CS to be a useful tool for discovering\ndifferences between defined groups. The algorithm was implemented as a part of\nthe RuleKit suite available at GitHub under GNU AGPL 3 licence\n(https://github.com/adaa-polsl/RuleKit).\n  Keywords: contrast sets, separate and conquer, regression, survival\n","authors":["Adam Gudyś","Marek Sikora","Łukasz Wróbel"],"pdf_url":"https://arxiv.org/pdf/2204.00497v3.pdf","comment":"36 pages, 6 figures, 3 tables, 3 algorithms"},{"id":"http://arxiv.org/abs/2303.02045v2","updated":"2023-03-13T07:41:15Z","published":"2023-03-03T16:12:59Z","title":"Uncertainty Estimation by Fisher Information-based Evidential Deep\n  Learning","summary":"  Uncertainty estimation is a key factor that makes deep learning reliable in\npractical applications. Recently proposed evidential neural networks explicitly\naccount for different uncertainties by treating the network's outputs as\nevidence to parameterize the Dirichlet distribution, and achieve impressive\nperformance in uncertainty estimation. However, for high data uncertainty\nsamples but annotated with the one-hot label, the evidence-learning process for\nthose mislabeled classes is over-penalized and remains hindered. To address\nthis problem, we propose a novel method, Fisher Information-based Evidential\nDeep Learning ($\\mathcal{I}$-EDL). In particular, we introduce Fisher\nInformation Matrix (FIM) to measure the informativeness of evidence carried by\neach sample, according to which we can dynamically reweight the objective loss\nterms to make the network more focused on the representation learning of\nuncertain classes. The generalization ability of our network is further\nimproved by optimizing the PAC-Bayesian bound. As demonstrated empirically, our\nproposed method consistently outperforms traditional EDL-related algorithms in\nmultiple uncertainty estimation tasks, especially in the more challenging\nfew-shot classification settings.\n","authors":["Danruo Deng","Guangyong Chen","Yang Yu","Furui Liu","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2303.02045v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05492v2","updated":"2023-03-13T07:40:08Z","published":"2022-12-11T12:37:31Z","title":"Client Selection for Federated Bayesian Learning","summary":"  Distributed Stein Variational Gradient Descent (DSVGD) is a non-parametric\ndistributed learning framework for federated Bayesian learning, where multiple\nclients jointly train a machine learning model by communicating a number of\nnon-random and interacting particles with the server. Since communication\nresources are limited, selecting the clients with most informative local\nlearning updates can improve the model convergence and communication\nefficiency. In this paper, we propose two selection schemes for DSVGD based on\nKernelized Stein Discrepancy (KSD) and Hilbert Inner Product (HIP). We derive\nthe upper bound on the decrease of the global free energy per iteration for\nboth schemes, which is then minimized to speed up the model convergence. We\nevaluate and compare our schemes with conventional schemes in terms of model\naccuracy, convergence speed, and stability using various learning tasks and\ndatasets.\n","authors":["Jiarong Yang","Yuan Liu","Rahif Kassab"],"pdf_url":"https://arxiv.org/pdf/2212.05492v2.pdf","comment":"To appear in IEEE Journal on Selected Areas in Communications Special\n  Issue on Communication-Efficient Distributed Learning over Networks"},{"id":"http://arxiv.org/abs/2208.12506v3","updated":"2023-03-13T07:35:42Z","published":"2022-08-26T08:56:33Z","title":"EGFR Mutation Prediction of Lung Biopsy Images using Deep Learning","summary":"  The standard diagnostic procedures for targeted therapies in lung cancer\ntreatment involve histological subtyping and subsequent detection of key driver\nmutations, such as EGFR. Even though molecular profiling can uncover the driver\nmutation, the process is often expensive and time-consuming. Deep\nlearning-oriented image analysis offers a more economical alternative for\ndiscovering driver mutations directly from whole slide images (WSIs). In this\nwork, we used customized deep learning pipelines with weak supervision to\nidentify the morphological correlates of EGFR mutation from hematoxylin and\neosin-stained WSIs, in addition to detecting tumor and histologically subtyping\nit. We demonstrate the effectiveness of our pipeline by conducting rigorous\nexperiments and ablation studies on two lung cancer datasets - TCGA and a\nprivate dataset from India. With our pipeline, we achieved an average area\nunder the curve (AUC) of 0.964 for tumor detection, and 0.942 for histological\nsubtyping between adenocarcinoma and squamous cell carcinoma on the TCGA\ndataset. For EGFR detection, we achieved an average AUC of 0.864 on the TCGA\ndataset and 0.783 on the dataset from India. Our key learning points include\nthe following. Firstly, there is no particular advantage of using a feature\nextractor layers trained on histology, if one is going to fine-tune the feature\nextractor on the target dataset. Secondly, selecting patches with high\ncellularity, presumably capturing tumor regions, is not always helpful, as the\nsign of a disease class may be present in the tumor-adjacent stroma.\n","authors":["Ravi Kant Gupta","Shivani Nandgaonkar","Nikhil Cherian Kurian","Swapnil Rane","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2208.12506v3.pdf","comment":"We need to improve"},{"id":"http://arxiv.org/abs/2303.06902v1","updated":"2023-03-13T07:32:37Z","published":"2023-03-13T07:32:37Z","title":"Molecular Property Prediction by Semantic-invariant Contrastive Learning","summary":"  Contrastive learning have been widely used as pretext tasks for\nself-supervised pre-trained molecular representation learning models in\nAI-aided drug design and discovery. However, exiting methods that generate\nmolecular views by noise-adding operations for contrastive learning may face\nthe semantic inconsistency problem, which leads to false positive pairs and\nconsequently poor prediction performance. To address this problem, in this\npaper we first propose a semantic-invariant view generation method by properly\nbreaking molecular graphs into fragment pairs. Then, we develop a\nFragment-based Semantic-Invariant Contrastive Learning (FraSICL) model based on\nthis view generation method for molecular property prediction. The FraSICL\nmodel consists of two branches to generate representations of views for\ncontrastive learning, meanwhile a multi-view fusion and an auxiliary similarity\nloss are introduced to make better use of the information contained in\ndifferent fragment-pair views. Extensive experiments on various benchmark\ndatasets show that with the least number of pre-training samples, FraSICL can\nachieve state-of-the-art performance, compared with major existing counterpart\nmodels.\n","authors":["Ziqiao Zhang","Ailin Xie","Jihong Guan","Shuigeng Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.06902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.00395v2","updated":"2023-03-13T07:15:18Z","published":"2022-02-01T13:22:26Z","title":"Is the Performance of My Deep Network Too Good to Be True? A Direct\n  Approach to Estimating the Bayes Error in Binary Classification","summary":"  There is a fundamental limitation in the prediction performance that a\nmachine learning model can achieve due to the inevitable uncertainty of the\nprediction target. In classification problems, this can be characterized by the\nBayes error, which is the best achievable error with any classifier. The Bayes\nerror can be used as a criterion to evaluate classifiers with state-of-the-art\nperformance and can be used to detect test set overfitting. We propose a simple\nand direct Bayes error estimator, where we just take the mean of the labels\nthat show \\emph{uncertainty} of the class assignments. Our flexible approach\nenables us to perform Bayes error estimation even for weakly supervised data.\nIn contrast to others, our method is model-free and even instance-free.\nMoreover, it has no hyperparameters and gives a more accurate estimate of the\nBayes error than several baselines empirically. Experiments using our method\nsuggest that recently proposed deep networks such as the Vision Transformer may\nhave reached, or is about to reach, the Bayes error for benchmark datasets.\nFinally, we discuss how we can study the inherent difficulty of the\nacceptance/rejection decision for scientific articles, by estimating the Bayes\nerror of the ICLR papers from 2017 to 2023.\n","authors":["Takashi Ishida","Ikko Yamane","Nontawat Charoenphakdee","Gang Niu","Masashi Sugiyama"],"pdf_url":"https://arxiv.org/pdf/2202.00395v2.pdf","comment":"ICLR 2023 (notable-top-5%)"},{"id":"http://arxiv.org/abs/2210.00215v4","updated":"2023-03-13T07:09:20Z","published":"2022-10-01T07:36:51Z","title":"Differentiable Parsing and Visual Grounding of Natural Language\n  Instructions for Object Placement","summary":"  We present a new method, PARsing And visual GrOuNding (ParaGon), for\ngrounding natural language in object placement tasks. Natural language\ngenerally describes objects and spatial relations with compositionality and\nambiguity, two major obstacles to effective language grounding. For\ncompositionality, ParaGon parses a language instruction into an object-centric\ngraph representation to ground objects individually. For ambiguity, ParaGon\nuses a novel particle-based graph neural network to reason about object\nplacements with uncertainty. Essentially, ParaGon integrates a parsing\nalgorithm into a probabilistic, data-driven learning framework. It is fully\ndifferentiable and trained end-to-end from data for robustness against complex,\nambiguous language input.\n","authors":["Zirui Zhao","Wee Sun Lee","David Hsu"],"pdf_url":"https://arxiv.org/pdf/2210.00215v4.pdf","comment":"To appear in ICRA 2023"},{"id":"http://arxiv.org/abs/2110.10423v4","updated":"2023-03-13T06:18:52Z","published":"2021-10-20T08:18:16Z","title":"ProxyBO: Accelerating Neural Architecture Search via Bayesian\n  Optimization with Zero-cost Proxies","summary":"  Designing neural architectures requires immense manual efforts. This has\npromoted the development of neural architecture search (NAS) to automate the\ndesign. While previous NAS methods achieve promising results but run slowly,\nzero-cost proxies run extremely fast but are less promising. Therefore, it is\nof great potential to accelerate NAS via those zero-cost proxies. The existing\nmethod has two limitations, which are unforeseeable reliability and one-shot\nusage. To address the limitations, we present ProxyBO, an efficient Bayesian\noptimization (BO) framework that utilizes the zero-cost proxies to accelerate\nneural architecture search. We apply the generalization ability measurement to\nestimate the fitness of proxies on the task during each iteration and design a\nnovel acquisition function to combine BO with zero-cost proxies based on their\ndynamic influence. Extensive empirical studies show that ProxyBO consistently\noutperforms competitive baselines on five tasks from three public benchmarks.\nConcretely, ProxyBO achieves up to 5.41x and 3.86x speedups over the\nstate-of-the-art approaches REA and BRP-NAS.\n","authors":["Yu Shen","Yang Li","Jian Zheng","Wentao Zhang","Peng Yao","Jixiang Li","Sen Yang","Ji Liu","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2110.10423v4.pdf","comment":"Accepted by AAAI 2023"},{"id":"http://arxiv.org/abs/2303.06879v1","updated":"2023-03-13T05:54:01Z","published":"2023-03-13T05:54:01Z","title":"Spacecraft Anomaly Detection with Attention Temporal Convolution Network","summary":"  Spacecraft faces various situations when carrying out exploration missions in\ncomplex space, thus monitoring the anomaly status of spacecraft is crucial to\nthe development of \\textcolor{blue}{the} aerospace industry. The time series\ntelemetry data generated by on-orbit spacecraft \\textcolor{blue}{contains}\nimportant information about the status of spacecraft. However, traditional\ndomain knowledge-based spacecraft anomaly detection methods are not effective\ndue to high dimensionality and complex correlation among variables. In this\nwork, we propose an anomaly detection framework for spacecraft multivariate\ntime-series data based on temporal convolution networks (TCNs). First, we\nemploy dynamic graph attention to model the complex correlation among variables\nand time series. Second, temporal convolution networks with parallel processing\nability are used to extract multidimensional \\textcolor{blue}{features} for\n\\textcolor{blue}{the} downstream prediction task. Finally, many potential\nanomalies are detected by the best threshold. Experiments on real NASA SMAP/MSL\nspacecraft datasets show the superiority of our proposed model with respect to\nstate-of-the-art methods.\n","authors":["Liang Liu","Ling Tian","Zhao Kang","Tianqi Wan"],"pdf_url":"https://arxiv.org/pdf/2303.06879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.03452v2","updated":"2023-03-13T05:52:05Z","published":"2021-12-07T02:28:12Z","title":"Location Leakage in Federated Signal Maps","summary":"  We consider the problem of predicting cellular network performance (signal\nmaps) from measurements collected by several mobile devices. We formulate the\nproblem within the online federated learning framework: (i) federated learning\n(FL) enables users to collaboratively train a model, while keeping their\ntraining data on their devices; (ii) measurements are collected as users move\naround over time and are used for local training in an online fashion. We\nconsider an honest-but-curious server, who observes the updates from target\nusers participating in FL and infers their location using a deep leakage from\ngradients (DLG) type of attack, originally developed to reconstruct training\ndata of DNN image classifiers. We make the key observation that a DLG attack,\napplied to our setting, infers the average location of a batch of local data,\nand can thus be used to reconstruct the target users' trajectory at a coarse\ngranularity. We build on this observation to protect location privacy, in our\nsetting, by revisiting and designing mechanisms within the federated learning\nframework including: tuning the FL parameters for averaging, curating local\nbatches so as to mislead the DLG attacker, and aggregating across multiple\nusers with different trajectories. We evaluate the performance of our\nalgorithms through both analysis and simulation based on real-world mobile\ndatasets, and we show that they achieve a good privacy-utility tradeoff.\n","authors":["Evita Bakopoulou","Jiang Zhang","Mengwei Yang","Konstantinos Psounis","Athina Markopoulou"],"pdf_url":"https://arxiv.org/pdf/2112.03452v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06871v1","updated":"2023-03-13T05:42:58Z","published":"2023-03-13T05:42:58Z","title":"Physics-driven machine learning models coupling PyTorch and Firedrake","summary":"  Partial differential equations (PDEs) are central to describing and modelling\ncomplex physical systems that arise in many disciplines across science and\nengineering. However, in many realistic applications PDE modelling provides an\nincomplete description of the physics of interest. PDE-based machine learning\ntechniques are designed to address this limitation. In this approach, the PDE\nis used as an inductive bias enabling the coupled model to rely on fundamental\nphysical laws while requiring less training data. The deployment of\nhigh-performance simulations coupling PDEs and machine learning to complex\nproblems necessitates the composition of capabilities provided by machine\nlearning and PDE-based frameworks. We present a simple yet effective coupling\nbetween the machine learning framework PyTorch and the PDE system Firedrake\nthat provides researchers, engineers and domain specialists with a high\nproductive way of specifying coupled models while only requiring trivial\nchanges to existing code.\n","authors":["Nacime Bouziani","David A. Ham"],"pdf_url":"https://arxiv.org/pdf/2303.06871v1.pdf","comment":"Accepted at the ICLR 2023 Workshop on Physics for Machine Learning"},{"id":"http://arxiv.org/abs/2303.06870v1","updated":"2023-03-13T05:37:46Z","published":"2023-03-13T05:37:46Z","title":"Three Guidelines You Should Know for Universally Slimmable\n  Self-Supervised Learning","summary":"  We propose universally slimmable self-supervised learning (dubbed as US3L) to\nachieve better accuracy-efficiency trade-offs for deploying self-supervised\nmodels across different devices. We observe that direct adaptation of\nself-supervised learning (SSL) to universally slimmable networks misbehaves as\nthe training process frequently collapses. We then discover that temporal\nconsistent guidance is the key to the success of SSL for universally slimmable\nnetworks, and we propose three guidelines for the loss design to ensure this\ntemporal consistency from a unified gradient perspective. Moreover, we propose\ndynamic sampling and group regularization strategies to simultaneously improve\ntraining efficiency and accuracy. Our US3L method has been empirically\nvalidated on both convolutional neural networks and vision transformers. With\nonly once training and one copy of weights, our method outperforms various\nstate-of-the-art methods (individually trained or not) on benchmarks including\nrecognition, object detection and instance segmentation. Our code is available\nat https://github.com/megvii-research/US3L-CVPR2023.\n","authors":["Yun-Hao Cao","Peiqin Sun","Shuchang Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.06870v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.06865v1","updated":"2023-03-13T05:19:28Z","published":"2023-03-13T05:19:28Z","title":"High-throughput Generative Inference of Large Language Models with a\n  Single GPU","summary":"  The high computational and memory requirements of large language model (LLM)\ninference traditionally make it feasible only with multiple high-end\naccelerators. Motivated by the emerging demand for latency-insensitive tasks\nwith batched processing, this paper initiates the study of high-throughput LLM\ninference using limited resources, such as a single commodity GPU. We present\nFlexGen, a high-throughput generation engine for running LLMs with limited GPU\nmemory. FlexGen can be flexibly configured under various hardware resource\nconstraints by aggregating memory and computation from the GPU, CPU, and disk.\nThrough a linear programming optimizer, it searches for efficient patterns to\nstore and access tensors. FlexGen further compresses these weights and the\nattention cache to 4 bits with negligible accuracy loss. These techniques\nenable FlexGen to have a larger space of batch size choices and thus\nsignificantly increase maximum throughput. As a result, when running OPT-175B\non a single 16GB GPU, FlexGen achieves significantly higher throughput compared\nto state-of-the-art offloading systems, reaching a generation throughput of 1\ntoken/s for the first time with an effective batch size of 144. On the HELM\nbenchmark, FlexGen can benchmark a 30B model with a 16GB GPU on 7\nrepresentative sub-scenarios in 21 hours. The code is available at\nhttps://github.com/FMInference/FlexGen\n","authors":["Ying Sheng","Lianmin Zheng","Binhang Yuan","Zhuohan Li","Max Ryabinin","Daniel Y. Fu","Zhiqiang Xie","Beidi Chen","Clark Barrett","Joseph E. Gonzalez","Percy Liang","Christopher Ré","Ion Stoica","Ce Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.06865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06856v1","updated":"2023-03-13T05:01:50Z","published":"2023-03-13T05:01:50Z","title":"Dynamic Neural Network for Multi-Task Learning Searching across Diverse\n  Network Topologies","summary":"  In this paper, we present a new MTL framework that searches for structures\noptimized for multiple tasks with diverse graph topologies and shares features\namong tasks. We design a restricted DAG-based central network with\nread-in/read-out layers to build topologically diverse task-adaptive structures\nwhile limiting search space and time. We search for a single optimized network\nthat serves as multiple task adaptive sub-networks using our three-stage\ntraining process. To make the network compact and discretized, we propose a\nflow-based reduction algorithm and a squeeze loss used in the training process.\nWe evaluate our optimized network on various public MTL datasets and show ours\nachieves state-of-the-art performance. An extensive ablation study\nexperimentally validates the effectiveness of the sub-module and schemes in our\nframework.\n","authors":["Wonhyeok Choi","Sunghoon Im"],"pdf_url":"https://arxiv.org/pdf/2303.06856v1.pdf","comment":"Accepted at CVPR 2023, 13 pages, 10 encapsulated postscript figures"},{"id":"http://arxiv.org/abs/2303.06854v1","updated":"2023-03-13T04:49:46Z","published":"2023-03-13T04:49:46Z","title":"Robust Contrastive Language-Image Pretraining against Adversarial\n  Attacks","summary":"  Contrastive vision-language representation learning has achieved\nstate-of-the-art performance for zero-shot classification, by learning from\nmillions of image-caption pairs crawled from the internet. However, the massive\ndata that powers large multimodal models such as CLIP, makes them extremely\nvulnerable to various types of adversarial attacks, including targeted and\nbackdoor data poisoning attacks. Despite this vulnerability, robust contrastive\nvision-language pretraining against adversarial attacks has remained\nunaddressed. In this work, we propose RoCLIP, the first effective method for\nrobust pretraining {and fine-tuning} multimodal vision-language models. RoCLIP\neffectively breaks the association between poisoned image-caption pairs by\nconsidering a pool of random examples, and (1) matching every image with the\ntext that is most similar to its caption in the pool, and (2) matching every\ncaption with the image that is most similar to its image in the pool. Our\nextensive experiments show that our method renders state-of-the-art targeted\ndata poisoning and backdoor attacks ineffective during pre-training or\nfine-tuning of CLIP. In particular, RoCLIP decreases the poison and backdoor\nattack success rates down to 0\\% during pre-training and 1\\%-4\\% during\nfine-tuning, and effectively improves the model's performance.\n","authors":["Wenhan Yang","Baharan Mirzasoleiman"],"pdf_url":"https://arxiv.org/pdf/2303.06854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06851v1","updated":"2023-03-13T04:46:08Z","published":"2023-03-13T04:46:08Z","title":"On the Regret of Online Edge Service Hosting","summary":"  We consider the problem of service hosting where a service provider can\ndynamically rent edge resources via short term contracts to ensure better\nquality of service to its customers. The service can also be partially hosted\nat the edge, in which case, customers' requests can be partially served at the\nedge. The total cost incurred by the system is modeled as a combination of the\nrent cost, the service cost incurred due to latency in serving customers, and\nthe fetch cost incurred as a result of the bandwidth used to fetch the\ncode/databases of the service from the cloud servers to host the service at the\nedge. In this paper, we compare multiple hosting policies with regret as a\nmetric, defined as the difference in the cost incurred by the policy and the\noptimal policy over some time horizon $T$. In particular we consider the Retro\nRenting (RR) and Follow The Perturbed Leader (FTPL) policies proposed in the\nliterature and provide performance guarantees on the regret of these policies.\nWe show that under i.i.d stochastic arrivals, RR policy has linear regret while\nFTPL policy has constant regret. Next, we propose a variant of FTPL, namely\nWait then FTPL (W-FTPL), which also has constant regret while demonstrating\nmuch better dependence on the fetch cost. We also show that under adversarial\narrivals, RR policy has linear regret while both FTPL and W-FTPL have regret\n$\\mathrm{O}(\\sqrt{T})$ which is order-optimal.\n","authors":["R Sri Prakash","Nikhil Karamchandani","Sharayu Moharir"],"pdf_url":"https://arxiv.org/pdf/2303.06851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.01970v5","updated":"2023-03-13T04:40:22Z","published":"2022-05-04T09:37:16Z","title":"Non-Stationary Bandit Learning via Predictive Sampling","summary":"  Thompson sampling has proven effective across a wide range of stationary\nbandit environments. However, as we demonstrate in this paper, it can perform\npoorly when applied to non-stationary environments. We show that such failures\nare attributed to the fact that, when exploring, the algorithm does not\ndifferentiate actions based on how quickly the information acquired loses its\nusefulness due to non-stationarity. Building upon this insight, we propose\npredictive sampling, an algorithm that deprioritizes acquiring information that\nquickly loses usefulness. Theoretical guarantee on the performance of\npredictive sampling is established through a Bayesian regret bound. We provide\nversions of predictive sampling for which computations tractably scale to\ncomplex bandit environments of practical interest. Through numerical\nsimulations, we demonstrate that predictive sampling outperforms Thompson\nsampling in all non-stationary environments examined.\n","authors":["Yueyang Liu","Benjamin Van Roy","Kuang Xu"],"pdf_url":"https://arxiv.org/pdf/2205.01970v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2008.02144v2","updated":"2023-03-13T04:38:12Z","published":"2020-08-05T14:05:37Z","title":"FRMDN: Flow-based Recurrent Mixture Density Network","summary":"  The class of recurrent mixture density networks is an important class of\nprobabilistic models used extensively in sequence modeling and\nsequence-to-sequence mapping applications. In this class of models, the density\nof a target sequence in each time-step is modeled by a Gaussian mixture model\nwith the parameters given by a recurrent neural network. In this paper, we\ngeneralize recurrent mixture density networks by defining a Gaussian mixture\nmodel on a non-linearly transformed target sequence in each time-step. The\nnon-linearly transformed space is created by normalizing flow. We observed that\nthis model significantly improves the fit to image sequences measured by the\nlog-likelihood. We also applied the proposed model on some speech and image\ndata, and observed that the model has significant modeling power outperforming\nother state-of-the-art methods in terms of the log-likelihood.\n","authors":["Seyedeh Fatemeh Razavi","Reshad Hosseini","Tina Behzad"],"pdf_url":"https://arxiv.org/pdf/2008.02144v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06847v1","updated":"2023-03-13T04:31:35Z","published":"2023-03-13T04:31:35Z","title":"Label Distribution Learning from Logical Label","summary":"  Label distribution learning (LDL) is an effective method to predict the label\ndescription degree (a.k.a. label distribution) of a sample. However, annotating\nlabel distribution (LD) for training samples is extremely costly. So recent\nstudies often first use label enhancement (LE) to generate the estimated label\ndistribution from the logical label and then apply external LDL algorithms on\nthe recovered label distribution to predict the label distribution for unseen\nsamples. But this step-wise manner overlooks the possible connections between\nLE and LDL. Moreover, the existing LE approaches may assign some description\ndegrees to invalid labels. To solve the above problems, we propose a novel\nmethod to learn an LDL model directly from the logical label, which unifies LE\nand LDL into a joint model, and avoids the drawbacks of the previous LE\nmethods. Extensive experiments on various datasets prove that the proposed\napproach can construct a reliable LDL model directly from the logical label,\nand produce more accurate label distribution than the state-of-the-art LE\nmethods.\n","authors":["Yuheng Jia","Jiawei Tang","Jiahao Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.06847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.01198v4","updated":"2023-03-13T04:29:53Z","published":"2022-09-02T17:49:34Z","title":"Estimation of Correlation Matrices from Limited time series Data using\n  Machine Learning","summary":"  Correlation matrices contain a wide variety of spatio-temporal information\nabout a dynamical system. Predicting correlation matrices from partial time\nseries information of a few nodes characterizes the spatio-temporal dynamics of\nthe entire underlying system. This information can help to predict the\nunderlying network structure, e.g., inferring neuronal connections from spiking\ndata, deducing causal dependencies between genes from expression data, and\ndiscovering long spatial range influences in climate variations. Traditional\nmethods of predicting correlation matrices utilize time series data of all the\nnodes of the underlying networks. Here, we use a supervised machine learning\ntechnique to predict the correlation matrix of entire systems from finite time\nseries information of a few randomly selected nodes. The accuracy of the\nprediction validates that only a limited time series of a subset of the entire\nsystem is enough to make good correlation matrix predictions. Furthermore,\nusing an unsupervised learning algorithm, we furnish insights into the success\nof the predictions from our model. Finally, we employ the machine learning\nmodel developed here to real-world data sets.\n","authors":["Nikhil Easaw","Woo Seok Lee","Prashant Singh Lohiya","Sarika Jalan","Priodyuti Pradhan"],"pdf_url":"https://arxiv.org/pdf/2209.01198v4.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.06845v1","updated":"2023-03-13T04:21:33Z","published":"2023-03-13T04:21:33Z","title":"Transformer Encoder with Multiscale Deep Learning for Pain\n  Classification Using Physiological Signals","summary":"  Pain is a serious worldwide health problem that affects a vast proportion of\nthe population. For efficient pain management and treatment, accurate\nclassification and evaluation of pain severity are necessary. However, this can\nbe challenging as pain is a subjective sensation-driven experience. Traditional\ntechniques for measuring pain intensity, e.g. self-report scales, are\nsusceptible to bias and unreliable in some instances. Consequently, there is a\nneed for more objective and automatic pain intensity assessment strategies. In\nthis research, we develop PainAttnNet (PAN), a novel transfomer-encoder\ndeep-learning framework for classifying pain intensities with physiological\nsignals as input. The proposed approach is comprised of three feature\nextraction architectures: multiscale convolutional networks (MSCN), a\nsqueeze-and-excitation residual network (SEResNet), and a transformer encoder\nblock. On the basis of pain stimuli, MSCN extracts short- and long-window\ninformation as well as sequential features. SEResNet highlights relevant\nextracted features by mapping the interdependencies among features. The third\narchitecture employs a transformer encoder consisting of three temporal\nconvolutional networks (TCN) with three multi-head attention (MHA) layers to\nextract temporal dependencies from the features. Using the publicly available\nBioVid pain dataset, we test the proposed PainAttnNet model and demonstrate\nthat our outcomes outperform state-of-the-art models. These results confirm\nthat our approach can be utilized for automated classification of pain\nintensity using physiological signals to improve pain management and treatment.\n","authors":["Zhenyuan Lu","Burcu Ozek","Sagar Kamarthi"],"pdf_url":"https://arxiv.org/pdf/2303.06845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06836v1","updated":"2023-03-13T03:46:37Z","published":"2023-03-13T03:46:37Z","title":"Label Information Bottleneck for Label Enhancement","summary":"  In this work, we focus on the challenging problem of Label Enhancement (LE),\nwhich aims to exactly recover label distributions from logical labels, and\npresent a novel Label Information Bottleneck (LIB) method for LE. For the\nrecovery process of label distributions, the label irrelevant information\ncontained in the dataset may lead to unsatisfactory recovery performance. To\naddress this limitation, we make efforts to excavate the essential label\nrelevant information to improve the recovery performance. Our method formulates\nthe LE problem as the following two joint processes: 1) learning the\nrepresentation with the essential label relevant information, 2) recovering\nlabel distributions based on the learned representation. The label relevant\ninformation can be excavated based on the \"bottleneck\" formed by the learned\nrepresentation. Significantly, both the label relevant information about the\nlabel assignments and the label relevant information about the label gaps can\nbe explored in our method. Evaluation experiments conducted on several\nbenchmark label distribution learning datasets verify the effectiveness and\ncompetitiveness of LIB. Our source codes are available\n\"https://github.com/qinghai-zheng/LIBLE\"\n","authors":["Qinghai Zheng","Jihua Zhu","Haoyu Tang"],"pdf_url":"https://arxiv.org/pdf/2303.06836v1.pdf","comment":"Accepted by CVPR 2023, our source codes are available at\n  \"https://github.com/qinghai-zheng/LIBLE\""},{"id":"http://arxiv.org/abs/2303.06833v1","updated":"2023-03-13T03:29:58Z","published":"2023-03-13T03:29:58Z","title":"Transformer-based Planning for Symbolic Regression","summary":"  Symbolic regression (SR) is a challenging task in machine learning that\ninvolves finding a mathematical expression for a function based on its values.\nRecent advancements in SR have demonstrated the efficacy of pretrained\ntransformer-based models for generating equations as sequences, which benefit\nfrom large-scale pretraining on synthetic datasets and offer considerable\nadvantages over GP-based methods in terms of inference time. However, these\nmodels focus on supervised pretraining goals borrowed from text generation and\nignore equation-specific objectives like accuracy and complexity. To address\nthis, we propose TPSR, a Transformer-based Planning strategy for Symbolic\nRegression that incorporates Monte Carlo Tree Search into the transformer\ndecoding process. TPSR, as opposed to conventional decoding strategies, allows\nfor the integration of non-differentiable feedback, such as fitting accuracy\nand complexity, as external sources of knowledge into the equation generation\nprocess. Extensive experiments on various datasets show that our approach\noutperforms state-of-the-art methods, enhancing the model's fitting-complexity\ntrade-off, extrapolation abilities, and robustness to noise. We also\ndemonstrate that the utilization of various caching mechanisms can further\nenhance the efficiency of TPSR.\n","authors":["Parshin Shojaee","Kazem Meidani","Amir Barati Farimani","Chandan K. Reddy"],"pdf_url":"https://arxiv.org/pdf/2303.06833v1.pdf","comment":"Parshin Shojaee and Kazem Meidani contributed equally to this work"},{"id":"http://arxiv.org/abs/2303.06832v1","updated":"2023-03-13T03:28:36Z","published":"2023-03-13T03:28:36Z","title":"ODIN: On-demand Data Formulation to Mitigate Dataset Lock-in","summary":"  ODIN is an innovative approach that addresses the problem of dataset\nconstraints by integrating generative AI models. Traditional zero-shot learning\nmethods are constrained by the training dataset. To fundamentally overcome this\nlimitation, ODIN attempts to mitigate the dataset constraints by generating\non-demand datasets based on user requirements. ODIN consists of three main\nmodules: a prompt generator, a text-to-image generator, and an image\npost-processor. To generate high-quality prompts and images, we adopted a large\nlanguage model (e.g., ChatGPT), and a text-to-image diffusion model (e.g.,\nStable Diffusion), respectively. We evaluated ODIN on various datasets in terms\nof model accuracy and data diversity to demonstrate its potential, and\nconducted post-experiments for further investigation. Overall, ODIN is a\nfeasible approach that enables Al to learn unseen knowledge beyond the training\ndataset.\n","authors":[" Spchoi","Jihoon Lee","HyeongSeok Ahn","Sanghee Jung","Bumsoo Kang"],"pdf_url":"https://arxiv.org/pdf/2303.06832v1.pdf","comment":"10pages"},{"id":"http://arxiv.org/abs/2301.01217v3","updated":"2023-03-13T03:27:49Z","published":"2022-12-31T04:26:25Z","title":"Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples","summary":"  There is a growing interest in developing unlearnable examples (UEs) against\nvisual privacy leaks on the Internet. UEs are training samples added with\ninvisible but unlearnable noise, which have been found can prevent unauthorized\ntraining of machine learning models. UEs typically are generated via a bilevel\noptimization framework with a surrogate model to remove (minimize) errors from\nthe original samples, and then applied to protect the data against unknown\ntarget models. However, existing UE generation methods all rely on an ideal\nassumption called label-consistency, where the hackers and protectors are\nassumed to hold the same label for a given sample. In this work, we propose and\npromote a more practical label-agnostic setting, where the hackers may exploit\nthe protected data quite differently from the protectors. E.g., a m-class\nunlearnable dataset held by the protector may be exploited by the hacker as a\nn-class dataset. Existing UE generation methods are rendered ineffective in\nthis challenging setting. To tackle this challenge, we present a novel\ntechnique called Unlearnable Clusters (UCs) to generate label-agnostic\nunlearnable examples with cluster-wise perturbations. Furthermore, we propose\nto leverage VisionandLanguage Pre-trained Models (VLPMs) like CLIP as the\nsurrogate model to improve the transferability of the crafted UCs to diverse\ndomains. We empirically verify the effectiveness of our proposed approach under\na variety of settings with different datasets, target models, and even\ncommercial platforms Microsoft Azure and Baidu PaddlePaddle. Code is available\nat \\url{https://github.com/jiamingzhang94/Unlearnable-Clusters}.\n","authors":["Jiaming Zhang","Xingjun Ma","Qi Yi","Jitao Sang","Yugang Jiang","Yaowei Wang","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2301.01217v3.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2201.06837v3","updated":"2023-03-13T03:00:12Z","published":"2022-01-18T09:27:13Z","title":"Landslide Susceptibility Modeling by Interpretable Neural Network","summary":"  Landslides are notoriously difficult to predict because numerous spatially\nand temporally varying factors contribute to slope stability. Artificial neural\nnetworks (ANN) have been shown to improve prediction accuracy but are largely\nuninterpretable. Here we introduce an additive ANN optimization framework to\nassess landslide susceptibility, as well as dataset division and outcome\ninterpretation techniques. We refer to our approach, which features full\ninterpretability, high accuracy, high generalizability and low model\ncomplexity, as superposable neural network (SNN) optimization. We validate our\napproach by training models on landslide inventory from three different\neasternmost Himalaya regions. Our SNN outperformed physically-based and\nstatistical models and achieved similar performance to state-of-the-art deep\nneural networks. The SNN models found the product of slope and precipitation\nand hillslope aspect to be important primary contributors to high landslide\nsusceptibility, which highlights the importance of strong slope-climate\ncouplings, along with microclimates, on landslide occurrences.\n","authors":["Khaled Youssef","Kevin Shao","Seulgi Moon","Louis-Serge Bouchard"],"pdf_url":"https://arxiv.org/pdf/2201.06837v3.pdf","comment":"79 pages (including SI section); 8 main figures; 12 supplementary\n  figures; 9 supplementary tables"},{"id":"http://arxiv.org/abs/2303.06827v1","updated":"2023-03-13T03:00:03Z","published":"2023-03-13T03:00:03Z","title":"Kernel Density Bayesian Inverse Reinforcement Learning","summary":"  Inverse reinforcement learning~(IRL) is a powerful framework to infer an\nagent's reward function by observing its behavior, but IRL algorithms that\nlearn point estimates of the reward function can be misleading because there\nmay be several functions that describe an agent's behavior equally well. A\nBayesian approach to IRL models a distribution over candidate reward functions,\nalleviating the shortcomings of learning a point estimate. However, several\nBayesian IRL algorithms use a $Q$-value function in place of the likelihood\nfunction. The resulting posterior is computationally intensive to calculate,\nhas few theoretical guarantees, and the $Q$-value function is often a poor\napproximation for the likelihood. We introduce kernel density Bayesian IRL\n(KD-BIRL), which uses conditional kernel density estimation to directly\napproximate the likelihood, providing an efficient framework that, with a\nmodified reward function parameterization, is applicable to environments with\ncomplex and infinite state spaces. We demonstrate KD-BIRL's benefits through a\nseries of experiments in Gridworld environments and a simulated sepsis\ntreatment task.\n","authors":["Aishwarya Mandyam","Didong Li","Diana Cai","Andrew Jones","Barbara E. Engelhardt"],"pdf_url":"https://arxiv.org/pdf/2303.06827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06825v1","updated":"2023-03-13T02:50:59Z","published":"2023-03-13T02:50:59Z","title":"Best-of-three-worlds Analysis for Linear Bandits with\n  Follow-the-regularized-leader Algorithm","summary":"  The linear bandit problem has been studied for many years in both stochastic\nand adversarial settings. Designing an algorithm that can optimize the\nenvironment without knowing the loss type attracts lots of interest.\n\\citet{LeeLWZ021} propose an algorithm that actively detects the loss type and\nthen switches between different algorithms specially designed for different\nsettings. However, such an approach requires meticulous designs to perform well\nin all settings. Follow-the-regularized-leader (FTRL) is another popular\nalgorithm type that can adapt to different environments. This algorithm is of\nsimple design and the regret bounds are shown to be optimal in traditional\nmulti-armed bandit problems compared with the detect-switch type algorithms.\nDesigning an FTRL-type algorithm for linear bandits is an important question\nthat has been open for a long time. In this paper, we prove that the FTRL-type\nalgorithm with a negative entropy regularizer can achieve the\nbest-of-three-world results for the linear bandit problem with the tacit\ncooperation between the choice of the learning rate and the specially designed\nself-bounding inequality.\n","authors":["Fang Kong","Canzhe Zhao","Shuai Li"],"pdf_url":"https://arxiv.org/pdf/2303.06825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.00233v3","updated":"2023-03-13T02:49:38Z","published":"2022-06-01T04:57:50Z","title":"DM$^2$: Decentralized Multi-Agent Reinforcement Learning for\n  Distribution Matching","summary":"  Current approaches to multi-agent cooperation rely heavily on centralized\nmechanisms or explicit communication protocols to ensure convergence. This\npaper studies the problem of distributed multi-agent learning without resorting\nto centralized components or explicit communication. It examines the use of\ndistribution matching to facilitate the coordination of independent agents. In\nthe proposed scheme, each agent independently minimizes the distribution\nmismatch to the corresponding component of a target visitation distribution.\nThe theoretical analysis shows that under certain conditions, each agent\nminimizing its individual distribution mismatch allows the convergence to the\njoint policy that generated the target distribution. Further, if the target\ndistribution is from a joint policy that optimizes a cooperative task, the\noptimal policy for a combination of this task reward and the distribution\nmatching reward is the same joint policy. This insight is used to formulate a\npractical algorithm (DM$^2$), in which each individual agent matches a target\ndistribution derived from concurrently sampled trajectories from a joint expert\npolicy. Experimental validation on the StarCraft domain shows that combining\n(1) a task reward, and (2) a distribution matching reward for expert\ndemonstrations for the same task, allows agents to outperform a naive\ndistributed baseline. Additional experiments probe the conditions under which\nexpert demonstrations need to be sampled to obtain the learning benefits.\n","authors":["Caroline Wang","Ishan Durugkar","Elad Liebman","Peter Stone"],"pdf_url":"https://arxiv.org/pdf/2206.00233v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06818v1","updated":"2023-03-13T02:25:59Z","published":"2023-03-13T02:25:59Z","title":"Backdoor Defense via Deconfounded Representation Learning","summary":"  Deep neural networks (DNNs) are recently shown to be vulnerable to backdoor\nattacks, where attackers embed hidden backdoors in the DNN model by injecting a\nfew poisoned examples into the training dataset. While extensive efforts have\nbeen made to detect and remove backdoors from backdoored DNNs, it is still not\nclear whether a backdoor-free clean model can be directly obtained from\npoisoned datasets. In this paper, we first construct a causal graph to model\nthe generation process of poisoned data and find that the backdoor attack acts\nas the confounder, which brings spurious associations between the input images\nand target labels, making the model predictions less reliable. Inspired by the\ncausal understanding, we propose the Causality-inspired Backdoor Defense (CBD),\nto learn deconfounded representations for reliable classification.\nSpecifically, a backdoored model is intentionally trained to capture the\nconfounding effects. The other clean model dedicates to capturing the desired\ncausal effects by minimizing the mutual information with the confounding\nrepresentations from the backdoored model and employing a sample-wise\nre-weighting scheme. Extensive experiments on multiple benchmark datasets\nagainst 6 state-of-the-art attacks verify that our proposed defense method is\neffective in reducing backdoor threats while maintaining high accuracy in\npredicting benign samples. Further analysis shows that CBD can also resist\npotential adaptive attacks. The code is available at\n\\url{https://github.com/zaixizhang/CBD}.\n","authors":["Zaixi Zhang","Qi Liu","Zhicai Wang","Zepu Lu","Qingyong Hu"],"pdf_url":"https://arxiv.org/pdf/2303.06818v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.06815v1","updated":"2023-03-13T02:14:42Z","published":"2023-03-13T02:14:42Z","title":"Provable Convergence of Tensor Decomposition-Based Neural Network\n  Training","summary":"  Advanced tensor decomposition, such as tensor train (TT), has been widely\nstudied for tensor decomposition-based neural network (NN) training, which is\none of the most common model compression methods. However, training NN with\ntensor decomposition always suffers significant accuracy loss and convergence\nissues. In this paper, a holistic framework is proposed for tensor\ndecomposition-based NN training by formulating TT decomposition-based NN\ntraining as a nonconvex optimization problem. This problem can be solved by the\nproposed tensor block coordinate descent (tenBCD) method, which is a\ngradient-free algorithm. The global convergence of tenBCD to a critical point\nat a rate of O(1/k) is established with the Kurdyka {\\L}ojasiewicz (K{\\L})\nproperty, where k is the number of iterations. The theoretical results can be\nextended to the popular residual neural networks (ResNets). The effectiveness\nand efficiency of our proposed framework are verified through an image\nclassification dataset, where our proposed method can converge efficiently in\ntraining and prevent overfitting.\n","authors":["Chenyang Li","Bo Shen"],"pdf_url":"https://arxiv.org/pdf/2303.06815v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2107.12422 by other authors"},{"id":"http://arxiv.org/abs/2209.14074v3","updated":"2023-03-13T02:00:41Z","published":"2022-09-28T13:15:03Z","title":"Recipro-CAM: Fast gradient-free visual explanations for convolutional\n  neural networks","summary":"  The Convolutional Neural Network (CNN) is a widely used deep learning\narchitecture for computer vision. However, its black box nature makes it\ndifficult to interpret the behavior of the model. To mitigate this issue, AI\npractitioners have explored explainable AI methods like Class Activation Map\n(CAM) and Grad-CAM. Although these methods have shown promise, they are limited\nby architectural constraints or the burden of gradient computing. To overcome\nthis issue, Score-CAM and Ablation-CAM have been proposed as gradient-free\nmethods, but they have longer execution times compared to CAM or Grad-CAM based\nmethods, making them unsuitable for real-world solution though they resolved\ngradient related issues and enabled inference mode XAI. To address this\nchallenge, we propose a fast gradient-free Reciprocal CAM (Recipro-CAM) method.\nOur approach involves spatially masking the extracted feature maps to exploit\nthe correlation between activation maps and network predictions for target\nclasses. Our proposed method has yielded promising results, outperforming\ncurrent state-of-the-art method in the Average Drop-Coherence-Complexity (ADCC)\nmetric by $1.78 \\%$ to $3.72 \\%$, excluding VGG-16 backbone. Moreover,\nRecipro-CAM generates saliency maps at a similar rate to Grad-CAM and is\napproximately $148$ times faster than Score-CAM. The source code for\nRecipro-CAM is available in our data analysis framework.\n","authors":["Seok-Yong Byun","Wonju Lee"],"pdf_url":"https://arxiv.org/pdf/2209.14074v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.14013v3","updated":"2023-03-13T01:32:16Z","published":"2023-02-27T18:12:56Z","title":"Revisiting Self-Training with Regularized Pseudo-Labeling for Tabular\n  Data","summary":"  Recent progress in semi- and self-supervised learning has caused a rift in\nthe long-held belief about the need for an enormous amount of labeled data for\nmachine learning and the irrelevancy of unlabeled data. Although it has been\nsuccessful in various data, there is no dominant semi- and self-supervised\nlearning method that can be generalized for tabular data (i.e. most of the\nexisting methods require appropriate tabular datasets and architectures). In\nthis paper, we revisit self-training which can be applied to any kind of\nalgorithm including the most widely used architecture, gradient boosting\ndecision tree, and introduce curriculum pseudo-labeling (a state-of-the-art\npseudo-labeling technique in image) for a tabular domain. Furthermore, existing\npseudo-labeling techniques do not assure the cluster assumption when computing\nconfidence scores of pseudo-labels generated from unlabeled data. To overcome\nthis issue, we propose a novel pseudo-labeling approach that regularizes the\nconfidence scores based on the likelihoods of the pseudo-labels so that more\nreliable pseudo-labels which lie in high density regions can be obtained. We\nexhaustively validate the superiority of our approaches using various models\nand tabular datasets.\n","authors":["Minwook Kim","Juseong Kim","Giltae Song"],"pdf_url":"https://arxiv.org/pdf/2302.14013v3.pdf","comment":"10 pages for the main part and 8 extra pages for the appendix. 2\n  figures and 3 tables for the main part"},{"id":"http://arxiv.org/abs/2209.12307v2","updated":"2023-03-13T01:15:29Z","published":"2022-09-25T19:26:30Z","title":"On the Stability Analysis of Open Federated Learning Systems","summary":"  We consider the open federated learning (FL) systems, where clients may join\nand/or leave the system during the FL process. Given the variability of the\nnumber of present clients, convergence to a fixed model cannot be guaranteed in\nopen systems. Instead, we resort to a new performance metric that we term the\nstability of open FL systems, which quantifies the magnitude of the learned\nmodel in open systems. Under the assumption that local clients' functions are\nstrongly convex and smooth, we theoretically quantify the radius of stability\nfor two FL algorithms, namely local SGD and local Adam. We observe that this\nradius relies on several key parameters, including the function condition\nnumber as well as the variance of the stochastic gradient. Our theoretical\nresults are further verified by numerical simulations on both synthetic and\nreal-world benchmark data-sets.\n","authors":["Youbang Sun","Heshan Fernando","Tianyi Chen","Shahin Shahrampour"],"pdf_url":"https://arxiv.org/pdf/2209.12307v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06799v1","updated":"2023-03-13T01:09:37Z","published":"2023-03-13T01:09:37Z","title":"Gaussian Process on the Product of Directional Manifolds","summary":"  We present a principled study on establishing Gaussian processes over\nvariables on the product of directional manifolds. As a basic functional\ncomponent, a manifold-adaptive kernel is presented based on the von Mises\ndistribution for Gaussian process regression on unit circles. Afterward, a\nnovel hypertoroidal von Mises kernel is introduced to enable topology-aware\nGaussian processes on hypertori with consideration of correlational circular\ncomponents. Based thereon, we enable multi-output regression for learning\nvector-valued functions on hypertori using intrinsic coregionalization model\nand provide analytical derivatives in hyperparameter optimization. The proposed\nmulti-output hypertoroidal Gaussian process is further embedded to a\ndata-driven recursive estimation scheme for learning unknown range sensing\nmodels of angle-of-arrival inputs. Evaluations on range-based localization show\nthat the proposed scheme enables superior tracking accuracy over parametric\nmodeling and common Gaussian processes.\n","authors":["Ziyu Cao","Kailai Li"],"pdf_url":"https://arxiv.org/pdf/2303.06799v1.pdf","comment":"6 pages"}],"Computation and Language":[{"id":"http://arxiv.org/abs/2212.02623v3","updated":"2023-03-13T17:42:44Z","published":"2022-12-05T22:14:49Z","title":"Unifying Vision, Text, and Layout for Universal Document Processing","summary":"  We propose Universal Document Processing (UDOP), a foundation Document AI\nmodel which unifies text, image, and layout modalities together with varied\ntask formats, including document understanding and generation. UDOP leverages\nthe spatial correlation between textual content and document image to model\nimage, text, and layout modalities with one uniform representation. With a\nnovel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain\ndownstream tasks into a prompt-based sequence generation scheme. UDOP is\npretrained on both large-scale unlabeled document corpora using innovative\nself-supervised objectives and diverse labeled data. UDOP also learns to\ngenerate document images from text and layout modalities via masked image\nreconstruction. To the best of our knowledge, this is the first time in the\nfield of document AI that one model simultaneously achieves high-quality neural\ndocument editing and content customization. Our method sets the\nstate-of-the-art on 8 Document AI tasks, e.g., document understanding and QA,\nacross diverse data domains like finance reports, academic papers, and\nwebsites. UDOP ranks first on the leaderboard of the Document Understanding\nBenchmark.\n","authors":["Zineng Tang","Ziyi Yang","Guoxin Wang","Yuwei Fang","Yang Liu","Chenguang Zhu","Michael Zeng","Cha Zhang","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2212.02623v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07320v1","updated":"2023-03-13T17:41:57Z","published":"2023-03-13T17:41:57Z","title":"Model-tuning Via Prompts Makes NLP Models Adversarially Robust","summary":"  In recent years, NLP practitioners have converged on the following practice:\n(i) import an off-the-shelf pretrained (masked) language model; (ii) append a\nmultilayer perceptron atop the CLS token's hidden representation (with randomly\ninitialized weights); and (iii) fine-tune the entire model on a downstream task\n(MLP). This procedure has produced massive gains on standard NLP benchmarks,\nbut these models remain brittle, even to mild adversarial perturbations, such\nas word-level synonym substitutions. In this work, we demonstrate surprising\ngains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an\nalternative method of adapting to downstream tasks. Rather than modifying the\nmodel (by appending an MLP head), MVP instead modifies the input (by appending\na prompt template). Across three classification datasets, MVP improves\nperformance against adversarial word-level synonym substitutions by an average\nof 8% over standard methods and even outperforms adversarial training-based\nstate-of-art defenses by 3.5%. By combining MVP with adversarial training, we\nachieve further improvements in robust accuracy while maintaining clean\naccuracy. Finally, we conduct ablations to investigate the mechanism underlying\nthese gains. Notably, we find that the main causes of vulnerability of MLP can\nbe attributed to the misalignment between pre-training and fine-tuning tasks,\nand the randomly initialized MLP parameters. Code is available at\nhttps://github.com/acmi-lab/mvp\n","authors":["Mrigank Raman","Pratyush Maini","J. Zico Kolter","Zachary C. Lipton","Danish Pruthi"],"pdf_url":"https://arxiv.org/pdf/2303.07320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07295v1","updated":"2023-03-13T17:17:11Z","published":"2023-03-13T17:17:11Z","title":"Meet in the Middle: A New Pre-training Paradigm","summary":"  Most language models (LMs) are trained and applied in an autoregressive\nleft-to-right fashion, assuming that the next token only depends on the\npreceding ones. However, this assumption ignores the potential benefits of\nusing the full sequence information during training, and the possibility of\nhaving context from both sides during inference. In this paper, we propose a\nnew pre-training paradigm with techniques that jointly improve the training\ndata efficiency and the capabilities of the LMs in the infilling task. The\nfirst is a training objective that aligns the predictions of a left-to-right LM\nwith those of a right-to-left LM, trained on the same data but in reverse\norder. The second is a bidirectional inference procedure that enables both LMs\nto meet in the middle. We show the effectiveness of our pre-training paradigm\nwith extensive experiments on both programming and natural language models,\noutperforming strong baselines.\n","authors":["Anh Nguyen","Nikos Karampatziakis","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2303.07295v1.pdf","comment":"24 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.07292v1","updated":"2023-03-13T17:12:03Z","published":"2023-03-13T17:12:03Z","title":"Transformer-based approaches to Sentiment Detection","summary":"  The use of transfer learning methods is largely responsible for the present\nbreakthrough in Natural Learning Processing (NLP) tasks across multiple\ndomains. In order to solve the problem of sentiment detection, we examined the\nperformance of four different types of well-known state-of-the-art transformer\nmodels for text classification. Models such as Bidirectional Encoder\nRepresentations from Transformers (BERT), Robustly Optimized BERT Pre-training\nApproach (RoBERTa), a distilled version of BERT (DistilBERT), and a large\nbidirectional neural network architecture (XLNet) were proposed. The\nperformance of the four models that were used to detect disaster in the text\nwas compared. All the models performed well enough, indicating that\ntransformer-based models are suitable for the detection of disaster in text.\nThe RoBERTa transformer model performs best on the test dataset with a score of\n82.6% and is highly recommended for quality predictions. Furthermore, we\ndiscovered that the learning algorithms' performance was influenced by the\npre-processing techniques, the nature of words in the vocabulary, unbalanced\nlabeling, and the model parameters.\n","authors":["Olumide Ebenezer Ojo","Hoang Thang Ta","Alexander Gelbukh","Hiram Calvo","Olaronke Oluwayemisi Adebanji","Grigori Sidorov"],"pdf_url":"https://arxiv.org/pdf/2303.07292v1.pdf","comment":"Publisher: Springer Nature Switzerland AG, Gewerbestrasse 11, 6330\n  Cham, Switzerland Published in Book Titled: Recent Developments and the New\n  Directions of Research, Foundations, and Applications: Selected Papers of the\n  8th World Conference on Soft Computing, February 03-05, 2022, Baku,\n  Azerbaijan"},{"id":"http://arxiv.org/abs/2303.07274v1","updated":"2023-03-13T16:49:43Z","published":"2023-03-13T16:49:43Z","title":"Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of\n  Synthetic and Compositional Images","summary":"  Weird, unusual, and uncanny images pique the curiosity of observers because\nthey challenge commonsense. For example, an image released during the 2022\nworld cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo\nplaying chess, which playfully violates our expectation that their competition\nshould occur on the football field. Humans can easily recognize and interpret\nthese unconventional images, but can AI models do the same? We introduce\nWHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is\ncomprised of purposefully commonsense-defying images created by designers using\npublicly-available image generation tools like Midjourney. We consider several\ntasks posed over the dataset. In addition to image captioning, cross-modal\nmatching, and visual question answering, we introduce a difficult explanation\ngeneration task, where models must identify and explain why a given image is\nunusual. Our results show that state-of-the-art models such as GPT3 and BLIP2\nstill lag behind human performance on WHOOPS!. We hope our dataset will inspire\nthe development of AI models with stronger visual commonsense reasoning\nabilities. Data, models and code are available at the project website:\nwhoops-benchmark.github.io\n","authors":["Nitzan Bitton-Guetta","Yonatan Bitton","Jack Hessel","Ludwig Schmidt","Yuval Elovici","Gabriel Stanovsky","Roy Schwartz"],"pdf_url":"https://arxiv.org/pdf/2303.07274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07247v1","updated":"2023-03-13T16:20:33Z","published":"2023-03-13T16:20:33Z","title":"Are Models Trained on Indian Legal Data Fair?","summary":"  Recent advances and applications of language technology and artificial\nintelligence have enabled much success across multiple domains like law,\nmedical and mental health. AI-based Language Models, like Judgement Prediction,\nhave recently been proposed for the legal sector. However, these models are\nstrife with encoded social biases picked up from the training data. While bias\nand fairness have been studied across NLP, most studies primarily locate\nthemselves within a Western context. In this work, we present an initial\ninvestigation of fairness from the Indian perspective in the legal domain. We\nhighlight the propagation of learnt algorithmic biases in the bail prediction\ntask for models trained on Hindi legal documents. We evaluate the fairness gap\nusing demographic parity and show that a decision tree model trained for the\nbail prediction task has an overall fairness disparity of 0.237 between input\nfeatures associated with Hindus and Muslims. Additionally, we highlight the\nneed for further research and studies in the avenues of fairness/bias in\napplying AI in the legal sector with a specific focus on the Indian context.\n","authors":["Sahil Girhepuje","Anmol Goel","Gokul Krishnan","Shreya Goyal","Satyendra Pandey","Ponnurangam Kumaraguru","Balaram Ravindran"],"pdf_url":"https://arxiv.org/pdf/2303.07247v1.pdf","comment":"Presented at the Symposium on AI and Law (SAIL) 2023"},{"id":"http://arxiv.org/abs/2303.05737v2","updated":"2023-03-13T16:19:43Z","published":"2023-03-10T06:46:23Z","title":"Clinical BERTScore: An Improved Measure of Automatic Speech Recognition\n  Performance in Clinical Settings","summary":"  Automatic Speech Recognition (ASR) in medical contexts has the potential to\nsave time, cut costs, increase report accuracy, and reduce physician burnout.\nHowever, the healthcare industry has been slower to adopt this technology, in\npart due to the importance of avoiding medically-relevant transcription\nmistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR\nmetric that penalizes clinically-relevant mistakes more than others. We\ndemonstrate that this metric more closely aligns with clinician preferences on\nmedical sentences as compared to other metrics (WER, BLUE, METEOR, etc),\nsometimes by wide margins. We collect a benchmark of 13 clinician preferences\non 149 realistic medical sentences called the Clinician Transcript Preference\nbenchmark (CTP), demonstrate that CBERTScore more closely matches what\nclinicians prefer, and release the benchmark for the community to further\ndevelop clinically-aware ASR metrics.\n","authors":["Joel Shor","Ruyue Agnes Bi","Subhashini Venugopalan","Steven Ibara","Roman Goldenberg","Ehud Rivlin"],"pdf_url":"https://arxiv.org/pdf/2303.05737v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.07027v2","updated":"2023-03-13T16:14:52Z","published":"2023-02-14T13:09:23Z","title":"AdapterSoup: Weight Averaging to Improve Generalization of Pretrained\n  Language Models","summary":"  Pretrained language models (PLMs) are trained on massive corpora, but often\nneed to specialize to specific domains. A parameter-efficient adaptation method\nsuggests training an adapter for each domain on the task of language modeling.\nThis leads to good in-domain scores but can be impractical for domain- or\nresource-restricted settings. A solution is to use a related-domain adapter for\nthe novel domain at test time. In this paper, we introduce AdapterSoup, an\napproach that performs weight-space averaging of adapters trained on different\ndomains. Our approach is embarrassingly parallel: first, we train a set of\ndomain-specific adapters; then, for each novel domain, we determine which\nadapters should be averaged at test time. We present extensive experiments\nshowing that AdapterSoup consistently improves performance to new domains\nwithout extra training. We also explore weight averaging of adapters trained on\nthe same domain with different hyper-parameters, and show that it preserves the\nperformance of a PLM on new domains while obtaining strong in-domain results.\nWe explore various approaches for choosing which adapters to combine, such as\ntext clustering and semantic similarity. We find that using clustering leads to\nthe most competitive results on novel domains.\n","authors":["Alexandra Chronopoulou","Matthew E. Peters","Alexander Fraser","Jesse Dodge"],"pdf_url":"https://arxiv.org/pdf/2302.07027v2.pdf","comment":"Accepted at EACL 2023; camera-ready version"},{"id":"http://arxiv.org/abs/2303.07240v1","updated":"2023-03-13T16:13:16Z","published":"2023-03-13T16:13:16Z","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical\n  Documents","summary":"  Foundation models trained on large-scale dataset gain a recent surge in CV\nand NLP. In contrast, development in biomedical domain lags far behind due to\ndata scarcity. To address this issue, we build and release PMC-OA, a biomedical\ndataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess\nsubset, which is 8 times larger than before. PMC-OA covers diverse modalities\nor diseases, with majority of the image-caption samples aligned at\nfiner-grained level, i.e., subfigure and subcaption. While pretraining a\nCLIP-style model on PMC-OA, our model named PMC-CLIP achieves state-of-the-art\nresults on various downstream tasks, including image-text retrieval on ROCO,\nMedMNIST image classification, Medical VQA, i.e. +8.1% R@10 on image-text\nretrieval, +3.9% accuracy on image classification.\n","authors":["Weixiong Lin","Ziheng Zhao","Xiaoman Zhang","Chaoyi Wu","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2303.07240v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2210.10488v4","updated":"2023-03-13T16:05:11Z","published":"2022-10-19T11:53:13Z","title":"Attribution and Obfuscation of Neural Text Authorship: A Data Mining\n  Perspective","summary":"  Two interlocking research questions of growing interest and importance in\nprivacy research are Authorship Attribution (AA) and Authorship Obfuscation\n(AO). Given an artifact, especially a text t in question, an AA solution aims\nto accurately attribute t to its true author out of many candidate authors\nwhile an AO solution aims to modify t to hide its true authorship.\nTraditionally, the notion of authorship and its accompanying privacy concern is\nonly toward human authors. However, in recent years, due to the explosive\nadvancements in Neural Text Generation (NTG) techniques in NLP, capable of\nsynthesizing human-quality open-ended texts (so-called \"neural texts\"), one has\nto now consider authorships by humans, machines, or their combination. Due to\nthe implications and potential threats of neural texts when used maliciously,\nit has become critical to understand the limitations of traditional AA/AO\nsolutions and develop novel AA/AO solutions in dealing with neural texts. In\nthis survey, therefore, we make a comprehensive review of recent literature on\nthe attribution and obfuscation of neural text authorship from a Data Mining\nperspective, and share our view on their limitations and promising research\ndirections.\n","authors":["Adaku Uchendu","Thai Le","Dongwon Lee"],"pdf_url":"https://arxiv.org/pdf/2210.10488v4.pdf","comment":"Accepted at ACM SIGKDD Explorations, Vol. 25, June 2023"},{"id":"http://arxiv.org/abs/2303.07226v1","updated":"2023-03-13T16:00:31Z","published":"2023-03-13T16:00:31Z","title":"Scaling Vision-Language Models with Sparse Mixture of Experts","summary":"  The field of natural language processing (NLP) has made significant strides\nin recent years, particularly in the development of large-scale vision-language\nmodels (VLMs). These models aim to bridge the gap between text and visual\ninformation, enabling a more comprehensive understanding of multimedia data.\nHowever, as these models become larger and more complex, they also become more\nchallenging to train and deploy. One approach to addressing this challenge is\nthe use of sparsely-gated mixture-of-experts (MoE) techniques, which divide the\nmodel into smaller, specialized sub-models that can jointly solve a task. In\nthis paper, we explore the effectiveness of MoE in scaling vision-language\nmodels, demonstrating its potential to achieve state-of-the-art performance on\na range of benchmarks over dense models of equivalent computational cost. Our\nresearch offers valuable insights into stabilizing the training of MoE models,\nunderstanding the impact of MoE on model interpretability, and balancing the\ntrade-offs between compute performance when scaling VLMs. We hope our work will\ninspire further research into the use of MoE for scaling large-scale\nvision-language models and other multimodal machine learning applications.\n","authors":["Sheng Shen","Zhewei Yao","Chunyuan Li","Trevor Darrell","Kurt Keutzer","Yuxiong He"],"pdf_url":"https://arxiv.org/pdf/2303.07226v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2211.05100v3","updated":"2023-03-13T15:55:30Z","published":"2022-11-09T18:48:09Z","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model","summary":"  Large language models (LLMs) have been shown to be able to perform new tasks\nbased on a few demonstrations or natural language instructions. While these\ncapabilities have led to widespread adoption, most LLMs are developed by\nresource-rich organizations and are frequently kept from the public. As a step\ntowards democratizing this powerful technology, we present BLOOM, a\n176B-parameter open-access language model designed and built thanks to a\ncollaboration of hundreds of researchers. BLOOM is a decoder-only Transformer\nlanguage model that was trained on the ROOTS corpus, a dataset comprising\nhundreds of sources in 46 natural and 13 programming languages (59 in total).\nWe find that BLOOM achieves competitive performance on a wide variety of\nbenchmarks, with stronger results after undergoing multitask prompted\nfinetuning. To facilitate future research and applications using LLMs, we\npublicly release our models and code under the Responsible AI License.\n","authors":["BigScience Workshop"," :","Teven Le Scao","Angela Fan","Christopher Akiki","Ellie Pavlick","Suzana Ilić","Daniel Hesslow","Roman Castagné","Alexandra Sasha Luccioni","François Yvon","Matthias Gallé","Jonathan Tow","Alexander M. Rush","Stella Biderman","Albert Webson","Pawan Sasanka Ammanamanchi","Thomas Wang","Benoît Sagot","Niklas Muennighoff","Albert Villanova del Moral","Olatunji Ruwase","Rachel Bawden","Stas Bekman","Angelina McMillan-Major","Iz Beltagy","Huu Nguyen","Lucile Saulnier","Samson Tan","Pedro Ortiz Suarez","Victor Sanh","Hugo Laurençon","Yacine Jernite","Julien Launay","Margaret Mitchell","Colin Raffel","Aaron Gokaslan","Adi Simhi","Aitor Soroa","Alham Fikri Aji","Amit Alfassy","Anna Rogers","Ariel Kreisberg Nitzav","Canwen Xu","Chenghao Mou","Chris Emezue","Christopher Klamm","Colin Leong","Daniel van Strien","David Ifeoluwa Adelani","Dragomir Radev","Eduardo González Ponferrada","Efrat Levkovizh","Ethan Kim","Eyal Bar Natan","Francesco De Toni","Gérard Dupont","Germán Kruszewski","Giada Pistilli","Hady Elsahar","Hamza Benyamina","Hieu Tran","Ian Yu","Idris Abdulmumin","Isaac Johnson","Itziar Gonzalez-Dios","Javier de la Rosa","Jenny Chim","Jesse Dodge","Jian Zhu","Jonathan Chang","Jörg Frohberg","Joseph Tobing","Joydeep Bhattacharjee","Khalid Almubarak","Kimbo Chen","Kyle Lo","Leandro Von Werra","Leon Weber","Long Phan","Loubna Ben allal","Ludovic Tanguy","Manan Dey","Manuel Romero Muñoz","Maraim Masoud","María Grandury","Mario Šaško","Max Huang","Maximin Coavoux","Mayank Singh","Mike Tian-Jian Jiang","Minh Chien Vu","Mohammad A. Jauhar","Mustafa Ghaleb","Nishant Subramani","Nora Kassner","Nurulaqilla Khamis","Olivier Nguyen","Omar Espejel","Ona de Gibert","Paulo Villegas","Peter Henderson","Pierre Colombo","Priscilla Amuok","Quentin Lhoest","Rheza Harliman","Rishi Bommasani","Roberto Luis López","Rui Ribeiro","Salomey Osei","Sampo Pyysalo","Sebastian Nagel","Shamik Bose","Shamsuddeen Hassan Muhammad","Shanya Sharma","Shayne Longpre","Somaieh Nikpoor","Stanislav Silberberg","Suhas Pai","Sydney Zink","Tiago Timponi Torrent","Timo Schick","Tristan Thrush","Valentin Danchev","Vassilina Nikoulina","Veronika Laippala","Violette Lepercq","Vrinda Prabhu","Zaid Alyafeai","Zeerak Talat","Arun Raja","Benjamin Heinzerling","Chenglei Si","Davut Emre Taşar","Elizabeth Salesky","Sabrina J. Mielke","Wilson Y. Lee","Abheesht Sharma","Andrea Santilli","Antoine Chaffin","Arnaud Stiegler","Debajyoti Datta","Eliza Szczechla","Gunjan Chhablani","Han Wang","Harshit Pandey","Hendrik Strobelt","Jason Alan Fries","Jos Rozen","Leo Gao","Lintang Sutawika","M Saiful Bari","Maged S. Al-shaibani","Matteo Manica","Nihal Nayak","Ryan Teehan","Samuel Albanie","Sheng Shen","Srulik Ben-David","Stephen H. Bach","Taewoon Kim","Tali Bers","Thibault Fevry","Trishala Neeraj","Urmish Thakker","Vikas Raunak","Xiangru Tang","Zheng-Xin Yong","Zhiqing Sun","Shaked Brody","Yallow Uri","Hadar Tojarieh","Adam Roberts","Hyung Won Chung","Jaesung Tae","Jason Phang","Ofir Press","Conglong Li","Deepak Narayanan","Hatim Bourfoune","Jared Casper","Jeff Rasley","Max Ryabinin","Mayank Mishra","Minjia Zhang","Mohammad Shoeybi","Myriam Peyrounette","Nicolas Patry","Nouamane Tazi","Omar Sanseviero","Patrick von Platen","Pierre Cornette","Pierre François Lavallée","Rémi Lacroix","Samyam Rajbhandari","Sanchit Gandhi","Shaden Smith","Stéphane Requena","Suraj Patil","Tim Dettmers","Ahmed Baruwa","Amanpreet Singh","Anastasia Cheveleva","Anne-Laure Ligozat","Arjun Subramonian","Aurélie Névéol","Charles Lovering","Dan Garrette","Deepak Tunuguntla","Ehud Reiter","Ekaterina Taktasheva","Ekaterina Voloshina","Eli Bogdanov","Genta Indra Winata","Hailey Schoelkopf","Jan-Christoph Kalo","Jekaterina Novikova","Jessica Zosa Forde","Jordan Clive","Jungo Kasai","Ken Kawamura","Liam Hazan","Marine Carpuat","Miruna Clinciu","Najoung Kim","Newton Cheng","Oleg Serikov","Omer Antverg","Oskar van der Wal","Rui Zhang","Ruochen Zhang","Sebastian Gehrmann","Shachar Mirkin","Shani Pais","Tatiana Shavrina","Thomas Scialom","Tian Yun","Tomasz Limisiewicz","Verena Rieser","Vitaly Protasov","Vladislav Mikhailov","Yada Pruksachatkun","Yonatan Belinkov","Zachary Bamberger","Zdeněk Kasner","Alice Rueda","Amanda Pestana","Amir Feizpour","Ammar Khan","Amy Faranak","Ana Santos","Anthony Hevia","Antigona Unldreaj","Arash Aghagol","Arezoo Abdollahi","Aycha Tammour","Azadeh HajiHosseini","Bahareh Behroozi","Benjamin Ajibade","Bharat Saxena","Carlos Muñoz Ferrandis","Danish Contractor","David Lansky","Davis David","Douwe Kiela","Duong A. Nguyen","Edward Tan","Emi Baylor","Ezinwanne Ozoani","Fatima Mirza","Frankline Ononiwu","Habib Rezanejad","Hessie Jones","Indrani Bhattacharya","Irene Solaiman","Irina Sedenko","Isar Nejadgholi","Jesse Passmore","Josh Seltzer","Julio Bonis Sanz","Livia Dutra","Mairon Samagaio","Maraim Elbadri","Margot Mieskes","Marissa Gerchick","Martha Akinlolu","Michael McKenna","Mike Qiu","Muhammed Ghauri","Mykola Burynok","Nafis Abrar","Nazneen Rajani","Nour Elkott","Nour Fahmy","Olanrewaju Samuel","Ran An","Rasmus Kromann","Ryan Hao","Samira Alizadeh","Sarmad Shubber","Silas Wang","Sourav Roy","Sylvain Viguier","Thanh Le","Tobi Oyebade","Trieu Le","Yoyo Yang","Zach Nguyen","Abhinav Ramesh Kashyap","Alfredo Palasciano","Alison Callahan","Anima Shukla","Antonio Miranda-Escalada","Ayush Singh","Benjamin Beilharz","Bo Wang","Caio Brito","Chenxi Zhou","Chirag Jain","Chuxin Xu","Clémentine Fourrier","Daniel León Periñán","Daniel Molano","Dian Yu","Enrique Manjavacas","Fabio Barth","Florian Fuhrimann","Gabriel Altay","Giyaseddin Bayrak","Gully Burns","Helena U. Vrabec","Imane Bello","Ishani Dash","Jihyun Kang","John Giorgi","Jonas Golde","Jose David Posada","Karthik Rangasai Sivaraman","Lokesh Bulchandani","Lu Liu","Luisa Shinzato","Madeleine Hahn de Bykhovetz","Maiko Takeuchi","Marc Pàmies","Maria A Castillo","Marianna Nezhurina","Mario Sänger","Matthias Samwald","Michael Cullan","Michael Weinberg","Michiel De Wolf","Mina Mihaljcic","Minna Liu","Moritz Freidank","Myungsun Kang","Natasha Seelam","Nathan Dahlberg","Nicholas Michio Broad","Nikolaus Muellner","Pascale Fung","Patrick Haller","Ramya Chandrasekhar","Renata Eisenberg","Robert Martin","Rodrigo Canalli","Rosaline Su","Ruisi Su","Samuel Cahyawijaya","Samuele Garda","Shlok S Deshmukh","Shubhanshu Mishra","Sid Kiblawi","Simon Ott","Sinee Sang-aroonsiri","Srishti Kumar","Stefan Schweter","Sushil Bharati","Tanmay Laud","Théo Gigant","Tomoya Kainuma","Wojciech Kusa","Yanis Labrak","Yash Shailesh Bajaj","Yash Venkatraman","Yifan Xu","Yingxin Xu","Yu Xu","Zhe Tan","Zhongli Xie","Zifan Ye","Mathilde Bras","Younes Belkada","Thomas Wolf"],"pdf_url":"https://arxiv.org/pdf/2211.05100v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07196v1","updated":"2023-03-13T15:34:19Z","published":"2023-03-13T15:34:19Z","title":"A Comprehensive Empirical Evaluation of Existing Word Embedding\n  Approaches","summary":"  Vector-based word representations help countless Natural Language Processing\n(NLP) tasks capture both semantic and syntactic regularities of the language.\nIn this paper, we present the characteristics of existing word embedding\napproaches and analyze them with regards to many classification tasks. We\ncategorize the methods into two main groups - Traditional approaches mostly use\nmatrix factorization to produce word representations, and they are not able to\ncapture the semantic and syntactic regularities of the language very well.\nNeural-Network based approaches, on the other hand, can capture sophisticated\nregularities of the language and preserve the word relationships in the\ngenerated word representations. We report experimental results on multiple\nclassification tasks and highlight the scenarios where one approach performs\nbetter than the rest.\n","authors":["Obaidullah Zaland","Muhammad Abulaish","Mohd. Fazil"],"pdf_url":"https://arxiv.org/pdf/2303.07196v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2211.16270v2","updated":"2023-03-13T14:18:42Z","published":"2022-11-29T14:57:23Z","title":"Neural Transducer Training: Reduced Memory Consumption with Sample-wise\n  Computation","summary":"  The neural transducer is an end-to-end model for automatic speech recognition\n(ASR). While the model is well-suited for streaming ASR, the training process\nremains challenging. During training, the memory requirements may quickly\nexceed the capacity of state-of-the-art GPUs, limiting batch size and sequence\nlengths. In this work, we analyze the time and space complexity of a typical\ntransducer training setup. We propose a memory-efficient training method that\ncomputes the transducer loss and gradients sample by sample. We present\noptimizations to increase the efficiency and parallelism of the sample-wise\nmethod. In a set of thorough benchmarks, we show that our sample-wise method\nsignificantly reduces memory usage, and performs at competitive speed when\ncompared to the default batched computation. As a highlight, we manage to\ncompute the transducer loss and gradients for a batch size of 1024, and audio\nlength of 40 seconds, using only 6 GB of memory.\n","authors":["Stefan Braun","Erik McDermott","Roger Hsiao"],"pdf_url":"https://arxiv.org/pdf/2211.16270v2.pdf","comment":"5 pages, 4 figures, 1 table, 1 algorithm"},{"id":"http://arxiv.org/abs/2303.07146v1","updated":"2023-03-13T14:16:59Z","published":"2023-03-13T14:16:59Z","title":"NeuroQL: A Neuro-Symbolic Language and Dataset for Inter-Subjective\n  Reasoning","summary":"  We present a new AI task and baseline solution for Inter-Subjective\nReasoning. We define inter-subjective information, to be a mixture of objective\nand subjective information possibly shared by different parties. Examples may\ninclude commodities and their objective properties as reported by IR\n(Information Retrieval) systems, that need to be cross-referenced with\nsubjective user reviews from an online forum. For an AI system to successfully\nreason about both, it needs to be able to combine symbolic reasoning of\nobjective facts with the shared consensus found on subjective user reviews. To\nthis end we introduce the NeuroQL dataset and DSL (Domain-specific Language) as\na baseline solution for this problem. NeuroQL is a neuro-symbolic language that\nextends logical unification with neural primitives for extraction and\nretrieval. It can function as a target for automatic translation of\ninter-subjective questions (posed in natural language) into the neuro-symbolic\ncode that can answer them.\n","authors":["Nick Papoulias"],"pdf_url":"https://arxiv.org/pdf/2303.07146v1.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.07142v1","updated":"2023-03-13T14:09:53Z","published":"2023-03-13T14:09:53Z","title":"Large Language Models in the Workplace: A Case Study on Prompt\n  Engineering for Job Type Classification","summary":"  This case study investigates the task of job classification in a real-world\nsetting, where the goal is to determine whether an English-language job posting\nis appropriate for a graduate or entry-level position. We explore multiple\napproaches to text classification, including supervised approaches such as\ntraditional models like Support Vector Machines (SVMs) and state-of-the-art\ndeep learning methods such as DeBERTa. We compare them with Large Language\nModels (LLMs) used in both few-shot and zero-shot classification settings. To\naccomplish this task, we employ prompt engineering, a technique that involves\ndesigning prompts to guide the LLMs towards the desired output. Specifically,\nwe evaluate the performance of two commercially available state-of-the-art\nGPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also\nconduct a detailed analysis of the impact of different aspects of prompt\nengineering on the model's performance. Our results show that, with a\nwell-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms all\nother models, achieving a 6% increase in Precision@95% Recall compared to the\nbest supervised approach. Furthermore, we observe that the wording of the\nprompt is a critical factor in eliciting the appropriate \"reasoning\" in the\nmodel, and that seemingly minor aspects of the prompt significantly affect the\nmodel's performance.\n","authors":["Benjamin Clavié","Alexandru Ciceu","Frederick Naylor","Guillaume Soulié","Thomas Brightwell"],"pdf_url":"https://arxiv.org/pdf/2303.07142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.08071v3","updated":"2023-03-13T13:55:30Z","published":"2022-01-20T09:10:20Z","title":"Temporal Sentence Grounding in Videos: A Survey and Future Directions","summary":"  Temporal sentence grounding in videos (TSGV), \\aka natural language video\nlocalization (NLVL) or video moment retrieval (VMR), aims to retrieve a\ntemporal moment that semantically corresponds to a language query from an\nuntrimmed video. Connecting computer vision and natural language, TSGV has\ndrawn significant attention from researchers in both communities. This survey\nattempts to provide a summary of fundamental concepts in TSGV and current\nresearch status, as well as future research directions. As the background, we\npresent a common structure of functional components in TSGV, in a tutorial\nstyle: from feature extraction from raw video and language query, to answer\nprediction of the target moment. Then we review the techniques for multimodal\nunderstanding and interaction, which is the key focus of TSGV for effective\nalignment between the two modalities. We construct a taxonomy of TSGV\ntechniques and elaborate the methods in different categories with their\nstrengths and weaknesses. Lastly, we discuss issues with the current TSGV\nresearch and share our insights about promising research directions.\n","authors":["Hao Zhang","Aixin Sun","Wei Jing","Joey Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2201.08071v3.pdf","comment":"Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)"},{"id":"http://arxiv.org/abs/2211.05103v2","updated":"2023-03-13T13:35:10Z","published":"2022-11-09T18:53:59Z","title":"Accidental Learners: Spoken Language Identification in Multilingual\n  Self-Supervised Models","summary":"  In this paper, we extend previous self-supervised approaches for language\nidentification by experimenting with Conformer based architecture in a\nmultilingual pre-training paradigm. We find that pre-trained speech models\noptimally encode language discriminatory information in lower layers. Further,\nwe demonstrate that the embeddings obtained from these layers are significantly\nrobust to classify unseen languages and different acoustic environments without\nadditional training. After fine-tuning a pre-trained Conformer model on the\nVoxLingua107 dataset, we achieve results similar to current state-of-the-art\nsystems for language identification. More, our model accomplishes this with 5x\nless parameters. We open-source the model through the NVIDIA NeMo toolkit.\n","authors":["Travis M. Bartley","Fei Jia","Krishna C. Puvvada","Samuel Kriman","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2211.05103v2.pdf","comment":"Submitted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2210.04029v2","updated":"2023-03-13T13:18:44Z","published":"2022-10-08T14:09:58Z","title":"EDU-level Extractive Summarization with Varying Summary Lengths","summary":"  Extractive models usually formulate text summarization as extracting fixed\ntop-$k$ salient sentences from the document as a summary. Few works exploited\nextracting finer-grained Elementary Discourse Unit (EDU) with little analysis\nand justification for the extractive unit selection. Further, the selection\nstrategy of the fixed top-$k$ salient sentences fits the summarization need\npoorly, as the number of salient sentences in different documents varies and\ntherefore a common or best $k$ does not exist in reality. To fill these gaps,\nthis paper first conducts the comparison analysis of oracle summaries based on\nEDUs and sentences, which provides evidence from both theoretical and\nexperimental perspectives to justify and quantify that EDUs make summaries with\nhigher automatic evaluation scores than sentences. Then, considering this merit\nof EDUs, this paper further proposes an EDU-level extractive model with Varying\nsummary Lengths and develops the corresponding learning algorithm. EDU-VL\nlearns to encode and predict probabilities of EDUs in the document, generate\nmultiple candidate summaries with varying lengths based on various $k$ values,\nand encode and score candidate summaries, in an end-to-end training manner.\nFinally, EDU-VL is experimented on single and multi-document benchmark datasets\nand shows improved performances on ROUGE scores in comparison with\nstate-of-the-art extractive models, and further human evaluation suggests that\nEDU-constituent summaries maintain good grammaticality and readability.\n","authors":["Yuping Wu","Ching-Hsun Tseng","Jiayu Shang","Shengzhong Mao","Goran Nenadic","Xiao-Jun Zeng"],"pdf_url":"https://arxiv.org/pdf/2210.04029v2.pdf","comment":"Accepted to EACL 2023 Findings"},{"id":"http://arxiv.org/abs/2303.07069v1","updated":"2023-03-13T12:45:01Z","published":"2023-03-13T12:45:01Z","title":"Generating multiple-choice questions for medical question answering with\n  distractors and cue-masking","summary":"  Medical multiple-choice question answering (MCQA) is particularly difficult.\nQuestions may describe patient symptoms and ask for the correct diagnosis,\nwhich requires domain knowledge and complex reasoning. Standard language\nmodeling pretraining alone is not sufficient to achieve the best results.\n\\citet{jin2020disease} showed that focusing masked language modeling on disease\nname prediction when using medical encyclopedic paragraphs as input leads to\nconsiderable MCQA accuracy improvement. In this work, we show that (1)\nfine-tuning on generated MCQA dataset outperforms the masked language modeling\nbased objective and (2) correctly masking the cues to the answers is critical\nfor good performance. We release new pretraining datasets and achieve\nstate-of-the-art results on 4 MCQA datasets, notably +5.7\\% with base-size\nmodel on MedQA-USMLE.\n","authors":["Damien Sileo","Kanimozhi Uma","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2303.07069v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.00258v2","updated":"2023-03-13T12:40:23Z","published":"2022-04-30T13:03:53Z","title":"EasyNLP: A Comprehensive and Easy-to-use Toolkit for Natural Language\n  Processing","summary":"  The success of Pre-Trained Models (PTMs) has reshaped the development of\nNatural Language Processing (NLP). Yet, it is not easy to obtain\nhigh-performing models and deploy them online for industrial practitioners. To\nbridge this gap, EasyNLP is designed to make it easy to build NLP applications,\nwhich supports a comprehensive suite of NLP algorithms. It further features\nknowledge-enhanced pre-training, knowledge distillation and few-shot learning\nfunctionalities for large-scale PTMs, and provides a unified framework of model\ntraining, inference and deployment for real-world applications. Currently,\nEasyNLP has powered over ten business units within Alibaba Group and is\nseamlessly integrated to the Platform of AI (PAI) products on Alibaba Cloud.\nThe source code of our EasyNLP toolkit is released at GitHub\n(https://github.com/alibaba/EasyNLP).\n","authors":["Chengyu Wang","Minghui Qiu","Chen Shi","Taolin Zhang","Tingting Liu","Lei Li","Jianing Wang","Ming Wang","Jun Huang","Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2205.00258v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2303.00969v2","updated":"2023-03-13T12:14:30Z","published":"2023-03-02T05:06:44Z","title":"Rethinking the Reasonability of the Test Set for Simultaneous Machine\n  Translation","summary":"  Simultaneous machine translation (SimulMT) models start translation before\nthe end of the source sentence, making the translation monotonically aligned\nwith the source sentence. However, the general full-sentence translation test\nset is acquired by offline translation of the entire source sentence, which is\nnot designed for SimulMT evaluation, making us rethink whether this will\nunderestimate the performance of SimulMT models. In this paper, we manually\nannotate a monotonic test set based on the MuST-C English-Chinese test set,\ndenoted as SiMuST-C. Our human evaluation confirms the acceptability of our\nannotated test set. Evaluations on three different SimulMT models verify that\nthe underestimation problem can be alleviated on our test set. Further\nexperiments show that finetuning on an automatically extracted monotonic\ntraining set improves SimulMT models by up to 3 BLEU points.\n","authors":["Mengge Liu","Wen Zhang","Xiang Li","Jian Luan","Bin Wang","Yuhang Guo","Shuoying Chen"],"pdf_url":"https://arxiv.org/pdf/2303.00969v2.pdf","comment":"Accepted by 48th IEEE International Conference on Acoustics, Speech,\n  and Signal Processing (ICASSP 2023)"},{"id":"http://arxiv.org/abs/2303.07024v1","updated":"2023-03-13T11:41:28Z","published":"2023-03-13T11:41:28Z","title":"Addressing Biases in the Texts using an End-to-End Pipeline Approach","summary":"  The concept of fairness is gaining popularity in academia and industry.\nSocial media is especially vulnerable to media biases and toxic language and\ncomments. We propose a fair ML pipeline that takes a text as input and\ndetermines whether it contains biases and toxic content. Then, based on\npre-trained word embeddings, it suggests a set of new words by substituting the\nbi-ased words, the idea is to lessen the effects of those biases by replacing\nthem with alternative words. We compare our approach to existing fairness\nmodels to determine its effectiveness. The results show that our proposed\npipeline can de-tect, identify, and mitigate biases in social media data\n","authors":["Shaina Raza","Syed Raza Bashir"," Sneha","Urooj Qamar"],"pdf_url":"https://arxiv.org/pdf/2303.07024v1.pdf","comment":"Accepted in Bias @ ECIR 2023"},{"id":"http://arxiv.org/abs/2303.06944v1","updated":"2023-03-13T09:22:48Z","published":"2023-03-13T09:22:48Z","title":"A Human Subject Study of Named Entity Recognition (NER) in\n  Conversational Music Recommendation Queries","summary":"  We conducted a human subject study of named entity recognition on a noisy\ncorpus of conversational music recommendation queries, with many irregular and\nnovel named entities. We evaluated the human NER linguistic behaviour in these\nchallenging conditions and compared it with the most common NER systems\nnowadays, fine-tuned transformers. Our goal was to learn about the task to\nguide the design of better evaluation methods and NER algorithms. The results\nshowed that NER in our context was quite hard for both human and algorithms\nunder a strict evaluation schema; humans had higher precision, while the model\nhigher recall because of entity exposure especially during pre-training; and\nentity types had different error patterns (e.g. frequent typing errors for\nartists). The released corpus goes beyond predefined frames of interaction and\ncan support future work in conversational music recommendation.\n","authors":["Elena V. Epure","Romain Hennequin"],"pdf_url":"https://arxiv.org/pdf/2303.06944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04426v2","updated":"2023-03-13T08:43:27Z","published":"2023-03-08T08:08:57Z","title":"NASTyLinker: NIL-Aware Scalable Transformer-based Entity Linker","summary":"  Entity Linking (EL) is the task of detecting mentions of entities in text and\ndisambiguating them to a reference knowledge base. Most prevalent EL approaches\nassume that the reference knowledge base is complete. In practice, however, it\nis necessary to deal with the case of linking to an entity that is not\ncontained in the knowledge base (NIL entity). Recent works have shown that,\ninstead of focusing only on affinities between mentions and entities,\nconsidering inter-mention affinities can be used to represent NIL entities by\nproducing clusters of mentions. At the same time, inter-mention affinities can\nhelp to substantially improve linking performance for known entities. With\nNASTyLinker, we introduce an EL approach that is aware of NIL entities and\nproduces corresponding mention clusters while maintaining high linking\nperformance for known entities. The approach clusters mentions and entities\nbased on dense representations from Transformers and resolves conflicts (if\nmore than one entity is assigned to a cluster) by computing transitive\nmention-entity affinities. We show the effectiveness and scalability of\nNASTyLinker on NILK, a dataset that is explicitly constructed to evaluate EL\nwith respect to NIL entities. Further, we apply the presented approach to an\nactual EL task, namely to knowledge graph population by linking entities in\nWikipedia listings, and provide an analysis of the outcome.\n","authors":["Nicolas Heist","Heiko Paulheim"],"pdf_url":"https://arxiv.org/pdf/2303.04426v2.pdf","comment":"Preprint of a paper in the research track of the 20th Extended\n  Semantic Web Conference (ESWC'23)"},{"id":"http://arxiv.org/abs/2303.06904v1","updated":"2023-03-13T07:46:41Z","published":"2023-03-13T07:46:41Z","title":"Contextually-rich human affect perception using multimodal scene\n  information","summary":"  The process of human affect understanding involves the ability to infer\nperson specific emotional states from various sources including images, speech,\nand language. Affect perception from images has predominantly focused on\nexpressions extracted from salient face crops. However, emotions perceived by\nhumans rely on multiple contextual cues including social settings, foreground\ninteractions, and ambient visual scenes. In this work, we leverage pretrained\nvision-language (VLN) models to extract descriptions of foreground context from\nimages. Further, we propose a multimodal context fusion (MCF) module to combine\nforeground cues with the visual scene and person-based contextual information\nfor emotion prediction. We show the effectiveness of our proposed modular\ndesign on two datasets associated with natural scenes and TV shows.\n","authors":["Digbalay Bose","Rajat Hebbar","Krishna Somandepalli","Shrikanth Narayanan"],"pdf_url":"https://arxiv.org/pdf/2303.06904v1.pdf","comment":"Accepted to IEEE International Conference on Acoustics, Speech and\n  Signal Processing (ICASSP), 2023"},{"id":"http://arxiv.org/abs/2301.13294v2","updated":"2023-03-13T06:43:18Z","published":"2023-01-30T21:17:15Z","title":"Adaptive Machine Translation with Large Language Models","summary":"  Consistency is a key requirement of high-quality translation. It is\nespecially important to adhere to pre-approved terminology and adapt to\ncorrected translations in domain-specific projects. Machine translation (MT)\nhas achieved significant progress in the area of domain adaptation. However,\nreal-time adaptation remains challenging. Large-scale language models (LLMs)\nhave recently shown interesting capabilities of in-context learning, where they\nlearn to replicate certain input-output text generation patterns, without\nfurther fine-tuning. By feeding an LLM at inference time with a prompt that\nconsists of a list of translation pairs, it can then simulate the domain and\nstyle characteristics. This work aims to investigate how we can utilize\nin-context learning to improve real-time adaptive MT. Our extensive experiments\nshow promising results at translation time. For example, GPT-3.5 can adapt to a\nset of in-domain sentence pairs and/or terminology while translating a new\nsentence. We observe that the translation quality with few-shot in-context\nlearning can surpass that of strong encoder-decoder MT systems, especially for\nhigh-resource languages. Moreover, we investigate whether we can combine MT\nfrom strong encoder-decoder models with fuzzy matches, which can further\nimprove translation quality, especially for less supported languages. We\nconduct our experiments across five diverse language pairs, namely\nEnglish-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French\n(EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).\n","authors":["Yasmin Moslem","Rejwanul Haque","John D. Kelleher","Andy Way"],"pdf_url":"https://arxiv.org/pdf/2301.13294v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06878v1","updated":"2023-03-13T05:53:42Z","published":"2023-03-13T05:53:42Z","title":"The System Description of dun_oscar team for The ICPR MSR Challenge","summary":"  This paper introduces the system submitted by dun_oscar team for the ICPR MSR\nChallenge. Three subsystems for task1-task3 are descripted respectively. In\ntask1, we develop a visual system which includes a OCR model, a text tracker,\nand a NLP classifier for distinguishing subtitles and non-subtitles. In task2,\nwe employ an ASR system which includes an AM with 18 layers and a 4-gram LM.\nSemi-supervised learning on unlabeled data is also vital. In task3, we employ\nthe ASR system to improve the visual system, some false subtitles can be\ncorrected by a fusion module.\n","authors":["Binbin Du","Rui Deng","Yingxin Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.06878v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.06574v3","updated":"2023-03-13T05:51:27Z","published":"2022-02-14T09:36:50Z","title":"I-Tuning: Tuning Frozen Language Models with Image for Lightweight Image\n  Captioning","summary":"  Image Captioning is a traditional vision-and-language task that aims to\ngenerate the language description of an image. Recent studies focus on scaling\nup the model size and the number of training data, which significantly increase\nthe cost of model training. Different to these heavy-cost models, we introduce\na lightweight image captioning framework (I-Tuning), which contains a small\nnumber of trainable parameters. We design a novel I-Tuning cross-attention\nmodule to connect the non-trainable pre-trained language decoder GPT2 and\nvision encoder CLIP-ViT. Since most parameters are not required to be updated\nduring training, our framework is lightweight and fast. Experimental results\nconducted on three image captioning benchmarks reveal that our framework\nachieves comparable or better performance than the large-scale baseline\nsystems. But our models contain up to 10 times fewer trainable parameters and\nrequire much fewer data for training compared with state-of-the-art baselines.\n","authors":["Ziyang Luo","Zhipeng Hu","Yadong Xi","Rongsheng Zhang","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2202.06574v3.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.06854v1","updated":"2023-03-13T04:49:46Z","published":"2023-03-13T04:49:46Z","title":"Robust Contrastive Language-Image Pretraining against Adversarial\n  Attacks","summary":"  Contrastive vision-language representation learning has achieved\nstate-of-the-art performance for zero-shot classification, by learning from\nmillions of image-caption pairs crawled from the internet. However, the massive\ndata that powers large multimodal models such as CLIP, makes them extremely\nvulnerable to various types of adversarial attacks, including targeted and\nbackdoor data poisoning attacks. Despite this vulnerability, robust contrastive\nvision-language pretraining against adversarial attacks has remained\nunaddressed. In this work, we propose RoCLIP, the first effective method for\nrobust pretraining {and fine-tuning} multimodal vision-language models. RoCLIP\neffectively breaks the association between poisoned image-caption pairs by\nconsidering a pool of random examples, and (1) matching every image with the\ntext that is most similar to its caption in the pool, and (2) matching every\ncaption with the image that is most similar to its image in the pool. Our\nextensive experiments show that our method renders state-of-the-art targeted\ndata poisoning and backdoor attacks ineffective during pre-training or\nfine-tuning of CLIP. In particular, RoCLIP decreases the poison and backdoor\nattack success rates down to 0\\% during pre-training and 1\\%-4\\% during\nfine-tuning, and effectively improves the model's performance.\n","authors":["Wenhan Yang","Baharan Mirzasoleiman"],"pdf_url":"https://arxiv.org/pdf/2303.06854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06841v1","updated":"2023-03-13T04:15:33Z","published":"2023-03-13T04:15:33Z","title":"Learning Transductions and Alignments with RNN Seq2seq Models","summary":"  The paper studies the capabilities of Recurrent-Neural-Network sequence to\nsequence (RNN seq2seq) models in learning four string-to-string transduction\ntasks: identity, reversal, total reduplication, and input-specified\nreduplication. These transductions are traditionally well studied under finite\nstate transducers and attributed with varying complexity. We find that RNN\nseq2seq models are only able to approximate a mapping that fits the training or\nin-distribution data. Attention helps significantly, but does not solve the\nout-of-distribution generalization limitation. Task complexity and RNN variants\nalso play a role in the results. Our results are best understood in terms of\nthe complexity hierarchy of formal languages as opposed to that of string\ntransductions.\n","authors":["Zhengxiang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.06841v1.pdf","comment":"24 pages; 9 figures; 7 tables"},{"id":"http://arxiv.org/abs/2207.01063v3","updated":"2023-03-13T02:13:38Z","published":"2022-07-03T15:07:41Z","title":"DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech","summary":"  The majority of current Text-to-Speech (TTS) datasets, which are collections\nof individual utterances, contain few conversational aspects. In this paper, we\nintroduce DailyTalk, a high-quality conversational speech dataset designed for\nconversational TTS. We sampled, modified, and recorded 2,541 dialogues from the\nopen-domain dialogue dataset DailyDialog inheriting its annotated attributes.\nOn top of our dataset, we extend prior work as our baseline, where a\nnon-autoregressive TTS is conditioned on historical information in a dialogue.\nFrom the baseline experiment with both general and our novel metrics, we show\nthat DailyTalk can be used as a general TTS dataset, and more than that, our\nbaseline can represent contextual information from DailyTalk. The DailyTalk\ndataset and baseline code are freely available for academic use with CC-BY-SA\n4.0 license.\n","authors":["Keon Lee","Kyumin Park","Daeyoung Kim"],"pdf_url":"https://arxiv.org/pdf/2207.01063v3.pdf","comment":"5 pages, 1 figures, 4 tables. Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2204.00175v2","updated":"2023-03-13T01:49:33Z","published":"2022-04-01T02:51:22Z","title":"Alternate Intermediate Conditioning with Syllable-level and\n  Character-level Targets for Japanese ASR","summary":"  End-to-end automatic speech recognition directly maps input speech to\ncharacters. However, the mapping can be problematic when several different\npronunciations should be mapped into one character or when one pronunciation is\nshared among many different characters. Japanese ASR suffers the most from such\nmany-to-one and one-to-many mapping problems due to Japanese kanji characters.\nTo alleviate the problems, we introduce explicit interaction between characters\nand syllables using Self-conditioned connectionist temporal classification\n(CTC), in which the upper layers are ``self-conditioned'' on the intermediate\npredictions from the lower layers. The proposed method utilizes character-level\nand syllable-level intermediate predictions as conditioning features to deal\nwith mutual dependency between characters and syllables. Experimental results\non Corpus of Spontaneous Japanese show that the proposed method outperformed\nthe conventional multi-task and Self-conditioned CTC methods.\n","authors":["Yusuke Fujita","Tatsuya Komatsu","Yusuke Kida"],"pdf_url":"https://arxiv.org/pdf/2204.00175v2.pdf","comment":"SLT 2022"},{"id":"http://arxiv.org/abs/2105.11115v3","updated":"2023-03-13T01:47:55Z","published":"2021-05-24T06:42:58Z","title":"Self-Attention Networks Can Process Bounded Hierarchical Languages","summary":"  Despite their impressive performance in NLP, self-attention networks were\nrecently proved to be limited for processing formal languages with hierarchical\nstructure, such as $\\mathsf{Dyck}_k$, the language consisting of well-nested\nparentheses of $k$ types. This suggested that natural language can be\napproximated well with models that are too weak for formal languages, or that\nthe role of hierarchy and recursion in natural language might be limited. We\nqualify this implication by proving that self-attention networks can process\n$\\mathsf{Dyck}_{k, D}$, the subset of $\\mathsf{Dyck}_{k}$ with depth bounded by\n$D$, which arguably better captures the bounded hierarchical structure of\nnatural language. Specifically, we construct a hard-attention network with\n$D+1$ layers and $O(\\log k)$ memory size (per token per layer) that recognizes\n$\\mathsf{Dyck}_{k, D}$, and a soft-attention network with two layers and\n$O(\\log k)$ memory size that generates $\\mathsf{Dyck}_{k, D}$. Experiments show\nthat self-attention networks trained on $\\mathsf{Dyck}_{k, D}$ generalize to\nlonger inputs with near-perfect accuracy, and also verify the theoretical\nmemory advantage of self-attention networks over recurrent networks.\n","authors":["Shunyu Yao","Binghui Peng","Christos Papadimitriou","Karthik Narasimhan"],"pdf_url":"https://arxiv.org/pdf/2105.11115v3.pdf","comment":"ACL 2021. 19 pages with extended appendix. Fixed a small typo in the\n  formula at the end of page 5 (thank to Gabriel Faria). Code:\n  https://github.com/princeton-nlp/dyck-transformer"},{"id":"http://arxiv.org/abs/2303.06806v1","updated":"2023-03-13T01:28:55Z","published":"2023-03-13T01:28:55Z","title":"Neural Diarization with Non-autoregressive Intermediate Attractors","summary":"  End-to-end neural diarization (EEND) with encoder-decoder-based attractors\n(EDA) is a promising method to handle the whole speaker diarization problem\nsimultaneously with a single neural network. While the EEND model can produce\nall frame-level speaker labels simultaneously, it disregards output label\ndependency. In this work, we propose a novel EEND model that introduces the\nlabel dependency between frames. The proposed method generates\nnon-autoregressive intermediate attractors to produce speaker labels at the\nlower layers and conditions the subsequent layers with these labels. While the\nproposed model works in a non-autoregressive manner, the speaker labels are\nrefined by referring to the whole sequence of intermediate labels. The\nexperiments with the two-speaker CALLHOME dataset show that the intermediate\nlabels with the proposed non-autoregressive intermediate attractors boost the\ndiarization performance. The proposed method with the deeper network benefits\nmore from the intermediate labels, resulting in better performance and training\nthroughput than EEND-EDA.\n","authors":["Yusuke Fujita","Tatsuya Komatsu","Robin Scheibler","Yusuke Kida","Tetsuji Ogawa"],"pdf_url":"https://arxiv.org/pdf/2303.06806v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.06791v1","updated":"2023-03-13T00:39:04Z","published":"2023-03-13T00:39:04Z","title":"Beyond Single Items: Exploring User Preferences in Item Sets with the\n  Conversational Playlist Curation Dataset","summary":"  Users in consumption domains, like music, are often able to more efficiently\nprovide preferences over a set of items (e.g. a playlist or radio) than over\nsingle items (e.g. songs). Unfortunately, this is an underexplored area of\nresearch, with most existing recommendation systems limited to understanding\npreferences over single items. Curating an item set exponentiates the search\nspace that recommender systems must consider (all subsets of items!): this\nmotivates conversational approaches-where users explicitly state or refine\ntheir preferences and systems elicit preferences in natural language-as an\nefficient way to understand user needs. We call this task conversational item\nset curation and present a novel data collection methodology that efficiently\ncollects realistic preferences about item sets in a conversational setting by\nobserving both item-level and set-level feedback. We apply this methodology to\nmusic recommendation to build the Conversational Playlist Curation Dataset\n(CPCD), where we show that it leads raters to express preferences that would\nnot be otherwise expressed. Finally, we propose a wide range of conversational\nretrieval models as baselines for this task and evaluate them on the dataset.\n","authors":["Arun Tejasvi Chaganty","Megan Leszczynski","Shu Zhang","Ravi Ganti","Krisztian Balog","Filip Radlinski"],"pdf_url":"https://arxiv.org/pdf/2303.06791v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.07347v1","updated":"2023-03-13T17:59:59Z","published":"2023-03-13T17:59:59Z","title":"TriDet: Temporal Action Detection with Relative Boundary Modeling","summary":"  In this paper, we present a one-stage framework TriDet for temporal action\ndetection. Existing methods often suffer from imprecise boundary predictions\ndue to the ambiguous action boundaries in videos. To alleviate this problem, we\npropose a novel Trident-head to model the action boundary via an estimated\nrelative probability distribution around the boundary. In the feature pyramid\nof TriDet, we propose an efficient Scalable-Granularity Perception (SGP) layer\nto mitigate the rank loss problem of self-attention that takes place in the\nvideo features and aggregate information across different temporal\ngranularities. Benefiting from the Trident-head and the SGP-based feature\npyramid, TriDet achieves state-of-the-art performance on three challenging\nbenchmarks: THUMOS14, HACS and EPIC-KITCHEN 100, with lower computational\ncosts, compared to previous methods. For example, TriDet hits an average mAP of\n$69.3\\%$ on THUMOS14, outperforming the previous best by $2.5\\%$, but with only\n$74.6\\%$ of its latency. The code is released to\nhttps://github.com/sssste/TriDet.\n","authors":["Dingfeng Shi","Yujie Zhong","Qiong Cao","Lin Ma","Jia Li","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.07347v1.pdf","comment":"CVPR2023; Temporal Action Detection; Temporal Action Localization"},{"id":"http://arxiv.org/abs/2303.07345v1","updated":"2023-03-13T17:59:55Z","published":"2023-03-13T17:59:55Z","title":"Erasing Concepts from Diffusion Models","summary":"  Motivated by recent advancements in text-to-image diffusion, we study erasure\nof specific concepts from the model's weights. While Stable Diffusion has shown\npromise in producing explicit or realistic artwork, it has raised concerns\nregarding its potential for misuse. We propose a fine-tuning method that can\nerase a visual concept from a pre-trained diffusion model, given only the name\nof the style and using negative guidance as a teacher. We benchmark our method\nagainst previous approaches that remove sexually explicit content and\ndemonstrate its effectiveness, performing on par with Safe Latent Diffusion and\ncensored training. To evaluate artistic style removal, we conduct experiments\nerasing five modern artists from the network and conduct a user study to assess\nthe human perception of the removed styles. Unlike previous methods, our\napproach can remove concepts from a diffusion model permanently rather than\nmodifying the output at the inference time, so it cannot be circumvented even\nif a user has access to model weights. Our code, data, and results are\navailable at https://erasing.baulab.info/\n","authors":["Rohit Gandikota","Joanna Materzynska","Jaden Fiotto-Kaufman","David Bau"],"pdf_url":"https://arxiv.org/pdf/2303.07345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07338v1","updated":"2023-03-13T17:59:02Z","published":"2023-03-13T17:59:02Z","title":"Revisiting Class-Incremental Learning with Pre-Trained Models:\n  Generalizability and Adaptivity are All You Need","summary":"  Class-incremental learning (CIL) aims to adapt to emerging new classes\nwithout forgetting old ones. Traditional CIL models are trained from scratch to\ncontinually acquire knowledge as data evolves. Recently, pre-training has\nachieved substantial progress, making vast pre-trained models (PTMs) accessible\nfor CIL. Contrary to traditional methods, PTMs possess generalizable\nembeddings, which can be easily transferred. In this work, we revisit CIL with\nPTMs and argue that the core factors in CIL are adaptivity for model updating\nand generalizability for knowledge transferring. 1) We first reveal that frozen\nPTM can already provide generalizable embeddings for CIL. Surprisingly, a\nsimple baseline (SimpleCIL) which continually sets the classifiers of PTM to\nprototype features can beat state-of-the-art even without training on the\ndownstream task. 2) Due to the distribution gap between pre-trained and\ndownstream datasets, PTM can be further cultivated with adaptivity via model\nadapting. We propose ADapt And Merge (ADAM), which aggregates the embeddings of\nPTM and adapted models for classifier construction. ADAM is a general framework\nthat can be orthogonally combined with any parameter-efficient tuning method,\nwhich holds the advantages of PTM's generalizability and adapted model's\nadaptivity. 3) Additionally, we find previous benchmarks are unsuitable in the\nera of PTM due to data overlapping and propose four new benchmarks for\nassessment, namely ImageNet-A, ObjectNet, OmniBenchmark, and VTAB. Extensive\nexperiments validate the effectiveness of ADAM with a unified and concise\nframework.\n","authors":["Da-Wei Zhou","Han-Jia Ye","De-Chuan Zhan","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2303.07338v1.pdf","comment":"Code is available at: https://github.com/zhoudw-zdw/RevisitingCIL"},{"id":"http://arxiv.org/abs/2303.07337v1","updated":"2023-03-13T17:58:54Z","published":"2023-03-13T17:58:54Z","title":"PoseExaminer: Automated Testing of Out-of-Distribution Robustness in\n  Human Pose and Shape Estimation","summary":"  Human pose and shape (HPS) estimation methods achieve remarkable results.\nHowever, current HPS benchmarks are mostly designed to test models in scenarios\nthat are similar to the training data. This can lead to critical situations in\nreal-world applications when the observed data differs significantly from the\ntraining data and hence is out-of-distribution (OOD). It is therefore important\nto test and improve the OOD robustness of HPS methods. To address this\nfundamental problem, we develop a simulator that can be controlled in a\nfine-grained manner using interpretable parameters to explore the manifold of\nimages of human pose, e.g. by varying poses, shapes, and clothes. We introduce\na learning-based testing method, termed PoseExaminer, that automatically\ndiagnoses HPS algorithms by searching over the parameter space of human pose\nimages to find the failure modes. Our strategy for exploring this\nhigh-dimensional parameter space is a multi-agent reinforcement learning\nsystem, in which the agents collaborate to explore different parts of the\nparameter space. We show that our PoseExaminer discovers a variety of\nlimitations in current state-of-the-art models that are relevant in real-world\nscenarios but are missed by current benchmarks. For example, it finds large\nregions of realistic human poses that are not predicted correctly, as well as\nreduced performance for humans with skinny and corpulent body shapes. In\naddition, we show that fine-tuning HPS methods by exploiting the failure modes\nfound by PoseExaminer improve their robustness and even their performance on\nstandard benchmarks by a significant margin. The code are available for\nresearch purposes.\n","authors":["Qihao Liu","Adam Kortylewski","Alan Yuille"],"pdf_url":"https://arxiv.org/pdf/2303.07337v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07335v1","updated":"2023-03-13T17:57:59Z","published":"2023-03-13T17:57:59Z","title":"Lite DETR : An Interleaved Multi-Scale Encoder for Efficient DETR","summary":"  Recent DEtection TRansformer-based (DETR) models have obtained remarkable\nperformance. Its success cannot be achieved without the re-introduction of\nmulti-scale feature fusion in the encoder. However, the excessively increased\ntokens in multi-scale features, especially for about 75\\% of low-level\nfeatures, are quite computationally inefficient, which hinders real\napplications of DETR models. In this paper, we present Lite DETR, a simple yet\nefficient end-to-end object detection framework that can effectively reduce the\nGFLOPs of the detection head by 60\\% while keeping 99\\% of the original\nperformance. Specifically, we design an efficient encoder block to update\nhigh-level features (corresponding to small-resolution feature maps) and\nlow-level features (corresponding to large-resolution feature maps) in an\ninterleaved way. In addition, to better fuse cross-scale features, we develop a\nkey-aware deformable attention to predict more reliable attention weights.\nComprehensive experiments validate the effectiveness and efficiency of the\nproposed Lite DETR, and the efficient encoder strategy can generalize well\nacross existing DETR-based models. The code will be available in\n\\url{https://github.com/IDEA-Research/Lite-DETR}.\n","authors":["Feng Li","Ailing Zeng","Shilong Liu","Hao Zhang","Hongyang Li","Lei Zhang","Lionel M. Ni"],"pdf_url":"https://arxiv.org/pdf/2303.07335v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07336v1","updated":"2023-03-13T17:57:59Z","published":"2023-03-13T17:57:59Z","title":"MP-Former: Mask-Piloted Transformer for Image Segmentation","summary":"  We present a mask-piloted Transformer which improves masked-attention in\nMask2Former for image segmentation. The improvement is based on our observation\nthat Mask2Former suffers from inconsistent mask predictions between consecutive\ndecoder layers, which leads to inconsistent optimization goals and low\nutilization of decoder queries. To address this problem, we propose a\nmask-piloted training approach, which additionally feeds noised ground-truth\nmasks in masked-attention and trains the model to reconstruct the original\nones. Compared with the predicted masks used in mask-attention, the\nground-truth masks serve as a pilot and effectively alleviate the negative\nimpact of inaccurate mask predictions in Mask2Former. Based on this technique,\nour \\M achieves a remarkable performance improvement on all three image\nsegmentation tasks (instance, panoptic, and semantic), yielding $+2.3$AP and\n$+1.6$mIoU on the Cityscapes instance and semantic segmentation tasks with a\nResNet-50 backbone. Our method also significantly speeds up the training,\noutperforming Mask2Former with half of the number of training epochs on ADE20K\nwith both a ResNet-50 and a Swin-L backbones. Moreover, our method only\nintroduces little computation during training and no extra computation during\ninference. Our code will be released at\n\\url{https://github.com/IDEA-Research/MP-Former}.\n","authors":["Hao Zhang","Feng Li","Huaizhe Xu","Shijia Huang","Shilong Liu","Lionel M. Ni","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07336v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07327v1","updated":"2023-03-13T17:45:39Z","published":"2023-03-13T17:45:39Z","title":"Unsupervised HDR Image and Video Tone Mapping via Contrastive Learning","summary":"  Capturing high dynamic range (HDR) images (videos) is attractive because it\ncan reveal the details in both dark and bright regions. Since the mainstream\nscreens only support low dynamic range (LDR) content, tone mapping algorithm is\nrequired to compress the dynamic range of HDR images (videos). Although image\ntone mapping has been widely explored, video tone mapping is lagging behind,\nespecially for the deep-learning-based methods, due to the lack of HDR-LDR\nvideo pairs. In this work, we propose a unified framework (IVTMNet) for\nunsupervised image and video tone mapping. To improve unsupervised training, we\npropose domain and instance based contrastive learning loss. Instead of using a\nuniversal feature extractor, such as VGG to extract the features for similarity\nmeasurement, we propose a novel latent code, which is an aggregation of the\nbrightness and contrast of extracted features, to measure the similarity of\ndifferent pairs. We totally construct two negative pairs and three positive\npairs to constrain the latent codes of tone mapped results. For video tone\nmapping, we propose a temporal-feature-replaced (TFR) module to efficiently\nutilize the temporal correlation and improve the temporal consistency of video\ntone-mapped results. We construct a large-scale unpaired HDR-LDR video dataset\nto facilitate the unsupervised training process for video tone mapping.\nExperimental results demonstrate that our method outperforms state-of-the-art\nimage and video tone mapping methods. Our code and dataset will be released\nafter the acceptance of this work.\n","authors":["Cong Cao","Huanjing Yue","Xin Liu","Jingyu Yang"],"pdf_url":"https://arxiv.org/pdf/2303.07327v1.pdf","comment":"12 pages,5 figures"},{"id":"http://arxiv.org/abs/2212.03241v2","updated":"2023-03-13T17:43:16Z","published":"2022-12-06T18:59:58Z","title":"PØDA: Prompt-driven Zero-shot Domain Adaptation","summary":"  Domain adaptation has been vastly investigated in computer vision but still\nrequires access to target images at train time, which might be intractable in\nsome uncommon conditions. In this paper, we propose the task of `Prompt-driven\nZero-shot Domain Adaptation', where we adapt a model trained on a source domain\nusing only a single general textual description of the target domain, i.e., a\nprompt. First, we leverage a pretrained contrastive vision-language model\n(CLIP) to optimize affine transformations of source features, steering them\ntowards target text embeddings, while preserving their content and semantics.\nSecond, we show that augmented features can be used to perform zero-shot domain\nadaptation for semantic segmentation. Experiments demonstrate that our method\nsignificantly outperforms CLIP-based style transfer baselines on several\ndatasets for the downstream task at hand. Our prompt-driven approach even\noutperforms one-shot unsupervised domain adaptation on some datasets, and gives\ncomparable results on others. Our code is available at\nhttps://github.com/astra-vision/PODA.\n","authors":["Mohammad Fahes","Tuan-Hung Vu","Andrei Bursuc","Patrick Pérez","Raoul de Charette"],"pdf_url":"https://arxiv.org/pdf/2212.03241v2.pdf","comment":"Project page: https://astra-vision.github.io/PODA/"},{"id":"http://arxiv.org/abs/2212.02623v3","updated":"2023-03-13T17:42:44Z","published":"2022-12-05T22:14:49Z","title":"Unifying Vision, Text, and Layout for Universal Document Processing","summary":"  We propose Universal Document Processing (UDOP), a foundation Document AI\nmodel which unifies text, image, and layout modalities together with varied\ntask formats, including document understanding and generation. UDOP leverages\nthe spatial correlation between textual content and document image to model\nimage, text, and layout modalities with one uniform representation. With a\nnovel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain\ndownstream tasks into a prompt-based sequence generation scheme. UDOP is\npretrained on both large-scale unlabeled document corpora using innovative\nself-supervised objectives and diverse labeled data. UDOP also learns to\ngenerate document images from text and layout modalities via masked image\nreconstruction. To the best of our knowledge, this is the first time in the\nfield of document AI that one model simultaneously achieves high-quality neural\ndocument editing and content customization. Our method sets the\nstate-of-the-art on 8 Document AI tasks, e.g., document understanding and QA,\nacross diverse data domains like finance reports, academic papers, and\nwebsites. UDOP ranks first on the leaderboard of the Document Understanding\nBenchmark.\n","authors":["Zineng Tang","Ziyi Yang","Guoxin Wang","Yuwei Fang","Yang Liu","Chenguang Zhu","Michael Zeng","Cha Zhang","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2212.02623v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07321v1","updated":"2023-03-13T17:42:11Z","published":"2023-03-13T17:42:11Z","title":"Collision Cross-entropy and EM Algorithm for Self-labeled Classification","summary":"  We propose \"collision cross-entropy\" as a robust alternative to the Shannon's\ncross-entropy in the context of self-labeled classification with posterior\nmodels. Assuming unlabeled data, self-labeling works by estimating latent\npseudo-labels, categorical distributions y, that optimize some discriminative\nclustering criteria, e.g. \"decisiveness\" and \"fairness\". All existing\nself-labeled losses incorporate Shannon's cross-entropy term targeting the\nmodel prediction, softmax, at the estimated distribution y. In fact, softmax is\ntrained to mimic the uncertainty in y exactly. Instead, we propose the negative\nlog-likelihood of \"collision\" to maximize the probability of equality between\ntwo random variables represented by distributions softmax and y. We show that\nour loss satisfies some properties of a generalized cross-entropy.\nInterestingly, it agrees with the Shannon's cross-entropy for one-hot\npseudo-labels y, but the training from softer labels weakens. For example, if y\nis a uniform distribution at some data point, it has zero contribution to the\ntraining. Our self-labeling loss combining collision cross entropy with basic\nclustering criteria is convex w.r.t. pseudo-labels, but non-trivial to optimize\nover the probability simplex. We derive a practical EM algorithm optimizing\npseudo-labels y significantly faster than generic methods, e.g. the projectile\ngradient descent. The collision cross-entropy consistently improves the results\non multiple self-labeled clustering examples using different DNNs.\n","authors":["Zhongwen Zhang","Yuri Boykov"],"pdf_url":"https://arxiv.org/pdf/2303.07321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07317v1","updated":"2023-03-13T17:38:58Z","published":"2023-03-13T17:38:58Z","title":"Nearest-Neighbor Inter-Intra Contrastive Learning from Unlabeled Videos","summary":"  Contrastive learning has recently narrowed the gap between self-supervised\nand supervised methods in image and video domain. State-of-the-art video\ncontrastive learning methods such as CVRL and $\\rho$-MoCo spatiotemporally\naugment two clips from the same video as positives. By only sampling positive\nclips locally from a single video, these methods neglect other semantically\nrelated videos that can also be useful. To address this limitation, we leverage\nnearest-neighbor videos from the global space as additional positive pairs,\nthus improving positive key diversity and introducing a more relaxed notion of\nsimilarity that extends beyond video and even class boundaries. Our method,\nInter-Intra Video Contrastive Learning (IIVCL), improves performance on a range\nof video tasks.\n","authors":["David Fan","Deyu Yang","Xinyu Li","Vimal Bhat","Rohith MV"],"pdf_url":"https://arxiv.org/pdf/2303.07317v1.pdf","comment":"Accepted to the ICLR 2023 Workshop on Mathematical and Empirical\n  Understanding of Foundation Models"},{"id":"http://arxiv.org/abs/2303.07308v1","updated":"2023-03-13T17:30:43Z","published":"2023-03-13T17:30:43Z","title":"NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial\n  Understanding with Objects","summary":"  We present NeuSE, a novel Neural SE(3)-Equivariant Embedding for objects, and\nillustrate how it supports object SLAM for consistent spatial understanding\nwith long-term scene changes. NeuSE is a set of latent object embeddings\ncreated from partial object observations. It serves as a compact point cloud\nsurrogate for complete object models, encoding full shape information while\ntransforming SE(3)-equivariantly in tandem with the object in the physical\nworld. With NeuSE, relative frame transforms can be directly derived from\ninferred latent codes. Our proposed SLAM paradigm, using NeuSE for object shape\nand pose characterization, can operate independently or in conjunction with\ntypical SLAM systems. It directly infers SE(3) camera pose constraints that are\ncompatible with general SLAM pose graph optimization, while also maintaining a\nlightweight object-centric map that adapts to real-world changes. Our approach\nis evaluated on synthetic and real-world sequences featuring changed objects\nand shows improved localization accuracy and change-aware mapping capability,\nwhen working either standalone or jointly with a common SLAM pipeline.\n","authors":["Jiahui Fu","Yilun Du","Kurran Singh","Joshua B. Tenenbaum","John J. Leonard"],"pdf_url":"https://arxiv.org/pdf/2303.07308v1.pdf","comment":"Project webpage: https://neuse-slam.github.io/neuse/"},{"id":"http://arxiv.org/abs/2211.11208v2","updated":"2023-03-13T17:08:01Z","published":"2022-11-21T06:40:46Z","title":"Next3D: Generative Neural Texture Rasterization for 3D-Aware Head\n  Avatars","summary":"  3D-aware generative adversarial networks (GANs) synthesize high-fidelity and\nmulti-view-consistent facial images using only collections of single-view 2D\nimagery. Towards fine-grained control over facial attributes, recent efforts\nincorporate 3D Morphable Face Model (3DMM) to describe deformation in\ngenerative radiance fields either explicitly or implicitly. Explicit methods\nprovide fine-grained expression control but cannot handle topological changes\ncaused by hair and accessories, while implicit ones can model varied topologies\nbut have limited generalization caused by the unconstrained deformation fields.\nWe propose a novel 3D GAN framework for unsupervised learning of generative,\nhigh-quality and 3D-consistent facial avatars from unstructured 2D images. To\nachieve both deformation accuracy and topological flexibility, we propose a 3D\nrepresentation called Generative Texture-Rasterized Tri-planes. The proposed\nrepresentation learns Generative Neural Textures on top of parametric mesh\ntemplates and then projects them into three orthogonal-viewed feature planes\nthrough rasterization, forming a tri-plane feature representation for volume\nrendering. In this way, we combine both fine-grained expression control of\nmesh-guided explicit deformation and the flexibility of implicit volumetric\nrepresentation. We further propose specific modules for modeling mouth interior\nwhich is not taken into account by 3DMM. Our method demonstrates\nstate-of-the-art 3D-aware synthesis quality and animation ability through\nextensive experiments. Furthermore, serving as 3D prior, our animatable 3D\nrepresentation boosts multiple applications including one-shot facial avatars\nand 3D-aware stylization.\n","authors":["Jingxiang Sun","Xuan Wang","Lizhen Wang","Xiaoyu Li","Yong Zhang","Hongwen Zhang","Yebin Liu"],"pdf_url":"https://arxiv.org/pdf/2211.11208v2.pdf","comment":"Accepted by CVPR 2023. Project page:\n  https://mrtornado24.github.io/Next3D/"},{"id":"http://arxiv.org/abs/2303.07284v1","updated":"2023-03-13T17:01:42Z","published":"2023-03-13T17:01:42Z","title":"Align and Attend: Multimodal Summarization with Dual Contrastive Losses","summary":"  The goal of multimodal summarization is to extract the most important\ninformation from different modalities to form summaries. Unlike unimodal\nsummarization, the multimodal summarization task explicitly leverages\ncross-modal information to help generate more reliable and high-quality\nsummaries. However, existing methods fail to leverage the temporal\ncorrespondence between different modalities and ignore the intrinsic\ncorrelation between different samples. To address this issue, we introduce\nAlign and Attend Multimodal Summarization (A2Summ), a unified multimodal\ntransformer-based model which can effectively align and attend the multimodal\ninput. In addition, we propose two novel contrastive losses to model both\ninter-sample and intra-sample correlations. Extensive experiments on two\nstandard video summarization datasets (TVSum and SumMe) and two multimodal\nsummarization datasets (Daily Mail and CNN) demonstrate the superiority of\nA2Summ, achieving state-of-the-art performances on all datasets. Moreover, we\ncollected a large-scale multimodal summarization dataset BLiSS, which contains\nlivestream videos and transcribed texts with annotated summaries. Our code and\ndataset are publicly available at ~\\url{https://boheumd.github.io/A2Summ/}.\n","authors":["Bo He","Jun Wang","Jielin Qiu","Trung Bui","Abhinav Shrivastava","Zhaowen Wang"],"pdf_url":"https://arxiv.org/pdf/2303.07284v1.pdf","comment":"Accepted at CVPR2023"},{"id":"http://arxiv.org/abs/2303.07280v1","updated":"2023-03-13T16:54:11Z","published":"2023-03-13T16:54:11Z","title":"Vision-Language Models as Success Detectors","summary":"  Detecting successful behaviour is crucial for training intelligent agents. As\nsuch, generalisable reward models are a prerequisite for agents that can learn\nto generalise their behaviour. In this work we focus on developing robust\nsuccess detectors that leverage large, pretrained vision-language models\n(Flamingo, Alayrac et al. (2022)) and human reward annotations. Concretely, we\ntreat success detection as a visual question answering (VQA) problem, denoted\nSuccessVQA. We study success detection across three vastly different domains:\n(i) interactive language-conditioned agents in a simulated household, (ii) real\nworld robotic manipulation, and (iii) \"in-the-wild\" human egocentric videos. We\ninvestigate the generalisation properties of a Flamingo-based success detection\nmodel across unseen language and visual changes in the first two domains, and\nfind that the proposed method is able to outperform bespoke reward models in\nout-of-distribution test scenarios with either variation. In the last domain of\n\"in-the-wild\" human videos, we show that success detection on unseen real\nvideos presents an even more challenging generalisation task warranting future\nwork. We hope our initial results encourage further work in real world success\ndetection and reward modelling.\n","authors":["Yuqing Du","Ksenia Konyushkova","Misha Denil","Akhil Raju","Jessica Landon","Felix Hill","Nando de Freitas","Serkan Cabi"],"pdf_url":"https://arxiv.org/pdf/2303.07280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10999v2","updated":"2023-03-13T16:51:03Z","published":"2022-11-20T15:27:55Z","title":"LA-VocE: Low-SNR Audio-visual Speech Enhancement using Neural Vocoders","summary":"  Audio-visual speech enhancement aims to extract clean speech from a noisy\nenvironment by leveraging not only the audio itself but also the target\nspeaker's lip movements. This approach has been shown to yield improvements\nover audio-only speech enhancement, particularly for the removal of interfering\nspeech. Despite recent advances in speech synthesis, most audio-visual\napproaches continue to use spectral mapping/masking to reproduce the clean\naudio, often resulting in visual backbones added to existing speech enhancement\narchitectures. In this work, we propose LA-VocE, a new two-stage approach that\npredicts mel-spectrograms from noisy audio-visual speech via a\ntransformer-based architecture, and then converts them into waveform audio\nusing a neural vocoder (HiFi-GAN). We train and evaluate our framework on\nthousands of speakers and 11+ different languages, and study our model's\nability to adapt to different levels of background noise and speech\ninterference. Our experiments show that LA-VocE outperforms existing methods\naccording to multiple metrics, particularly under very noisy scenarios.\n","authors":["Rodrigo Mira","Buye Xu","Jacob Donley","Anurag Kumar","Stavros Petridis","Vamsi Krishna Ithapu","Maja Pantic"],"pdf_url":"https://arxiv.org/pdf/2211.10999v2.pdf","comment":"accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07274v1","updated":"2023-03-13T16:49:43Z","published":"2023-03-13T16:49:43Z","title":"Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of\n  Synthetic and Compositional Images","summary":"  Weird, unusual, and uncanny images pique the curiosity of observers because\nthey challenge commonsense. For example, an image released during the 2022\nworld cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo\nplaying chess, which playfully violates our expectation that their competition\nshould occur on the football field. Humans can easily recognize and interpret\nthese unconventional images, but can AI models do the same? We introduce\nWHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is\ncomprised of purposefully commonsense-defying images created by designers using\npublicly-available image generation tools like Midjourney. We consider several\ntasks posed over the dataset. In addition to image captioning, cross-modal\nmatching, and visual question answering, we introduce a difficult explanation\ngeneration task, where models must identify and explain why a given image is\nunusual. Our results show that state-of-the-art models such as GPT3 and BLIP2\nstill lag behind human performance on WHOOPS!. We hope our dataset will inspire\nthe development of AI models with stronger visual commonsense reasoning\nabilities. Data, models and code are available at the project website:\nwhoops-benchmark.github.io\n","authors":["Nitzan Bitton-Guetta","Yonatan Bitton","Jack Hessel","Ludwig Schmidt","Yuval Elovici","Gabriel Stanovsky","Roy Schwartz"],"pdf_url":"https://arxiv.org/pdf/2303.07274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07269v1","updated":"2023-03-13T16:45:41Z","published":"2023-03-13T16:45:41Z","title":"InPL: Pseudo-labeling the Inliers First for Imbalanced Semi-supervised\n  Learning","summary":"  Recent state-of-the-art methods in imbalanced semi-supervised learning (SSL)\nrely on confidence-based pseudo-labeling with consistency regularization. To\nobtain high-quality pseudo-labels, a high confidence threshold is typically\nadopted. However, it has been shown that softmax-based confidence scores in\ndeep networks can be arbitrarily high for samples far from the training data,\nand thus, the pseudo-labels for even high-confidence unlabeled samples may\nstill be unreliable. In this work, we present a new perspective of\npseudo-labeling for imbalanced SSL. Without relying on model confidence, we\npropose to measure whether an unlabeled sample is likely to be\n``in-distribution''; i.e., close to the current training data. To decide\nwhether an unlabeled sample is ``in-distribution'' or ``out-of-distribution'',\nwe adopt the energy score from out-of-distribution detection literature. As\ntraining progresses and more unlabeled samples become in-distribution and\ncontribute to training, the combined labeled and pseudo-labeled data can better\napproximate the true class distribution to improve the model. Experiments\ndemonstrate that our energy-based pseudo-labeling method, \\textbf{InPL}, albeit\nconceptually simple, significantly outperforms confidence-based methods on\nimbalanced SSL benchmarks. For example, it produces around 3\\% absolute\naccuracy improvement on CIFAR10-LT. When combined with state-of-the-art\nlong-tailed SSL methods, further improvements are attained. In particular, in\none of the most challenging scenarios, InPL achieves a 6.9\\% accuracy\nimprovement over the best competitor.\n","authors":["Zhuoran Yu","Yin Li","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2303.07269v1.pdf","comment":"Accepted by ICLR 2023"},{"id":"http://arxiv.org/abs/2303.07264v1","updated":"2023-03-13T16:44:15Z","published":"2023-03-13T16:44:15Z","title":"A Surface-normal Based Neural Framework for Colonoscopy Reconstruction","summary":"  Reconstructing a 3D surface from colonoscopy video is challenging due to\nillumination and reflectivity variation in the video frame that can cause\ndefective shape predictions. Aiming to overcome this challenge, we utilize the\ncharacteristics of surface normal vectors and develop a two-step neural\nframework that significantly improves the colonoscopy reconstruction quality.\nThe normal-based depth initialization network trained with self-supervised\nnormal consistency loss provides depth map initialization to the normal-depth\nrefinement module, which utilizes the relationship between illumination and\nsurface normals to refine the frame-wise normal and depth predictions\nrecursively. Our framework's depth accuracy performance on phantom colonoscopy\ndata demonstrates the value of exploiting the surface normals in colonoscopy\nreconstruction, especially on en face views. Due to its low depth error, the\nprediction result from our framework will require limited post-processing to be\nclinically applicable for real-time colonoscopy reconstruction.\n","authors":["Shuxian Wang","Yubo Zhang","Sarah K. McGill","Julian G. Rosenman","Jan-Michael Frahm","Soumyadip Sengupta","Stephen M. Pizer"],"pdf_url":"https://arxiv.org/pdf/2303.07264v1.pdf","comment":"Accepted at IPMI 2023; first two authors contributed equally"},{"id":"http://arxiv.org/abs/2205.08745v2","updated":"2023-03-13T16:37:07Z","published":"2022-05-18T06:38:18Z","title":"Validation of a photogrammetric approach for the objective study of\n  ancient bowed instruments","summary":"  Some early violins have been reduced during their history to fit imposed\nmorphological standards, while more recent ones have been built directly to\nthese standards. We propose an objective photogrammetric approach to\ndifferentiate between a reduced and an unreduced instrument, whereby a\nthree-dimensional mesh is studied geometrically by examining 2D slices. Our\ncontribution is twofold. First, we validate the quality of the photogrammetric\nmesh through a comparison with reference images obtained by medical imaging,\nand conclude that a sub-millimetre accuracy is achieved. Then, we show how\nquantitative and qualitative features such as contour lines, channel of minima\nand a measure of asymmetry between the upper and lower surfaces of a violin can\nbe automatically extracted from the validated photogrammetric meshes, allowing\nto successfully highlight differences between instruments.\n","authors":["Philémon Beghin","Anne-Emmanuelle Ceulemans","Paul Fisette","François Glineur"],"pdf_url":"https://arxiv.org/pdf/2205.08745v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07240v1","updated":"2023-03-13T16:13:16Z","published":"2023-03-13T16:13:16Z","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical\n  Documents","summary":"  Foundation models trained on large-scale dataset gain a recent surge in CV\nand NLP. In contrast, development in biomedical domain lags far behind due to\ndata scarcity. To address this issue, we build and release PMC-OA, a biomedical\ndataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess\nsubset, which is 8 times larger than before. PMC-OA covers diverse modalities\nor diseases, with majority of the image-caption samples aligned at\nfiner-grained level, i.e., subfigure and subcaption. While pretraining a\nCLIP-style model on PMC-OA, our model named PMC-CLIP achieves state-of-the-art\nresults on various downstream tasks, including image-text retrieval on ROCO,\nMedMNIST image classification, Medical VQA, i.e. +8.1% R@10 on image-text\nretrieval, +3.9% accuracy on image classification.\n","authors":["Weixiong Lin","Ziheng Zhao","Xiaoman Zhang","Chaoyi Wu","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2303.07240v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.02328v2","updated":"2023-03-13T16:04:17Z","published":"2023-03-04T05:23:11Z","title":"Decompose, Adjust, Compose: Effective Normalization by Playing with\n  Frequency for Domain Generalization","summary":"  Domain generalization (DG) is a principal task to evaluate the robustness of\ncomputer vision models. Many previous studies have used normalization for DG.\nIn normalization, statistics and normalized features are regarded as style and\ncontent, respectively. However, it has a content variation problem when\nremoving style because the boundary between content and style is unclear. This\nstudy addresses this problem from the frequency domain perspective, where\namplitude and phase are considered as style and content, respectively. First,\nwe verify the quantitative phase variation of normalization through the\nmathematical derivation of the Fourier transform formula. Then, based on this,\nwe propose a novel normalization method, PCNorm, which eliminates style only as\nthe preserving content through spectral decomposition. Furthermore, we propose\nadvanced PCNorm variants, CCNorm and SCNorm, which adjust the degrees of\nvariations in content and style, respectively. Thus, they can learn\ndomain-agnostic representations for DG. With the normalization methods, we\npropose ResNet-variant models, DAC-P and DAC-SC, which are robust to the domain\ngap. The proposed models outperform other recent DG methods. The DAC-SC\nachieves an average state-of-the-art performance of 65.6% on five datasets:\nPACS, VLCS, Office-Home, DomainNet, and TerraIncognita.\n","authors":["Sangrok Lee","Jongseong Bae","Ha Young Kim"],"pdf_url":"https://arxiv.org/pdf/2303.02328v2.pdf","comment":"10 pages,6 figures, Conference on Computer Vision and Pattern\n  Recognition 2023"},{"id":"http://arxiv.org/abs/2303.07226v1","updated":"2023-03-13T16:00:31Z","published":"2023-03-13T16:00:31Z","title":"Scaling Vision-Language Models with Sparse Mixture of Experts","summary":"  The field of natural language processing (NLP) has made significant strides\nin recent years, particularly in the development of large-scale vision-language\nmodels (VLMs). These models aim to bridge the gap between text and visual\ninformation, enabling a more comprehensive understanding of multimedia data.\nHowever, as these models become larger and more complex, they also become more\nchallenging to train and deploy. One approach to addressing this challenge is\nthe use of sparsely-gated mixture-of-experts (MoE) techniques, which divide the\nmodel into smaller, specialized sub-models that can jointly solve a task. In\nthis paper, we explore the effectiveness of MoE in scaling vision-language\nmodels, demonstrating its potential to achieve state-of-the-art performance on\na range of benchmarks over dense models of equivalent computational cost. Our\nresearch offers valuable insights into stabilizing the training of MoE models,\nunderstanding the impact of MoE on model interpretability, and balancing the\ntrade-offs between compute performance when scaling VLMs. We hope our work will\ninspire further research into the use of MoE for scaling large-scale\nvision-language models and other multimodal machine learning applications.\n","authors":["Sheng Shen","Zhewei Yao","Chunyuan Li","Trevor Darrell","Kurt Keutzer","Yuxiong He"],"pdf_url":"https://arxiv.org/pdf/2303.07226v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2303.07224v1","updated":"2023-03-13T15:58:15Z","published":"2023-03-13T15:58:15Z","title":"Efficient Semantic Segmentation by Altering Resolutions for Compressed\n  Videos","summary":"  Video semantic segmentation (VSS) is a computationally expensive task due to\nthe per-frame prediction for videos of high frame rates. In recent work,\ncompact models or adaptive network strategies have been proposed for efficient\nVSS. However, they did not consider a crucial factor that affects the\ncomputational cost from the input side: the input resolution. In this paper, we\npropose an altering resolution framework called AR-Seg for compressed videos to\nachieve efficient VSS. AR-Seg aims to reduce the computational cost by using\nlow resolution for non-keyframes. To prevent the performance degradation caused\nby downsampling, we design a Cross Resolution Feature Fusion (CReFF) module,\nand supervise it with a novel Feature Similarity Training (FST) strategy.\nSpecifically, CReFF first makes use of motion vectors stored in a compressed\nvideo to warp features from high-resolution keyframes to low-resolution\nnon-keyframes for better spatial alignment, and then selectively aggregates the\nwarped features with local attention mechanism. Furthermore, the proposed FST\nsupervises the aggregated features with high-resolution features through an\nexplicit similarity loss and an implicit constraint from the shared decoding\nlayer. Extensive experiments on CamVid and Cityscapes show that AR-Seg achieves\nstate-of-the-art performance and is compatible with different segmentation\nbackbones. On CamVid, AR-Seg saves 67% computational cost (measured in GFLOPs)\nwith the PSPNet18 backbone while maintaining high segmentation accuracy. Code:\nhttps://github.com/THU-LYJ-Lab/AR-Seg.\n","authors":["Yubin Hu","Yuze He","Yanghao Li","Jisheng Li","Yuxing Han","Jiangtao Wen","Yong-Jin Liu"],"pdf_url":"https://arxiv.org/pdf/2303.07224v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07223v1","updated":"2023-03-13T15:58:00Z","published":"2023-03-13T15:58:00Z","title":"PromptFusion: Decoupling Stability and Plasticity for Continual Learning","summary":"  Continual learning refers to the capability of continuously learning from a\nstream of data. Current research mainly focuses on relieving catastrophic\nforgetting, and most of their success is at the cost of limiting the\nperformance of newly incoming tasks. Such a trade-off is referred to as the\nstabilityplasticity dilemma and is a more general and challenging problem for\ncontinual learning. However, the inherent conflict between these two concepts\nmakes it seemingly impossible to devise a satisfactory solution to both of them\nsimultaneously. Therefore, we ask, \"is it possible to divide them into two\nproblems to conquer independently?\" To this end, we propose a\nprompt-tuning-based method termed PromptFusion to enable the decoupling of\nstability and plasticity. Specifically, PromptFusion consists of a carefully\ndesigned Stabilizer module that deals with catastrophic forgetting and a\nBooster module to learn new knowledge concurrently. During training,\nPromptFusion first passes an input image to the two modules separately. Then\nthe resulting logits are further fused with a learnable weight parameter.\nFinally, a weight mask is applied to the derived logits to balance between old\nand new classes. Extensive experiments show that our method achieves promising\nresults on popular continual learning datasets for both class-incremental and\ndomain incremental settings. Especially on Split-Imagenet-R, one of the most\nchallenging datasets for class-incremental learning, our method exceeds\nstate-of-the-art prompt-based methods L2P and DualPrompt by more than 10%.\n","authors":["Haoran Chen","Zuxuan Wu","Xintong Han","Menglin Jia","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.07223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07216v1","updated":"2023-03-13T15:51:38Z","published":"2023-03-13T15:51:38Z","title":"Parallel Vertex Diffusion for Unified Visual Grounding","summary":"  Unified visual grounding pursues a simple and generic technical route to\nleverage multi-task data with less task-specific design. The most advanced\nmethods typically present boxes and masks as vertex sequences to model\nreferring detection and segmentation as an autoregressive sequential vertex\ngeneration paradigm. However, generating high-dimensional vertex sequences\nsequentially is error-prone because the upstream of the sequence remains static\nand cannot be refined based on downstream vertex information, even if there is\na significant location gap. Besides, with limited vertexes, the inferior\nfitting of objects with complex contours restricts the performance upper bound.\nTo deal with this dilemma, we propose a parallel vertex generation paradigm for\nsuperior high-dimension scalability with a diffusion model by simply modifying\nthe noise dimension. An intuitive materialization of our paradigm is Parallel\nVertex Diffusion (PVD) to directly set vertex coordinates as the generation\ntarget and use a diffusion model to train and infer. We claim that it has two\nflaws: (1) unnormalized coordinate caused a high variance of loss value; (2)\nthe original training objective of PVD only considers point consistency but\nignores geometry consistency. To solve the first flaw, Center Anchor Mechanism\n(CAM) is designed to convert coordinates as normalized offset values to\nstabilize the training loss value. For the second flaw, Angle summation loss\n(ASL) is designed to constrain the geometry difference of prediction and ground\ntruth vertexes for geometry-level consistency. Empirical results show that our\nPVD achieves state-of-the-art in both referring detection and segmentation, and\nour paradigm is more scalable and efficient than sequential vertex generation\nwith high-dimension data.\n","authors":["Zesen Cheng","Kehan Li","Peng Jin","Xiangyang Ji","Li Yuan","Chang Liu","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2303.07216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07189v1","updated":"2023-03-13T15:30:28Z","published":"2023-03-13T15:30:28Z","title":"Optimizing Convolutional Neural Networks for Chronic Obstructive\n  Pulmonary Disease Detection in Clinical Computed Tomography Imaging","summary":"  Chronic Obstructive Pulmonary Disease (COPD) is a leading cause of death\nworldwide, yet early detection and treatment can prevent the progression of the\ndisease. In contrast to the conventional method of detecting COPD with\nspirometry tests, X-ray Computed Tomography (CT) scans of the chest provide a\nmeasure of morphological changes in the lung. It has been shown that automated\ndetection of COPD can be performed with deep learning models. However, the\npotential of incorporating optimal window setting selection, typically carried\nout by clinicians during examination of CT scans for COPD, is generally\noverlooked in deep learning approaches. We aim to optimize the binary\nclassification of COPD with densely connected convolutional neural networks\n(DenseNets) through implementation of manual and automated Window-Setting\nOptimization (WSO) steps. Our dataset consisted of 78 CT scans from the\nKlinikum rechts der Isar research hospital. Repeated inference on the test set\nshowed that without WSO, the plain DenseNet resulted in a mean slice-level AUC\nof 0.80$\\pm$0.05. With input images manually adjusted to the emphysema window\nsetting, the plain DenseNet model predicted COPD with a mean AUC of\n0.86$\\pm$0.04. By automating the WSO through addition of a customized layer to\nthe DenseNet, an optimal window setting in the proximity of the emphysema\nwindow setting was learned and a mean AUC of 0.82$\\pm$0.04 was achieved.\nDetection of COPD with DenseNet models was optimized by WSO of CT data to the\nemphysema window setting range, demonstrating the importance of implementing\noptimal window setting selection in the deep learning pipeline.\n","authors":["Tina Dorosti","Manuel Schultheiss","Felix Hofmann","Luisa Kirchner","Theresa Urban","Franz Pfeiffer","Johannes Thalhammer","Florian Schaff","Tobias Lasser","Daniela Pfeiffer"],"pdf_url":"https://arxiv.org/pdf/2303.07189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.13096v2","updated":"2023-03-13T15:28:11Z","published":"2022-04-27T17:46:55Z","title":"3D Magic Mirror: Clothing Reconstruction from a Single Image via a\n  Causal Perspective","summary":"  This research aims to study a self-supervised 3D clothing reconstruction\nmethod, which recovers the geometry shape and texture of human clothing from a\nsingle image. Compared with existing methods, we observe that three primary\nchallenges remain: (1) 3D ground-truth meshes of clothing are usually\ninaccessible due to annotation difficulties and time costs; (2) Conventional\ntemplate-based methods are limited to modeling non-rigid objects, e.g.,\nhandbags and dresses, which are common in fashion images; (3) The inherent\nambiguity compromises the model training, such as the dilemma between a large\nshape with a remote camera or a small shape with a close camera.\n  In an attempt to address the above limitations, we propose a causality-aware\nself-supervised learning method to adaptively reconstruct 3D non-rigid objects\nfrom 2D images without 3D annotations. In particular, to solve the inherent\nambiguity among four implicit variables, i.e., camera position, shape, texture,\nand illumination, we introduce an explainable structural causal map (SCM) to\nbuild our model. The proposed model structure follows the spirit of the causal\nmap, which explicitly considers the prior template in the camera estimation and\nshape prediction. When optimization, the causality intervention tool, i.e., two\nexpectation-maximization loops, is deeply embedded in our algorithm to (1)\ndisentangle four encoders and (2) facilitate the prior template. Extensive\nexperiments on two 2D fashion benchmarks (ATR and Market-HQ) show that the\nproposed method could yield high-fidelity 3D reconstruction. Furthermore, we\nalso verify the scalability of the proposed method on a fine-grained bird\ndataset, i.e., CUB. The code is available at https://github.com/layumi/\n3D-Magic-Mirror .\n","authors":["Zhedong Zheng","Jiayin Zhu","Wei Ji","Yi Yang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2204.13096v2.pdf","comment":"Update results. Report person re-id performance. Add details in\n  Appendix"},{"id":"http://arxiv.org/abs/2303.07182v1","updated":"2023-03-13T15:24:06Z","published":"2023-03-13T15:24:06Z","title":"Mobile Mapping Mesh Change Detection and Update","summary":"  Mobile mapping, in particular, Mobile Lidar Scanning (MLS) is increasingly\nwidespread to monitor and map urban scenes at city scale with unprecedented\nresolution and accuracy. The resulting point cloud sampling of the scene\ngeometry can be meshed in order to create a continuous representation for\ndifferent applications: visualization, simulation, navigation, etc. Because of\nthe highly dynamic nature of these urban scenes, long term mapping should rely\non frequent map updates. A trivial solution is to simply replace old data with\nnewer data each time a new acquisition is made. However it has two drawbacks:\n1) the old data may be of higher quality (resolution, precision) than the new\nand 2) the coverage of the scene might be different in various acquisitions,\nincluding varying occlusions. In this paper, we propose a fully automatic\npipeline to address these two issues by formulating the problem of merging\nmeshes with different quality, coverage and acquisition time. Our method is\nbased on a combined distance and visibility based change detection, a time\nseries analysis to assess the sustainability of changes, a mesh mosaicking\nbased on a global boolean optimization and finally a stitching of the resulting\nmesh pieces boundaries with triangle strips. Finally, our method is\ndemonstrated on Robotcar and Stereopolis datasets.\n","authors":["Teng Wu","Bruno Vallet","Cédric Demonceaux"],"pdf_url":"https://arxiv.org/pdf/2303.07182v1.pdf","comment":"6 pages without reference"},{"id":"http://arxiv.org/abs/2303.07180v1","updated":"2023-03-13T15:22:50Z","published":"2023-03-13T15:22:50Z","title":"Incomplete Multi-View Multi-Label Learning via Label-Guided Masked View-\n  and Category-Aware Transformers","summary":"  As we all know, multi-view data is more expressive than single-view data and\nmulti-label annotation enjoys richer supervision information than single-label,\nwhich makes multi-view multi-label learning widely applicable for various\npattern recognition tasks. In this complex representation learning problem,\nthree main challenges can be characterized as follows: i) How to learn\nconsistent representations of samples across all views? ii) How to exploit and\nutilize category correlations of multi-label to guide inference? iii) How to\navoid the negative impact resulting from the incompleteness of views or labels?\nTo cope with these problems, we propose a general multi-view multi-label\nlearning framework named label-guided masked view- and category-aware\ntransformers in this paper. First, we design two transformer-style based\nmodules for cross-view features aggregation and multi-label classification,\nrespectively. The former aggregates information from different views in the\nprocess of extracting view-specific features, and the latter learns subcategory\nembedding to improve classification performance. Second, considering the\nimbalance of expressive power among views, an adaptively weighted view fusion\nmodule is proposed to obtain view-consistent embedding features. Third, we\nimpose a label manifold constraint in sample-level representation learning to\nmaximize the utilization of supervised information. Last but not least, all the\nmodules are designed under the premise of incomplete views and labels, which\nmakes our method adaptable to arbitrary multi-view and multi-label data.\nExtensive experiments on five datasets confirm that our method has clear\nadvantages over other state-of-the-art methods.\n","authors":["Chengliang Liu","Jie Wen","Xiaoling Luo","Yong Xu"],"pdf_url":"https://arxiv.org/pdf/2303.07180v1.pdf","comment":"Accepted to AAAI-23"},{"id":"http://arxiv.org/abs/2302.07570v2","updated":"2023-03-13T15:22:31Z","published":"2023-02-15T10:21:38Z","title":"Super-Resolution of BVOC Maps by Adapting Deep Learning Methods","summary":"  Biogenic Volatile Organic Compounds (BVOCs) play a critical role in\nbiosphere-atmosphere interactions, being a key factor in the physical and\nchemical properties of the atmosphere and climate. Acquiring large and\nfine-grained BVOC emission maps is expensive and time-consuming, so most\navailable BVOC data are obtained on a loose and sparse sampling grid or on\nsmall regions. However, high-resolution BVOC data are desirable in many\napplications, such as air quality, atmospheric chemistry, and climate\nmonitoring. In this work, we investigate the possibility of enhancing BVOC\nacquisitions, further explaining the relationships between the environment and\nthese compounds. We do so by comparing the performances of several\nstate-of-the-art neural networks proposed for image Super-Resolution (SR),\nadapting them to overcome the challenges posed by the large dynamic range of\nthe emission and reduce the impact of outliers in the prediction. Moreover, we\nalso consider realistic scenarios, considering both temporal and geographical\nconstraints. Finally, we present possible future developments regarding SR\ngeneralization, considering the scale-invariance property and super-resolving\nemissions from unseen compounds.\n","authors":["Antonio Giganti","Sara Mandelli","Paolo Bestagini","Marco Marcon","Stefano Tubaro"],"pdf_url":"https://arxiv.org/pdf/2302.07570v2.pdf","comment":"5 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2204.09804v3","updated":"2023-03-13T15:20:30Z","published":"2022-04-20T22:48:05Z","title":"Weighted Bayesian Gaussian Mixture Model for Roadside LiDAR Object\n  Detection","summary":"  Background modeling is widely used for intelligent surveillance systems to\ndetect moving targets by subtracting the static background components. Most\nroadside LiDAR object detection methods filter out foreground points by\ncomparing new data points to pre-trained background references based on\ndescriptive statistics over many frames (e.g., voxel density, number of\nneighbors, maximum distance). However, these solutions are inefficient under\nheavy traffic, and parameter values are hard to transfer from one scenario to\nanother. In early studies, the probabilistic background modeling methods widely\nused for the video-based system were considered unsuitable for roadside LiDAR\nsurveillance systems due to the sparse and unstructured point cloud data. In\nthis paper, the raw LiDAR data were transformed into a structured\nrepresentation based on the elevation and azimuth value of each LiDAR point.\nWith this high-order tensor representation, we break the barrier to allow\nefficient high-dimensional multivariate analysis for roadside LiDAR background\nmodeling. The Bayesian Nonparametric (BNP) approach integrates the intensity\nvalue and 3D measurements to exploit the measurement data using 3D and\nintensity info entirely. The proposed method was compared against two\nstate-of-the-art roadside LiDAR background models, computer vision benchmark,\nand deep learning baselines, evaluated at point, object, and path levels under\nheavy traffic and challenging weather. This multimodal Weighted Bayesian\nGaussian Mixture Model (GMM) can handle dynamic backgrounds with noisy\nmeasurements and substantially enhances the infrastructure-based LiDAR object\ndetection, whereby various 3D modeling for smart city applications could be\ncreated.\n","authors":["Tianya Zhang","Yi Ge","Peter J. Jin"],"pdf_url":"https://arxiv.org/pdf/2204.09804v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07172v1","updated":"2023-03-13T15:14:26Z","published":"2023-03-13T15:14:26Z","title":"Evaluating Visual Number Discrimination in Deep Neural Networks","summary":"  The ability to discriminate between large and small quantities is a core\naspect of basic numerical competence in both humans and animals. In this work,\nwe examine the extent to which the state-of-the-art neural networks designed\nfor vision exhibit this basic ability. Motivated by studies in animal and\ninfant numerical cognition, we use the numerical bisection procedure to test\nnumber discrimination in different families of neural architectures. Our\nresults suggest that vision-specific inductive biases are helpful in numerosity\ndiscrimination, as models with such biases have lowest test errors on the task,\nand often have psychometric curves that qualitatively resemble those of humans\nand animals performing the task. However, even the strongest models, as\nmeasured on standard metrics of performance, fail to discriminate quantities in\ntransfer experiments with differing training and testing conditions, indicating\nthat such inductive biases might not be sufficient.\n","authors":["Ivana Kajić","Aida Nematzadeh"],"pdf_url":"https://arxiv.org/pdf/2303.07172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07169v1","updated":"2023-03-13T15:12:30Z","published":"2023-03-13T15:12:30Z","title":"Dynamic Event-based Optical Flow Identification and Communication","summary":"  Optical identification is often done with spatial or temporal visual pattern\nrecognition and localization. Temporal pattern recognition, depending on the\ntechnology, involves a trade-off between communication frequency, range and\naccurate tracking. We propose a solution with light-emitting beacons that\nimproves this trade-off by exploiting fast event-based cameras and, for\ntracking, sparse neuromorphic optical flow computed with spiking neurons. In an\nasset monitoring use case, we demonstrate that the system, embedded in a\nsimulated drone, is robust to relative movements and enables simultaneous\ncommunication with, and tracking of, multiple moving beacons. Finally, in a\nhardware lab prototype, we achieve state-of-the-art optical camera\ncommunication frequencies in the kHz magnitude.\n","authors":["Axel von Arnim","Jules Lecomte","Stanislaw Wozniak","Naima Elosegui Borras","Angeliki Pantazi"],"pdf_url":"https://arxiv.org/pdf/2303.07169v1.pdf","comment":"5 pages, 6 figures and 1 table"},{"id":"http://arxiv.org/abs/2303.07151v1","updated":"2023-03-13T14:25:39Z","published":"2023-03-13T14:25:39Z","title":"Amélioration de la qualité d'images avec un algorithme\n  d'optimisation inspirée par la nature","summary":"  Reproducible images preprocessing is important in the field of computer\nvision, for efficient algorithms comparison or for new images corpus\npreparation. In this paper, we propose a method to obtain an explicit and\nordered sequence of transformations that improves a given image: the\ncomputation is performed via a nature-inspired optimization algorithm based on\nquality assessment techniques. Preliminary tests show the impact of the\napproach on different state-of-the-art data sets.\n  --\n  L'application de pr\\'etraitements explicites et reproductibles est\nfondamentale dans le domaine de la vision par ordinateur, pour pouvoir comparer\nefficacement des algorithmes ou pour pr\\'eparer un nouveau corpus d'images.\nDans cet article, nous proposons une m\\'ethode pour obtenir une s\\'equence\nreproductible de transformations qui am\\'eliore une image donn\\'ee: le calcul\nest r\\'ealis\\'e via un algorithme d'optimisation inspir\\'ee par la nature et\nbas\\'e sur des techniques d'\\'evaluation de la qualit\\'e. Des tests montrent\nl'impact de l'approche sur diff\\'erents ensembles d'images de l'\\'etat de\nl'art.\n","authors":["Olivier Parisot","Thomas Tamisier"],"pdf_url":"https://arxiv.org/pdf/2303.07151v1.pdf","comment":"8 pages, in French language"},{"id":"http://arxiv.org/abs/2303.07150v1","updated":"2023-03-13T14:23:39Z","published":"2023-03-13T14:23:39Z","title":"Multi PILOT: Learned Feasible Multiple Acquisition Trajectories for\n  Dynamic MRI","summary":"  Dynamic Magnetic Resonance Imaging (MRI) is known to be a powerful and\nreliable technique for the dynamic imaging of internal organs and tissues,\nmaking it a leading diagnostic tool. A major difficulty in using MRI in this\nsetting is the relatively long acquisition time (and, hence, increased cost)\nrequired for imaging in high spatio-temporal resolution, leading to the\nappearance of related motion artifacts and decrease in resolution. Compressed\nSensing (CS) techniques have become a common tool to reduce MRI acquisition\ntime by subsampling images in the k-space according to some acquisition\ntrajectory. Several studies have particularly focused on applying deep learning\ntechniques to learn these acquisition trajectories in order to attain better\nimage reconstruction, rather than using some predefined set of trajectories. To\nthe best of our knowledge, learning acquisition trajectories has been only\nexplored in the context of static MRI. In this study, we consider acquisition\ntrajectory learning in the dynamic imaging setting. We design an end-to-end\npipeline for the joint optimization of multiple per-frame acquisition\ntrajectories along with a reconstruction neural network, and demonstrate\nimproved image reconstruction quality in shorter acquisition times. The code\nfor reproducing all experiments is accessible at\nhttps://github.com/tamirshor7/MultiPILOT.\n","authors":["Tamir Shor","Tomer Weiss","Dor Noti","Alex Bronstein"],"pdf_url":"https://arxiv.org/pdf/2303.07150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07141v1","updated":"2023-03-13T14:09:40Z","published":"2023-03-13T14:09:40Z","title":"An Improved Baseline Framework for Pose Estimation Challenge at ECCV\n  2022 Visual Perception for Navigation in Human Environments Workshop","summary":"  This technical report describes our first-place solution to the pose\nestimation challenge at ECCV 2022 Visual Perception for Navigation in Human\nEnvironments Workshop. In this challenge, we aim to estimate human poses from\nin-the-wild stitched panoramic images. Our method is built based on Faster\nR-CNN for human detection, and HRNet for human pose estimation. We describe\ntechnical details for the JRDB-Pose dataset, together with some experimental\nresults. In the competition, we achieved 0.303 $\\text{OSPA}_{\\text{IOU}}$ and\n64.047\\% $\\text{AP}_{\\text{0.5}}$ on the test set of JRDB-Pose.\n","authors":["Jiajun Fu","Yonghao Dang","Ruoqi Yin","Shaojie Zhang","Feng Zhou","Wending Zhao","Jianqin Yin"],"pdf_url":"https://arxiv.org/pdf/2303.07141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.03127v3","updated":"2023-03-13T14:09:06Z","published":"2022-11-06T14:08:04Z","title":"StuArt: Individualized Classroom Observation of Students with Automatic\n  Behavior Recognition and Tracking","summary":"  Each student matters, but it is hardly for instructors to observe all the\nstudents during the courses and provide helps to the needed ones immediately.\nIn this paper, we present StuArt, a novel automatic system designed for the\nindividualized classroom observation, which empowers instructors to concern the\nlearning status of each student. StuArt can recognize five representative\nstudent behaviors (hand-raising, standing, sleeping, yawning, and smiling) that\nare highly related to the engagement and track their variation trends during\nthe course. To protect the privacy of students, all the variation trends are\nindexed by the seat numbers without any personal identification information.\nFurthermore, StuArt adopts various user-friendly visualization designs to help\ninstructors quickly understand the individual and whole learning status.\nExperimental results on real classroom videos have demonstrated the superiority\nand robustness of the embedded algorithms. We expect our system promoting the\ndevelopment of large-scale individualized guidance of students. More\ninformation is in https://github.com/hnuzhy/StuArt.\n","authors":["Huayi Zhou","Fei Jiang","Jiaxin Si","Lili Xiong","Hongtao Lu"],"pdf_url":"https://arxiv.org/pdf/2211.03127v3.pdf","comment":"accepted by ICASSP2023. Novel pedagogical approaches in signal\n  processing for K-12 education"},{"id":"http://arxiv.org/abs/2212.07652v2","updated":"2023-03-13T14:06:14Z","published":"2022-12-15T08:19:02Z","title":"Body-Part Joint Detection and Association via Extended Object\n  Representation","summary":"  The detection of human body and its related parts (e.g., face, head or hands)\nhave been intensively studied and greatly improved since the breakthrough of\ndeep CNNs. However, most of these detectors are trained independently, making\nit a challenging task to associate detected body parts with people. This paper\nfocuses on the problem of joint detection of human body and its corresponding\nparts. Specifically, we propose a novel extended object representation that\nintegrates the center location offsets of body or its parts, and construct a\ndense single-stage anchor-based Body-Part Joint Detector (BPJDet). Body-part\nassociations in BPJDet are embedded into the unified representation which\ncontains both the semantic and geometric information. Therefore, BPJDet does\nnot suffer from error-prone association post-matching, and has a better\naccuracy-speed trade-off. Furthermore, BPJDet can be seamlessly generalized to\njointly detect any body part. To verify the effectiveness and superiority of\nour method, we conduct extensive experiments on the CityPersons, CrowdHuman and\nBodyHands datasets. The proposed BPJDet detector achieves state-of-the-art\nassociation performance on these three benchmarks while maintains high accuracy\nof detection. Code is in https://github.com/hnuzhy/BPJDet.\n","authors":["Huayi Zhou","Fei Jiang","Hongtao Lu"],"pdf_url":"https://arxiv.org/pdf/2212.07652v2.pdf","comment":"accepted by ICME2023"},{"id":"http://arxiv.org/abs/2303.07130v1","updated":"2023-03-13T13:59:47Z","published":"2023-03-13T13:59:47Z","title":"Enhancing COVID-19 Severity Analysis through Ensemble Methods","summary":"  Computed Tomography (CT) scans provide a detailed image of the lungs,\nallowing clinicians to observe the extent of damage caused by COVID-19. The CT\nseverity score (CTSS) of COVID-19 can be categorized based on the extent of\nlung involvement observed on a CT scan. This paper proposes a domain\nknowledge-based pipeline to extract the infection regions using diverse\nimage-processing algorithms and a pre-trained UNET model. An ensemble of three\nmachine-learning models, Random Forest (RF), Extremely Randomized Trees (ERT),\nand Support Vector Machine (SVM), is employed to classify the CT scans into\ndifferent severity classes. The proposed system achieved a macro F1 score of\n57.47% on the validation dataset in the AI-Enabled Medical Image Analysis\nWorkshop and COVID-19 Diagnosis Competition (AI-MIA-COV19D).\n","authors":["Anand Thyagachandran","Hema A Murthy"],"pdf_url":"https://arxiv.org/pdf/2303.07130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07126v1","updated":"2023-03-13T13:57:29Z","published":"2023-03-13T13:57:29Z","title":"Mirror U-Net: Marrying Multimodal Fission with Multi-task Learning for\n  Semantic Segmentation in Medical Imaging","summary":"  Positron Emission Tomography (PET) and Computer Tomography (CT) are routinely\nused together to detect tumors. PET/CT segmentation models can automate tumor\ndelineation, however, current multimodal models do not fully exploit the\ncomplementary information in each modality, as they either concatenate PET and\nCT data or fuse them at the decision level. To combat this, we propose Mirror\nU-Net, which replaces traditional fusion methods with multimodal fission by\nfactorizing the multimodal representation into modality-specific branches and\nan auxiliary multimodal decoder. At these branches, Mirror U-Net assigns a task\ntailored to each modality to reinforce unimodal features while preserving\nmultimodal features in the shared representation. In contrast to previous\nmethods that use either fission or multi-task learning, Mirror U-Net combines\nboth paradigms in a unified framework. We explore various task combinations and\nexamine which parameters to share in the model. We evaluate Mirror U-Net on the\nAutoPET PET/CT and on the multimodal MSD BrainTumor datasets, demonstrating its\neffectiveness in multimodal segmentation and achieving state-of-the-art\nperformance on both datasets. Our code will be made publicly available.\n","authors":["Zdravko Marinov","Simon Reiß","David Kersting","Jens Kleesiek","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2303.07126v1.pdf","comment":"8 pages; 8 figures; 5 tables"},{"id":"http://arxiv.org/abs/2210.04087v2","updated":"2023-03-13T13:56:31Z","published":"2022-10-08T18:49:58Z","title":"Symmetry Defense Against CNN Adversarial Perturbation Attacks","summary":"  Convolutional neural network classifiers (CNNs) are susceptible to\nadversarial attacks that perturb original samples to fool classifiers such as\nan autonomous vehicle's road sign image classifier. CNNs also lack invariance\nin the classification of symmetric samples because CNNs can classify symmetric\nsamples differently. Considered together, the CNN lack of adversarial\nrobustness and the CNN lack of invariance mean that the classification of\nsymmetric adversarial samples can differ from their incorrect classification.\nCould symmetric adversarial samples revert to their correct classification?\nThis paper answers this question by designing a symmetry defense that inverts\nor horizontally flips adversarial samples before classification against\nadversaries unaware of the defense. Against adversaries aware of the defense,\nthe defense devises a Klein four symmetry subgroup that includes the horizontal\nflip and pixel inversion symmetries. The symmetry defense uses the subgroup\nsymmetries in accuracy evaluation and the subgroup closure property to confine\nthe transformations that an adaptive adversary can apply before or after\ngenerating the adversarial sample. Without changing the preprocessing,\nparameters, or model, the proposed symmetry defense counters the Projected\nGradient Descent (PGD) and AutoAttack attacks with near-default accuracies for\nImageNet. Without using attack knowledge or adversarial samples, the proposed\ndefense exceeds the current best defense, which trains on adversarial samples.\nThe defense maintains and even improves the classification accuracy of\nnon-adversarial samples.\n","authors":["Blerta Lindqvist"],"pdf_url":"https://arxiv.org/pdf/2210.04087v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2303.07125v1","updated":"2023-03-13T13:56:20Z","published":"2023-03-13T13:56:20Z","title":"Don't PANIC: Prototypical Additive Neural Network for Interpretable\n  Classification of Alzheimer's Disease","summary":"  Alzheimer's disease (AD) has a complex and multifactorial etiology, which\nrequires integrating information about neuroanatomy, genetics, and\ncerebrospinal fluid biomarkers for accurate diagnosis. Hence, recent deep\nlearning approaches combined image and tabular information to improve\ndiagnostic performance. However, the black-box nature of such neural networks\nis still a barrier for clinical applications, in which understanding the\ndecision of a heterogeneous model is integral. We propose PANIC, a prototypical\nadditive neural network for interpretable AD classification that integrates 3D\nimage and tabular data. It is interpretable by design and, thus, avoids the\nneed for post-hoc explanations that try to approximate the decision of a\nnetwork. Our results demonstrate that PANIC achieves state-of-the-art\nperformance in AD classification, while directly providing local and global\nexplanations. Finally, we show that PANIC extracts biologically meaningful\nsignatures of AD, and satisfies a set of desirable desiderata for trustworthy\nmachine learning. Our implementation is available at\n\\url{https://github.com/ai-med/PANIC}.\n","authors":["Tom Nuno Wolf","Sebastian Pölster","Christian Wachinger"],"pdf_url":"https://arxiv.org/pdf/2303.07125v1.pdf","comment":"To be published in proceedings of Information Processing In Medical\n  Imaging 2023"},{"id":"http://arxiv.org/abs/2303.07123v1","updated":"2023-03-13T13:56:11Z","published":"2023-03-13T13:56:11Z","title":"Modality-Agnostic Debiasing for Single Domain Generalization","summary":"  Deep neural networks (DNNs) usually fail to generalize well to outside of\ndistribution (OOD) data, especially in the extreme case of single domain\ngeneralization (single-DG) that transfers DNNs from single domain to multiple\nunseen domains. Existing single-DG techniques commonly devise various\ndata-augmentation algorithms, and remould the multi-source domain\ngeneralization methodology to learn domain-generalized (semantic) features.\nNevertheless, these methods are typically modality-specific, thereby being only\napplicable to one single modality (e.g., image). In contrast, we target a\nversatile Modality-Agnostic Debiasing (MAD) framework for single-DG, that\nenables generalization for different modalities. Technically, MAD introduces a\nnovel two-branch classifier: a biased-branch encourages the classifier to\nidentify the domain-specific (superficial) features, and a general-branch\ncaptures domain-generalized features based on the knowledge from biased-branch.\nOur MAD is appealing in view that it is pluggable to most single-DG models. We\nvalidate the superiority of our MAD in a variety of single-DG scenarios with\ndifferent modalities, including recognition on 1D texts, 2D images, 3D point\nclouds, and semantic segmentation on 2D images. More remarkably, for\nrecognition on 3D point clouds and semantic segmentation on 2D images, MAD\nimproves DSU by 2.82\\% and 1.5\\% in accuracy and mIOU.\n","authors":["Sanqing Qu","Yingwei Pan","Guang Chen","Ting Yao","Changjun Jiang","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2303.07123v1.pdf","comment":"To appear in CVPR-2023"},{"id":"http://arxiv.org/abs/2303.04940v2","updated":"2023-03-13T13:56:09Z","published":"2023-03-08T23:23:44Z","title":"Non-aligned supervision for Real Image Dehazing","summary":"  Removing haze from real-world images is challenging due to unpredictable\nweather conditions, resulting in misaligned hazy and clear image pairs. In this\npaper, we propose a non-aligned supervision framework that consists of three\nnetworks - dehazing, airlight, and transmission. In particular, we explore a\nnon-alignment setting by utilizing a clear reference image that is not aligned\nwith the hazy input image to supervise the dehazing network through a\nmulti-scale reference loss that compares the features of the two images. Our\nsetting makes it easier to collect hazy/clear image pairs in real-world\nenvironments, even under conditions of misalignment and shift views. To\ndemonstrate this, we have created a new hazy dataset called \"Phone-Hazy\", which\nwas captured using mobile phones in both rural and urban areas. Additionally,\nwe present a mean and variance self-attention network to model the infinite\nairlight using dark channel prior as position guidance, and employ a channel\nattention network to estimate the three-channel transmission. Experimental\nresults show that our framework outperforms current state-of-the-art methods in\nthe real-world image dehazing. Phone-Hazy and code will be available at\nhttps://github.com/hello2377/NSDNet.\n","authors":["Junkai Fan","Fei Guo","Jianjun Qian","Xiang Li","Jun Li","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2303.04940v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.08071v3","updated":"2023-03-13T13:55:30Z","published":"2022-01-20T09:10:20Z","title":"Temporal Sentence Grounding in Videos: A Survey and Future Directions","summary":"  Temporal sentence grounding in videos (TSGV), \\aka natural language video\nlocalization (NLVL) or video moment retrieval (VMR), aims to retrieve a\ntemporal moment that semantically corresponds to a language query from an\nuntrimmed video. Connecting computer vision and natural language, TSGV has\ndrawn significant attention from researchers in both communities. This survey\nattempts to provide a summary of fundamental concepts in TSGV and current\nresearch status, as well as future research directions. As the background, we\npresent a common structure of functional components in TSGV, in a tutorial\nstyle: from feature extraction from raw video and language query, to answer\nprediction of the target moment. Then we review the techniques for multimodal\nunderstanding and interaction, which is the key focus of TSGV for effective\nalignment between the two modalities. We construct a taxonomy of TSGV\ntechniques and elaborate the methods in different categories with their\nstrengths and weaknesses. Lastly, we discuss issues with the current TSGV\nresearch and share our insights about promising research directions.\n","authors":["Hao Zhang","Aixin Sun","Wei Jing","Joey Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2201.08071v3.pdf","comment":"Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)"},{"id":"http://arxiv.org/abs/2303.07115v1","updated":"2023-03-13T13:47:57Z","published":"2023-03-13T13:47:57Z","title":"NeurEPDiff: Neural Operators to Predict Geodesics in Deformation Spaces","summary":"  This paper presents NeurEPDiff, a novel network to fast predict the geodesics\nin deformation spaces generated by a well known Euler-Poincar\\'e differential\nequation (EPDiff). To achieve this, we develop a neural operator that for the\nfirst time learns the evolving trajectory of geodesic deformations\nparameterized in the tangent space of diffeomorphisms(a.k.a velocity fields).\nIn contrast to previous methods that purely fit the training images, our\nproposed NeurEPDiff learns a nonlinear mapping function between the\ntime-dependent velocity fields. A composition of integral operators and smooth\nactivation functions is formulated in each layer of NeurEPDiff to effectively\napproximate such mappings. The fact that NeurEPDiff is able to rapidly provide\nthe numerical solution of EPDiff (given any initial condition) results in a\nsignificantly reduced computational cost of geodesic shooting of\ndiffeomorphisms in a high-dimensional image space. Additionally, the properties\nof discretiztion/resolution-invariant of NeurEPDiff make its performance\ngeneralizable to multiple image resolutions after being trained offline. We\ndemonstrate the effectiveness of NeurEPDiff in registering two image datasets:\n2D synthetic data and 3D brain resonance imaging (MRI). The registration\naccuracy and computational efficiency are compared with the state-of-the-art\ndiffeomophic registration algorithms with geodesic shooting.\n","authors":["Nian Wu","Miaomiao Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07110v1","updated":"2023-03-13T13:44:04Z","published":"2023-03-13T13:44:04Z","title":"Upcycling Models under Domain and Category Shift","summary":"  Deep neural networks (DNNs) often perform poorly in the presence of domain\nshift and category shift. How to upcycle DNNs and adapt them to the target task\nremains an important open problem. Unsupervised Domain Adaptation (UDA),\nespecially recently proposed Source-free Domain Adaptation (SFDA), has become a\npromising technology to address this issue. Nevertheless, existing SFDA methods\nrequire that the source domain and target domain share the same label space,\nconsequently being only applicable to the vanilla closed-set setting. In this\npaper, we take one step further and explore the Source-free Universal Domain\nAdaptation (SF-UniDA). The goal is to identify \"known\" data samples under both\ndomain and category shift, and reject those \"unknown\" data samples (not present\nin source classes), with only the knowledge from standard pre-trained source\nmodel. To this end, we introduce an innovative global and local clustering\nlearning technique (GLC). Specifically, we design a novel, adaptive one-vs-all\nglobal clustering algorithm to achieve the distinction across different target\nclasses and introduce a local k-NN clustering strategy to alleviate negative\ntransfer. We examine the superiority of our GLC on multiple benchmarks with\ndifferent category shift scenarios, including partial-set, open-set, and\nopen-partial-set DA. Remarkably, in the most challenging open-partial-set DA\nscenario, GLC outperforms UMAD by 14.8\\% on the VisDA benchmark. The code is\navailable at https://github.com/ispc-lab/GLC.\n","authors":["Sanqing Qu","Tianpei Zou","Florian Roehrbein","Cewu Lu","Guang Chen","Dacheng Tao","Changjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.07110v1.pdf","comment":"To appear in CVPR 2023. The code has been made public"},{"id":"http://arxiv.org/abs/2303.07100v1","updated":"2023-03-13T13:40:09Z","published":"2023-03-13T13:40:09Z","title":"A Feature-based Approach for the Recognition of Image Quality\n  Degradation in Automotive Applications","summary":"  Cameras play a crucial role in modern driver assistance systems and are an\nessential part of the sensor technology for automated driving. The quality of\nimages captured by in-vehicle cameras highly influences the performance of\nvisual perception systems. This paper presents a feature-based algorithm to\ndetect certain effects that can degrade image quality in automotive\napplications. The algorithm is based on an intelligent selection of significant\nfeatures. Due to the small number of features, the algorithm performs well even\nwith small data sets. Experiments with different data sets show that the\nalgorithm can detect soiling adhering to camera lenses and classify different\ntypes of image degradation.\n","authors":["Florian Bauer"],"pdf_url":"https://arxiv.org/pdf/2303.07100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07096v1","updated":"2023-03-13T13:30:59Z","published":"2023-03-13T13:30:59Z","title":"Prototype-based Embedding Network for Scene Graph Generation","summary":"  Current Scene Graph Generation (SGG) methods explore contextual information\nto predict relationships among entity pairs. However, due to the diverse visual\nappearance of numerous possible subject-object combinations, there is a large\nintra-class variation within each predicate category, e.g., \"man-eating-pizza,\ngiraffe-eating-leaf\", and the severe inter-class similarity between different\nclasses, e.g., \"man-holding-plate, man-eating-pizza\", in model's latent space.\nThe above challenges prevent current SGG methods from acquiring robust features\nfor reliable relation prediction. In this paper, we claim that the predicate's\ncategory-inherent semantics can serve as class-wise prototypes in the semantic\nspace for relieving the challenges. To the end, we propose the Prototype-based\nEmbedding Network (PE-Net), which models entities/predicates with\nprototype-aligned compact and distinctive representations and thereby\nestablishes matching between entity pairs and predicates in a common embedding\nspace for relation recognition. Moreover, Prototype-guided Learning (PL) is\nintroduced to help PE-Net efficiently learn such entitypredicate matching, and\nPrototype Regularization (PR) is devised to relieve the ambiguous\nentity-predicate matching caused by the predicate's semantic overlap. Extensive\nexperiments demonstrate that our method gains superior relation recognition\ncapability on SGG, achieving new state-of-the-art performances on both Visual\nGenome and Open Images datasets.\n","authors":["Chaofan Zheng","Xinyu Lyu","Lianli Gao","Bo Dai","Jingkuan Song"],"pdf_url":"https://arxiv.org/pdf/2303.07096v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2210.07182v6","updated":"2023-03-13T13:27:02Z","published":"2022-10-13T17:03:36Z","title":"PDEBENCH: An Extensive Benchmark for Scientific Machine Learning","summary":"  Machine learning-based modeling of physical systems has experienced increased\ninterest in recent years. Despite some impressive progress, there is still a\nlack of benchmarks for Scientific ML that are easy to use but still challenging\nand representative of a wide range of problems. We introduce PDEBench, a\nbenchmark suite of time-dependent simulation tasks based on Partial\nDifferential Equations (PDEs). PDEBench comprises both code and data to\nbenchmark the performance of novel machine learning models against both\nclassical numerical simulations and machine learning baselines. Our proposed\nset of benchmark problems contribute the following unique features: (1) A much\nwider range of PDEs compared to existing benchmarks, ranging from relatively\ncommon examples to more realistic and difficult problems; (2) much larger\nready-to-use datasets compared to prior work, comprising multiple simulation\nruns across a larger number of initial and boundary conditions and PDE\nparameters; (3) more extensible source codes with user-friendly APIs for data\ngeneration and baseline results with popular machine learning models (FNO,\nU-Net, PINN, Gradient-Based Inverse Method). PDEBench allows researchers to\nextend the benchmark freely for their own purposes using a standardized API and\nto compare the performance of new models to existing baseline methods. We also\npropose new evaluation metrics with the aim to provide a more holistic\nunderstanding of learning methods in the context of Scientific ML. With those\nmetrics we identify tasks which are challenging for recent ML methods and\npropose these tasks as future challenges for the community. The code is\navailable at https://github.com/pdebench/PDEBench.\n","authors":["Makoto Takamoto","Timothy Praditia","Raphael Leiteritz","Dan MacKinlay","Francesco Alesiani","Dirk Pflüger","Mathias Niepert"],"pdf_url":"https://arxiv.org/pdf/2210.07182v6.pdf","comment":"16 pages (main body) + 34 pages (supplemental material), accepted for\n  publication in NeurIPS 2022 Track Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2208.02998v3","updated":"2023-03-13T13:25:27Z","published":"2022-08-05T05:48:28Z","title":"Localized Sparse Incomplete Multi-view Clustering","summary":"  Incomplete multi-view clustering, which aims to solve the clustering problem\non the incomplete multi-view data with partial view missing, has received more\nand more attention in recent years. Although numerous methods have been\ndeveloped, most of the methods either cannot flexibly handle the incomplete\nmulti-view data with arbitrary missing views or do not consider the negative\nfactor of information imbalance among views. Moreover, some methods do not\nfully explore the local structure of all incomplete views. To tackle these\nproblems, this paper proposes a simple but effective method, named localized\nsparse incomplete multi-view clustering (LSIMVC). Different from the existing\nmethods, LSIMVC intends to learn a sparse and structured consensus latent\nrepresentation from the incomplete multi-view data by optimizing a sparse\nregularized and novel graph embedded multi-view matrix factorization model.\nSpecifically, in such a novel model based on the matrix factorization, a l1\nnorm based sparse constraint is introduced to obtain the sparse low-dimensional\nindividual representations and the sparse consensus representation. Moreover, a\nnovel local graph embedding term is introduced to learn the structured\nconsensus representation. Different from the existing works, our local graph\nembedding term aggregates the graph embedding task and consensus representation\nlearning task into a concise term. Furthermore, to reduce the imbalance factor\nof incomplete multi-view learning, an adaptive weighted learning scheme is\nintroduced to LSIMVC. Finally, an efficient optimization strategy is given to\nsolve the optimization problem of our proposed model. Comprehensive\nexperimental results performed on six incomplete multi-view databases verify\nthat the performance of our LSIMVC is superior to the state-of-the-art IMC\napproaches. The code is available in https://github.com/justsmart/LSIMVC.\n","authors":["Chengliang Liu","Zhihao Wu","Jie Wen","Chao Huang","Yong Xu"],"pdf_url":"https://arxiv.org/pdf/2208.02998v3.pdf","comment":"Published in IEEE Transactions on Multimedia (TMM). The code is\n  available at Github https://github.com/justsmart/LSIMVC"},{"id":"http://arxiv.org/abs/2303.07093v1","updated":"2023-03-13T13:23:57Z","published":"2023-03-13T13:23:57Z","title":"Weakly Unsupervised Domain Adaptation for Vestibular Schwannoma\n  Segmentation","summary":"  Vestibular schwannoma (VS) is a non-cancerous tumor located next to the ear\nthat can cause hearing loss. Most brain MRI images acquired from patients are\ncontrast-enhanced T1 (ceT1), with a growing interest in high-resolution T2\nimages (hrT2) to replace ceT1, which involves the use of a contrast agent. As\nhrT2 images are currently scarce, it is less likely to train robust machine\nlearning models to segment VS or other brain structures. In this work, we\npropose a weakly supervised machine learning approach that learns from only\nceT1 scans and adapts to segment two structures from hrT2 scans: the VS and the\ncochlea from the crossMoDA dataset. Our model 1) generates fake hrT2 scans from\nceT1 images and segmentation masks, 2) is trained using the fake hrT2 scans, 3)\npredicts the augmented real hrT2 scans, and 4) is retrained again using both\nthe fake and real hrT2. The final result of this model has been computed on an\nunseen testing dataset provided by the 2022 crossMoDA challenge organizers. The\nmean dice score and average symmetric surface distance (ASSD) are 0.78 and\n0.46, respectively. The predicted segmentation masks achieved a dice score of\n0.83 and an ASSD of 0.56 on the VS, and a dice score of 0.74 and an ASSD of\n0.35 on the cochleas.\n","authors":["Shahad Hardan","Hussain Alasmawi","Xiangjian Hou","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2303.07093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10916v3","updated":"2023-03-13T13:14:55Z","published":"2022-11-20T09:20:32Z","title":"ECM-OPCC: Efficient Context Model for Octree-based Point Cloud\n  Compression","summary":"  Recently, deep learning methods have shown promising results in point cloud\ncompression. For octree-based point cloud compression, previous works show that\nthe information of ancestor nodes and sibling nodes are equally important for\npredicting current node. However, those works either adopt insufficient context\nor bring intolerable decoding complexity (e.g. >600s). To address this problem,\nwe propose a sufficient yet efficient context model and design an efficient\ndeep learning codec for point clouds. Specifically, we first propose a\nwindow-constrained multi-group coding strategy to exploit the autoregressive\ncontext while maintaining decoding efficiency. Then, we propose a dual\ntransformer architecture to utilize the dependency of current node on its\nancestors and siblings. We also propose a random-masking pre-train method to\nenhance our model. Experimental results show that our approach achieves\nstate-of-the-art performance for both lossy and lossless point cloud\ncompression. Moreover, our multi-group coding strategy saves 98% decoding time\ncompared with previous octree-based compression method.\n","authors":["Yiqi Jin","Ziyu Zhu","Tongda Xu","Yuhuan Lin","Yan Wang"],"pdf_url":"https://arxiv.org/pdf/2211.10916v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07084v1","updated":"2023-03-13T13:08:20Z","published":"2023-03-13T13:08:20Z","title":"The challenge of representation learning: Improved accuracy in deep\n  vision models does not come with better predictions of perceptual similarity","summary":"  Over the last years, advancements in deep learning models for computer vision\nhave led to a dramatic improvement in their image classification accuracy.\nHowever, models with a higher accuracy in the task they were trained on do not\nnecessarily develop better image representations that allow them to also\nperform better in other tasks they were not trained on. In order to investigate\nthe representation learning capabilities of prominent high-performing computer\nvision models, we investigated how well they capture various indices of\nperceptual similarity from large-scale behavioral datasets. We find that higher\nimage classification accuracy rates are not associated with a better\nperformance on these datasets, and in fact we observe no improvement in\nperformance since GoogLeNet (released 2015) and VGG-M (released 2014). We\nspeculate that more accurate classification may result from hyper-engineering\ntowards very fine-grained distinctions between highly similar classes, which\ndoes not incentivize the models to capture overall perceptual similarities.\n","authors":["Fritz Günther","Marco Marelli","Marco Alessandro Petilli"],"pdf_url":"https://arxiv.org/pdf/2303.07084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07080v1","updated":"2023-03-13T13:05:33Z","published":"2023-03-13T13:05:33Z","title":"Bag of Tricks with Quantized Convolutional Neural Networks for image\n  classification","summary":"  Deep neural networks have been proven effective in a wide range of tasks.\nHowever, their high computational and memory costs make them impractical to\ndeploy on resource-constrained devices. To address this issue, quantization\nschemes have been proposed to reduce the memory footprint and improve inference\nspeed. While numerous quantization methods have been proposed, they lack\nsystematic analysis for their effectiveness. To bridge this gap, we collect and\nimprove existing quantization methods and propose a gold guideline for\npost-training quantization. We evaluate the effectiveness of our proposed\nmethod with two popular models, ResNet50 and MobileNetV2, on the ImageNet\ndataset. By following our guidelines, no accuracy degradation occurs even after\ndirectly quantizing the model to 8-bits without additional training. A\nquantization-aware training based on the guidelines can further improve the\naccuracy in lower-bits quantization. Moreover, we have integrated a multi-stage\nfine-tuning strategy that works harmoniously with existing pruning techniques\nto reduce costs even further. Remarkably, our results reveal that a quantized\nMobileNetV2 with 30\\% sparsity actually surpasses the performance of the\nequivalent full-precision model, underscoring the effectiveness and resilience\nof our proposed scheme.\n","authors":["Jie Hu","Mengze Zeng","Enhua Wu"],"pdf_url":"https://arxiv.org/pdf/2303.07080v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07077v1","updated":"2023-03-13T12:59:53Z","published":"2023-03-13T12:59:53Z","title":"Spatial Attention and Syntax Rule Enhanced Tree Decoder for Offine\n  Handwritten Mathematical Expression Recognition","summary":"  Offline Handwritten Mathematical Expression Recognition (HMER) has been\ndramatically advanced recently by employing tree decoders as part of the\nencoder-decoder method. Despite the tree decoder-based methods regard the\nexpressions as a tree and parse 2D spatial structure to the tree nodes\nsequence, the performance of existing works is still poor due to the inevitable\ntree nodes prediction errors. Besides, they lack syntax rules to regulate the\noutput of expressions. In this paper, we propose a novel model called Spatial\nAttention and Syntax Rule Enhanced Tree Decoder (SS-TD), which is equipped with\nspatial attention mechanism to alleviate the prediction error of tree structure\nand use syntax masks (obtained from the transformation of syntax rules) to\nconstrain the occurrence of ungrammatical mathematical expression. In this way,\nour model can effectively describe tree structure and increase the accuracy of\noutput expression. Experiments show that SS-TD achieves better recognition\nperformance than prior models on CROHME 14/16/19 datasets, demonstrating the\neffectiveness of our model.\n","authors":["Zihao Lin","Jinrong Li","Fan Yang","Shuangping Huang","Xu Yang","Jianmin Lin","Ming Yang"],"pdf_url":"https://arxiv.org/pdf/2303.07077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07065v1","updated":"2023-03-13T12:39:59Z","published":"2023-03-13T12:39:59Z","title":"MSINet: Twins Contrastive Search of Multi-Scale Interaction for Object\n  ReID","summary":"  Neural Architecture Search (NAS) has been increasingly appealing to the\nsociety of object Re-Identification (ReID), for that task-specific\narchitectures significantly improve the retrieval performance. Previous works\nexplore new optimizing targets and search spaces for NAS ReID, yet they neglect\nthe difference of training schemes between image classification and ReID. In\nthis work, we propose a novel Twins Contrastive Mechanism (TCM) to provide more\nappropriate supervision for ReID architecture search. TCM reduces the category\noverlaps between the training and validation data, and assists NAS in\nsimulating real-world ReID training schemes. We then design a Multi-Scale\nInteraction (MSI) search space to search for rational interaction operations\nbetween multi-scale features. In addition, we introduce a Spatial Alignment\nModule (SAM) to further enhance the attention consistency confronted with\nimages from different sources. Under the proposed NAS scheme, a specific\narchitecture is automatically searched, named as MSINet. Extensive experiments\ndemonstrate that our method surpasses state-of-the-art ReID methods on both\nin-domain and cross-domain scenarios. Source code available in\nhttps://github.com/vimar-gu/MSINet.\n","authors":["Jianyang Gu","Kai Wang","Hao Luo","Chen Chen","Wei Jiang","Yuqiang Fang","Shanghang Zhang","Yang You","Jian Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.07065v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.07064v1","updated":"2023-03-13T12:38:07Z","published":"2023-03-13T12:38:07Z","title":"A Generalized Multi-Modal Fusion Detection Framework","summary":"  LiDAR point clouds have become the most common data source in autonomous\ndriving. However, due to the sparsity of point clouds, accurate and reliable\ndetection cannot be achieved in specific scenarios. Because of their\ncomplementarity with point clouds, images are getting increasing attention.\nAlthough with some success, existing fusion methods either perform hard fusion\nor do not fuse in a direct manner. In this paper, we propose a generic 3D\ndetection framework called MMFusion, using multi-modal features. The framework\naims to achieve accurate fusion between LiDAR and images to improve 3D\ndetection in complex scenes. Our framework consists of two separate streams:\nthe LiDAR stream and the camera stream, which can be compatible with any\nsingle-modal feature extraction network. The Voxel Local Perception Module in\nthe LiDAR stream enhances local feature representation, and then the\nMulti-modal Feature Fusion Module selectively combines feature output from\ndifferent streams to achieve better fusion. Extensive experiments have shown\nthat our framework not only outperforms existing benchmarks but also improves\ntheir detection, especially for detecting cyclists and pedestrians on KITTI\nbenchmarks, with strong robustness and generalization capabilities. Hopefully,\nour work will stimulate more research into multi-modal fusion for autonomous\ndriving tasks.\n","authors":["Leichao Cui","Xiuxian Li","Min Meng","Xiaoyu Mo"],"pdf_url":"https://arxiv.org/pdf/2303.07064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01162v4","updated":"2023-03-13T12:37:15Z","published":"2023-02-02T15:37:46Z","title":"Get3DHuman: Lifting StyleGAN-Human into a 3D Generative Model using\n  Pixel-aligned Reconstruction Priors","summary":"  Fast generation of high-quality 3D digital humans is important to a vast\nnumber of applications ranging from entertainment to professional concerns.\nRecent advances in differentiable rendering have enabled the training of 3D\ngenerative models without requiring 3D ground truths. However, the quality of\nthe generated 3D humans still has much room to improve in terms of both\nfidelity and diversity. In this paper, we present Get3DHuman, a novel 3D human\nframework that can significantly boost the realism and diversity of the\ngenerated outcomes by only using a limited budget of 3D ground-truth data. Our\nkey observation is that the 3D generator can profit from human-related priors\nlearned through 2D human generators and 3D reconstructors. Specifically, we\nbridge the latent space of Get3DHuman with that of StyleGAN-Human via a\nspecially-designed prior network, where the input latent code is mapped to the\nshape and texture feature volumes spanned by the pixel-aligned 3D\nreconstructor. The outcomes of the prior network are then leveraged as the\nsupervisory signals for the main generator network. To ensure effective\ntraining, we further propose three tailored losses applied to the generated\nfeature volumes and the intermediate feature maps. Extensive experiments\ndemonstrate that Get3DHuman greatly outperforms the other state-of-the-art\napproaches and can support a wide range of applications including shape\ninterpolation, shape re-texturing, and single-view reconstruction through\nlatent inversion.\n","authors":["Zhangyang Xiong","Di Kang","Derong Jin","Weikai Chen","Linchao Bao","Shuguang Cui","Xiaoguang Han"],"pdf_url":"https://arxiv.org/pdf/2302.01162v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.06592v2","updated":"2023-03-13T12:21:16Z","published":"2021-12-13T12:18:43Z","title":"CR-FIQA: Face Image Quality Assessment by Learning Sample Relative\n  Classifiability","summary":"  The quality of face images significantly influences the performance of\nunderlying face recognition algorithms. Face image quality assessment (FIQA)\nestimates the utility of the captured image in achieving reliable and accurate\nrecognition performance. In this work, we propose a novel learning paradigm\nthat learns internal network observations during the training process. Based on\nthat, our proposed CR-FIQA uses this paradigm to estimate the face image\nquality of a sample by predicting its relative classifiability. This\nclassifiability is measured based on the allocation of the training sample\nfeature representation in angular space with respect to its class center and\nthe nearest negative class center. We experimentally illustrate the correlation\nbetween the face image quality and the sample relative classifiability. As such\nproperty is only observable for the training dataset, we propose to learn this\nproperty from the training dataset and utilize it to predict the quality\nmeasure on unseen samples. This training is performed simultaneously while\noptimizing the class centers by an angular margin penalty-based softmax loss\nused for face recognition model training. Through extensive evaluation\nexperiments on eight benchmarks and four face recognition models, we\ndemonstrate the superiority of our proposed CR-FIQA over state-of-the-art\n(SOTA) FIQA algorithms.\n","authors":["Fadi Boutros","Meiling Fang","Marcel Klemt","Biying Fu","Naser Damer"],"pdf_url":"https://arxiv.org/pdf/2112.06592v2.pdf","comment":"Accepted at the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition 2023 (CVPR2023)"},{"id":"http://arxiv.org/abs/2303.07035v1","updated":"2023-03-13T11:54:16Z","published":"2023-03-13T11:54:16Z","title":"FireRisk: A Remote Sensing Dataset for Fire Risk Assessment with\n  Benchmarks Using Supervised and Self-supervised Learning","summary":"  In recent decades, wildfires, as widespread and extremely destructive natural\ndisasters, have caused tremendous property losses and fatalities, as well as\nextensive damage to forest ecosystems. Many fire risk assessment projects have\nbeen proposed to prevent wildfires, but GIS-based methods are inherently\nchallenging to scale to different geographic areas due to variations in data\ncollection and local conditions. Inspired by the abundance of publicly\navailable remote sensing projects and the burgeoning development of deep\nlearning in computer vision, our research focuses on assessing fire risk using\nremote sensing imagery.\n  In this work, we propose a novel remote sensing dataset, FireRisk, consisting\nof 7 fire risk classes with a total of 91872 labelled images for fire risk\nassessment. This remote sensing dataset is labelled with the fire risk classes\nsupplied by the Wildfire Hazard Potential (WHP) raster dataset, and remote\nsensing images are collected using the National Agriculture Imagery Program\n(NAIP), a high-resolution remote sensing imagery program. On FireRisk, we\npresent benchmark performance for supervised and self-supervised\nrepresentations, with Masked Autoencoders (MAE) pre-trained on ImageNet1k\nachieving the highest classification accuracy, 65.29%.\n  This remote sensing dataset, FireRisk, provides a new direction for fire risk\nassessment, and we make it publicly available on\nhttps://github.com/CharmonyShen/FireRisk.\n","authors":["Shuchang Shen","Sachith Seneviratne","Xinye Wanyan","Michael Kirley"],"pdf_url":"https://arxiv.org/pdf/2303.07035v1.pdf","comment":"10 pages, 6 figures, 1 table, 1 equation"},{"id":"http://arxiv.org/abs/2303.07034v1","updated":"2023-03-13T11:53:40Z","published":"2023-03-13T11:53:40Z","title":"Pretrained ViTs Yield Versatile Representations For Medical Images","summary":"  Convolutional Neural Networks (CNNs) have reigned for a decade as the de\nfacto approach to automated medical image diagnosis, pushing the\nstate-of-the-art in classification, detection and segmentation tasks. Over the\nlast years, vision transformers (ViTs) have appeared as a competitive\nalternative to CNNs, yielding impressive levels of performance in the natural\nimage domain, while possessing several interesting properties that could prove\nbeneficial for medical imaging tasks. In this work, we explore the benefits and\ndrawbacks of transformer-based models for medical image classification. We\nconduct a series of experiments on several standard 2D medical image benchmark\ndatasets and tasks. Our findings show that, while CNNs perform better if\ntrained from scratch, off-the-shelf vision transformers can perform on par with\nCNNs when pretrained on ImageNet, both in a supervised and self-supervised\nsetting, rendering them as a viable alternative to CNNs.\n","authors":["Christos Matsoukas","Johan Fredin Haslum","Magnus Söderberg","Kevin Smith"],"pdf_url":"https://arxiv.org/pdf/2303.07034v1.pdf","comment":"Extended version of \"Is it Time to Replace CNNs with Transformers for\n  Medical Images?\" (Matsoukas et al. 2022) originally published at the ICCV\n  2021 Workshop on Computer Vision for Automated Medical Diagnosis"},{"id":"http://arxiv.org/abs/2303.07033v1","updated":"2023-03-13T11:47:24Z","published":"2023-03-13T11:47:24Z","title":"SelfPromer: Self-Prompt Dehazing Transformers with Depth-Consistency","summary":"  This work presents an effective depth-consistency self-prompt Transformer for\nimage dehazing. It is motivated by an observation that the estimated depths of\nan image with haze residuals and its clear counterpart vary. Enforcing the\ndepth consistency of dehazed images with clear ones, therefore, is essential\nfor dehazing. For this purpose, we develop a prompt based on the features of\ndepth differences between the hazy input images and corresponding clear\ncounterparts that can guide dehazing models for better restoration.\nSpecifically, we first apply deep features extracted from the input images to\nthe depth difference features for generating the prompt that contains the haze\nresidual information in the input. Then we propose a prompt embedding module\nthat is designed to perceive the haze residuals, by linearly adding the prompt\nto the deep features. Further, we develop an effective prompt attention module\nto pay more attention to haze residuals for better removal. By incorporating\nthe prompt, prompt embedding, and prompt attention into an encoder-decoder\nnetwork based on VQGAN, we can achieve better perception quality. As the depths\nof clear images are not available at inference, and the dehazed images with\none-time feed-forward execution may still contain a portion of haze residuals,\nwe propose a new continuous self-prompt inference that can iteratively correct\nthe dehazing model towards better haze-free image generation. Extensive\nexperiments show that our method performs favorably against the\nstate-of-the-art approaches on both synthetic and real-world datasets in terms\nof perception metrics including NIQE, PI, and PIQE.\n","authors":["Cong Wang","Jinshan Pan","Wanyu Lin","Jiangxin Dong","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2303.07033v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07016v1","updated":"2023-03-13T11:25:32Z","published":"2023-03-13T11:25:32Z","title":"HOOV: Hand Out-Of-View Tracking for Proprioceptive Interaction using\n  Inertial Sensing","summary":"  Current Virtual Reality systems are designed for interaction under visual\ncontrol. Using built-in cameras, headsets track the user's hands or hand-held\ncontrollers while they are inside the field of view. Current systems thus\nignore the user's interaction with off-screen content -- virtual objects that\nthe user could quickly access through proprioception without requiring\nlaborious head motions to bring them into focus. In this paper, we present\nHOOV, a wrist-worn sensing method that allows VR users to interact with objects\noutside their field of view. Based on the signals of a single wrist-worn\ninertial sensor, HOOV continuously estimates the user's hand position in\n3-space to complement the headset's tracking as the hands leave the tracking\nrange. Our novel data-driven method predicts hand positions and trajectories\nfrom just the continuous estimation of hand orientation, which by itself is\nstable based solely on inertial observations. Our inertial sensing\nsimultaneously detects finger pinching to register off-screen selection events,\nconfirms them using a haptic actuator inside our wrist device, and thus allows\nusers to select, grab, and drop virtual content. We compared HOOV's performance\nwith a camera-based optical motion capture system in two folds. In the first\nevaluation, participants interacted based on tracking information from the\nmotion capture system to assess the accuracy of their proprioceptive input,\nwhereas in the second, they interacted based on HOOV's real-time estimations.\nWe found that HOOV's target-agnostic estimations had a mean tracking error of\n7.7 cm, which allowed participants to reliably access virtual objects around\ntheir body without first bringing them into focus. We demonstrate several\napplications that leverage the larger input space HOOV opens up for quick\nproprioceptive interaction, and conclude by discussing the potential of our\ntechnique.\n","authors":["Paul Streli","Rayan Armani","Yi Fei Cheng","Christian Holz"],"pdf_url":"https://arxiv.org/pdf/2303.07016v1.pdf","comment":"Accepted at 2023 CHI Conference on Human Factors in Computing Systems"},{"id":"http://arxiv.org/abs/2212.00935v2","updated":"2023-03-13T11:24:49Z","published":"2022-12-02T02:47:30Z","title":"Dunhuang murals contour generation network based on convolution and\n  self-attention fusion","summary":"  Dunhuang murals are a collection of Chinese style and national style, forming\na self-contained Chinese-style Buddhist art. It has very high historical and\ncultural value and research significance. Among them, the lines of Dunhuang\nmurals are highly general and expressive. It reflects the character's\ndistinctive character and complex inner emotions. Therefore, the outline\ndrawing of murals is of great significance to the research of Dunhuang Culture.\nThe contour generation of Dunhuang murals belongs to image edge detection,\nwhich is an important branch of computer vision, aims to extract salient\ncontour information in images. Although convolution-based deep learning\nnetworks have achieved good results in image edge extraction by exploring the\ncontextual and semantic features of images. However, with the enlargement of\nthe receptive field, some local detail information is lost. This makes it\nimpossible for them to generate reasonable outline drawings of murals. In this\npaper, we propose a novel edge detector based on self-attention combined with\nconvolution to generate line drawings of Dunhuang murals. Compared with\nexisting edge detection methods, firstly, a new residual self-attention and\nconvolution mixed module (Ramix) is proposed to fuse local and global features\nin feature maps. Secondly, a novel densely connected backbone extraction\nnetwork is designed to efficiently propagate rich edge feature information from\nshallow layers into deep layers. Compared with existing methods, it is shown on\ndifferent public datasets that our method is able to generate sharper and\nricher edge maps. In addition, testing on the Dunhuang mural dataset shows that\nour method can achieve very competitive performance.\n","authors":["Baokai Liu","Fengjie He","Shiqiang Du","Kaiwu Zhang","Jianhua Wang"],"pdf_url":"https://arxiv.org/pdf/2212.00935v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07014v1","updated":"2023-03-13T11:22:37Z","published":"2023-03-13T11:22:37Z","title":"Reference-Guided Large-Scale Face Inpainting with Identity and Texture\n  Control","summary":"  Face inpainting aims at plausibly predicting missing pixels of face images\nwithin a corrupted region. Most existing methods rely on generative models\nlearning a face image distribution from a big dataset, which produces\nuncontrollable results, especially with large-scale missing regions. To\nintroduce strong control for face inpainting, we propose a novel\nreference-guided face inpainting method that fills the large-scale missing\nregion with identity and texture control guided by a reference face image.\nHowever, generating high-quality results under imposing two control signals is\nchallenging. To tackle such difficulty, we propose a dual control one-stage\nframework that decouples the reference image into two levels for flexible\ncontrol: High-level identity information and low-level texture information,\nwhere the identity information figures out the shape of the face and the\ntexture information depicts the component-aware texture. To synthesize\nhigh-quality results, we design two novel modules referred to as Half-AdaIN and\nComponent-Wise Style Injector (CWSI) to inject the two kinds of control\ninformation into the inpainting processing. Our method produces realistic\nresults with identity and texture control faithful to reference images. To the\nbest of our knowledge, it is the first work to concurrently apply identity and\ncomponent-level controls in face inpainting to promise more precise and\ncontrollable results. Code is available at\nhttps://github.com/WuyangLuo/RefFaceInpainting\n","authors":["Wuyang Luo","Su Yang","Weishan Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07014v1.pdf","comment":"accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology"},{"id":"http://arxiv.org/abs/2303.07012v1","updated":"2023-03-13T11:18:41Z","published":"2023-03-13T11:18:41Z","title":"AGTGAN: Unpaired Image Translation for Photographic Ancient Character\n  Generation","summary":"  The study of ancient writings has great value for archaeology and philology.\nEssential forms of material are photographic characters, but manual\nphotographic character recognition is extremely time-consuming and\nexpertise-dependent. Automatic classification is therefore greatly desired.\nHowever, the current performance is limited due to the lack of annotated data.\nData generation is an inexpensive but useful solution for data scarcity.\nNevertheless, the diverse glyph shapes and complex background textures of\nphotographic ancient characters make the generation task difficult, leading to\nthe unsatisfactory results of existing methods. In this paper, we propose an\nunsupervised generative adversarial network called AGTGAN. By the explicit\nglobal and local glyph shape style modeling followed by the stroke-aware\ntexture transfer, as well as an associate adversarial learning mechanism, our\nmethod can generate characters with diverse glyphs and realistic textures. We\nevaluate our approach on the photographic ancient character datasets, e.g.,\nOBC306 and CSDD. Our method outperforms the state-of-the-art approaches in\nvarious metrics and performs much better in terms of the diversity and\nauthenticity of generated samples. With our generated images, experiments on\nthe largest photographic oracle bone character dataset show that our method can\nachieve a significant increase in classification accuracy, up to 16.34%.\n","authors":["Hongxiang Huang","Daihui Yang","Gang Dai","Zhen Han","Yuyi Wang","Kin-Man Lam","Fan Yang","Shuangping Huang","Yongge Liu","Mengchao He"],"pdf_url":"https://arxiv.org/pdf/2303.07012v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07011v1","updated":"2023-03-13T11:11:41Z","published":"2023-03-13T11:11:41Z","title":"OSIS: Efficient One-stage Network for 3D Instance Segmentation","summary":"  Current 3D instance segmentation models generally use multi-stage methods to\nextract instance objects, including clustering, feature extraction, and\npost-processing processes. However, these multi-stage approaches rely on\nhyperparameter settings and hand-crafted processes, which restrict the\ninference speed of the model. In this paper, we propose a new 3D point cloud\ninstance segmentation network, named OSIS. OSIS is a one-stage network, which\ndirectly segments instances from 3D point cloud data using neural network. To\nsegment instances directly from the network, we propose an instance decoder,\nwhich decodes instance features from the network into instance segments. Our\nproposed OSIS realizes the end-to-end training by bipartite matching,\ntherefore, our network does not require computationally expensive\npost-processing steps such as non maximum suppression (NMS) and clustering\nduring inference. The results show that our network finally achieves excellent\nperformance in the commonly used indoor scene instance segmentation dataset,\nand the inference speed of our network is only an average of 138ms per scene,\nwhich substantially exceeds the previous fastest method.\n","authors":["Chuan Tang","Xi Yang"],"pdf_url":"https://arxiv.org/pdf/2303.07011v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10420v2","updated":"2023-03-13T11:00:33Z","published":"2023-02-21T03:16:22Z","title":"HCGMNET: A Hierarchical Change Guiding Map Network For Change Detection","summary":"  Very-high-resolution (VHR) remote sensing (RS) image change detection (CD)\nhas been a challenging task for its very rich spatial information and sample\nimbalance problem. In this paper, we have proposed a hierarchical change\nguiding map network (HCGMNet) for change detection. The model uses hierarchical\nconvolution operations to extract multiscale features, continuously merges\nmulti-scale features layer by layer to improve the expression of global and\nlocal information, and guides the model to gradually refine edge features and\ncomprehensive performance by a change guide module (CGM), which is a\nself-attention with changing guide map. Extensive experiments on two CD\ndatasets show that the proposed HCGMNet architecture achieves better CD\nperformance than existing state-of-the-art (SOTA) CD methods.\n","authors":["Chengxi Han","Chen Wu","Bo Du"],"pdf_url":"https://arxiv.org/pdf/2302.10420v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06999v1","updated":"2023-03-13T10:54:52Z","published":"2023-03-13T10:54:52Z","title":"Identifying Label Errors in Object Detection Datasets by Loss Inspection","summary":"  Labeling datasets for supervised object detection is a dull and\ntime-consuming task. Errors can be easily introduced during annotation and\noverlooked during review, yielding inaccurate benchmarks and performance\ndegradation of deep neural networks trained on noisy labels. In this work, we\nfor the first time introduce a benchmark for label error detection methods on\nobject detection datasets as well as a label error detection method and a\nnumber of baselines. We simulate four different types of randomly introduced\nlabel errors on train and test sets of well-labeled object detection datasets.\nFor our label error detection method we assume a two-stage object detector to\nbe given and consider the sum of both stages' classification and regression\nlosses. The losses are computed with respect to the predictions and the noisy\nlabels including simulated label errors, aiming at detecting the latter. We\ncompare our method to three baselines: a naive one without deep learning, the\nobject detector's score and the entropy of the classification softmax\ndistribution. We outperform all baselines and demonstrate that among the\nconsidered methods, ours is the only one that detects label errors of all four\ntypes efficiently. Furthermore, we detect real label errors a) on commonly used\ntest datasets in object detection and b) on a proprietary dataset. In both\ncases we achieve low false positives rates, i.e., when considering 200\nproposals from our method, we detect label errors with a precision for a) of up\nto 71.5% and for b) with 97%.\n","authors":["Marius Schubert","Tobias Riedlinger","Karsten Kahl","Daniel Kröll","Sebastian Schoenen","Siniša Šegvić","Matthias Rottmann"],"pdf_url":"https://arxiv.org/pdf/2303.06999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06994v1","updated":"2023-03-13T10:49:59Z","published":"2023-03-13T10:49:59Z","title":"Synthesizing Realistic Image Restoration Training Pairs: A Diffusion\n  Approach","summary":"  In supervised image restoration tasks, one key issue is how to obtain the\naligned high-quality (HQ) and low-quality (LQ) training image pairs.\nUnfortunately, such HQ-LQ training pairs are hard to capture in practice, and\nhard to synthesize due to the complex unknown degradation in the wild. While\nseveral sophisticated degradation models have been manually designed to\nsynthesize LQ images from their HQ counterparts, the distribution gap between\nthe synthesized and real-world LQ images remains large. We propose a new\napproach to synthesizing realistic image restoration training pairs using the\nemerging denoising diffusion probabilistic model (DDPM).\n  First, we train a DDPM, which could convert a noisy input into the desired LQ\nimage, with a large amount of collected LQ images, which define the target data\ndistribution. Then, for a given HQ image, we synthesize an initial LQ image by\nusing an off-the-shelf degradation model, and iteratively add proper Gaussian\nnoises to it. Finally, we denoise the noisy LQ image using the pre-trained DDPM\nto obtain the final LQ image, which falls into the target distribution of\nreal-world LQ images. Thanks to the strong capability of DDPM in distribution\napproximation, the synthesized HQ-LQ image pairs can be used to train robust\nmodels for real-world image restoration tasks, such as blind face image\nrestoration and blind image super-resolution. Experiments demonstrated the\nsuperiority of our proposed approach to existing degradation models. Code and\ndata will be released.\n","authors":["Tao Yang","Peiran Ren","Xuansong xie","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.06994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.17042v2","updated":"2023-03-13T10:31:11Z","published":"2022-11-30T14:43:35Z","title":"Spatio-Temporal Crop Aggregation for Video Representation Learning","summary":"  We propose Spatio-temporal Crop Aggregation for video representation LEarning\n(SCALE), a novel method that enjoys high scalability at both training and\ninference time. Our model builds long-range video features by learning from\nsets of video clip-level features extracted with a pre-trained backbone. To\ntrain the model, we propose a self-supervised objective consisting of masked\nclip feature prediction. We apply sparsity to both the input, by extracting a\nrandom set of video clips, and to the loss function, by only reconstructing the\nsparse inputs. Moreover, we use dimensionality reduction by working in the\nlatent space of a pre-trained backbone applied to single video clips. These\ntechniques make our method not only extremely efficient to train but also\nhighly effective in transfer learning. We demonstrate that our video\nrepresentation yields state-of-the-art performance with linear, non-linear, and\nKNN probing on common action classification and video understanding datasets.\n","authors":["Sepehr Sameni","Simon Jenni","Paolo Favaro"],"pdf_url":"https://arxiv.org/pdf/2211.17042v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14310v2","updated":"2023-03-13T10:25:22Z","published":"2022-11-25T18:59:54Z","title":"Efficient 3D Reconstruction, Streaming and Visualization of Static and\n  Dynamic Scene Parts for Multi-client Live-telepresence in Large-scale\n  Environments","summary":"  Despite the impressive progress of telepresence systems for room-scale scenes\nwith static and dynamic scene entities, expanding their capabilities to\nscenarios with larger dynamic environments beyond a fixed size of a few\nsquare-meters remains challenging.\n  In this paper, we aim at sharing 3D live-telepresence experiences in\nlarge-scale environments beyond room scale with both static and dynamic scene\nentities at practical bandwidth requirements only based on light-weight scene\ncapture with a single moving consumer-grade RGB-D camera. To this end, we\npresent a system which is built upon a novel hybrid volumetric scene\nrepresentation in terms of the combination of a voxel-based scene\nrepresentation for the static contents, that not only stores the reconstructed\nsurface geometry but also contains information about the object semantics as\nwell as their accumulated dynamic movement over time, and a point-cloud-based\nrepresentation for dynamic scene parts, where the respective separation from\nstatic parts is achieved based on semantic and instance information extracted\nfor the input frames. With an independent yet simultaneous streaming of both\nstatic and dynamic content, where we seamlessly integrate potentially moving\nbut currently static scene entities in the static model until they are becoming\ndynamic again, as well as the fusion of static and dynamic data at the remote\nclient, our system is able to achieve VR-based live-telepresence at close to\nreal-time rates. Our evaluation demonstrates the potential of our novel\napproach in terms of visual quality, performance, and ablation studies\nregarding involved design choices.\n","authors":["Leif Van Holland","Patrick Stotko","Stefan Krumpen","Reinhard Klein","Michael Weinmann"],"pdf_url":"https://arxiv.org/pdf/2211.14310v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.04788v2","updated":"2023-03-13T10:13:00Z","published":"2022-04-10T22:58:02Z","title":"Representation Learning by Detecting Incorrect Location Embeddings","summary":"  In this paper, we introduce a novel self-supervised learning (SSL) loss for\nimage representation learning. There is a growing belief that generalization in\ndeep neural networks is linked to their ability to discriminate object shapes.\nSince object shape is related to the location of its parts, we propose to\ndetect those that have been artificially misplaced. We represent object parts\nwith image tokens and train a ViT to detect which token has been combined with\nan incorrect positional embedding. We then introduce sparsity in the inputs to\nmake the model more robust to occlusions and to speed up the training. We call\nour method DILEMMA, which stands for Detection of Incorrect Location EMbeddings\nwith MAsked inputs. We apply DILEMMA to MoCoV3, DINO and SimCLR and show an\nimprovement in their performance of respectively 4.41%, 3.97%, and 0.5% under\nthe same training time and with a linear probing transfer on ImageNet-1K. We\nalso show full fine-tuning improvements of MAE combined with our method on\nImageNet-100. We evaluate our method via fine-tuning on common SSL benchmarks.\nMoreover, we show that when downstream tasks are strongly reliant on shape\n(such as in the YOGA-82 pose dataset), our pre-trained features yield a\nsignificant gain over prior work.\n","authors":["Sepehr Sameni","Simon Jenni","Paolo Favaro"],"pdf_url":"https://arxiv.org/pdf/2204.04788v2.pdf","comment":"accepted at AAAI2023, https://github.com/Separius/DILEMMA"},{"id":"http://arxiv.org/abs/2303.06088v2","updated":"2023-03-13T10:05:01Z","published":"2023-03-10T17:09:04Z","title":"Improving Domain-Invariance in Self-Supervised Learning via Batch Styles\n  Standardization","summary":"  The recent rise of Self-Supervised Learning (SSL) as one of the preferred\nstrategies for learning with limited labeled data, and abundant unlabeled data\nhas led to the widespread use of these models. They are usually pretrained,\nfinetuned, and evaluated on the same data distribution, i.e., within an\nin-distribution setting. However, they tend to perform poorly in\nout-of-distribution evaluation scenarios, a challenge that Unsupervised Domain\nGeneralization (UDG) seeks to address.\n  This paper introduces a novel method to standardize the styles of images in a\nbatch. Batch styles standardization, relying on Fourier-based augmentations,\npromotes domain invariance in SSL by preventing spurious correlations from\nleaking into the features. The combination of batch styles standardization with\nthe well-known contrastive-based method SimCLR leads to a novel UDG method\nnamed CLaSSy ($\\textbf{C}$ontrastive $\\textbf{L}$e$\\textbf{a}$rning with\n$\\textbf{S}$tandardized $\\textbf{S}$t$\\textbf{y}$les). CLaSSy offers serious\nadvantages over prior methods, as it does not rely on domain labels and is\nscalable to handle a large number of domains. Experimental results on various\nUDG datasets demonstrate the superior performance of CLaSSy compared to\nexisting UDG methods. Finally, the versatility of the proposed batch styles\nstandardization is demonstrated by extending respectively the contrastive-based\nand non-contrastive-based SSL methods, SWaV and MSN, while considering\ndifferent backbone architectures (convolutional-based, transformers-based).\n","authors":["Marin Scalbert","Maria Vakalopoulou","Florent Couzinié-Devy"],"pdf_url":"https://arxiv.org/pdf/2303.06088v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10984v2","updated":"2023-03-13T09:50:01Z","published":"2022-11-20T14:08:47Z","title":"DAQE: Enhancing the Quality of Compressed Images by Exploiting the\n  Inherent Characteristic of Defocus","summary":"  Image defocus is inherent in the physics of image formation caused by the\noptical aberration of lenses, providing plentiful information on image quality.\nUnfortunately, existing quality enhancement approaches for compressed images\nneglect the inherent characteristic of defocus, resulting in inferior\nperformance. This paper finds that in compressed images, significantly\ndefocused regions have better compression quality, and two regions with\ndifferent defocus values possess diverse texture patterns. These observations\nmotivate our defocus-aware quality enhancement (DAQE) approach. Specifically,\nwe propose a novel dynamic region-based deep learning architecture of the DAQE\napproach, which considers the regionwise defocus difference of compressed\nimages in two aspects. (1) The DAQE approach employs fewer computational\nresources to enhance the quality of significantly defocused regions and more\nresources to enhance the quality of other regions; (2) The DAQE approach learns\nto separately enhance diverse texture patterns for regions with different\ndefocus values, such that texture-specific enhancement can be achieved.\nExtensive experiments validate the superiority of our DAQE approach over\nstate-of-the-art approaches in terms of quality enhancement and resource\nsavings.\n","authors":["Qunliang Xing","Mai Xu","Xin Deng","Yichen Guo"],"pdf_url":"https://arxiv.org/pdf/2211.10984v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06949v1","updated":"2023-03-13T09:34:08Z","published":"2023-03-13T09:34:08Z","title":"Improving Table Structure Recognition with Visual-Alignment Sequential\n  Coordinate Modeling","summary":"  Table structure recognition aims to extract the logical and physical\nstructure of unstructured table images into a machine-readable format. The\nlatest end-to-end image-to-text approaches simultaneously predict the two\nstructures by two decoders, where the prediction of the physical structure (the\nbounding boxes of the cells) is based on the representation of the logical\nstructure. However, the previous methods struggle with imprecise bounding boxes\nas the logical representation lacks local visual information. To address this\nissue, we propose an end-to-end sequential modeling framework for table\nstructure recognition called VAST. It contains a novel coordinate sequence\ndecoder triggered by the representation of the non-empty cell from the logical\nstructure decoder. In the coordinate sequence decoder, we model the bounding\nbox coordinates as a language sequence, where the left, top, right and bottom\ncoordinates are decoded sequentially to leverage the inter-coordinate\ndependency. Furthermore, we propose an auxiliary visual-alignment loss to\nenforce the logical representation of the non-empty cells to contain more local\nvisual details, which helps produce better cell bounding boxes. Extensive\nexperiments demonstrate that our proposed method can achieve state-of-the-art\nresults in both logical and physical structure recognition. The ablation study\nalso validates that the proposed coordinate sequence decoder and the\nvisual-alignment loss are the keys to the success of our method.\n","authors":["Yongshuai Huang","Ning Lu","Dapeng Chen","Yibo Li","Zecheng Xie","Shenggao Zhu","Liangcai Gao","Wei Peng"],"pdf_url":"https://arxiv.org/pdf/2303.06949v1.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2303.06942v1","updated":"2023-03-13T09:17:56Z","published":"2023-03-13T09:17:56Z","title":"Guiding the Guidance: A Comparative Analysis of User Guidance Signals\n  for Interactive Segmentation of Volumetric Images","summary":"  Interactive segmentation reduces the annotation time of medical images and\nallows annotators to iteratively refine labels with corrective interactions,\nsuch as clicks. While existing interactive models transform clicks into user\nguidance signals, which are combined with images to form (image, guidance)\npairs, the question of how to best represent the guidance has not been fully\nexplored. To address this, we conduct a comparative study of existing guidance\nsignals by training interactive models with different signals and parameter\nsettings to identify crucial parameters for the model's design. Based on our\nfindings, we design a guidance signal that retains the benefits of other\nsignals while addressing their limitations. We propose an adaptive Gaussian\nheatmaps guidance signal that utilizes the geodesic distance transform to\ndynamically adapt the radius of each heatmap when encoding clicks. We conduct\nour study on the MSD Spleen and the AutoPET datasets to explore the\nsegmentation of both anatomy (spleen) and pathology (tumor lesions). Our\nresults show that choosing the guidance signal is crucial for interactive\nsegmentation as we improve the performance by 14% Dice with our adaptive\nheatmaps on the challenging AutoPET dataset when compared to non-interactive\nmodels. This brings interactive models one step closer to deployment on\nclinical workflows. We will make our code publically available.\n","authors":["Zdravko Marinov","Rainer Stiefelhagen","Jens Kleesiek"],"pdf_url":"https://arxiv.org/pdf/2303.06942v1.pdf","comment":"8 pages; 2 figures; 2 tables"},{"id":"http://arxiv.org/abs/2207.02625v5","updated":"2023-03-13T09:12:31Z","published":"2022-07-06T12:34:33Z","title":"$L_2$BN: Enhancing Batch Normalization by Equalizing the $L_2$ Norms of\n  Features","summary":"  In this paper, we show that the difference in $l_2$ norms of sample features\ncan hinder batch normalization from obtaining more distinguished inter-class\nfeatures and more compact intra-class features. To address this issue, we\npropose an intuitive but effective method to equalize the $l_2$ norms of sample\nfeatures. Concretely, we $l_2$-normalize each sample feature before feeding\nthem into batch normalization, and therefore the features are of the same\nmagnitude. Since the proposed method combines the $l_2$ normalization and batch\nnormalization, we name our method $L_2$BN. The $L_2$BN can strengthen the\ncompactness of intra-class features and enlarge the discrepancy of inter-class\nfeatures. The $L_2$BN is easy to implement and can exert its effect without any\nadditional parameters or hyper-parameters. Therefore, it can be used as a basic\nnormalization method for neural networks. We evaluate the effectiveness of\n$L_2$BN through extensive experiments with various models on image\nclassification and acoustic scene classification tasks. The results demonstrate\nthat the $L_2$BN can boost the generalization ability of various neural network\nmodels and achieve considerable performance improvements.\n","authors":["Zhennan Wang","Kehan Li","Runyi Yu","Yian Zhao","Pengchong Qiao","Fan Xu","Guoli Song","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2207.02625v5.pdf","comment":"Equation (4) and Figure 5 are confusing"},{"id":"http://arxiv.org/abs/2301.01123v2","updated":"2023-03-13T08:59:01Z","published":"2023-01-03T14:43:40Z","title":"MGTAB: A Multi-Relational Graph-Based Twitter Account Detection\n  Benchmark","summary":"  The development of social media user stance detection and bot detection\nmethods rely heavily on large-scale and high-quality benchmarks. However, in\naddition to low annotation quality, existing benchmarks generally have\nincomplete user relationships, suppressing graph-based account detection\nresearch. To address these issues, we propose a Multi-Relational Graph-Based\nTwitter Account Detection Benchmark (MGTAB), the first standardized graph-based\nbenchmark for account detection. To our knowledge, MGTAB was built based on the\nlargest original data in the field, with over 1.55 million users and 130\nmillion tweets. MGTAB contains 10,199 expert-annotated users and 7 types of\nrelationships, ensuring high-quality annotation and diversified relations. In\nMGTAB, we extracted the 20 user property features with the greatest information\ngain and user tweet features as the user features. In addition, we performed a\nthorough evaluation of MGTAB and other public datasets. Our experiments found\nthat graph-based approaches are generally more effective than feature-based\napproaches and perform better when introducing multiple relations. By analyzing\nexperiment results, we identify effective approaches for account detection and\nprovide potential future research directions in this field. Our benchmark and\nstandardized evaluation procedures are freely available at:\nhttps://github.com/GraphDetec/MGTAB.\n","authors":["Shuhao Shi","Kai Qiao","Jian Chen","Shuai Yang","Jie Yang","Baojie Song","Linyuan Wang","Bin Yan"],"pdf_url":"https://arxiv.org/pdf/2301.01123v2.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2211.07157v2","updated":"2023-03-13T08:57:52Z","published":"2022-11-14T07:22:55Z","title":"ParCNetV2: Oversized Kernel with Enhanced Attention","summary":"  Transformers have achieved tremendous success in various computer vision\ntasks. By borrowing design concepts from transformers, many studies\nrevolutionized CNNs and showed remarkable results. This paper falls in this\nline of studies. More specifically, we introduce a convolutional neural network\narchitecture named ParCNetV2, which extends position-aware circular convolution\n(ParCNet) with oversized convolutions and strengthens attention through\nbifurcate gate units. The oversized convolution utilizes a kernel with\n$2\\times$ the input size to model long-range dependencies through a global\nreceptive field. Simultaneously, it achieves implicit positional encoding by\nremoving the shift-invariant property from convolutional kernels, i.e., the\neffective kernels at different spatial locations are different when the kernel\nsize is twice as large as the input size. The bifurcate gate unit implements an\nattention mechanism similar to self-attention in transformers. It splits the\ninput into two branches, one serves as feature transformation while the other\nserves as attention weights. The attention is applied through element-wise\nmultiplication of the two branches. Besides, we introduce a unified\nlocal-global convolution block to unify the design of the early and late stage\nconvolutional blocks. Extensive experiments demonstrate that our method\noutperforms other pure convolutional neural networks as well as neural networks\nhybridizing CNNs and transformers.\n","authors":["Ruihan Xu","Haokui Zhang","Wenze Hu","Shiliang Zhang","Xiaoyu Wang"],"pdf_url":"https://arxiv.org/pdf/2211.07157v2.pdf","comment":"16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2303.06930v1","updated":"2023-03-13T08:53:47Z","published":"2023-03-13T08:53:47Z","title":"Twin Contrastive Learning with Noisy Labels","summary":"  Learning from noisy data is a challenging task that significantly degenerates\nthe model performance. In this paper, we present TCL, a novel twin contrastive\nlearning model to learn robust representations and handle noisy labels for\nclassification. Specifically, we construct a Gaussian mixture model (GMM) over\nthe representations by injecting the supervised model predictions into GMM to\nlink label-free latent variables in GMM with label-noisy annotations. Then, TCL\ndetects the examples with wrong labels as the out-of-distribution examples by\nanother two-component GMM, taking into account the data distribution. We\nfurther propose a cross-supervision with an entropy regularization loss that\nbootstraps the true targets from model predictions to handle the noisy labels.\nAs a result, TCL can learn discriminative representations aligned with\nestimated labels through mixup and contrastive learning. Extensive experimental\nresults on several standard benchmarks and real-world datasets demonstrate the\nsuperior performance of TCL. In particular, TCL achieves 7.5\\% improvements on\nCIFAR-10 with 90\\% noisy label -- an extremely noisy scenario. The source code\nis available at \\url{https://github.com/Hzzone/TCL}.\n","authors":["Zhizhong Huang","Junping Zhang","Hongming Shan"],"pdf_url":"https://arxiv.org/pdf/2303.06930v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.06925v1","updated":"2023-03-13T08:41:24Z","published":"2023-03-13T08:41:24Z","title":"Super-Resolution Information Enhancement For Crowd Counting","summary":"  Crowd counting is a challenging task due to the heavy occlusions, scales, and\ndensity variations. Existing methods handle these challenges effectively while\nignoring low-resolution (LR) circumstances. The LR circumstances weaken the\ncounting performance deeply for two crucial reasons: 1) limited detail\ninformation; 2) overlapping head regions accumulate in density maps and result\nin extreme ground-truth values. An intuitive solution is to employ\nsuper-resolution (SR) pre-processes for the input LR images. However, it\ncomplicates the inference steps and thus limits application potentials when\nrequiring real-time. We propose a more elegant method termed Multi-Scale\nSuper-Resolution Module (MSSRM). It guides the network to estimate the lost de\ntails and enhances the detailed information in the feature space. Noteworthy\nthat the MSSRM is plug-in plug-out and deals with the LR problems with no\ninference cost. As the proposed method requires SR labels, we further propose a\nSuper-Resolution Crowd Counting dataset (SR-Crowd). Extensive experiments on\nthree datasets demonstrate the superiority of our method. The code will be\navailable at https://github.com/PRIS-CV/MSSRM.git.\n","authors":["Jiahao Xie","Wei Xu","Dingkang Liang","Zhanyu Ma","Kongming Liang","Weidong Liu","Rui Wang","Ling Jin"],"pdf_url":"https://arxiv.org/pdf/2303.06925v1.pdf","comment":"Accepted by ICASSP 2023. The code will be available at\n  https://github.com/PRIS-CV/MSSRM.git"},{"id":"http://arxiv.org/abs/2211.12194v2","updated":"2023-03-13T08:40:32Z","published":"2022-11-22T11:35:07Z","title":"SadTalker: Learning Realistic 3D Motion Coefficients for Stylized\n  Audio-Driven Single Image Talking Face Animation","summary":"  Generating talking head videos through a face image and a piece of speech\naudio still contains many challenges. ie, unnatural head movement, distorted\nexpression, and identity modification. We argue that these issues are mainly\nbecause of learning from the coupled 2D motion fields. On the other hand,\nexplicitly using 3D information also suffers problems of stiff expression and\nincoherent video. We present SadTalker, which generates 3D motion coefficients\n(head pose, expression) of the 3DMM from audio and implicitly modulates a novel\n3D-aware face render for talking head generation. To learn the realistic motion\ncoefficients, we explicitly model the connections between audio and different\ntypes of motion coefficients individually. Precisely, we present ExpNet to\nlearn the accurate facial expression from audio by distilling both coefficients\nand 3D-rendered faces. As for the head pose, we design PoseVAE via a\nconditional VAE to synthesize head motion in different styles. Finally, the\ngenerated 3D motion coefficients are mapped to the unsupervised 3D keypoints\nspace of the proposed face render, and synthesize the final video. We conducted\nextensive experiments to demonstrate the superiority of our method in terms of\nmotion and video quality.\n","authors":["Wenxuan Zhang","Xiaodong Cun","Xuan Wang","Yong Zhang","Xi Shen","Yu Guo","Ying Shan","Fei Wang"],"pdf_url":"https://arxiv.org/pdf/2211.12194v2.pdf","comment":"Accepted by CVPR 2023, Project page: https://sadtalker.github.io,\n  Code: https://github.com/Winfredy/SadTalker"},{"id":"http://arxiv.org/abs/2303.06920v1","updated":"2023-03-13T08:37:59Z","published":"2023-03-13T08:37:59Z","title":"Pixel-wise Gradient Uncertainty for Convolutional Neural Networks\n  applied to Out-of-Distribution Segmentation","summary":"  In recent years, deep neural networks have defined the state-of-the-art in\nsemantic segmentation where their predictions are constrained to a predefined\nset of semantic classes. They are to be deployed in applications such as\nautomated driving, although their categorically confined expressive power runs\ncontrary to such open world scenarios. Thus, the detection and segmentation of\nobjects from outside their predefined semantic space, i.e., out-of-distribution\n(OoD) objects, is of highest interest. Since uncertainty estimation methods\nlike softmax entropy or Bayesian models are sensitive to erroneous predictions,\nthese methods are a natural baseline for OoD detection. Here, we present a\nmethod for obtaining uncertainty scores from pixel-wise loss gradients which\ncan be computed efficiently during inference. Our approach is simple to\nimplement for a large class of models, does not require any additional training\nor auxiliary data and can be readily used on pre-trained segmentation models.\nOur experiments show the ability of our method to identify wrong pixel\nclassifications and to estimate prediction quality. In particular, we observe\nsuperior performance in terms of OoD segmentation to comparable baselines on\nthe SegmentMeIfYouCan benchmark, clearly outperforming methods which are\nsimilarly flexible to implement.\n","authors":["Kira Maag","Tobias Riedlinger"],"pdf_url":"https://arxiv.org/pdf/2303.06920v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06919v1","updated":"2023-03-13T08:36:30Z","published":"2023-03-13T08:36:30Z","title":"NeRFLiX: High-Quality Neural View Synthesis by Learning a\n  Degradation-Driven Inter-viewpoint MiXer","summary":"  Neural radiance fields (NeRF) show great success in novel view synthesis.\nHowever, in real-world scenes, recovering high-quality details from the source\nimages is still challenging for the existing NeRF-based approaches, due to the\npotential imperfect calibration information and scene representation\ninaccuracy. Even with high-quality training frames, the synthetic novel views\nproduced by NeRF models still suffer from notable rendering artifacts, such as\nnoise, blur, etc. Towards to improve the synthesis quality of NeRF-based\napproaches, we propose NeRFLiX, a general NeRF-agnostic restorer paradigm by\nlearning a degradation-driven inter-viewpoint mixer. Specially, we design a\nNeRF-style degradation modeling approach and construct large-scale training\ndata, enabling the possibility of effectively removing NeRF-native rendering\nartifacts for existing deep neural networks. Moreover, beyond the degradation\nremoval, we propose an inter-viewpoint aggregation framework that is able to\nfuse highly related high-quality training images, pushing the performance of\ncutting-edge NeRF models to entirely new levels and producing highly\nphoto-realistic synthetic views.\n","authors":["Kun Zhou","Wenbo Li","Yi Wang","Tao Hu","Nianjuan Jiang","Xiaoguang Han","Jiangbo Lu"],"pdf_url":"https://arxiv.org/pdf/2303.06919v1.pdf","comment":"Accepted to CVPR 2023; Project Page: see\n  https://redrock303.github.io/nerflix/"},{"id":"http://arxiv.org/abs/2303.06911v1","updated":"2023-03-13T08:02:12Z","published":"2023-03-13T08:02:12Z","title":"ViM: Vision Middleware for Unified Downstream Transferring","summary":"  Foundation models are pre-trained on massive data and transferred to\ndownstream tasks via fine-tuning. This work presents Vision Middleware (ViM), a\nnew learning paradigm that targets unified transferring from a single\nfoundation model to a variety of downstream tasks. ViM consists of a zoo of\nlightweight plug-in modules, each of which is independently learned on a\nmidstream dataset with a shared frozen backbone. Downstream tasks can then\nbenefit from an adequate aggregation of the module zoo thanks to the rich\nknowledge inherited from midstream tasks. There are three major advantages of\nsuch a design. From the efficiency aspect, the upstream backbone can be trained\nonly once and reused for all downstream tasks without tuning. From the\nscalability aspect, we can easily append additional modules to ViM with no\ninfluence on existing modules. From the performance aspect, ViM can include as\nmany midstream tasks as possible, narrowing the task gap between upstream and\ndownstream. Considering these benefits, we believe that ViM, which the\ncommunity could maintain and develop together, would serve as a powerful tool\nto assist foundation models.\n","authors":["Yutong Feng","Biao Gong","Jianwen Jiang","Yiliang Lv","Yujun Shen","Deli Zhao","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.06911v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06908v1","updated":"2023-03-13T07:54:29Z","published":"2023-03-13T07:54:29Z","title":"CrossFormer++: A Versatile Vision Transformer Hinging on Cross-scale\n  Attention","summary":"  While features of different scales are perceptually important to visual\ninputs, existing vision transformers do not yet take advantage of them\nexplicitly. To this end, we first propose a cross-scale vision transformer,\nCrossFormer. It introduces a cross-scale embedding layer (CEL) and a long-short\ndistance attention (LSDA). On the one hand, CEL blends each token with multiple\npatches of different scales, providing the self-attention module itself with\ncross-scale features. On the other hand, LSDA splits the self-attention module\ninto a short-distance one and a long-distance counterpart, which not only\nreduces the computational burden but also keeps both small-scale and\nlarge-scale features in the tokens. Moreover, through experiments on\nCrossFormer, we observe another two issues that affect vision transformers'\nperformance, i.e. the enlarging self-attention maps and amplitude explosion.\nThus, we further propose a progressive group size (PGS) paradigm and an\namplitude cooling layer (ACL) to alleviate the two issues, respectively. The\nCrossFormer incorporating with PGS and ACL is called CrossFormer++. Extensive\nexperiments show that CrossFormer++ outperforms the other vision transformers\non image classification, object detection, instance segmentation, and semantic\nsegmentation tasks. The code will be available at:\nhttps://github.com/cheerss/CrossFormer.\n","authors":["Wenxiao Wang","Wei Chen","Qibo Qiu","Long Chen","Boxi Wu","Binbin Lin","Xiaofei He","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2303.06908v1.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.05095v2","updated":"2023-03-13T07:50:57Z","published":"2023-03-09T08:13:06Z","title":"Trajectory-Aware Body Interaction Transformer for Multi-Person Pose\n  Forecasting","summary":"  Multi-person pose forecasting remains a challenging problem, especially in\nmodeling fine-grained human body interaction in complex crowd scenarios.\nExisting methods typically represent the whole pose sequence as a temporal\nseries, yet overlook interactive influences among people based on skeletal body\nparts. In this paper, we propose a novel Trajectory-Aware Body Interaction\nTransformer (TBIFormer) for multi-person pose forecasting via effectively\nmodeling body part interactions. Specifically, we construct a Temporal Body\nPartition Module that transforms all the pose sequences into a Multi-Person\nBody-Part sequence to retain spatial and temporal information based on body\nsemantics. Then, we devise a Social Body Interaction Self-Attention (SBI-MSA)\nmodule, utilizing the transformed sequence to learn body part dynamics for\ninter- and intra-individual interactions. Furthermore, different from prior\nEuclidean distance-based spatial encodings, we present a novel and efficient\nTrajectory-Aware Relative Position Encoding for SBI-MSA to offer discriminative\nspatial information and additional interactive clues. On both short- and\nlong-term horizons, we empirically evaluate our framework on CMU-Mocap,\nMuPoTS-3D as well as synthesized datasets (6 ~ 10 persons), and demonstrate\nthat our method greatly outperforms the state-of-the-art methods. Code will be\nmade publicly available upon acceptance.\n","authors":["Xiaogang Peng","Siyuan Mao","Zizhao Wu"],"pdf_url":"https://arxiv.org/pdf/2303.05095v2.pdf","comment":"Accepted by CVPR2023, 8 pages, 6 figures. arXiv admin note: text\n  overlap with arXiv:2208.09224"},{"id":"http://arxiv.org/abs/2303.06907v1","updated":"2023-03-13T07:48:46Z","published":"2023-03-13T07:48:46Z","title":"ST360IQ: No-Reference Omnidirectional Image Quality Assessment with\n  Spherical Vision Transformers","summary":"  Omnidirectional images, aka 360 images, can deliver immersive and interactive\nvisual experiences. As their popularity has increased dramatically in recent\nyears, evaluating the quality of 360 images has become a problem of interest\nsince it provides insights for capturing, transmitting, and consuming this new\nmedia. However, directly adapting quality assessment methods proposed for\nstandard natural images for omnidirectional data poses certain challenges.\nThese models need to deal with very high-resolution data and implicit\ndistortions due to the spherical form of the images. In this study, we present\na method for no-reference 360 image quality assessment. Our proposed ST360IQ\nmodel extracts tangent viewports from the salient parts of the input\nomnidirectional image and employs a vision-transformers based module processing\nsaliency selective patches/tokens that estimates a quality score from each\nviewport. Then, it aggregates these scores to give a final quality score. Our\nexperiments on two benchmark datasets, namely OIQA and CVIQ datasets,\ndemonstrate that as compared to the state-of-the-art, our approach predicts the\nquality of an omnidirectional image correlated with the human-perceived image\nquality. The code has been available on\nhttps://github.com/Nafiseh-Tofighi/ST360IQ\n","authors":["Nafiseh Jabbari Tofighi","Mohamed Hedi Elfkir","Nevrez Imamoglu","Cagri Ozcinar","Erkut Erdem","Aykut Erdem"],"pdf_url":"https://arxiv.org/pdf/2303.06907v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.06905v1","updated":"2023-03-13T07:47:18Z","published":"2023-03-13T07:47:18Z","title":"DEHRFormer: Real-time Transformer for Depth Estimation and Haze Removal\n  from Varicolored Haze Scenes","summary":"  Varicolored haze caused by chromatic casts poses haze removal and depth\nestimation challenges. Recent learning-based depth estimation methods are\nmainly targeted at dehazing first and estimating depth subsequently from\nhaze-free scenes. This way, the inner connections between colored haze and\nscene depth are lost. In this paper, we propose a real-time transformer for\nsimultaneous single image Depth Estimation and Haze Removal (DEHRFormer).\nDEHRFormer consists of a single encoder and two task-specific decoders. The\ntransformer decoders with learnable queries are designed to decode coupling\nfeatures from the task-agnostic encoder and project them into clean image and\ndepth map, respectively. In addition, we introduce a novel learning paradigm\nthat utilizes contrastive learning and domain consistency learning to tackle\nweak-generalization problem for real-world dehazing, while predicting the same\ndepth map from the same scene with varicolored haze. Experiments demonstrate\nthat DEHRFormer achieves significant performance improvement across diverse\nvaricolored haze scenes over previous depth estimation networks and dehazing\napproaches.\n","authors":["Sixiang Chen","Tian Ye","Jun Shi","Yun Liu","JingXia Jiang","Erkang Chen","Peng Chen"],"pdf_url":"https://arxiv.org/pdf/2303.06905v1.pdf","comment":"Accepted to ICASSP'2023"},{"id":"http://arxiv.org/abs/2303.06904v1","updated":"2023-03-13T07:46:41Z","published":"2023-03-13T07:46:41Z","title":"Contextually-rich human affect perception using multimodal scene\n  information","summary":"  The process of human affect understanding involves the ability to infer\nperson specific emotional states from various sources including images, speech,\nand language. Affect perception from images has predominantly focused on\nexpressions extracted from salient face crops. However, emotions perceived by\nhumans rely on multiple contextual cues including social settings, foreground\ninteractions, and ambient visual scenes. In this work, we leverage pretrained\nvision-language (VLN) models to extract descriptions of foreground context from\nimages. Further, we propose a multimodal context fusion (MCF) module to combine\nforeground cues with the visual scene and person-based contextual information\nfor emotion prediction. We show the effectiveness of our proposed modular\ndesign on two datasets associated with natural scenes and TV shows.\n","authors":["Digbalay Bose","Rajat Hebbar","Krishna Somandepalli","Shrikanth Narayanan"],"pdf_url":"https://arxiv.org/pdf/2303.06904v1.pdf","comment":"Accepted to IEEE International Conference on Acoustics, Speech and\n  Signal Processing (ICASSP), 2023"},{"id":"http://arxiv.org/abs/2208.12506v3","updated":"2023-03-13T07:35:42Z","published":"2022-08-26T08:56:33Z","title":"EGFR Mutation Prediction of Lung Biopsy Images using Deep Learning","summary":"  The standard diagnostic procedures for targeted therapies in lung cancer\ntreatment involve histological subtyping and subsequent detection of key driver\nmutations, such as EGFR. Even though molecular profiling can uncover the driver\nmutation, the process is often expensive and time-consuming. Deep\nlearning-oriented image analysis offers a more economical alternative for\ndiscovering driver mutations directly from whole slide images (WSIs). In this\nwork, we used customized deep learning pipelines with weak supervision to\nidentify the morphological correlates of EGFR mutation from hematoxylin and\neosin-stained WSIs, in addition to detecting tumor and histologically subtyping\nit. We demonstrate the effectiveness of our pipeline by conducting rigorous\nexperiments and ablation studies on two lung cancer datasets - TCGA and a\nprivate dataset from India. With our pipeline, we achieved an average area\nunder the curve (AUC) of 0.964 for tumor detection, and 0.942 for histological\nsubtyping between adenocarcinoma and squamous cell carcinoma on the TCGA\ndataset. For EGFR detection, we achieved an average AUC of 0.864 on the TCGA\ndataset and 0.783 on the dataset from India. Our key learning points include\nthe following. Firstly, there is no particular advantage of using a feature\nextractor layers trained on histology, if one is going to fine-tune the feature\nextractor on the target dataset. Secondly, selecting patches with high\ncellularity, presumably capturing tumor regions, is not always helpful, as the\nsign of a disease class may be present in the tumor-adjacent stroma.\n","authors":["Ravi Kant Gupta","Shivani Nandgaonkar","Nikhil Cherian Kurian","Swapnil Rane","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2208.12506v3.pdf","comment":"We need to improve"},{"id":"http://arxiv.org/abs/2210.00215v4","updated":"2023-03-13T07:09:20Z","published":"2022-10-01T07:36:51Z","title":"Differentiable Parsing and Visual Grounding of Natural Language\n  Instructions for Object Placement","summary":"  We present a new method, PARsing And visual GrOuNding (ParaGon), for\ngrounding natural language in object placement tasks. Natural language\ngenerally describes objects and spatial relations with compositionality and\nambiguity, two major obstacles to effective language grounding. For\ncompositionality, ParaGon parses a language instruction into an object-centric\ngraph representation to ground objects individually. For ambiguity, ParaGon\nuses a novel particle-based graph neural network to reason about object\nplacements with uncertainty. Essentially, ParaGon integrates a parsing\nalgorithm into a probabilistic, data-driven learning framework. It is fully\ndifferentiable and trained end-to-end from data for robustness against complex,\nambiguous language input.\n","authors":["Zirui Zhao","Wee Sun Lee","David Hsu"],"pdf_url":"https://arxiv.org/pdf/2210.00215v4.pdf","comment":"To appear in ICRA 2023"},{"id":"http://arxiv.org/abs/2303.05715v2","updated":"2023-03-13T07:09:03Z","published":"2023-03-10T05:46:25Z","title":"Context-Based Trit-Plane Coding for Progressive Image Compression","summary":"  Trit-plane coding enables deep progressive image compression, but it cannot\nuse autoregressive context models. In this paper, we propose the context-based\ntrit-plane coding (CTC) algorithm to achieve progressive compression more\ncompactly. First, we develop the context-based rate reduction module to\nestimate trit probabilities of latent elements accurately and thus encode the\ntrit-planes compactly. Second, we develop the context-based distortion\nreduction module to refine partial latent tensors from the trit-planes and\nimprove the reconstructed image quality. Third, we propose a retraining scheme\nfor the decoder to attain better rate-distortion tradeoffs. Extensive\nexperiments show that CTC outperforms the baseline trit-plane codec\nsignificantly in BD-rate on the Kodak lossless dataset, while increasing the\ntime complexity only marginally. Our codes are available at\nhttps://github.com/seungminjeon-github/CTC.\n","authors":["Seungmin Jeon","Kwang Pyo Choi","Youngo Park","Chang-Su Kim"],"pdf_url":"https://arxiv.org/pdf/2303.05715v2.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2211.03177v2","updated":"2023-03-13T07:06:55Z","published":"2022-11-06T17:05:04Z","title":"Measurement-Consistent Networks via a Deep Implicit Layer for Solving\n  Inverse Problems","summary":"  End-to-end deep neural networks (DNNs) have become the state-of-the-art\n(SOTA) for solving inverse problems. Despite their outstanding performance,\nduring deployment, such networks are sensitive to minor variations in the\ntesting pipeline and often fail to reconstruct small but important details, a\nfeature critical in medical imaging, astronomy, or defence. Such instabilities\nin DNNs can be explained by the fact that they ignore the forward measurement\nmodel during deployment, and thus fail to enforce consistency between their\noutput and the input measurements. To overcome this, we propose a framework\nthat transforms any DNN for inverse problems into a measurement-consistent one.\nThis is done by appending to it an implicit layer (or deep equilibrium network)\ndesigned to solve a model-based optimization problem. The implicit layer\nconsists of a shallow learnable network that can be integrated into the\nend-to-end training while keeping the SOTA DNN fixed. Experiments on\nsingle-image super-resolution show that the proposed framework leads to\nsignificant improvements in reconstruction quality and robustness over the SOTA\nDNNs.\n","authors":["Rahul Mourya","João F. C. Mota"],"pdf_url":"https://arxiv.org/pdf/2211.03177v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.01567v3","updated":"2023-03-13T07:05:21Z","published":"2022-11-03T03:12:52Z","title":"Galaxy Image Deconvolution for Weak Gravitational Lensing with Unrolled\n  Plug-and-Play ADMM","summary":"  Removing optical and atmospheric blur from galaxy images significantly\nimproves galaxy shape measurements for weak gravitational lensing and galaxy\nevolution studies. This ill-posed linear inverse problem is usually solved with\ndeconvolution algorithms enhanced by regularisation priors or deep learning. We\nintroduce a so-called \"physics-informed deep learning\" approach to the Point\nSpread Function (PSF) deconvolution problem in galaxy surveys. We apply\nalgorithm unrolling and the Plug-and-Play technique to the Alternating\nDirection Method of Multipliers (ADMM), in which a neural network learns\nappropriate hyperparameters and denoising priors from simulated galaxy images.\nWe characterise the time-performance trade-off of several methods for galaxies\nof differing brightness levels as well as our method's robustness to systematic\nPSF errors and network ablations. We show an improvement in reduced shear\nellipticity error of 38.6% (SNR=20)/45.0% (SNR=200) compared to classic methods\nand 7.4% (SNR=20)/33.2% (SNR=200) compared to modern methods.\n","authors":["Tianao Li","Emma Alexander"],"pdf_url":"https://arxiv.org/pdf/2211.01567v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.12047v2","updated":"2023-03-13T07:02:14Z","published":"2022-01-28T11:23:29Z","title":"Exploring Object-Aware Attention Guided Frame Association for RGB-D SLAM","summary":"  Deep learning models as an emerging topic have shown great progress in\nvarious fields. Especially, visualization tools such as class activation\nmapping methods provided visual explanation on the reasoning of convolutional\nneural networks (CNNs). By using the gradients of the network layers, it is\npossible to demonstrate where the networks pay attention during a specific\nimage recognition task. Moreover, these gradients can be integrated with CNN\nfeatures for localizing more generalized task dependent attentive (salient)\nobjects in scenes. Despite this progress, there is not much explicit usage of\nthis gradient (network attention) information to integrate with CNN\nrepresentations for object semantics. This can be very useful for visual tasks\nsuch as simultaneous localization and mapping (SLAM) where CNN representations\nof spatially attentive object locations may lead to improved performance.\nTherefore, in this work, we propose the use of task specific network attention\nfor RGB-D indoor SLAM. To do so, we integrate layer-wise object attention\ninformation (layer gradients) with CNN layer representations to improve frame\nassociation performance in an RGB-D indoor SLAM method. Experiments show\npromising results with improved performance over the baseline.\n","authors":["Ali Caglayan","Nevrez Imamoglu","Oguzhan Guclu","Ali Osman Serhatoglu","Weimin Wang","Ahmet Burak Can","Ryosuke Nakamura"],"pdf_url":"https://arxiv.org/pdf/2201.12047v2.pdf","comment":"5 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2301.04011v2","updated":"2023-03-13T06:37:31Z","published":"2023-01-08T09:27:41Z","title":"Learning Support and Trivial Prototypes for Interpretable Image\n  Classification","summary":"  Prototypical part network (ProtoPNet) methods have been designed to achieve\ninterpretable classification by associating predictions with a set of training\nprototypes, which we refer to as trivial prototypes because they are trained to\nlie far from the classification boundary in the feature space. Note that it is\npossible to make an analogy between ProtoPNet and support vector machine (SVM)\ngiven that the classification from both methods relies on computing similarity\nwith a set of training points (i.e., trivial prototypes in ProtoPNet, and\nsupport vectors in SVM). However, while trivial prototypes are located far from\nthe classification boundary, support vectors are located close to this\nboundary, and we argue that this discrepancy with the well-established SVM\ntheory can result in ProtoPNet models with inferior classification accuracy. In\nthis paper, we aim to improve the classification of ProtoPNet with a new method\nto learn support prototypes that lie near the classification boundary in the\nfeature space, as suggested by the SVM theory. In addition, we target the\nimprovement of classification results with a new model, named ST-ProtoPNet,\nwhich exploits our support prototypes and the trivial prototypes to provide\nmore effective classification. Experimental results on CUB-200-2011, Stanford\nCars, and Stanford Dogs datasets demonstrate that ST-ProtoPNet achieves\nstate-of-the-art classification accuracy and interpretability results. We also\nshow that the proposed support prototypes tend to be better localised in the\nobject of interest rather than in the background region.\n","authors":["Chong Wang","Yuyuan Liu","Yuanhong Chen","Fengbei Liu","Yu Tian","Davis J. McCarthy","Helen Frazer","Gustavo Carneiro"],"pdf_url":"https://arxiv.org/pdf/2301.04011v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05021v2","updated":"2023-03-13T06:18:14Z","published":"2023-03-09T03:48:24Z","title":"DiffusionDepth: Diffusion Denoising Approach for Monocular Depth\n  Estimation","summary":"  Monocular depth estimation is a challenging task that predicts the pixel-wise\ndepth from a single 2D image. Current methods typically model this problem as a\nregression or classification task. We propose DiffusionDepth, a new approach\nthat reformulates monocular depth estimation as a denoising diffusion process.\nIt learns an iterative denoising process to `denoise' random depth distribution\ninto a depth map with the guidance of monocular visual conditions. The process\nis performed in the latent space encoded by a dedicated depth encoder and\ndecoder. Instead of diffusing ground truth (GT) depth, the model learns to\nreverse the process of diffusing the refined depth of itself into random depth\ndistribution. This self-diffusion formulation overcomes the difficulty of\napplying generative models to sparse GT depth scenarios. The proposed approach\nbenefits this task by refining depth estimation step by step, which is superior\nfor generating accurate and highly detailed depth maps. Experimental results on\nKITTI and NYU-Depth-V2 datasets suggest that a simple yet efficient diffusion\napproach could reach state-of-the-art performance in both indoor and outdoor\nscenarios with acceptable inference time.\n","authors":["Yiqun Duan","Zheng Zhu","Xianda Guo"],"pdf_url":"https://arxiv.org/pdf/2303.05021v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06885v1","updated":"2023-03-13T06:05:18Z","published":"2023-03-13T06:05:18Z","title":"DR2: Diffusion-based Robust Degradation Remover for Blind Face\n  Restoration","summary":"  Blind face restoration usually synthesizes degraded low-quality data with a\npre-defined degradation model for training, while more complex cases could\nhappen in the real world. This gap between the assumed and actual degradation\nhurts the restoration performance where artifacts are often observed in the\noutput. However, it is expensive and infeasible to include every type of\ndegradation to cover real-world cases in the training data. To tackle this\nrobustness issue, we propose Diffusion-based Robust Degradation Remover (DR2)\nto first transform the degraded image to a coarse but degradation-invariant\nprediction, then employ an enhancement module to restore the coarse prediction\nto a high-quality image. By leveraging a well-performing denoising diffusion\nprobabilistic model, our DR2 diffuses input images to a noisy status where\nvarious types of degradation give way to Gaussian noise, and then captures\nsemantic information through iterative denoising steps. As a result, DR2 is\nrobust against common degradation (e.g. blur, resize, noise and compression)\nand compatible with different designs of enhancement modules. Experiments in\nvarious settings show that our framework outperforms state-of-the-art methods\non heavily degraded synthetic and real-world datasets.\n","authors":["Zhixin Wang","Xiaoyun Zhang","Ziying Zhang","Huangjie Zheng","Mingyuan Zhou","Ya Zhang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.06885v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.06884v1","updated":"2023-03-13T06:03:23Z","published":"2023-03-13T06:03:23Z","title":"SCPNet: Semantic Scene Completion on Point Cloud","summary":"  Training deep models for semantic scene completion (SSC) is challenging due\nto the sparse and incomplete input, a large quantity of objects of diverse\nscales as well as the inherent label noise for moving objects. To address the\nabove-mentioned problems, we propose the following three solutions: 1)\nRedesigning the completion sub-network. We design a novel completion\nsub-network, which consists of several Multi-Path Blocks (MPBs) to aggregate\nmulti-scale features and is free from the lossy downsampling operations. 2)\nDistilling rich knowledge from the multi-frame model. We design a novel\nknowledge distillation objective, dubbed Dense-to-Sparse Knowledge Distillation\n(DSKD). It transfers the dense, relation-based semantic knowledge from the\nmulti-frame teacher to the single-frame student, significantly improving the\nrepresentation learning of the single-frame model. 3) Completion label\nrectification. We propose a simple yet effective label rectification strategy,\nwhich uses off-the-shelf panoptic segmentation labels to remove the traces of\ndynamic objects in completion labels, greatly improving the performance of deep\nmodels especially for those moving objects. Extensive experiments are conducted\nin two public SSC benchmarks, i.e., SemanticKITTI and SemanticPOSS. Our SCPNet\nranks 1st on SemanticKITTI semantic scene completion challenge and surpasses\nthe competitive S3CNet by 7.2 mIoU. SCPNet also outperforms previous completion\nalgorithms on the SemanticPOSS dataset. Besides, our method also achieves\ncompetitive results on SemanticKITTI semantic segmentation tasks, showing that\nknowledge learned in the scene completion is beneficial to the segmentation\ntask.\n","authors":["Zhaoyang Xia","Youquan Liu","Xin Li","Xinge Zhu","Yuexin Ma","Yikang Li","Yuenan Hou","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2303.06884v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.06881v1","updated":"2023-03-13T05:56:36Z","published":"2023-03-13T05:56:36Z","title":"OverlapNetVLAD: A Coarse-to-Fine Framework for LiDAR-based Place\n  Recognition","summary":"  Place recognition is a challenging yet crucial task in robotics. Existing 3D\nLiDAR place recognition methods suffer from limited feature representation\ncapability and long search times. To address these challenges, we propose a\nnovel coarse-to-fine framework for 3D LiDAR place recognition that combines\nBirds' Eye View (BEV) feature extraction, coarse-grained matching, and\nfine-grained verification. In the coarse stage, our framework leverages the\nrich contextual information contained in BEV features to produce global\ndescriptors. Then the top-\\textit{K} most similar candidates are identified via\ndescriptor matching, which is fast but coarse-grained. In the fine stage, our\noverlap estimation network reuses the corresponding BEV features to predict the\noverlap region, enabling meticulous and precise matching. Experimental results\non the KITTI odometry benchmark demonstrate that our framework achieves leading\nperformance compared to state-of-the-art methods. Our code is available at:\n\\url{https://github.com/fcchit/OverlapNetVLAD}.\n","authors":["Chencan Fu","Lin Li","Linpeng Peng","Yukai Ma","Xiangrui Zhao","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2303.06881v1.pdf","comment":"Submitted to IROS2023"},{"id":"http://arxiv.org/abs/2303.06880v1","updated":"2023-03-13T05:54:13Z","published":"2023-03-13T05:54:13Z","title":"Uni3D: A Unified Baseline for Multi-dataset 3D Object Detection","summary":"  Current 3D object detection models follow a single dataset-specific training\nand testing paradigm, which often faces a serious detection accuracy drop when\nthey are directly deployed in another dataset. In this paper, we study the task\nof training a unified 3D detector from multiple datasets. We observe that this\nappears to be a challenging task, which is mainly due to that these datasets\npresent substantial data-level differences and taxonomy-level variations caused\nby different LiDAR types and data acquisition standards. Inspired by such\nobservation, we present a Uni3D which leverages a simple data-level correction\noperation and a designed semantic-level coupling-and-recoupling module to\nalleviate the unavoidable data-level and taxonomy-level differences,\nrespectively. Our method is simple and easily combined with many 3D object\ndetection baselines such as PV-RCNN and Voxel-RCNN, enabling them to\neffectively learn from multiple off-the-shelf 3D datasets to obtain more\ndiscriminative and generalizable representations. Experiments are conducted on\nmany dataset consolidation settings including Waymo-nuScenes, nuScenes-KITTI,\nWaymo-KITTI, and Waymo-nuScenes-KITTI consolidations. Their results demonstrate\nthat Uni3D exceeds a series of individual detectors trained on a single\ndataset, with a 1.04x parameter increase over a selected baseline detector. We\nexpect this work will inspire the research of 3D generalization since it will\npush the limits of perceptual performance.\n","authors":["Bo Zhang","Jiakang Yuan","Botian Shi","Tao Chen","Yikang Li","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2303.06880v1.pdf","comment":"Accepted by CVPR-2023, and our code is available at\n  https://github.com/PJLab-ADG/3DTrans"},{"id":"http://arxiv.org/abs/2303.06879v1","updated":"2023-03-13T05:54:01Z","published":"2023-03-13T05:54:01Z","title":"Spacecraft Anomaly Detection with Attention Temporal Convolution Network","summary":"  Spacecraft faces various situations when carrying out exploration missions in\ncomplex space, thus monitoring the anomaly status of spacecraft is crucial to\nthe development of \\textcolor{blue}{the} aerospace industry. The time series\ntelemetry data generated by on-orbit spacecraft \\textcolor{blue}{contains}\nimportant information about the status of spacecraft. However, traditional\ndomain knowledge-based spacecraft anomaly detection methods are not effective\ndue to high dimensionality and complex correlation among variables. In this\nwork, we propose an anomaly detection framework for spacecraft multivariate\ntime-series data based on temporal convolution networks (TCNs). First, we\nemploy dynamic graph attention to model the complex correlation among variables\nand time series. Second, temporal convolution networks with parallel processing\nability are used to extract multidimensional \\textcolor{blue}{features} for\n\\textcolor{blue}{the} downstream prediction task. Finally, many potential\nanomalies are detected by the best threshold. Experiments on real NASA SMAP/MSL\nspacecraft datasets show the superiority of our proposed model with respect to\nstate-of-the-art methods.\n","authors":["Liang Liu","Ling Tian","Zhao Kang","Tianqi Wan"],"pdf_url":"https://arxiv.org/pdf/2303.06879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06877v1","updated":"2023-03-13T05:53:11Z","published":"2023-03-13T05:53:11Z","title":"Progressive Open Space Expansion for Open-Set Model Attribution","summary":"  Despite the remarkable progress in generative technology, the Janus-faced\nissues of intellectual property protection and malicious content supervision\nhave arisen. Efforts have been paid to manage synthetic images by attributing\nthem to a set of potential source models. However, the closed-set\nclassification setting limits the application in real-world scenarios for\nhandling contents generated by arbitrary models. In this study, we focus on a\nchallenging task, namely Open-Set Model Attribution (OSMA), to simultaneously\nattribute images to known models and identify those from unknown ones. Compared\nto existing open-set recognition (OSR) tasks focusing on semantic novelty, OSMA\nis more challenging as the distinction between images from known and unknown\nmodels may only lie in visually imperceptible traces. To this end, we propose a\nProgressive Open Space Expansion (POSE) solution, which simulates open-set\nsamples that maintain the same semantics as closed-set samples but embedded\nwith different imperceptible traces. Guided by a diversity constraint, the open\nspace is simulated progressively by a set of lightweight augmentation models.\nWe consider three real-world scenarios and construct an OSMA benchmark dataset,\nincluding unknown models trained with different random seeds, architectures,\nand datasets from known ones. Extensive experiments on the dataset demonstrate\nPOSE is superior to both existing model attribution methods and off-the-shelf\nOSR methods.\n","authors":["Tianyun Yang","Danding Wang","Fan Tang","Xinying Zhao","Juan Cao","Sheng Tang"],"pdf_url":"https://arxiv.org/pdf/2303.06877v1.pdf","comment":"accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2303.01904v3","updated":"2023-03-13T05:52:56Z","published":"2023-03-03T13:05:30Z","title":"EcoTTA: Memory-Efficient Continual Test-time Adaptation via\n  Self-distilled Regularization","summary":"  This paper presents a simple yet effective approach that improves continual\ntest-time adaptation (TTA) in a memory-efficient manner. TTA may primarily be\nconducted on edge devices with limited memory, so reducing memory is crucial\nbut has been overlooked in previous TTA studies. In addition, long-term\nadaptation often leads to catastrophic forgetting and error accumulation, which\nhinders applying TTA in real-world deployments. Our approach consists of two\ncomponents to address these issues. First, we present lightweight meta networks\nthat can adapt the frozen original networks to the target domain. This novel\narchitecture minimizes memory consumption by decreasing the size of\nintermediate activations required for backpropagation. Second, our novel\nself-distilled regularization controls the output of the meta networks not to\ndeviate significantly from the output of the frozen original networks, thereby\npreserving well-trained knowledge from the source domain. Without additional\nmemory, this regularization prevents error accumulation and catastrophic\nforgetting, resulting in stable performance even in long-term test-time\nadaptation. We demonstrate that our simple yet effective strategy outperforms\nother state-of-the-art methods on various benchmarks for image classification\nand semantic segmentation tasks. Notably, our proposed method with ResNet-50\nand WideResNet-40 takes 86% and 80% less memory than the recent\nstate-of-the-art method, CoTTA.\n","authors":["Junha Song","Jungsoo Lee","In So Kweon","Sungha Choi"],"pdf_url":"https://arxiv.org/pdf/2303.01904v3.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.06876v1","updated":"2023-03-13T05:51:35Z","published":"2023-03-13T05:51:35Z","title":"Revisiting model self-interpretability in a decision-theoretic way for\n  binary medical image classification","summary":"  Interpretability is highly desired for deep neural network-based classifiers,\nespecially when addressing high-stake decisions in medical imaging. Commonly\nused post-hoc interpretability methods may not be always useful because\ndifferent such methods can produce several plausible but different\ninterpretations of a given model, leading to confusion about which one to\nchoose. {In this work, an {inherently} interpretable encoder-decoder model\ncoupled with a single-layer fully connected network with unity weights is\nproposed for binary medical image classification problems. The feature\nextraction component of a trained black-box network for the same task is\nemployed as the pre-trained encoder of the interpretable model. The model is\ntrained to estimate the decision statistic of the given trained black-box deep\nbinary classifier to maintain a similar accuracy.} The decoder output\nrepresents a transformed version of the to-be-classified image that, when\nprocessed by the fixed fully connected layer, produces the same decision\nstatistic value as the original classifier. This is accomplished by minimizing\nthe mean squared error between the decision statistic values of the black-box\nmodel and encoder-decoder based model during training. The decoder output image\nis referred to as an equivalency map. Because the single-layer network is fully\ninterpretable, the equivalency map provides a visualization of the transformed\nimage features that contribute to the decision statistic value and, moreover,\npermits quantification of their relative contributions. Unlike the traditional\npost-hoc interpretability methods, the proposed method is inherently\ninterpretable, quantitative, and fundamentally based on decision theory.\n","authors":["Sourya Sengupta","Mark A. Anastasio"],"pdf_url":"https://arxiv.org/pdf/2303.06876v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.06574v3","updated":"2023-03-13T05:51:27Z","published":"2022-02-14T09:36:50Z","title":"I-Tuning: Tuning Frozen Language Models with Image for Lightweight Image\n  Captioning","summary":"  Image Captioning is a traditional vision-and-language task that aims to\ngenerate the language description of an image. Recent studies focus on scaling\nup the model size and the number of training data, which significantly increase\nthe cost of model training. Different to these heavy-cost models, we introduce\na lightweight image captioning framework (I-Tuning), which contains a small\nnumber of trainable parameters. We design a novel I-Tuning cross-attention\nmodule to connect the non-trainable pre-trained language decoder GPT2 and\nvision encoder CLIP-ViT. Since most parameters are not required to be updated\nduring training, our framework is lightweight and fast. Experimental results\nconducted on three image captioning benchmarks reveal that our framework\nachieves comparable or better performance than the large-scale baseline\nsystems. But our models contain up to 10 times fewer trainable parameters and\nrequire much fewer data for training compared with state-of-the-art baselines.\n","authors":["Ziyang Luo","Zhipeng Hu","Yadong Xi","Rongsheng Zhang","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2202.06574v3.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.06873v1","updated":"2023-03-13T05:46:56Z","published":"2023-03-13T05:46:56Z","title":"Interventional Bag Multi-Instance Learning On Whole-Slide Pathological\n  Images","summary":"  Multi-instance learning (MIL) is an effective paradigm for whole-slide\npathological images (WSIs) classification to handle the gigapixel resolution\nand slide-level label. Prevailing MIL methods primarily focus on improving the\nfeature extractor and aggregator. However, one deficiency of these methods is\nthat the bag contextual prior may trick the model into capturing spurious\ncorrelations between bags and labels. This deficiency is a confounder that\nlimits the performance of existing MIL methods. In this paper, we propose a\nnovel scheme, Interventional Bag Multi-Instance Learning (IBMIL), to achieve\ndeconfounded bag-level prediction. Unlike traditional likelihood-based\nstrategies, the proposed scheme is based on the backdoor adjustment to achieve\nthe interventional training, thus is capable of suppressing the bias caused by\nthe bag contextual prior. Note that the principle of IBMIL is orthogonal to\nexisting bag MIL methods. Therefore, IBMIL is able to bring consistent\nperformance boosting to existing schemes, achieving new state-of-the-art\nperformance. Code is available at https://github.com/HHHedo/IBMIL.\n","authors":["Tiancheng Lin","Zhimiao Yu","Hongyu Hu","Yi Xu","Chang Wen Chen"],"pdf_url":"https://arxiv.org/pdf/2303.06873v1.pdf","comment":"Accepted by CVPR 2023; Code at https://github.com/HHHedo/IBMIL"},{"id":"http://arxiv.org/abs/2303.06872v1","updated":"2023-03-13T05:46:21Z","published":"2023-03-13T05:46:21Z","title":"FusionLoc: Camera-2D LiDAR Fusion Using Multi-Head Self-Attention for\n  End-to-End Serving Robot Relocalization","summary":"  With the recent development of autonomous driving technology, as the pursuit\nof efficiency for repetitive tasks and the value of non-face-to-face services\nincrease, mobile service robots such as delivery robots and serving robots\nattract attention, and their demands are increasing day by day. However, when\nsomething goes wrong, most commercial serving robots need to return to their\nstarting position and orientation to operate normally again. In this paper, we\nfocus on end-to-end relocalization of serving robots to address the problem. It\nis to predict robot pose directly from only the onboard sensor data using\nneural networks. In particular, we propose a deep neural network architecture\nfor the relocalization based on camera-2D LiDAR sensor fusion. We call the\nproposed method FusionLoc. In the proposed method, the multi-head\nself-attention complements different types of information captured by the two\nsensors. Our experiments on a dataset collected by a commercial serving robot\ndemonstrate that FusionLoc can provide better performances than previous\nrelocalization methods taking only a single image or a 2D LiDAR point cloud as\nwell as a straightforward fusion method concatenating their features.\n","authors":["Jieun Lee","Hakjun Lee","Jiyong Oh"],"pdf_url":"https://arxiv.org/pdf/2303.06872v1.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.06870v1","updated":"2023-03-13T05:37:46Z","published":"2023-03-13T05:37:46Z","title":"Three Guidelines You Should Know for Universally Slimmable\n  Self-Supervised Learning","summary":"  We propose universally slimmable self-supervised learning (dubbed as US3L) to\nachieve better accuracy-efficiency trade-offs for deploying self-supervised\nmodels across different devices. We observe that direct adaptation of\nself-supervised learning (SSL) to universally slimmable networks misbehaves as\nthe training process frequently collapses. We then discover that temporal\nconsistent guidance is the key to the success of SSL for universally slimmable\nnetworks, and we propose three guidelines for the loss design to ensure this\ntemporal consistency from a unified gradient perspective. Moreover, we propose\ndynamic sampling and group regularization strategies to simultaneously improve\ntraining efficiency and accuracy. Our US3L method has been empirically\nvalidated on both convolutional neural networks and vision transformers. With\nonly once training and one copy of weights, our method outperforms various\nstate-of-the-art methods (individually trained or not) on benchmarks including\nrecognition, object detection and instance segmentation. Our code is available\nat https://github.com/megvii-research/US3L-CVPR2023.\n","authors":["Yun-Hao Cao","Peiqin Sun","Shuchang Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.06870v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.06869v1","updated":"2023-03-13T05:37:40Z","published":"2023-03-13T05:37:40Z","title":"Adaptive Data-Free Quantization","summary":"  Data-free quantization (DFQ) recovers the performance of quantized network\n(Q) without accessing the real data, but generates the fake sample via a\ngenerator (G) by learning from full-precision network (P) instead. However,\nsuch sample generation process is totally independent of Q, overlooking the\nadaptability of the knowledge from generated samples, i.e., informative or not\nto the learning process of Q, resulting into the overflow of generalization\nerror. Building on this, several critical questions -- how to measure the\nsample adaptability to Q under varied bit-width scenarios? how to generate the\nsamples with large adaptability to improve Q's generalization? whether the\nlargest adaptability is the best? To answer the above questions, in this paper,\nwe propose an Adaptive Data-Free Quantization (AdaDFQ) method, which\nreformulates DFQ as a zero-sum game upon the sample adaptability between two\nplayers -- a generator and a quantized network. Following this viewpoint, we\nfurther define the disagreement and agreement samples to form two boundaries,\nwhere the margin is optimized to address the over-and-under fitting issues, so\nas to generate the samples with the desirable adaptability to Q. Our AdaDFQ\nreveals: 1) the largest adaptability is NOT the best for sample generation to\nbenefit Q's generalization; 2) the knowledge of the generated sample should not\nbe informative to Q only, but also related to the category and distribution\ninformation of the training data for P. The theoretical and empirical analysis\nvalidate the advantages of AdaDFQ over the state-of-the-arts. Our code is\navailable at https: github.com/hfutqian/AdaDFQ.\n","authors":["Biao Qian","Yang Wang","Richang Hong","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.06869v1.pdf","comment":"9 pages, 6 figures, accepted by CVPR 2023. arXiv admin note:\n  substantial text overlap with arXiv:2302.09572"},{"id":"http://arxiv.org/abs/2303.06868v1","updated":"2023-03-13T05:33:28Z","published":"2023-03-13T05:33:28Z","title":"Deep Learning-based Eye-Tracking Analysis for Diagnosis of Alzheimer's\n  Disease Using 3D Comprehensive Visual Stimuli","summary":"  Alzheimer's Disease (AD) causes a continuous decline in memory, thinking, and\njudgment. Traditional diagnoses are usually based on clinical experience, which\nis limited by some realistic factors. In this paper, we focus on exploiting\ndeep learning techniques to diagnose AD based on eye-tracking behaviors. Visual\nattention, as typical eye-tracking behavior, is of great clinical value to\ndetect cognitive abnormalities in AD patients. To better analyze the\ndifferences in visual attention between AD patients and normals, we first\nconduct a 3D comprehensive visual task on a non-invasive eye-tracking system to\ncollect visual attention heatmaps. We then propose a multi-layered comparison\nconvolution neural network (MC-CNN) to distinguish the visual attention\ndifferences between AD patients and normals. In MC-CNN, the multi-layered\nrepresentations of heatmaps are obtained by hierarchical convolution to better\nencode eye-movement behaviors, which are further integrated into a distance\nvector to benefit the comprehensive visual task. Extensive experimental results\non the collected dataset demonstrate that MC-CNN achieves consistent validity\nin classifying AD patients and normals with eye-tracking data.\n","authors":["Fangyu Zuo","Peiguang Jing","Jinglin Sun"," Jizhong"," Duan","Yong Ji","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2303.06868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.09590v4","updated":"2023-03-13T05:19:15Z","published":"2022-11-17T15:36:48Z","title":"Hypergraph Transformer for Skeleton-based Action Recognition","summary":"  Skeleton-based action recognition aims to recognize human actions given human\njoint coordinates with skeletal interconnections. By defining a graph with\njoints as vertices and their natural connections as edges, previous works\nsuccessfully adopted Graph Convolutional networks (GCNs) to model joint\nco-occurrences and achieved superior performance. More recently, a limitation\nof GCNs is identified, i.e., the topology is fixed after training. To relax\nsuch a restriction, Self-Attention (SA) mechanism has been adopted to make the\ntopology of GCNs adaptive to the input, resulting in the state-of-the-art\nhybrid models. Concurrently, attempts with plain Transformers have also been\nmade, but they still lag behind state-of-the-art GCN-based methods due to the\nlack of structural prior. Unlike hybrid models, we propose a more elegant\nsolution to incorporate the bone connectivity into Transformer via a graph\ndistance embedding. Our embedding retains the information of skeletal structure\nduring training, whereas GCNs merely use it for initialization. More\nimportantly, we reveal an underlying issue of graph models in general, i.e.,\npairwise aggregation essentially ignores the high-order kinematic dependencies\nbetween body joints. To fill this gap, we propose a new self-attention (SA)\nmechanism on hypergraph, termed Hypergraph Self-Attention (HyperSA), to\nincorporate intrinsic higher-order relations into the model. We name the\nresulting model Hyperformer, and it beats state-of-the-art graph models w.r.t.\naccuracy and efficiency on NTU RGB+D, NTU RGB+D 120, and Northwestern-UCLA\ndatasets.\n","authors":["Yuxuan Zhou","Zhi-Qi Cheng","Chao Li","Yanwen Fang","Yifeng Geng","Xuansong Xie","Margret Keuper"],"pdf_url":"https://arxiv.org/pdf/2211.09590v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06862v1","updated":"2023-03-13T05:13:47Z","published":"2023-03-13T05:13:47Z","title":"OTOV2: Automatic, Generic, User-Friendly","summary":"  The existing model compression methods via structured pruning typically\nrequire complicated multi-stage procedures. Each individual stage necessitates\nnumerous engineering efforts and domain-knowledge from the end-users which\nprevent their wider applications onto broader scenarios. We propose the second\ngeneration of Only-Train-Once (OTOv2), which first automatically trains and\ncompresses a general DNN only once from scratch to produce a more compact model\nwith competitive performance without fine-tuning. OTOv2 is automatic and\npluggable into various deep learning applications, and requires almost minimal\nengineering efforts from the users. Methodologically, OTOv2 proposes two major\nimprovements: (i) Autonomy: automatically exploits the dependency of general\nDNNs, partitions the trainable variables into Zero-Invariant Groups (ZIGs), and\nconstructs the compressed model; and (ii) Dual Half-Space Projected Gradient\n(DHSPG): a novel optimizer to more reliably solve structured-sparsity problems.\nNumerically, we demonstrate the generality and autonomy of OTOv2 on a variety\nof model architectures such as VGG, ResNet, CARN, ConvNeXt, DenseNet and\nStackedUnets, the majority of which cannot be handled by other methods without\nextensive handcrafting efforts. Together with benchmark datasets including\nCIFAR10/100, DIV2K, Fashion-MNIST, SVNH and ImageNet, its effectiveness is\nvalidated by performing competitively or even better than the\nstate-of-the-arts. The source code is available at\nhttps://github.com/tianyic/only_train_once.\n","authors":["Tianyi Chen","Luming Liang","Tianyu Ding","Zhihui Zhu","Ilya Zharkov"],"pdf_url":"https://arxiv.org/pdf/2303.06862v1.pdf","comment":"Published on ICLR 2023. Remark here that a few images of dependency\n  graphs can not be included in arXiv due to exceeding size limit"},{"id":"http://arxiv.org/abs/2303.06860v1","updated":"2023-03-13T05:08:25Z","published":"2023-03-13T05:08:25Z","title":"View Adaptive Light Field Deblurring Networks with Depth Perception","summary":"  The Light Field (LF) deblurring task is a challenging problem as the blur\nimages are caused by different reasons like the camera shake and the object\nmotion. The single image deblurring method is a possible way to solve this\nproblem. However, since it deals with each view independently and cannot\neffectively utilize and maintain the LF structure, the restoration effect is\nusually not ideal. Besides, the LF blur is more complex because the degree is\naffected by the views and depth. Therefore, we carefully designed a novel LF\ndeblurring network based on the LF blur characteristics. On one hand, since the\nblur degree varies a lot in different views, we design a novel view adaptive\nspatial convolution to deblur blurred LFs, which calculates the exclusive\nconvolution kernel for each view. On the other hand, because the blur degree\nalso varies with the depth of the object, a depth perception view attention is\ndesigned to deblur different depth areas by selectively integrating information\nfrom different views. Besides, we introduce an angular position embedding to\nmaintain the LF structure better, which ensures the model correctly restores\nthe view information. Quantitative and qualitative experimental results on\nsynthetic and real images show that the deblurring effect of our method is\nbetter than other state-of-the-art methods.\n","authors":["Zeqi Shen","Shuo Zhang","Zhuhao Zhang","Qihua Chen","Xueyao Dong","Youfang Lin"],"pdf_url":"https://arxiv.org/pdf/2303.06860v1.pdf","comment":"9 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.06859v1","updated":"2023-03-13T05:04:18Z","published":"2023-03-13T05:04:18Z","title":"Learning Distortion Invariant Representation for Image Restoration from\n  A Causality Perspective","summary":"  In recent years, we have witnessed the great advancement of Deep neural\nnetworks (DNNs) in image restoration. However, a critical limitation is that\nthey cannot generalize well to real-world degradations with different degrees\nor types. In this paper, we are the first to propose a novel training strategy\nfor image restoration from the causality perspective, to improve the\ngeneralization ability of DNNs for unknown degradations. Our method, termed\nDistortion Invariant representation Learning (DIL), treats each distortion type\nand degree as one specific confounder, and learns the distortion-invariant\nrepresentation by eliminating the harmful confounding effect of each\ndegradation. We derive our DIL with the back-door criterion in causality by\nmodeling the interventions of different distortions from the optimization\nperspective. Particularly, we introduce counterfactual distortion augmentation\nto simulate the virtual distortion types and degrees as the confounders. Then,\nwe instantiate the intervention of each distortion with a virtual model\nupdating based on corresponding distorted images, and eliminate them from the\nmeta-learning perspective. Extensive experiments demonstrate the effectiveness\nof our DIL on the generalization capability for unseen distortion types and\ndegrees. Our code will be available at\nhttps://github.com/lixinustc/Casual-IRDIL.\n","authors":["Xin Li","Bingchen Li","Xin Jin","Cuiling Lan","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2303.06859v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.06857v1","updated":"2023-03-13T05:02:34Z","published":"2023-03-13T05:02:34Z","title":"An automated pipeline to create an atlas of in situ hybridization gene\n  expression data in the adult marmoset brain","summary":"  We present the first automated pipeline to create an atlas of in situ\nhybridization gene expression in the adult marmoset brain in the same\nstereotaxic space. The pipeline consists of segmentation of gene expression\nfrom microscopy images and registration of images to a standard space.\nAutomation of this pipeline is necessary to analyze the large volume of data in\nthe genome-wide whole-brain dataset, and to process images that have varying\nintensity profiles and expression patterns with minimal human bias. To reduce\nthe number of labelled images required for training, we develop a\nsemi-supervised segmentation model. We further develop an iterative algorithm\nto register images to a standard space, enabling comparative analysis between\ngenes and concurrent visualization with other datasets, thereby facilitating a\nmore holistic understanding of primate brain structure and function.\n","authors":["Charissa Poon","Muhammad Febrian Rachmadi","Michal Byra","Matthias Schlachter","Binbin Xu","Tomomi Shimogori","Henrik Skibbe"],"pdf_url":"https://arxiv.org/pdf/2303.06857v1.pdf","comment":"5 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.06856v1","updated":"2023-03-13T05:01:50Z","published":"2023-03-13T05:01:50Z","title":"Dynamic Neural Network for Multi-Task Learning Searching across Diverse\n  Network Topologies","summary":"  In this paper, we present a new MTL framework that searches for structures\noptimized for multiple tasks with diverse graph topologies and shares features\namong tasks. We design a restricted DAG-based central network with\nread-in/read-out layers to build topologically diverse task-adaptive structures\nwhile limiting search space and time. We search for a single optimized network\nthat serves as multiple task adaptive sub-networks using our three-stage\ntraining process. To make the network compact and discretized, we propose a\nflow-based reduction algorithm and a squeeze loss used in the training process.\nWe evaluate our optimized network on various public MTL datasets and show ours\nachieves state-of-the-art performance. An extensive ablation study\nexperimentally validates the effectiveness of the sub-module and schemes in our\nframework.\n","authors":["Wonhyeok Choi","Sunghoon Im"],"pdf_url":"https://arxiv.org/pdf/2303.06856v1.pdf","comment":"Accepted at CVPR 2023, 13 pages, 10 encapsulated postscript figures"},{"id":"http://arxiv.org/abs/2303.06854v1","updated":"2023-03-13T04:49:46Z","published":"2023-03-13T04:49:46Z","title":"Robust Contrastive Language-Image Pretraining against Adversarial\n  Attacks","summary":"  Contrastive vision-language representation learning has achieved\nstate-of-the-art performance for zero-shot classification, by learning from\nmillions of image-caption pairs crawled from the internet. However, the massive\ndata that powers large multimodal models such as CLIP, makes them extremely\nvulnerable to various types of adversarial attacks, including targeted and\nbackdoor data poisoning attacks. Despite this vulnerability, robust contrastive\nvision-language pretraining against adversarial attacks has remained\nunaddressed. In this work, we propose RoCLIP, the first effective method for\nrobust pretraining {and fine-tuning} multimodal vision-language models. RoCLIP\neffectively breaks the association between poisoned image-caption pairs by\nconsidering a pool of random examples, and (1) matching every image with the\ntext that is most similar to its caption in the pool, and (2) matching every\ncaption with the image that is most similar to its image in the pool. Our\nextensive experiments show that our method renders state-of-the-art targeted\ndata poisoning and backdoor attacks ineffective during pre-training or\nfine-tuning of CLIP. In particular, RoCLIP decreases the poison and backdoor\nattack success rates down to 0\\% during pre-training and 1\\%-4\\% during\nfine-tuning, and effectively improves the model's performance.\n","authors":["Wenhan Yang","Baharan Mirzasoleiman"],"pdf_url":"https://arxiv.org/pdf/2303.06854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06852v1","updated":"2023-03-13T04:47:15Z","published":"2023-03-13T04:47:15Z","title":"One-Shot Segmentation of Novel White Matter Tracts via Extensive Data\n  Augmentation","summary":"  Deep learning based methods have achieved state-of-the-art performance for\nautomated white matter (WM) tract segmentation. In these methods, the\nsegmentation model needs to be trained with a large number of manually\nannotated scans, which can be accumulated throughout time. When novel WM\ntracts, i.e., tracts not included in the existing annotated WM tracts, are to\nbe segmented, additional annotations of these novel WM tracts need to be\ncollected. Since tract annotation is time-consuming and costly, it is desirable\nto make only a few annotations of novel WM tracts for training the segmentation\nmodel, and previous work has addressed this problem by transferring the\nknowledge learned for segmenting existing WM tracts to the segmentation of\nnovel WM tracts. However, accurate segmentation of novel WM tracts can still be\nchallenging in the one-shot setting, where only one scan is annotated for the\nnovel WM tracts. In this work, we explore the problem of one-shot segmentation\nof novel WM tracts. Since in the one-shot setting the annotated training data\nis extremely scarce, based on the existing knowledge transfer framework, we\npropose to further perform extensive data augmentation for the single annotated\nscan, where synthetic annotated training data is produced. We have designed\nseveral different strategies that mask out regions in the single annotated scan\nfor data augmentation. Our method was evaluated on public and in-house\ndatasets. The experimental results show that our method improves the accuracy\nof one-shot segmentation of novel WM tracts.\n","authors":["Wan Liu","Qi Lu","ZhiZheng Zhuo","Yaou Liu","Chuyang Ye"],"pdf_url":"https://arxiv.org/pdf/2303.06852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05970v2","updated":"2023-03-13T04:41:36Z","published":"2023-03-10T15:01:51Z","title":"Exploring Recurrent Long-term Temporal Fusion for Multi-view 3D\n  Perception","summary":"  Long-term temporal fusion is a crucial but often overlooked technique in\ncamera-based Bird's-Eye-View (BEV) 3D perception. Existing methods are mostly\nin a parallel manner. While parallel fusion can benefit from long-term\ninformation, it suffers from increasing computational and memory overheads as\nthe fusion window size grows. Alternatively, BEVFormer adopts a recurrent\nfusion pipeline so that history information can be efficiently integrated, yet\nit fails to benefit from longer temporal frames. In this paper, we explore an\nembarrassingly simple long-term recurrent fusion strategy built upon the\nLSS-based methods and find it already able to enjoy the merits from both sides,\ni.e., rich long-term information and efficient fusion pipeline. A temporal\nembedding module is further proposed to improve the model's robustness against\noccasionally missed frames in practical scenarios. We name this simple but\neffective fusing pipeline VideoBEV. Experimental results on the nuScenes\nbenchmark show that VideoBEV obtains leading performance on various\ncamera-based 3D perception tasks, including object detection (55.4% mAP and\n62.9% NDS), segmentation (48.6% vehicle mIoU), tracking (54.8% AMOTA), and\nmotion prediction (0.80m minADE and 0.463 EPA). Code will be available.\n","authors":["Chunrui Han","Jianjian Sun","Zheng Ge","Jinrong Yang","Runpei Dong","Hongyu Zhou","Weixin Mao","Yuang Peng","Xiangyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.05970v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06842v1","updated":"2023-03-13T04:16:42Z","published":"2023-03-13T04:16:42Z","title":"Scene Graph Generation from Hierarchical Relationship Reasoning","summary":"  This paper describes a novel approach to deducing relationships between\nobjects in a visual scene. It explicitly exploits an informative hierarchical\nstructure that can be imposed to divide the object and relationship categories\ninto disjoint super-categories. Specifically, our proposed scheme implements a\nBayes prediction head to jointly predict the super-category or type of\nrelationship between the two objects, along with the detailed relationship\nwithin that super-category. This design reduces the impact of class imbalance\nproblems. We present experimental results on the Visual Genome and OpenImage V6\ndatasets showing that this factorized approach allows a relatively simple model\nto achieve competitive performance, especially on predicate classification and\nzero-shot tasks.\n","authors":["Bowen Jiang","Camillo J. Taylor"],"pdf_url":"https://arxiv.org/pdf/2303.06842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06840v1","updated":"2023-03-13T04:06:42Z","published":"2023-03-13T04:06:42Z","title":"DDFM: Denoising Diffusion Model for Multi-Modality Image Fusion","summary":"  Multi-modality image fusion aims to combine different modalities to produce\nfused images that retain the complementary features of each modality, such as\nfunctional highlights and texture details. To leverage strong generative priors\nand address challenges such as unstable training and lack of interpretability\nfor GAN-based generative methods, we propose a novel fusion algorithm based on\nthe denoising diffusion probabilistic model (DDPM). The fusion task is\nformulated as a conditional generation problem under the DDPM sampling\nframework, which is further divided into an unconditional generation subproblem\nand a maximum likelihood subproblem. The latter is modeled in a hierarchical\nBayesian manner with latent variables and inferred by the\nexpectation-maximization algorithm. By integrating the inference solution into\nthe diffusion sampling iteration, our method can generate high-quality fused\nimages with natural image generative priors and cross-modality information from\nsource images. Note that all we required is an unconditional pre-trained\ngenerative model, and no fine-tuning is needed. Our extensive experiments\nindicate that our approach yields promising fusion results in infrared-visible\nimage fusion and medical image fusion. The code will be released.\n","authors":["Zixiang Zhao","Haowen Bai","Yuanzhi Zhu","Jiangshe Zhang","Shuang Xu","Yulun Zhang","Kai Zhang","Deyu Meng","Radu Timofte","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2303.06840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.00648v3","updated":"2023-03-13T04:06:24Z","published":"2022-12-01T16:49:53Z","title":"One-shot recognition of any material anywhere using contrastive learning\n  with physics-based rendering","summary":"  Visual recognition of materials and their states is essential for\nunderstanding most aspects of the world, from determining whether food is\ncooked, metal is rusted, or a chemical reaction has occurred. However, current\nimage recognition methods are limited to specific classes and properties and\ncan't handle the vast number of material states and textures in the world. To\naddress this, we present MatSim: the first dataset and benchmark for computer\nvision-based recognition of similarities and transitions between materials and\ntextures, focusing on identifying any material under any conditions using one\nor a few examples. The dataset contains synthetic and real images. The\nsynthetic images were rendered using giant collections of textures, objects,\nand environments generated by computer graphics artists. We use mixtures and\ngradual transitions between materials to allow the system to learn cases with\nsmooth transitions between states (like gradually cooked food). We also render\nimages with materials inside transparent containers to support beverage and\nchemistry lab use cases. We use this dataset to train a siamese net that\nidentifies the same material in different objects, mixtures, and environments.\nThe descriptor generated by this net can be used to identify the states of\nmaterials and their subclasses using a single image. We also present the first\nfew-shot material recognition benchmark with images from a wide range of\nfields, including the state of metals and chemical reactions types of ground\nand many other use cases. We show that a net trained on the MatSim synthetic\ndataset outperforms state-of-the-art models like Clip on the benchmark and also\nachieves good results on other unsupervised material classification tasks.\n","authors":["Manuel S. Drehwald","Sagi Eppel","Jolina Li","Han Hao","Alan Aspuru-Guzik"],"pdf_url":"https://arxiv.org/pdf/2212.00648v3.pdf","comment":"for associated code and dataset, see\n  https://zenodo.org/record/7390166#.Y5ku6mHMJH4 or\n  https://e1.pcloud.link/publink/show?code=kZIiSQZCYU5M4HOvnQykql9jxF4h0KiC5MX\n  and https://icedrive.net/s/A13FWzZ8V2aP9T4ufGQ1N3fBZxDF"},{"id":"http://arxiv.org/abs/2303.06834v1","updated":"2023-03-13T03:31:29Z","published":"2023-03-13T03:31:29Z","title":"DarkVisionNet: Low-Light Imaging via RGB-NIR Fusion with Deep\n  Inconsistency Prior","summary":"  RGB-NIR fusion is a promising method for low-light imaging. However,\nhigh-intensity noise in low-light images amplifies the effect of structure\ninconsistency between RGB-NIR images, which fails existing algorithms. To\nhandle this, we propose a new RGB-NIR fusion algorithm called Dark Vision Net\n(DVN) with two technical novelties: Deep Structure and Deep Inconsistency Prior\n(DIP). The Deep Structure extracts clear structure details in deep multiscale\nfeature space rather than raw input space, which is more robust to noisy\ninputs. Based on the deep structures from both RGB and NIR domains, we\nintroduce the DIP to leverage the structure inconsistency to guide the fusion\nof RGB-NIR. Benefiting from this, the proposed DVN obtains high-quality\nlowlight images without the visual artifacts. We also propose a new dataset\ncalled Dark Vision Dataset (DVD), consisting of aligned RGB-NIR image pairs, as\nthe first public RGBNIR fusion benchmark. Quantitative and qualitative results\non the proposed benchmark show that DVN significantly outperforms other\ncomparison algorithms in PSNR and SSIM, especially in extremely low light\nconditions.\n","authors":["Shuangping Jin","Bingbing Yu","Minhao Jing","Yi Zhou","Jiajun Liang","Renhe Ji"],"pdf_url":"https://arxiv.org/pdf/2303.06834v1.pdf","comment":"Accepted to AAAI 2022"},{"id":"http://arxiv.org/abs/2212.13906v2","updated":"2023-03-13T03:21:43Z","published":"2022-12-24T17:59:01Z","title":"DiP: Learning Discriminative Implicit Parts for Person Re-Identification","summary":"  In person re-identification (ReID) tasks, many works explore the learning of\npart features to improve the performance over global image features. Existing\nmethods explicitly extract part features by either using a hand-designed image\ndivision or keypoints obtained with external visual systems. In this work, we\npropose to learn Discriminative implicit Parts (DiPs) which are decoupled from\nexplicit body parts. Therefore, DiPs can learn to extract any discriminative\nfeatures that can benefit in distinguishing identities, which is beyond\npredefined body parts (such as accessories). Moreover, we propose a novel\nimplicit position to give a geometric interpretation for each DiP. The implicit\nposition can also serve as a learning signal to encourage DiPs to be more\nposition-equivariant with the identity in the image. Lastly, an additional DiP\nweighting is introduced to handle the invisible or occluded situation and\nfurther improve the feature representation of DiPs. Extensive experiments show\nthat the proposed method achieves state-of-the-art performance on multiple\nperson ReID benchmarks.\n","authors":["Dengjie Li","Siyu Chen","Yujie Zhong","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2212.13906v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06823v1","updated":"2023-03-13T02:49:50Z","published":"2023-03-13T02:49:50Z","title":"Instate: Predicting the State of Residence From Last Name","summary":"  India has twenty-two official languages. Serving such a diverse language base\nis a challenge for survey statisticians, call center operators, software\ndevelopers, and other such service providers. To help provide better services\nto different language communities via better localization, we introduce a new\nmachine learning model that predicts the language(s) that the user can speak\nfrom their name. Using nearly 438M records spanning 33 Indian states and 1.13M\nunique last names from the Indian Electoral Rolls Corpus (?), we build a\ncharacter-level transformer-based machine-learning model that predicts the\nstate of residence based on the last name. The model has a top-3 accuracy of\n85.3% on unseen names. We map the states to languages using the Indian census\nto infer languages understood by the respondent. We provide open-source\nsoftware that implements the method discussed in the paper.\n","authors":["Atul Dhingra","Gaurav Sood"],"pdf_url":"https://arxiv.org/pdf/2303.06823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06821v1","updated":"2023-03-13T02:48:54Z","published":"2023-03-13T02:48:54Z","title":"SDF-3DGAN: A 3D Object Generative Method Based on Implicit Signed\n  Distance Function","summary":"  In this paper, we develop a new method, termed SDF-3DGAN, for 3D object\ngeneration and 3D-Aware image synthesis tasks, which introduce implicit Signed\nDistance Function (SDF) as the 3D object representation method in the\ngenerative field. We apply SDF for higher quality representation of 3D object\nin space and design a new SDF neural renderer, which has higher efficiency and\nhigher accuracy. To train only on 2D images, we first generate the objects,\nwhich are represented by SDF, from Gaussian distribution. Then we render them\nto 2D images and use them to apply GAN training method together with 2D images\nin the dataset. In the new rendering method, we relieve all the potential of\nSDF mathematical property to alleviate computation pressure in the previous SDF\nneural renderer. In specific, our new SDF neural renderer can solve the problem\nof sampling ambiguity when the number of sampling point is not enough, \\ie use\nthe less points to finish higher quality sampling task in the rendering\npipeline. And in this rendering pipeline, we can locate the surface easily.\nTherefore, we apply normal loss on it to control the smoothness of generated\nobject surface, which can make our method enjoy the much higher generation\nquality. Quantitative and qualitative experiments conducted on public\nbenchmarks demonstrate favorable performance against the state-of-the-art\nmethods in 3D object generation task and 3D-Aware image synthesis task. Our\ncodes will be released at https://github.com/lutao2021/SDF-3DGAN.\n","authors":["Lutao Jiang","Ruyi Ji","Libo Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.06821v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05933v2","updated":"2023-03-13T02:45:40Z","published":"2023-03-10T14:11:09Z","title":"Boosting Open-Set Domain Adaptation with Threshold Self-Tuning and\n  Cross-Domain Mixup","summary":"  Open-set domain adaptation (OSDA) aims to not only recognize target samples\nbelonging to common classes shared by source and target domains but also\nperceive unknown class samples. Existing OSDA methods suffer from two\nobstacles. Firstly, a tedious process of manually tuning a hyperparameter\n$threshold$ is required for most OSDA approaches to separate common and unknown\nclasses. It is difficult to determine a proper threshold when the target domain\ndata is unlabeled. Secondly, most OSDA methods rely only on confidence values\nto distinguish between common and unknown classes, using limited source and\ntarget samples to train models, leading to unsatisfactory performance when the\ntarget domain has mostly unknown classes. Our studies demonstrate that\nexploiting multiple criteria within a more continuous latent space is\nbeneficial for the model's performance. In this paper, we design a novel\nthreshold self-tuning and cross-domain mixup (TSCM) method to overcome the two\ndrawbacks. TSCM can automatically tune a proper threshold utilizing unlabeled\ntarget samples rather than manually setting an empirical hyperparameter. Our\nmethod considers multiple criteria instead of only the confidence and uses the\nthreshold generated by itself to separate common and unknown classes in the\ntarget domain. Moreover, we introduce a cross-domain mixup method designed for\nOSDA scenarios to learn domain-invariant features in a more continuous latent\nspace. Comprehensive experiments illustrate that our method consistently\nachieves superior performance on different benchmarks compared with various\nstate-of-the-art methods.\n","authors":["Xinghong Liu","Yi Zhou","Tao Zhou","Jie Qin","Shengcai Liao"],"pdf_url":"https://arxiv.org/pdf/2303.05933v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06820v1","updated":"2023-03-13T02:33:34Z","published":"2023-03-13T02:33:34Z","title":"Continuous sign language recognition based on cross-resolution knowledge\n  distillation","summary":"  The goal of continuous sign language recognition(CSLR) research is to apply\nCSLR models as a communication tool in real life, and the real-time requirement\nof the models is important. In this paper, we address the model real-time\nproblem through cross-resolution knowledge distillation. In our study, we found\nthat keeping the frame-level feature scales consistent between the output of\nthe student network and the teacher network is better than recovering the\nframe-level feature sizes for feature distillation. Based on this finding, we\npropose a new frame-level feature extractor that keeps the output frame-level\nfeatures at the same scale as the output of by the teacher network. We further\ncombined with the TSCM+2D hybrid convolution proposed in our previous study to\nform a new lightweight end-to-end CSLR network-Low resolution input\nnet(LRINet). It is then used to combine cross-resolution knowledge distillation\nand traditional knowledge distillation methods to form a CSLR model based on\ncross-resolution knowledge distillation (CRKD). The CRKD uses high-resolution\nframes as input to the teacher network for training, locks the weights after\ntraining, and then uses low-resolution frames as input to the student network\nLRINet to perform knowledge distillation on frame-level features and\nclassification features respectively. Experiments on two large-scale continuous\nsign language datasets have proved the effectiveness of CRKD. Compared with the\nmodel with high-resolution data as input, the calculation amount, parameter\namount and inference time of the model have been significantly reduced under\nthe same experimental conditions, while ensuring the accuracy of the model, and\nhas achieved very competitive results in comparison with other advanced\nmethods.\n","authors":["Qidan Zhu","Jing Li","Fei Yuan","Quan Gan"],"pdf_url":"https://arxiv.org/pdf/2303.06820v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.06819v1","updated":"2023-03-13T02:27:45Z","published":"2023-03-13T02:27:45Z","title":"TranSG: Transformer-Based Skeleton Graph Prototype Contrastive Learning\n  with Structure-Trajectory Prompted Reconstruction for Person\n  Re-Identification","summary":"  Person re-identification (re-ID) via 3D skeleton data is an emerging topic\nwith prominent advantages. Existing methods usually design skeleton descriptors\nwith raw body joints or perform skeleton sequence representation learning.\nHowever, they typically cannot concurrently model different body-component\nrelations, and rarely explore useful semantics from fine-grained\nrepresentations of body joints. In this paper, we propose a generic\nTransformer-based Skeleton Graph prototype contrastive learning (TranSG)\napproach with structure-trajectory prompted reconstruction to fully capture\nskeletal relations and valuable spatial-temporal semantics from skeleton graphs\nfor person re-ID. Specifically, we first devise the Skeleton Graph Transformer\n(SGT) to simultaneously learn body and motion relations within skeleton graphs,\nso as to aggregate key correlative node features into graph representations.\nThen, we propose the Graph Prototype Contrastive learning (GPC) to mine the\nmost typical graph features (graph prototypes) of each identity, and contrast\nthe inherent similarity between graph representations and different prototypes\nfrom both skeleton and sequence levels to learn discriminative graph\nrepresentations. Last, a graph Structure-Trajectory Prompted Reconstruction\n(STPR) mechanism is proposed to exploit the spatial and temporal contexts of\ngraph nodes to prompt skeleton graph reconstruction, which facilitates\ncapturing more valuable patterns and graph semantics for person re-ID.\nEmpirical evaluations demonstrate that TranSG significantly outperforms\nexisting state-of-the-art methods. We further show its generality under\ndifferent graph modeling, RGB-estimated skeletons, and unsupervised scenarios.\n","authors":["Haocong Rao","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2303.06819v1.pdf","comment":"Accepted by CVPR 2023. Codes are available at\n  https://github.com/Kali-Hac/TranSG. Supplemental material is included in the\n  conference proceedings"},{"id":"http://arxiv.org/abs/2303.06817v1","updated":"2023-03-13T02:21:38Z","published":"2023-03-13T02:21:38Z","title":"Transformation-Invariant Network for Few-Shot Object Detection in Remote\n  Sensing Images","summary":"  Object detection in remote sensing images relies on a large amount of labeled\ndata for training. The growing new categories and class imbalance render\nexhaustive annotation non-scalable. Few-shot object detection~(FSOD) tackles\nthis issue by meta-learning on seen base classes and then fine-tuning on novel\nclasses with few labeled samples. However, the object's scale and orientation\nvariations are particularly large in remote sensing images, thus posing\nchallenges to existing few-shot object detection methods. To tackle these\nchallenges, we first propose to integrate a feature pyramid network and use\nprototype features to highlight query features to improve upon existing FSOD\nmethods. We refer to the modified FSOD as a Strong Baseline which is\ndemonstrated to perform significantly better than the original baselines. To\nimprove the robustness of orientation variation, we further propose a\ntransformation-invariant network (TINet) to allow the network to be invariant\nto geometric transformations. Extensive experiments on three widely used remote\nsensing object detection datasets, i.e., NWPU VHR-10.v2, DIOR, and HRRSD\ndemonstrated the effectiveness of the proposed method. Finally, we reproduced\nmultiple FSOD methods for remote sensing images to create an extensive\nbenchmark for follow-up works.\n","authors":["Nanqing Liu","Xun Xu","Turgay Celik","Zongxin Gan","Heng-Chao Li"],"pdf_url":"https://arxiv.org/pdf/2303.06817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.14074v3","updated":"2023-03-13T02:00:41Z","published":"2022-09-28T13:15:03Z","title":"Recipro-CAM: Fast gradient-free visual explanations for convolutional\n  neural networks","summary":"  The Convolutional Neural Network (CNN) is a widely used deep learning\narchitecture for computer vision. However, its black box nature makes it\ndifficult to interpret the behavior of the model. To mitigate this issue, AI\npractitioners have explored explainable AI methods like Class Activation Map\n(CAM) and Grad-CAM. Although these methods have shown promise, they are limited\nby architectural constraints or the burden of gradient computing. To overcome\nthis issue, Score-CAM and Ablation-CAM have been proposed as gradient-free\nmethods, but they have longer execution times compared to CAM or Grad-CAM based\nmethods, making them unsuitable for real-world solution though they resolved\ngradient related issues and enabled inference mode XAI. To address this\nchallenge, we propose a fast gradient-free Reciprocal CAM (Recipro-CAM) method.\nOur approach involves spatially masking the extracted feature maps to exploit\nthe correlation between activation maps and network predictions for target\nclasses. Our proposed method has yielded promising results, outperforming\ncurrent state-of-the-art method in the Average Drop-Coherence-Complexity (ADCC)\nmetric by $1.78 \\%$ to $3.72 \\%$, excluding VGG-16 backbone. Moreover,\nRecipro-CAM generates saliency maps at a similar rate to Grad-CAM and is\napproximately $148$ times faster than Score-CAM. The source code for\nRecipro-CAM is available in our data analysis framework.\n","authors":["Seok-Yong Byun","Wonju Lee"],"pdf_url":"https://arxiv.org/pdf/2209.14074v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06810v1","updated":"2023-03-13T01:56:53Z","published":"2023-03-13T01:56:53Z","title":"Dynamic Clustering and Cluster Contrastive Learning for Unsupervised\n  Person Re-identification","summary":"  Unsupervised Re-ID methods aim at learning robust and discriminative features\nfrom unlabeled data. However, existing methods often ignore the relationship\nbetween module parameters of Re-ID framework and feature distributions, which\nmay lead to feature misalignment and hinder the model performance. To address\nthis problem, we propose a dynamic clustering and cluster contrastive learning\n(DCCC) method. Specifically, we first design a dynamic clustering parameters\nscheduler (DCPS) which adjust the hyper-parameter of clustering to fit the\nvariation of intra- and inter-class distances. Then, a dynamic cluster\ncontrastive learning (DyCL) method is designed to match the cluster\nrepresentation vectors' weights with the local feature association. Finally, a\nlabel smoothing soft contrastive loss ($L_{ss}$) is built to keep the balance\nbetween cluster contrastive learning and self-supervised learning with low\ncomputational consumption and high computational efficiency. Experiments on\nseveral widely used public datasets validate the effectiveness of our proposed\nDCCC which outperforms previous state-of-the-art methods by achieving the best\nperformance.\n","authors":["Ziqi He","Mengjia Xue","Yunhao Du","Zhicheng Zhao","Fei Su"],"pdf_url":"https://arxiv.org/pdf/2303.06810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06807v1","updated":"2023-03-13T01:42:29Z","published":"2023-03-13T01:42:29Z","title":"Vessel-Promoted OCT to OCTA Image Translation by Heuristic Contextual\n  Constraints","summary":"  Optical Coherence Tomography Angiography (OCTA) has become increasingly vital\nin the clinical screening of fundus diseases due to its ability to capture\naccurate 3D imaging of blood vessels in a non-contact scanning manner. However,\nthe acquisition of OCTA images remains challenging due to the requirement of\nexclusive sensors and expensive devices. In this paper, we propose a novel\nframework, TransPro, that translates 3D Optical Coherence Tomography (OCT)\nimages into exclusive 3D OCTA images using an image translation pattern. Our\nmain objective is to address two issues in existing image translation\nbaselines, namely, the aimlessness in the translation process and\nincompleteness of the translated object. The former refers to the overall\nquality of the translated OCTA images being satisfactory, but the retinal\nvascular quality being low. The latter refers to incomplete objects in\ntranslated OCTA images due to the lack of global contexts. TransPro merges a 2D\nretinal vascular segmentation model and a 2D OCTA image translation model into\na 3D image translation baseline for the 2D projection map projected by the\ntranslated OCTA images. The 2D retinal vascular segmentation model can improve\nattention to the retinal vascular, while the 2D OCTA image translation model\nintroduces beneficial heuristic contextual information. Extensive experimental\nresults on two challenging datasets demonstrate that TransPro can consistently\noutperform existing approaches with minimal computational overhead during\ntraining and none during testing.\n","authors":["Shuhan Li","Dong Zhang","Xiaomeng Li","Chubin Ou","Lin An","Yanwu Xu","Kwang-Ting Cheng"],"pdf_url":"https://arxiv.org/pdf/2303.06807v1.pdf","comment":"Code is available at: https://github.com/ustlsh/TransPro"},{"id":"http://arxiv.org/abs/2303.06800v1","updated":"2023-03-13T01:10:50Z","published":"2023-03-13T01:10:50Z","title":"Object-Centric Multi-Task Learning for Human Instances","summary":"  Human is one of the most essential classes in visual recognition tasks such\nas detection, segmentation, and pose estimation. Although much effort has been\nput into individual tasks, multi-task learning for these three tasks has been\nrarely studied. In this paper, we explore a compact multi-task network\narchitecture that maximally shares the parameters of the multiple tasks via\nobject-centric learning. To this end, we propose a novel query design to encode\nthe human instance information effectively, called human-centric query (HCQ).\nHCQ enables for the query to learn explicit and structural information of human\nas well such as keypoints. Besides, we utilize HCQ in prediction heads of the\ntarget tasks directly and also interweave HCQ with the deformable attention in\nTransformer decoders to exploit a well-learned object-centric representation.\nExperimental results show that the proposed multi-task network achieves\ncomparable accuracy to state-of-the-art task-specific models in human\ndetection, segmentation, and pose estimation task, while it consumes less\ncomputational costs.\n","authors":["Hyeongseok Son","Sangil Jung","Solae Lee","Seongeun Kim","Seung-In Park","ByungIn Yoo"],"pdf_url":"https://arxiv.org/pdf/2303.06800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06797v1","updated":"2023-03-13T01:07:32Z","published":"2023-03-13T01:07:32Z","title":"Orthogonal Transform Domain Approaches for the Convolutional Layer","summary":"  In this paper, we propose a set of transform-based neural network layers as\nan alternative to the $3\\times3$ Conv2D layers in Convolutional Neural Networks\n(CNNs). The proposed layers can be implemented based on orthogonal transforms\nsuch as Discrete Cosine Transform (DCT) and Hadamard transform (HT), and the\nbiorthogonal Block Wavelet Transform (BWT). Convolutional filtering operations\nare performed in the transform domain using element-wise multiplications by\ntaking advantage of the convolution theorems. Trainable soft-thresholding\nlayers that remove noise in the transform domain bring nonlinearity to the\ntransform domain layers. Compared to the Conv2D layer which is spatial-agnostic\nand channel-specific, the proposed layers are location-specific and\nchannel-specific. The proposed layers reduce the number of parameters and\nmultiplications significantly while improving the accuracy results of regular\nResNets on the ImageNet-1K classification task. Furthermore, the proposed\nlayers can be inserted with a batch normalization layer before the global\naverage pooling layer in the conventional ResNets as an additional layer to\nimprove classification accuracy with a negligible increase in the number of\nparameters and computational cost.\n","authors":["Hongyi Pan","Xin Zhu","Salih Atici","Ahmet Enis Cetin"],"pdf_url":"https://arxiv.org/pdf/2303.06797v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2211.08577"},{"id":"http://arxiv.org/abs/2303.06796v1","updated":"2023-03-13T01:02:28Z","published":"2023-03-13T01:02:28Z","title":"Ins-ATP: Deep Estimation of ATP for Organoid Based on High Throughput\n  Microscopic Images","summary":"  Adenosine triphosphate (ATP) is a high-energy phosphate compound and the most\ndirect energy source in organisms. ATP is an essential biomarker for evaluating\ncell viability in biology. Researchers often use ATP bioluminescence to measure\nthe ATP of organoid after drug to evaluate the drug efficacy. However, ATP\nbioluminescence has some limitations, leading to unreliable drug screening\nresults. Performing ATP bioluminescence causes cell lysis of organoids, so it\nis impossible to observe organoids' long-term viability changes after\nmedication continually. To overcome the disadvantages of ATP bioluminescence,\nwe propose Ins-ATP, a non-invasive strategy, the first organoid ATP estimation\nmodel based on the high-throughput microscopic image. Ins-ATP directly\nestimates the ATP of organoids from high-throughput microscopic images, so that\nit does not influence the drug reactions of organoids. Therefore, the ATP\nchange of organoids can be observed for a long time to obtain more stable\nresults. Experimental results show that the ATP estimation by Ins-ATP is in\ngood agreement with those determined by ATP bioluminescence. Specifically, the\npredictions of Ins-ATP are consistent with the results measured by ATP\nbioluminescence in the efficacy evaluation experiments of different drugs.\n","authors":["Xuesheng Bian","Cheng Wang","Shuting Chen","Weiquan Liu","Sen Xu","Jinxin Zhu","Rugang Wang","Zexin Chen","Min Huang","Gang Li"],"pdf_url":"https://arxiv.org/pdf/2303.06796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06015v2","updated":"2023-03-13T00:32:29Z","published":"2023-03-10T16:19:34Z","title":"Dynamic Y-KD: A Hybrid Approach to Continual Instance Segmentation","summary":"  Despite the success of deep learning methods on instance segmentation, these\nmodels still suffer from catastrophic forgetting in continual learning\nscenarios. In this paper, our contributions for continual instance segmentation\nare threefold. First, we propose the Y-knowledge distillation (Y-KD), a\nknowledge distillation strategy that shares a common feature extractor between\nthe teacher and student networks. As the teacher is also updated with new data\nin Y-KD, the increased plasticity results in new modules that are specialized\non new classes. Second, our Y-KD approach is supported by a dynamic\narchitecture method that grows new modules for each task and uses all of them\nfor inference with a unique instance segmentation head, which significantly\nreduces forgetting. Third, we complete our approach by leveraging checkpoint\naveraging as a simple method to manually balance the trade-off between the\nperformance on the various sets of classes, thus increasing the control over\nthe model's behavior without any additional cost. These contributions are\nunited in our model that we name the Dynamic Y-KD network.\n  We perform extensive experiments on several single-step and multi-steps\nscenarios on Pascal-VOC, and we show that our approach outperforms previous\nmethods both on past and new classes. For instance, compared to recent work,\nour method obtains +2.1% mAP on old classes in 15-1, +7.6% mAP on new classes\nin 19-1 and reaches 91.5% of the mAP obtained by joint-training on all classes\nin 15-5.\n","authors":["Mathieu Pagé-Fortin","Brahim Chaib-draa"],"pdf_url":"https://arxiv.org/pdf/2303.06015v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.07347v1","updated":"2023-03-13T17:59:59Z","published":"2023-03-13T17:59:59Z","title":"TriDet: Temporal Action Detection with Relative Boundary Modeling","summary":"  In this paper, we present a one-stage framework TriDet for temporal action\ndetection. Existing methods often suffer from imprecise boundary predictions\ndue to the ambiguous action boundaries in videos. To alleviate this problem, we\npropose a novel Trident-head to model the action boundary via an estimated\nrelative probability distribution around the boundary. In the feature pyramid\nof TriDet, we propose an efficient Scalable-Granularity Perception (SGP) layer\nto mitigate the rank loss problem of self-attention that takes place in the\nvideo features and aggregate information across different temporal\ngranularities. Benefiting from the Trident-head and the SGP-based feature\npyramid, TriDet achieves state-of-the-art performance on three challenging\nbenchmarks: THUMOS14, HACS and EPIC-KITCHEN 100, with lower computational\ncosts, compared to previous methods. For example, TriDet hits an average mAP of\n$69.3\\%$ on THUMOS14, outperforming the previous best by $2.5\\%$, but with only\n$74.6\\%$ of its latency. The code is released to\nhttps://github.com/sssste/TriDet.\n","authors":["Dingfeng Shi","Yujie Zhong","Qiong Cao","Lin Ma","Jia Li","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.07347v1.pdf","comment":"CVPR2023; Temporal Action Detection; Temporal Action Localization"},{"id":"http://arxiv.org/abs/2303.07240v1","updated":"2023-03-13T16:13:16Z","published":"2023-03-13T16:13:16Z","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical\n  Documents","summary":"  Foundation models trained on large-scale dataset gain a recent surge in CV\nand NLP. In contrast, development in biomedical domain lags far behind due to\ndata scarcity. To address this issue, we build and release PMC-OA, a biomedical\ndataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess\nsubset, which is 8 times larger than before. PMC-OA covers diverse modalities\nor diseases, with majority of the image-caption samples aligned at\nfiner-grained level, i.e., subfigure and subcaption. While pretraining a\nCLIP-style model on PMC-OA, our model named PMC-CLIP achieves state-of-the-art\nresults on various downstream tasks, including image-text retrieval on ROCO,\nMedMNIST image classification, Medical VQA, i.e. +8.1% R@10 on image-text\nretrieval, +3.9% accuracy on image classification.\n","authors":["Weixiong Lin","Ziheng Zhao","Xiaoman Zhang","Chaoyi Wu","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2303.07240v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2201.08071v3","updated":"2023-03-13T13:55:30Z","published":"2022-01-20T09:10:20Z","title":"Temporal Sentence Grounding in Videos: A Survey and Future Directions","summary":"  Temporal sentence grounding in videos (TSGV), \\aka natural language video\nlocalization (NLVL) or video moment retrieval (VMR), aims to retrieve a\ntemporal moment that semantically corresponds to a language query from an\nuntrimmed video. Connecting computer vision and natural language, TSGV has\ndrawn significant attention from researchers in both communities. This survey\nattempts to provide a summary of fundamental concepts in TSGV and current\nresearch status, as well as future research directions. As the background, we\npresent a common structure of functional components in TSGV, in a tutorial\nstyle: from feature extraction from raw video and language query, to answer\nprediction of the target moment. Then we review the techniques for multimodal\nunderstanding and interaction, which is the key focus of TSGV for effective\nalignment between the two modalities. We construct a taxonomy of TSGV\ntechniques and elaborate the methods in different categories with their\nstrengths and weaknesses. Lastly, we discuss issues with the current TSGV\nresearch and share our insights about promising research directions.\n","authors":["Hao Zhang","Aixin Sun","Wei Jing","Joey Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2201.08071v3.pdf","comment":"Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)"},{"id":"http://arxiv.org/abs/2202.03861v4","updated":"2023-03-13T07:29:12Z","published":"2022-02-08T13:46:42Z","title":"Towards Making a Trojan-horse Attack on Text-to-Image Retrieval","summary":"  While deep learning based image retrieval is reported to be vulnerable to\nadversarial attacks, existing works are mainly on image-to-image retrieval with\ntheir attacks performed at the front end via query modification. By contrast,\nwe present in this paper the first study about a threat that occurs at the back\nend of a text-to-image retrieval (T2IR) system. Our study is motivated by the\nfact that the image collection indexed by the system will be regularly updated\ndue to the arrival of new images from various sources such as web crawlers and\nadvertisers. With malicious images indexed, it is possible for an attacker to\nindirectly interfere with the retrieval process, letting users see certain\nimages that are completely irrelevant w.r.t. their queries. We put this thought\ninto practice by proposing a novel Trojan-horse attack (THA). In particular, we\nconstruct a set of Trojan-horse images by first embedding word-specific\nadversarial information into a QR code and then putting the code on benign\nadvertising images. A proof-of-concept evaluation, conducted on two popular\nT2IR datasets (Flickr30k and MS-COCO), shows the effectiveness of the proposed\nTHA in a white-box mode.\n","authors":["Fan Hu","Aozhu Chen","Xirong Li"],"pdf_url":"https://arxiv.org/pdf/2202.03861v4.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2206.07331v2","updated":"2023-03-13T06:52:04Z","published":"2022-06-15T07:26:27Z","title":"ETMA: Efficient Transformer Based Multilevel Attention framework for\n  Multimodal Fake News Detection","summary":"  In this new digital era, social media has created a severe impact on the\nlives of people. In recent times, fake news content on social media has become\none of the major challenging problems for society. The dissemination of\nfabricated and false news articles includes multimodal data in the form of text\nand images. The previous methods have mainly focused on unimodal analysis.\nMoreover, for multimodal analysis, researchers fail to keep the unique\ncharacteristics corresponding to each modality. This paper aims to overcome\nthese limitations by proposing an Efficient Transformer based Multilevel\nAttention (ETMA) framework for multimodal fake news detection, which comprises\nthe following components: visual attention-based encoder, textual\nattention-based encoder, and joint attention-based learning. Each component\nutilizes the different forms of attention mechanism and uniquely deals with\nmultimodal data to detect fraudulent content. The efficacy of the proposed\nnetwork is validated by conducting several experiments on four real-world fake\nnews datasets: Twitter, Jruvika Fake News Dataset, Pontes Fake News Dataset,\nand Risdal Fake News Dataset using multiple evaluation metrics. The results\nshow that the proposed method outperforms the baseline methods on all four\ndatasets. Further, the computation time of the model is also lower than the\nstate-of-the-art methods.\n","authors":["Ashima Yadav","Shivani Gaba","Haneef Khan","Ishan Budhiraja","Akansha Singh","Krishan Kant Singh"],"pdf_url":"https://arxiv.org/pdf/2206.07331v2.pdf","comment":"Accepted for publication in IEEE Transactions on Computational Social\n  Systems"},{"id":"http://arxiv.org/abs/2303.06859v1","updated":"2023-03-13T05:04:18Z","published":"2023-03-13T05:04:18Z","title":"Learning Distortion Invariant Representation for Image Restoration from\n  A Causality Perspective","summary":"  In recent years, we have witnessed the great advancement of Deep neural\nnetworks (DNNs) in image restoration. However, a critical limitation is that\nthey cannot generalize well to real-world degradations with different degrees\nor types. In this paper, we are the first to propose a novel training strategy\nfor image restoration from the causality perspective, to improve the\ngeneralization ability of DNNs for unknown degradations. Our method, termed\nDistortion Invariant representation Learning (DIL), treats each distortion type\nand degree as one specific confounder, and learns the distortion-invariant\nrepresentation by eliminating the harmful confounding effect of each\ndegradation. We derive our DIL with the back-door criterion in causality by\nmodeling the interventions of different distortions from the optimization\nperspective. Particularly, we introduce counterfactual distortion augmentation\nto simulate the virtual distortion types and degrees as the confounders. Then,\nwe instantiate the intervention of each distortion with a virtual model\nupdating based on corresponding distorted images, and eliminate them from the\nmeta-learning perspective. Extensive experiments demonstrate the effectiveness\nof our DIL on the generalization capability for unseen distortion types and\ndegrees. Our code will be available at\nhttps://github.com/lixinustc/Casual-IRDIL.\n","authors":["Xin Li","Bingchen Li","Xin Jin","Cuiling Lan","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2303.06859v1.pdf","comment":"Accepted by CVPR2023"}]},"2023-03-12T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2201.05333v3","updated":"2023-03-12T17:54:21Z","published":"2022-01-14T08:21:28Z","title":"Attention over Self-attention:Intention-aware Re-ranking with Dynamic\n  Transformer Encoders for Recommendation","summary":"  Re-ranking models refine item recommendation lists generated by the prior\nglobal ranking model, which have demonstrated their effectiveness in improving\nthe recommendation quality. However, most existing re-ranking solutions only\nlearn from implicit feedback with a shared prediction model, which regrettably\nignore inter-item relationships under diverse user intentions. In this paper,\nwe propose a novel Intention-aware Re-ranking Model with Dynamic Transformer\nEncoder (RAISE), aiming to perform user-specific prediction for each individual\nuser based on her intentions. Specifically, we first propose to mine latent\nuser intentions from text reviews with an intention discovering module (IDM).\nBy differentiating the importance of review information with a co-attention\nnetwork, the latent user intention can be explicitly modeled for each user-item\npair. We then introduce a dynamic transformer encoder (DTE) to capture\nuser-specific inter-item relationships among item candidates by seamlessly\naccommodating the learned latent user intentions via IDM. As such, one can not\nonly achieve more personalized recommendations but also obtain corresponding\nexplanations by constructing RAISE upon existing recommendation engines.\nEmpirical study on four public datasets shows the superiority of our proposed\nRAISE, with up to 13.95%, 9.60%, and 13.03% relative improvements evaluated by\nPrecision@5, MAP@5, and NDCG@5 respectively.\n","authors":["Zhuoyi Lin","Sheng Zang","Rundong Wang","Zhu Sun","J. Senthilnath","Chi Xu","Chee-Keong Kwoh"],"pdf_url":"https://arxiv.org/pdf/2201.05333v3.pdf","comment":"Accepted by IEEE TKDE"},{"id":"http://arxiv.org/abs/2007.14129v6","updated":"2023-03-12T16:39:57Z","published":"2020-07-28T11:18:36Z","title":"COMET: Convolutional Dimension Interaction for Collaborative Filtering","summary":"  Representation learning-based recommendation models play a dominant role\namong recommendation techniques. However, most of the existing methods assume\nboth historical interactions and embedding dimensions are independent of each\nother, and thus regrettably ignore the high-order interaction information among\nhistorical interactions and embedding dimensions. In this paper, we propose a\nnovel representation learning-based model called COMET (COnvolutional diMEnsion\ninTeraction), which simultaneously models the high-order interaction patterns\namong historical interactions and embedding dimensions. To be specific, COMET\nstacks the embeddings of historical interactions horizontally at first, which\nresults in two \"embedding maps\". In this way, internal interactions and\ndimensional interactions can be exploited by convolutional neural networks\n(CNN) with kernels of different sizes simultaneously. A fully-connected\nmulti-layer perceptron (MLP) is then applied to obtain two interaction vectors.\nLastly, the representations of users and items are enriched by the learnt\ninteraction vectors, which can further be used to produce the final prediction.\nExtensive experiments and ablation studies on various public implicit feedback\ndatasets clearly demonstrate the effectiveness and rationality of our proposed\nmethod.\n","authors":["Zhuoyi Lin","Lei Feng","Xingzhi Guo","Yu Zhang","Rui Yin","Chee Keong Kwoh","Chi Xu"],"pdf_url":"https://arxiv.org/pdf/2007.14129v6.pdf","comment":"Accepted by ACM TIST"},{"id":"http://arxiv.org/abs/2302.14096v2","updated":"2023-03-12T15:44:41Z","published":"2023-02-27T19:14:37Z","title":"A Dataset for Learning Graph Representations to Predict Customer Returns\n  in Fashion Retail","summary":"  We present a novel dataset collected by ASOS (a major online fashion\nretailer) to address the challenge of predicting customer returns in a fashion\nretail ecosystem. With the release of this substantial dataset we hope to\nmotivate further collaboration between research communities and the fashion\nindustry. We first explore the structure of this dataset with a focus on the\napplication of Graph Representation Learning in order to exploit the natural\ndata structure and provide statistical insights into particular features within\nthe data. In addition to this, we show examples of a return prediction\nclassification task with a selection of baseline models (i.e. with no\nintermediate representation learning step) and a graph representation based\nmodel. We show that in a downstream return prediction classification task, an\nF1-score of 0.792 can be found using a Graph Neural Network (GNN), improving\nupon other models discussed in this work. Alongside this increased F1-score, we\nalso present a lower cross-entropy loss by recasting the data into a graph\nstructure, indicating more robust predictions from a GNN based solution. These\nresults provide evidence that GNNs could provide more impactful and usable\nclassifications than other baseline models on the presented dataset and with\nthis motivation, we hope to encourage further research into graph-based\napproaches using the ASOS GraphReturns dataset.\n","authors":["Jamie McGowan","Elizabeth Guest","Ziyang Yan","Cong Zheng","Neha Patel","Mason Cusack","Charlie Donaldson","Sofie de Cnudde","Gabriel Facini","Fabon Dzogang"],"pdf_url":"https://arxiv.org/pdf/2302.14096v2.pdf","comment":"The ASOS GraphReturns dataset can be found at https://osf.io/c793h/.\n  Accepted at FashionXRecSys 2022 workshop. Published Version"},{"id":"http://arxiv.org/abs/2303.06660v1","updated":"2023-03-12T13:42:50Z","published":"2023-03-12T13:42:50Z","title":"P-MMF: Provider Max-min Fairness Re-ranking in Recommender System","summary":"  In this paper, we address the issue of recommending fairly from the aspect of\nproviders, which has become increasingly essential in multistakeholder\nrecommender systems. Existing studies on provider fairness usually focused on\ndesigning proportion fairness (PF) metrics that first consider systematic\nfairness. However, sociological researches show that to make the market more\nstable, max-min fairness (MMF) is a better metric. The main reason is that MMF\naims to improve the utility of the worst ones preferentially, guiding the\nsystem to support the providers in weak market positions. When applying MMF to\nrecommender systems, how to balance user preferences and provider fairness in\nan online recommendation scenario is still a challenging problem. In this\npaper, we proposed an online re-ranking model named Provider Max-min Fairness\nRe-ranking (P-MMF) to tackle the problem. Specifically, P-MMF formulates\nprovider fair recommendation as a resource allocation problem, where the\nexposure slots are considered the resources to be allocated to providers and\nthe max-min fairness is used as the regularizer during the process. We show\nthat the problem can be further represented as a regularized online optimizing\nproblem and solved efficiently in its dual space. During the online re-ranking\nphase, a momentum gradient descent method is designed to conduct the dynamic\nre-ranking. Theoretical analysis showed that the regret of P-MMF can be\nbounded. Experimental results on four public recommender datasets demonstrated\nthat P-MMF can outperformed the state-of-the-art baselines. Experimental\nresults also show that P-MMF can retain small computationally costs on a corpus\nwith the large number of items.\n","authors":["Chen Xu","Sirui Chen","Jun Xu","Weiran Shen","Xiao Zhang","Gang Wang","Zhenghua Dong"],"pdf_url":"https://arxiv.org/pdf/2303.06660v1.pdf","comment":"Accepted in WWW23"},{"id":"http://arxiv.org/abs/2303.06611v1","updated":"2023-03-12T08:36:15Z","published":"2023-03-12T08:36:15Z","title":"AutoDenoise: Automatic Data Instance Denoising for Recommendations","summary":"  Historical user-item interaction datasets are essential in training modern\nrecommender systems for predicting user preferences. However, the arbitrary\nuser behaviors in most recommendation scenarios lead to a large volume of noisy\ndata instances being recorded, which cannot fully represent their true\ninterests. While a large number of denoising studies are emerging in the\nrecommender system community, all of them suffer from highly dynamic data\ndistributions. In this paper, we propose a Deep Reinforcement Learning (DRL)\nbased framework, AutoDenoise, with an Instance Denoising Policy Network, for\ndenoising data instances with an instance selection manner in deep recommender\nsystems. To be specific, AutoDenoise serves as an agent in DRL to adaptively\nselect noise-free and predictive data instances, which can then be utilized\ndirectly in training representative recommendation models. In addition, we\ndesign an alternate two-phase optimization strategy to train and validate the\nAutoDenoise properly. In the searching phase, we aim to train the policy\nnetwork with the capacity of instance denoising; in the validation phase, we\nfind out and evaluate the denoised subset of data instances selected by the\ntrained policy network, so as to validate its denoising ability. We conduct\nextensive experiments to validate the effectiveness of AutoDenoise combined\nwith multiple representative recommender system models.\n","authors":["Weilin Lin","Xiangyu Zhao","Yejing Wang","Yuanshao Zhu","Wanyu Wang"],"pdf_url":"https://arxiv.org/pdf/2303.06611v1.pdf","comment":"9 pages, 4 figures, 5 tables, conference"},{"id":"http://arxiv.org/abs/2303.06588v1","updated":"2023-03-12T06:39:40Z","published":"2023-03-12T06:39:40Z","title":"MobileRec: A Large-Scale Dataset for Mobile Apps Recommendation","summary":"  Recommender systems have become ubiquitous in our digital lives, from\nrecommending products on e-commerce websites to suggesting movies and music on\nstreaming platforms. Existing recommendation datasets, such as Amazon Product\nReviews and MovieLens, greatly facilitated the research and development of\nrecommender systems in their respective domains. While the number of mobile\nusers and applications (aka apps) has increased exponentially over the past\ndecade, research in mobile app recommender systems has been significantly\nconstrained, primarily due to the lack of high-quality benchmark datasets, as\nopposed to recommendations for products, movies, and news. To facilitate\nresearch for app recommendation systems, we introduce a large-scale dataset,\ncalled MobileRec. We constructed MobileRec from users' activity on the Google\nplay store. MobileRec contains 19.3 million user interactions (i.e., user\nreviews on apps) with over 10K unique apps across 48 categories. MobileRec\nrecords the sequential activity of a total of 0.7 million distinct users. Each\nof these users has interacted with no fewer than five distinct apps, which\nstands in contrast to previous datasets on mobile apps that recorded only a\nsingle interaction per user. Furthermore, MobileRec presents users' ratings as\nwell as sentiments on installed apps, and each app contains rich metadata such\nas app name, category, description, and overall rating, among others. We\ndemonstrate that MobileRec can serve as an excellent testbed for app\nrecommendation through a comparative study of several state-of-the-art\nrecommendation approaches. The quantitative results can act as a baseline for\nother researchers to compare their results against. The MobileRec dataset is\navailable at https://huggingface.co/datasets/recmeapp/mobilerec.\n","authors":["M. H. Maqbool","Umar Farooq","Adib Mosharrof","A. B. Siddique","Hassan Foroosh"],"pdf_url":"https://arxiv.org/pdf/2303.06588v1.pdf","comment":"10 pages, 4 tables, 4 figures, Under submission at SIGIR'23"},{"id":"http://arxiv.org/abs/2303.06586v1","updated":"2023-03-12T06:23:10Z","published":"2023-03-12T06:23:10Z","title":"Proactive Prioritization of App Issues via Contrastive Learning","summary":"  Mobile app stores produce a tremendous amount of data in the form of user\nreviews, which is a huge source of user requirements and sentiments; such\nreviews allow app developers to proactively address issues in their apps.\nHowever, only a small number of reviews capture common issues and sentiments\nwhich creates a need for automatically identifying prominent reviews.\nUnfortunately, most existing work in text ranking and popularity prediction\nfocuses on social contexts where other signals are available, which renders\nsuch works ineffective in the context of app reviews. In this work, we propose\na new framework, PPrior, that enables proactive prioritization of app issues\nthrough identifying prominent reviews (ones predicted to receive a large number\nof votes in a given time window). Predicting highly-voted reviews is\nchallenging given that, unlike social posts, social network features of users\nare not available. Moreover, there is an issue of class imbalance, since a\nlarge number of user reviews receive little to no votes. PPrior employs a\npre-trained T5 model and works in three phases. Phase one adapts the\npre-trained T5 model to the user reviews data in a self-supervised fashion. In\nphase two, we leverage contrastive training to learn a generic and\ntask-independent representation of user reviews. Phase three uses radius\nneighbors classifier t o m ake t he final predictions. This phase also uses\nFAISS index for scalability and efficient search. To conduct extensive\nexperiments, we acquired a large dataset of over 2.1 million user reviews from\nGoogle Play. Our experimental results demonstrate the effectiveness of the\nproposed framework when compared against several state-of-the-art approaches.\nMoreover, the accuracy of PPrior in predicting prominent reviews is comparable\nto that of experienced app developers.\n","authors":["Moghis Fereidouni","Adib Mosharrof","Umar Farooq","AB Siddique"],"pdf_url":"https://arxiv.org/pdf/2303.06586v1.pdf","comment":"10 pages, 2022 IEEE International Conference on Big Data (Big Data)"},{"id":"http://arxiv.org/abs/2303.06573v1","updated":"2023-03-12T05:08:16Z","published":"2023-03-12T05:08:16Z","title":"Large Language Models Know Your Contextual Search Intent: A Prompting\n  Framework for Conversational Search","summary":"  In this paper, we present a prompting framework called LLMCS that leverages\nlarge language models, such as code-davinci-002 of GPT-3, to perform few-shot\nconversational query rewriting for conversational search. We explore three\nprompting methods to generate multiple query rewrites and hypothetical\nresponses, and propose aggregating them into an integrated representation that\ncan robustly represent the user's real contextual search intent. Experimental\nresults on two conversational search datasets, including CAst-19 and CAsT-20,\nshow that our approach achieves significant improvements in search\neffectiveness over existing baselines and manual rewrites. Notably, LLMCS can\nsignificantly outperform the state-of-the-art baselines by up to +5.9\\% and\n+32.9\\% w.r.t. NDCG@3 on CAsT-19 and CAsT-20, highlighting the vast potential\nof large language models for conversational search. Our code will be released\nat https://github.com/kyriemao/LLMCS.\n","authors":["Kelong Mao","Zhicheng Dou","Haonan Chen","Fengran Mo","Hongjin Qian"],"pdf_url":"https://arxiv.org/pdf/2303.06573v1.pdf","comment":"Work in progress"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2303.06783v1","updated":"2023-03-12T23:51:51Z","published":"2023-03-12T23:51:51Z","title":"Asynchronous Decentralized Federated Lifelong Learning for Landmark\n  Localization in Medical Imaging","summary":"  Federated learning is a recent development in the machine learning area that\nallows a system of devices to train on one or more tasks without sharing their\ndata to a single location or device. However, this framework still requires a\ncentralized global model to consolidate individual models into one, and the\ndevices train synchronously, which both can be potential bottlenecks for using\nfederated learning. In this paper, we propose a novel method of asynchronous\ndecentralized federated lifelong learning (ADFLL) method that inherits the\nmerits of federated learning and can train on multiple tasks simultaneously\nwithout the need for a central node or synchronous training. Thus, overcoming\nthe potential drawbacks of conventional federated learning. We demonstrate\nexcellent performance on the brain tumor segmentation (BRATS) dataset for\nlocalizing the left ventricle on multiple image sequences and image\norientation. Our framework allows agents to achieve the best performance with a\nmean distance error of 7.81, better than the conventional all-knowing agent's\nmean distance error of 11.78, and significantly (p=0.01) better than a\nconventional lifelong learning agent with a distance error of 15.17 after eight\nrounds of training. In addition, all ADFLL agents have comparable or better\nperformance than a conventional LL agent. In conclusion, we developed an ADFLL\nframework with excellent performance and speed-up compared to conventional RL\nagents.\n","authors":["Guangyao Zheng","Michael A. Jacobs","Vladimir Braverman","Vishwa S. Parekh"],"pdf_url":"https://arxiv.org/pdf/2303.06783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06782v1","updated":"2023-03-12T23:46:04Z","published":"2023-03-12T23:46:04Z","title":"AidUI: Toward Automated Recognition of Dark Patterns in User Interfaces","summary":"  Past studies have illustrated the prevalence of UI dark patterns, or user\ninterfaces that can lead end-users toward (unknowingly) taking actions that\nthey may not have intended. Such deceptive UI designs can result in adverse\neffects on end users, such as oversharing personal information or financial\nloss. While significant research progress has been made toward the development\nof dark pattern taxonomies, developers and users currently lack guidance to\nhelp recognize, avoid, and navigate these often subtle design motifs. However,\nautomated recognition of dark patterns is a challenging task, as the\ninstantiation of a single type of pattern can take many forms, leading to\nsignificant variability.\n  In this paper, we take the first step toward understanding the extent to\nwhich common UI dark patterns can be automatically recognized in modern\nsoftware applications. To do this, we introduce AidUI, a novel automated\napproach that uses computer vision and natural language processing techniques\nto recognize a set of visual and textual cues in application screenshots that\nsignify the presence of ten unique UI dark patterns, allowing for their\ndetection, classification, and localization. To evaluate our approach, we have\nconstructed ContextDP, the current largest dataset of fully-localized UI dark\npatterns that spans 175 mobile and 83 web UI screenshots containing 301 dark\npattern instances. The results of our evaluation illustrate that \\AidUI\nachieves an overall precision of 0.66, recall of 0.67, F1-score of 0.65 in\ndetecting dark pattern instances, reports few false positives, and is able to\nlocalize detected patterns with an IoU score of ~0.84. Furthermore, a\nsignificant subset of our studied dark patterns can be detected quite reliably\n(F1 score of over 0.82), and future research directions may allow for improved\ndetection of additional patterns.\n","authors":["SM Hasan Mansur","Sabiha Salma","Damilola Awofisayo","Kevin Moran"],"pdf_url":"https://arxiv.org/pdf/2303.06782v1.pdf","comment":"13 pages, Accepted at The 45th IEEE/ACM International Conference on\n  Software Engineering (ICSE 2023), Melbourne, Australia, May 14th-20th, 2023"},{"id":"http://arxiv.org/abs/2110.15829v4","updated":"2023-03-12T23:36:54Z","published":"2021-10-29T14:46:32Z","title":"Holistic Deep Learning","summary":"  This paper presents a novel holistic deep learning framework that\nsimultaneously addresses the challenges of vulnerability to input\nperturbations, overparametrization, and performance instability from different\ntrain-validation splits. The proposed framework holistically improves accuracy,\nrobustness, sparsity, and stability over standard deep learning models, as\ndemonstrated by extensive experiments on both tabular and image data sets. The\nresults are further validated by ablation experiments and SHAP value analysis,\nwhich reveal the interactions and trade-offs between the different evaluation\nmetrics. To support practitioners applying our framework, we provide a\nprescriptive approach that offers recommendations for selecting an appropriate\ntraining loss function based on their specific objectives. All the code to\nreproduce the results can be found at https://github.com/kimvc7/HDL.\n","authors":["Dimitris Bertsimas","Kimberly Villalobos Carballo","Léonard Boussioux","Michael Lingzhi Li","Alex Paskov","Ivan Paskov"],"pdf_url":"https://arxiv.org/pdf/2110.15829v4.pdf","comment":"Submitted to Machine Learning"},{"id":"http://arxiv.org/abs/2005.04646v4","updated":"2023-03-12T23:04:10Z","published":"2020-05-10T12:37:26Z","title":"An FPGA-Based On-Device Reinforcement Learning Approach using Online\n  Sequential Learning","summary":"  DQN (Deep Q-Network) is a method to perform Q-learning for reinforcement\nlearning using deep neural networks. DQNs require a large buffer and batch\nprocessing for an experience replay and rely on a backpropagation based\niterative optimization, making them difficult to be implemented on\nresource-limited edge devices. In this paper, we propose a lightweight\non-device reinforcement learning approach for low-cost FPGA devices. It\nexploits a recently proposed neural-network based on-device learning approach\nthat does not rely on the backpropagation method but uses OS-ELM (Online\nSequential Extreme Learning Machine) based training algorithm. In addition, we\npropose a combination of L2 regularization and spectral normalization for the\non-device reinforcement learning so that output values of the neural network\ncan be fit into a certain range and the reinforcement learning becomes stable.\nThe proposed reinforcement learning approach is designed for PYNQ-Z1 board as a\nlow-cost FPGA platform. The evaluation results using OpenAI Gym demonstrate\nthat the proposed algorithm and its FPGA implementation complete a CartPole-v0\ntask 29.77x and 89.40x faster than a conventional DQN-based approach when the\nnumber of hidden-layer nodes is 64.\n","authors":["Hirohisa Watanabe","Mineto Tsukada","Hiroki Matsutani"],"pdf_url":"https://arxiv.org/pdf/2005.04646v4.pdf","comment":"RAW'21"},{"id":"http://arxiv.org/abs/2211.10525v2","updated":"2023-03-12T22:32:57Z","published":"2022-11-18T22:48:09Z","title":"Differentiable Uncalibrated Imaging","summary":"  We propose a differentiable imaging framework to address uncertainty in\nmeasurement coordinates such as sensor locations and projection angles. We\nformulate the problem as measurement interpolation at unknown nodes supervised\nthrough the forward operator. To solve it we apply implicit neural networks,\nalso known as neural fields, which are naturally differentiable with respect to\nthe input coordinates. We also develop differentiable spline interpolators\nwhich perform as well as neural networks, require less time to optimize and\nhave well-understood properties. Differentiability is key as it allows us to\njointly fit a measurement representation, optimize over the uncertain\nmeasurement coordinates, and perform image reconstruction which in turn ensures\nconsistent calibration. We apply our approach to 2D and 3D computed tomography\nand show that it produces improved reconstructions compared to baselines that\ndo not account for the lack of calibration. The flexibility of the proposed\nframework makes it easy to apply to almost arbitrary imaging problems.\n","authors":["Sidharth Gupta","Konik Kothari","Valentin Debarnot","Ivan Dokmanić"],"pdf_url":"https://arxiv.org/pdf/2211.10525v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2008.08915v2","updated":"2023-03-12T22:27:29Z","published":"2020-08-19T05:47:12Z","title":"LOCUS: A Novel Decomposition Method for Brain Network Connectivity\n  Matrices using Low-rank Structure with Uniform Sparsity","summary":"  Network-oriented research has been increasingly popular in many scientific\nareas. In neuroscience research, imaging-based network connectivity measures\nhave become the key for understanding brain organizations, potentially serving\nas individual neural fingerprints. There are major challenges in analyzing\nconnectivity matrices including the high dimensionality of brain networks,\nunknown latent sources underlying the observed connectivity, and the large\nnumber of brain connections leading to spurious findings. In this paper, we\npropose a novel blind source separation method with low-rank structure and\nuniform sparsity (LOCUS) as a fully data-driven decomposition method for\nnetwork measures. Compared with the existing method that vectorizes\nconnectivity matrices ignoring brain network topology, LOCUS achieves more\nefficient and accurate source separation for connectivity matrices using\nlow-rank structure. We propose a novel angle-based uniform sparsity\nregularization that demonstrates better performance than the existing sparsity\ncontrols for low-rank tensor methods. We propose a highly efficient iterative\nNode-Rotation algorithm that exploits the block multi-convexity of the\nobjective function to solve the non-convex optimization problem for learning\nLOCUS. We illustrate the advantage of LOCUS through extensive simulation\nstudies. Application of LOCUS to Philadelphia Neurodevelopmental Cohort\nneuroimaging study reveals biologically insightful connectivity traits which\nare not found using the existing method.\n","authors":["Yikai Wang","Ying Guo"],"pdf_url":"https://arxiv.org/pdf/2008.08915v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2102.10019v2","updated":"2023-03-12T22:25:58Z","published":"2021-02-19T16:40:47Z","title":"The Disparate Impact of Uncertainty: Affirmative Action vs. Affirmative\n  Information","summary":"  Critical decisions like loan approvals, medical interventions, and college\nadmissions are guided by predictions made in the presence of uncertainty. In\nthis paper, we prove that uncertainty has a disparate impact. While it imparts\nerrors across all demographic groups, the types of errors vary systematically:\nGroups with higher average outcomes are typically assigned higher false\npositive rates, while those with lower average outcomes are assigned higher\nfalse negative rates. We show that additional data acquisition can eliminate\nthe disparity and broaden access to opportunity. The strategy, which we call\nAffirmative Information, could stand as an alternative to Affirmative Action.\n","authors":["Claire Lazar Reich"],"pdf_url":"https://arxiv.org/pdf/2102.10019v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.08253v2","updated":"2023-03-12T22:00:41Z","published":"2022-11-15T15:59:43Z","title":"HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization","summary":"  Due to domain shift, machine learning systems typically fail to generalize\nwell to domains different from those of training data, which is what domain\ngeneralization (DG) aims to address. Although various DG methods have been\ndeveloped, most of them lack interpretability and require domain labels that\nare not available in many real-world scenarios. This paper presents a novel DG\nmethod, called HMOE: Hypernetwork-based Mixture of Experts (MoE), which does\nnot rely on domain labels and is more interpretable. MoE proves effective in\nidentifying heterogeneous patterns in data. For the DG problem, heterogeneity\narises exactly from domain shift. HMOE uses hypernetworks taking vectors as\ninput to generate experts' weights, which allows experts to share useful\nmeta-knowledge and enables exploring experts' similarities in a low-dimensional\nvector space. We compare HMOE with other DG algorithms under a fair and unified\nbenchmark-DomainBed. Our extensive experiments show that HMOE can divide\nmixed-domain data into distinct clusters that are surprisingly more consistent\nwith human intuition than original domain labels. Compared to other DG methods,\nHMOE shows competitive performance and achieves SOTA results in some cases.\n","authors":["Jingang Qu","Thibault Faney","Ze Wang","Patrick Gallinari","Soleiman Yousef","Jean-Charles de Hemptinne"],"pdf_url":"https://arxiv.org/pdf/2211.08253v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.08767v2","updated":"2023-03-12T21:34:05Z","published":"2022-08-18T11:05:55Z","title":"Evaluating Continual Test-Time Adaptation for Contextual and Semantic\n  Domain Shifts","summary":"  In this paper, our goal is to adapt a pre-trained convolutional neural\nnetwork to domain shifts at test time. We do so continually with the incoming\nstream of test batches, without labels. The existing literature mostly operates\non artificial shifts obtained via adversarial perturbations of a test image.\nMotivated by this, we evaluate the state of the art on two realistic and\nchallenging sources of domain shifts, namely contextual and semantic shifts.\nContextual shifts correspond to the environment types, for example, a model\npre-trained on indoor context has to adapt to the outdoor context on CORe-50.\nSemantic shifts correspond to the capture types, for example a model\npre-trained on natural images has to adapt to cliparts, sketches, and paintings\non DomainNet. We include in our analysis recent techniques such as\nPrediction-Time Batch Normalization (BN), Test Entropy Minimization (TENT) and\nContinual Test-Time Adaptation (CoTTA). Our findings are three-fold: i)\nTest-time adaptation methods perform better and forget less on contextual\nshifts compared to semantic shifts, ii) TENT outperforms other methods on\nshort-term adaptation, whereas CoTTA outpeforms other methods on long-term\nadaptation, iii) BN is most reliable and robust. Our code is available at\nhttps://github.com/tommiekerssies/Evaluating-Continual-Test-Time-Adaptation-for-Contextual-and-Semantic-Domain-Shifts.\n","authors":["Tommie Kerssies","Mert Kılıçkaya","Joaquin Vanschoren"],"pdf_url":"https://arxiv.org/pdf/2208.08767v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06753v1","updated":"2023-03-12T21:01:54Z","published":"2023-03-12T21:01:54Z","title":"Module-Wise Network Quantization for 6D Object Pose Estimation","summary":"  Many edge applications, such as collaborative robotics and spacecraft\nrendezvous, can benefit from 6D object pose estimation, but must do so on\nembedded platforms. Unfortunately, existing 6D pose estimation networks are\ntypically too large for deployment in such situations and must therefore be\ncompressed, while maintaining reliable performance. In this work, we present an\napproach to doing so by quantizing such networks. More precisely, we introduce\na module-wise quantization strategy that, in contrast to uniform and\nmixed-precision quantization, accounts for the modular structure of typical 6D\npose estimation frameworks. We demonstrate that uniquely compressing these\nmodules outperforms uniform and mixed-precision quantization techniques.\nMoreover, our experiments evidence that module-wise quantization can lead to a\nsignificant accuracy boost. We showcase the generality of our approach using\ndifferent datasets, quantization methodologies, and network architectures,\nincluding the recent ZebraPose.\n","authors":["Saqib Javed","Andrew Price","Yinlin Hu","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2303.06753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06740v1","updated":"2023-03-12T19:52:34Z","published":"2023-03-12T19:52:34Z","title":"Fine-tuning Strategies for Faster Inference using Speech Self-Supervised\n  Models: A Comparative Study","summary":"  Self-supervised learning (SSL) has allowed substantial progress in Automatic\nSpeech Recognition (ASR) performance in low-resource settings. In this context,\nit has been demonstrated that larger self-supervised feature extractors are\ncrucial for achieving lower downstream ASR error rates. Thus, better\nperformance might be sanctioned with longer inferences. This article explores\ndifferent approaches that may be deployed during the fine-tuning to reduce the\ncomputations needed in the SSL encoder, leading to faster inferences. We adapt\na number of existing techniques to common ASR settings and benchmark them,\ndisplaying performance drops and gains in inference times. Interestingly, we\nfound that given enough downstream data, a simple downsampling of the input\nsequences outperforms the other methods with both low performance drops and\nhigh computational savings, reducing computations by 61.3% with an WER increase\nof only 0.81. Finally, we analyze the robustness of the comparison to changes\nin dataset conditions, revealing sensitivity to dataset size.\n","authors":["Salah Zaiem","Robin Algayres","Titouan Parcollet","Slim Essid","Mirco Ravanelli"],"pdf_url":"https://arxiv.org/pdf/2303.06740v1.pdf","comment":"Submitted to ICASSP \"Self-supervision in Audio, Speech and Beyond\"\n  workshop"},{"id":"http://arxiv.org/abs/2303.06726v1","updated":"2023-03-12T18:44:29Z","published":"2023-03-12T18:44:29Z","title":"Global Optimality of Elman-type RNN in the Mean-Field Regime","summary":"  We analyze Elman-type Recurrent Reural Networks (RNNs) and their training in\nthe mean-field regime. Specifically, we show convergence of gradient descent\ntraining dynamics of the RNN to the corresponding mean-field formulation in the\nlarge width limit. We also show that the fixed points of the limiting\ninfinite-width dynamics are globally optimal, under some assumptions on the\ninitialization of the weights. Our results establish optimality for\nfeature-learning with wide RNNs in the mean-field regime\n","authors":["Andrea Agazzi","Jianfeng Lu","Sayan Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2303.06726v1.pdf","comment":"31 pages, 2 figures"},{"id":"http://arxiv.org/abs/2111.00843v3","updated":"2023-03-12T18:08:11Z","published":"2021-11-01T11:23:44Z","title":"How I Learned to Stop Worrying and Love Retraining","summary":"  Many Neural Network Pruning approaches consist of several iterative training\nand pruning steps, seemingly losing a significant amount of their performance\nafter pruning and then recovering it in the subsequent retraining phase. Recent\nworks of Renda et al. (2020) and Le & Hua (2021) demonstrate the significance\nof the learning rate schedule during the retraining phase and propose specific\nheuristics for choosing such a schedule for IMP (Han et al., 2015). We place\nthese findings in the context of the results of Li et al. (2020) regarding the\ntraining of models within a fixed training budget and demonstrate that,\nconsequently, the retraining phase can be massively shortened using a simple\nlinear learning rate schedule. Improving on existing retraining approaches, we\nadditionally propose a method to adaptively select the initial value of the\nlinear schedule. Going a step further, we propose similarly imposing a budget\non the initial dense training phase and show that the resulting simple and\nefficient method is capable of outperforming significantly more complex or\nheavily parameterized state-of-the-art approaches that attempt to sparsify the\nnetwork during training. These findings not only advance our understanding of\nthe retraining phase, but more broadly question the belief that one should aim\nto avoid the need for retraining and reduce the negative effects of 'hard'\npruning by incorporating the sparsification process into the standard training.\n","authors":["Max Zimmer","Christoph Spiegel","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2111.00843v3.pdf","comment":"ICLR2023 camera-ready version, 9 pages main text, 34 pages appendix,\n  2 tables, 3 figures in main text"},{"id":"http://arxiv.org/abs/2303.06721v1","updated":"2023-03-12T18:00:12Z","published":"2023-03-12T18:00:12Z","title":"Knowledge-integrated AutoEncoder Model","summary":"  Data encoding is a common and central operation in most data analysis tasks.\nThe performance of other models, downstream in the computational process,\nhighly depends on the quality of data encoding. One of the most powerful ways\nto encode data is using the neural network AutoEncoder (AE) architecture.\nHowever, the developers of AE are not able to easily influence the produced\nembedding space, as it is usually treated as a \\textit{black box} technique,\nwhich makes it uncontrollable and not necessarily has desired properties for\ndownstream tasks. In this paper, we introduce a novel approach for developing\nAE models that can integrate external knowledge sources into the learning\nprocess, possibly leading to more accurate results. The proposed\n\\methodNamefull{} (\\methodName{}) model is able to leverage domain-specific\ninformation to make sure the desired distance and neighborhood properties\nbetween samples are preservative in the embedding space. The proposed model is\nevaluated on three large-scale datasets from three different scientific fields\nand is compared to nine existing encoding models. The results demonstrate that\nthe \\methodName{} model effectively captures the underlying structures and\nrelationships between the input data and external knowledge, meaning it\ngenerates a more useful representation. This leads to outperforming the rest of\nthe models in terms of reconstruction accuracy.\n","authors":["Teddy Lazebnik","Liron Simon-Keren"],"pdf_url":"https://arxiv.org/pdf/2303.06721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06710v1","updated":"2023-03-12T17:22:54Z","published":"2023-03-12T17:22:54Z","title":"Decision Making for Human-in-the-loop Robotic Agents via\n  Uncertainty-Aware Reinforcement Learning","summary":"  In a Human-in-the-Loop paradigm, a robotic agent is able to act mostly\nautonomously in solving a task, but can request help from an external expert\nwhen needed. However, knowing when to request such assistance is critical: too\nfew requests can lead to the robot making mistakes, but too many requests can\noverload the expert. In this paper, we present a Reinforcement Learning based\napproach to this problem, where a semi-autonomous agent asks for external\nassistance when it has low confidence in the eventual success of the task. The\nconfidence level is computed by estimating the variance of the return from the\ncurrent state. We show that this estimate can be iteratively improved during\ntraining using a Bellman-like recursion. On discrete navigation problems with\nboth fully- and partially-observable state information, we show that our method\nmakes effective use of a limited budget of expert calls at run-time, despite\nhaving no access to the expert at training time.\n","authors":["Siddharth Singi","Zhanpeng He","Alvin Pan","Sandip Patel","Gunnar A. Sigurdsson","Robinson Piramuthu","Shuran Song","Matei Ciocarlie"],"pdf_url":"https://arxiv.org/pdf/2303.06710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07738v3","updated":"2023-03-12T16:49:24Z","published":"2022-12-15T11:40:40Z","title":"A large-scale and PCR-referenced vocal audio dataset for COVID-19","summary":"  The UK COVID-19 Vocal Audio Dataset is designed for the training and\nevaluation of machine learning models that classify SARS-CoV-2 infection status\nor associated respiratory symptoms using vocal audio. The UK Health Security\nAgency recruited voluntary participants through the national Test and Trace\nprogramme and the REACT-1 survey in England from March 2021 to March 2022,\nduring dominant transmission of the Alpha and Delta SARS-CoV-2 variants and\nsome Omicron variant sublineages. Audio recordings of volitional coughs,\nexhalations, and speech were collected in the 'Speak up to help beat\ncoronavirus' digital survey alongside demographic, self-reported symptom and\nrespiratory condition data, and linked to SARS-CoV-2 test results. The UK\nCOVID-19 Vocal Audio Dataset represents the largest collection of SARS-CoV-2\nPCR-referenced audio recordings to date. PCR results were linked to 70,794 of\n72,999 participants and 24,155 of 25,776 positive cases. Respiratory symptoms\nwere reported by 45.62% of participants. This dataset has additional potential\nuses for bioacoustics research, with 11.30% participants reporting asthma, and\n27.20% with linked influenza PCR test results.\n","authors":["Jobie Budd","Kieran Baker","Emma Karoune","Harry Coppock","Selina Patel","Ana Tendero Cañadas","Alexander Titcomb","Richard Payne","David Hurley","Sabrina Egglestone","Lorraine Butler","Jonathon Mellor","George Nicholson","Ivan Kiskin","Vasiliki Koutra","Radka Jersakova","Rachel A. McKendry","Peter Diggle","Sylvia Richardson","Björn W. Schuller","Steven Gilmour","Davide Pigoli","Stephen Roberts","Josef Packham","Tracey Thornley","Chris Holmes"],"pdf_url":"https://arxiv.org/pdf/2212.07738v3.pdf","comment":"37 pages, 4 figures"},{"id":"http://arxiv.org/abs/2007.14129v6","updated":"2023-03-12T16:39:57Z","published":"2020-07-28T11:18:36Z","title":"COMET: Convolutional Dimension Interaction for Collaborative Filtering","summary":"  Representation learning-based recommendation models play a dominant role\namong recommendation techniques. However, most of the existing methods assume\nboth historical interactions and embedding dimensions are independent of each\nother, and thus regrettably ignore the high-order interaction information among\nhistorical interactions and embedding dimensions. In this paper, we propose a\nnovel representation learning-based model called COMET (COnvolutional diMEnsion\ninTeraction), which simultaneously models the high-order interaction patterns\namong historical interactions and embedding dimensions. To be specific, COMET\nstacks the embeddings of historical interactions horizontally at first, which\nresults in two \"embedding maps\". In this way, internal interactions and\ndimensional interactions can be exploited by convolutional neural networks\n(CNN) with kernels of different sizes simultaneously. A fully-connected\nmulti-layer perceptron (MLP) is then applied to obtain two interaction vectors.\nLastly, the representations of users and items are enriched by the learnt\ninteraction vectors, which can further be used to produce the final prediction.\nExtensive experiments and ablation studies on various public implicit feedback\ndatasets clearly demonstrate the effectiveness and rationality of our proposed\nmethod.\n","authors":["Zhuoyi Lin","Lei Feng","Xingzhi Guo","Yu Zhang","Rui Yin","Chee Keong Kwoh","Chi Xu"],"pdf_url":"https://arxiv.org/pdf/2007.14129v6.pdf","comment":"Accepted by ACM TIST"},{"id":"http://arxiv.org/abs/2303.06698v1","updated":"2023-03-12T16:23:58Z","published":"2023-03-12T16:23:58Z","title":"Branch & Learn with Post-hoc Correction for Predict+Optimize with\n  Unknown Parameters in Constraints","summary":"  Combining machine learning and constrained optimization, Predict+Optimize\ntackles optimization problems containing parameters that are unknown at the\ntime of solving. Prior works focus on cases with unknowns only in the\nobjectives. A new framework was recently proposed to cater for unknowns also in\nconstraints by introducing a loss function, called Post-hoc Regret, that takes\ninto account the cost of correcting an unsatisfiable prediction. Since Post-hoc\nRegret is non-differentiable, the previous work computes only its\napproximation. While the notion of Post-hoc Regret is general, its specific\nimplementation is applicable to only packing and covering linear programming\nproblems. In this paper, we first show how to compute Post-hoc Regret exactly\nfor any optimization problem solvable by a recursive algorithm satisfying\nsimple conditions. Experimentation demonstrates substantial improvement in the\nquality of solutions as compared to the earlier approximation approach.\nFurthermore, we show experimentally the empirical behavior of different\ncombinations of correction and penalty functions used in the Post-hoc Regret of\nthe same benchmarks. Results provide insights for defining the appropriate\nPost-hoc Regret in different application scenarios.\n","authors":["Xinyi Hu","Jasper C. H. Lee","Jimmy H. M. Lee"],"pdf_url":"https://arxiv.org/pdf/2303.06698v1.pdf","comment":null}],"Computation and Language":[{"id":"http://arxiv.org/abs/2210.15173v2","updated":"2023-03-12T20:28:46Z","published":"2022-10-27T05:07:04Z","title":"Articulation GAN: Unsupervised modeling of articulatory learning","summary":"  Generative deep neural networks are widely used for speech synthesis, but\nmost existing models directly generate waveforms or spectral outputs. Humans,\nhowever, produce speech by controlling articulators, which results in the\nproduction of speech sounds through physical properties of sound propagation.\nWe introduce the Articulatory Generator to the Generative Adversarial Network\nparadigm, a new unsupervised generative model of speech production/synthesis.\nThe Articulatory Generator more closely mimics human speech production by\nlearning to generate articulatory representations (electromagnetic\narticulography or EMA) in a fully unsupervised manner. A separate pre-trained\nphysical model (ema2wav) then transforms the generated EMA representations to\nspeech waveforms, which get sent to the Discriminator for evaluation.\nArticulatory analysis suggests that the network learns to control articulators\nin a similar manner to humans during speech production. Acoustic analysis of\nthe outputs suggests that the network learns to generate words that are both\npresent and absent in the training distribution. We additionally discuss\nimplications of articulatory representations for cognitive models of human\nlanguage and speech technology in general.\n","authors":["Gašper Beguš","Alan Zhou","Peter Wu","Gopala K Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2210.15173v2.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2110.05006v3","updated":"2023-03-12T17:56:44Z","published":"2021-10-11T05:30:30Z","title":"Pre-trained Language Models in Biomedical Domain: A Systematic Survey","summary":"  Pre-trained language models (PLMs) have been the de facto paradigm for most\nnatural language processing (NLP) tasks. This also benefits biomedical domain:\nresearchers from informatics, medicine, and computer science (CS) communities\npropose various PLMs trained on biomedical datasets, e.g., biomedical text,\nelectronic health records, protein, and DNA sequences for various biomedical\ntasks. However, the cross-discipline characteristics of biomedical PLMs hinder\ntheir spreading among communities; some existing works are isolated from each\nother without comprehensive comparison and discussions. It expects a survey\nthat not only systematically reviews recent advances of biomedical PLMs and\ntheir applications but also standardizes terminology and benchmarks. In this\npaper, we summarize the recent progress of pre-trained language models in the\nbiomedical domain and their applications in biomedical downstream tasks.\nParticularly, we discuss the motivations and propose a taxonomy of existing\nbiomedical PLMs. Their applications in biomedical downstream tasks are\nexhaustively discussed. At last, we illustrate various limitations and future\ntrends, which we hope can provide inspiration for the future research of the\nresearch community.\n","authors":["Benyou Wang","Qianqian Xie","Jiahuan Pei","Zhihong Chen","Prayag Tiwari","Zhao Li","Jie fu"],"pdf_url":"https://arxiv.org/pdf/2110.05006v3.pdf","comment":"An improved version"},{"id":"http://arxiv.org/abs/2302.07267v3","updated":"2023-03-12T17:32:22Z","published":"2023-02-13T17:57:50Z","title":"\"Correct answers\" from the psychology of artificial intelligence","summary":"  Large Language Models have vastly grown in capabilities. One proposed\napplication of such AI systems is to support data collection in the social and\ncognitive sciences, where perfect experimental control is currently unfeasible\nand the collection of large, representative datasets is generally expensive. In\nthis paper, we re-replicate 14 studies from the Many Labs 2 replication project\nwith OpenAI's text-davinci-003 model, colloquially known as GPT3.5. We\ncollected responses from the default setting of GPT3.5 by inputting each\nstudy's survey as text. Among the eight studies we could analyse, our GPT\nsample replicated 37.5% of the original results as well as 37.5% of the Many\nLabs 2 results. Unexpectedly, we could not analyse the remaining six studies as\nwe had planned in our pre-registration. This was because for each of these six\nstudies, GPT3.5 answered at least one of the survey questions (either a\ndependent variable or a condition variable) in an extremely predetermined way:\nan unexpected phenomenon we call the \"correct answer\" effect. Different runs of\nGPT3.5 answered nuanced questions probing political orientation, economic\npreference, judgement, and moral philosophy with zero or near-zero variation in\nresponses: with the supposedly \"correct answer.\" For example, our survey\nquestions found the default setting of GPT3.5 to almost always self-identify as\na maximally strong conservative (99.6%, N=1,030), and to always be morally\ndeontological in opposing the hypothetical pushing of a large man in front of\nan incoming trolley to save the lives of five people (100%, N=1,030). Since AI\nmodels of the future may be trained on much of the same data as GPT3.5,\ntraining data from which GPT3.5 may have learned its supposedly \"correct\nanswers,\" our results raise concerns that a hypothetical AI-led future may in\ncertain ways be subject to a diminished diversity of thought.\n","authors":["Peter S. Park","Philipp Schoenegger","Chongyang Zhu"],"pdf_url":"https://arxiv.org/pdf/2302.07267v3.pdf","comment":"52 pages (31-page main text, 21-page SI); nine visualizations (three\n  tables and two figures in the main text, four figures in the SI); added\n  corrections regarding the previously erroneous survey for Study 4's\n  replication of Graham et al. (2009); preregistered OSF database is available\n  at https://osf.io/dzp8t/"},{"id":"http://arxiv.org/abs/2303.02846v2","updated":"2023-03-12T14:51:17Z","published":"2023-03-06T02:52:37Z","title":"Reducing Spurious Correlations for Aspect-Based Sentiment Analysis with\n  Variational Information Bottleneck and Contrastive Learning","summary":"  Deep learning techniques have dominated the literature on aspect-based\nsentiment analysis (ABSA), yielding state-of-the-art results. However, these\ndeep models generally suffer from spurious correlation problems between input\nfeatures and output labels, which creates significant barriers to robustness\nand generalization capability. In this paper, we propose a novel Contrastive\nVariational Information Bottleneck framework (called CVIB) to reduce spurious\ncorrelations for ABSA. The proposed CVIB framework is composed of an original\nnetwork and a self-pruned network, and these two networks are optimized\nsimultaneously via contrastive learning. Concretely, we employ the Variational\nInformation Bottleneck (VIB) principle to learn an informative and compressed\nnetwork (self-pruned network) from the original network, which discards the\nsuperfluous patterns or spurious correlations between input features and\nprediction labels. Then, self-pruning contrastive learning is devised to pull\ntogether semantically similar positive pairs and push away dissimilar pairs,\nwhere the representations of the anchor learned by the original and self-pruned\nnetworks respectively are regarded as a positive pair while the representations\nof two different sentences within a mini-batch are treated as a negative pair.\nTo verify the effectiveness of our CVIB method, we conduct extensive\nexperiments on five benchmark ABSA datasets and the experimental results show\nthat our approach achieves better performance than the strong competitors in\nterms of overall prediction performance, robustness, and generalization.\n","authors":["Mingshan Chang","Min Yang","Qingshan Jiang","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2303.02846v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06675v1","updated":"2023-03-12T14:31:44Z","published":"2023-03-12T14:31:44Z","title":"LUKE-Graph: A Transformer-based Approach with Gated Relational Graph\n  Attention for Cloze-style Reading Comprehension","summary":"  Incorporating prior knowledge can improve existing pre-training models in\ncloze-style machine reading and has become a new trend in recent studies.\nNotably, most of the existing models have integrated external knowledge graphs\n(KG) and transformer-based models, such as BERT into a unified data structure.\nHowever, selecting the most relevant ambiguous entities in KG and extracting\nthe best subgraph remains a challenge. In this paper, we propose the\nLUKE-Graph, a model that builds a heterogeneous graph based on the intuitive\nrelationships between entities in a document without using any external KG. We\nthen use a Relational Graph Attention (RGAT) network to fuse the graph's\nreasoning information and the contextual representation encoded by the\npre-trained LUKE model. In this way, we can take advantage of LUKE, to derive\nan entity-aware representation; and a graph model - to exploit relation-aware\nrepresentation. Moreover, we propose Gated-RGAT by augmenting RGAT with a\ngating mechanism that regulates the question information for the graph\nconvolution operation. This is very similar to human reasoning processing\nbecause they always choose the best entity candidate based on the question\ninformation. Experimental results demonstrate that the LUKE-Graph achieves\nstate-of-the-art performance on the ReCoRD dataset with commonsense reasoning.\n","authors":["Shima Foolad","Kourosh Kiani"],"pdf_url":"https://arxiv.org/pdf/2303.06675v1.pdf","comment":"submitted for neurocomputing journal"},{"id":"http://arxiv.org/abs/2303.06662v1","updated":"2023-03-12T13:51:38Z","published":"2023-03-12T13:51:38Z","title":"Fuzzy Alignments in Directed Acyclic Graph for Non-Autoregressive\n  Machine Translation","summary":"  Non-autoregressive translation (NAT) reduces the decoding latency but suffers\nfrom performance degradation due to the multi-modality problem. Recently, the\nstructure of directed acyclic graph has achieved great success in NAT, which\ntackles the multi-modality problem by introducing dependency between vertices.\nHowever, training it with negative log-likelihood loss implicitly requires a\nstrict alignment between reference tokens and vertices, weakening its ability\nto handle multiple translation modalities. In this paper, we hold the view that\nall paths in the graph are fuzzily aligned with the reference sentence. We do\nnot require the exact alignment but train the model to maximize a fuzzy\nalignment score between the graph and reference, which takes captured\ntranslations in all modalities into account. Extensive experiments on major WMT\nbenchmarks show that our method substantially improves translation performance\nand increases prediction confidence, setting a new state of the art for NAT on\nthe raw training data.\n","authors":["Zhengrui Ma","Chenze Shao","Shangtong Gui","Min Zhang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2303.06662v1.pdf","comment":"ICLR 2023"},{"id":"http://arxiv.org/abs/2303.06623v1","updated":"2023-03-12T09:35:42Z","published":"2023-03-12T09:35:42Z","title":"MWE as WSD: Solving Multiword Expression Identification with Word Sense\n  Disambiguation","summary":"  Recent work in word sense disambiguation (WSD) utilizes encodings of the\nsense gloss (definition text), in addition to the input words and context, to\nimprove performance. In this work we demonstrate that this approach can be\nadapted for use in multiword expression (MWE) identification by training a\nBi-encoder model which uses gloss and context information to filter MWE\ncandidates produced from a simple rule-based extraction pipeline. We achieve\nstate-of-the-art results in MWE identification on the DiMSUM dataset, and\ncompetitive results on the PARSEME 1.1 English dataset using this method. Our\nmodel also retains most of its ability to perform WSD, demonstrating that a\nsingle model can successfully be applied to both of these tasks. Additionally,\nwe experiment with applying Poly-encoder models to MWE identification and WSD,\nintroducing a modified Poly-encoder architecture which outperforms the standard\nPoly-encoder on these tasks.\n","authors":["Joshua Tanner","Jacob Hoffman"],"pdf_url":"https://arxiv.org/pdf/2303.06623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06585v1","updated":"2023-03-12T06:11:44Z","published":"2023-03-12T06:11:44Z","title":"Improving the Intent Classification accuracy in Noisy Environment","summary":"  Intent classification is a fundamental task in the spoken language\nunderstanding field that has recently gained the attention of the scientific\ncommunity, mainly because of the feasibility of approaching it with end-to-end\nneural models. In this way, avoiding using intermediate steps, i.e. automatic\nspeech recognition, is possible, thus the propagation of errors due to\nbackground noise, spontaneous speech, speaking styles of users, etc. Towards\nthe development of solutions applicable in real scenarios, it is interesting to\ninvestigate how environmental noise and related noise reduction techniques to\naddress the intent classification task with end-to-end neural models. In this\npaper, we experiment with a noisy version of the fluent speech command data\nset, combining the intent classifier with a time-domain speech enhancement\nsolution based on Wave-U-Net and considering different training strategies.\nExperimental results reveal that, for this task, the use of speech enhancement\ngreatly improves the classification accuracy in noisy conditions, in particular\nwhen the classification model is trained on enhanced signals.\n","authors":["Mohamed Nabih Ali","Alessio Brutti","Daniele Falavigna"],"pdf_url":"https://arxiv.org/pdf/2303.06585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06580v1","updated":"2023-03-12T05:27:22Z","published":"2023-03-12T05:27:22Z","title":"Towards General Purpose Medical AI: Continual Learning Medical\n  Foundation Model","summary":"  Inevitable domain and task discrepancies in real-world scenarios can impair\nthe generalization performance of the pre-trained deep models for medical data.\nTherefore, we audaciously propose that we should build a general-purpose\nmedical AI system that can be seamlessly adapted to downstream domains/tasks.\nSince the domain/task adaption procedures usually involve additional labeling\nwork for the target data, designing a data-efficient adaption algorithm is\ndesired to save the cost of transferring the learned knowledge. Our recent work\nfound that vision-language models (VLMs) are efficient learners with\nextraordinary cross-domain ability. Therefore, in this work, we further explore\nthe possibility of leveraging pre-trained VLMs as medical foundation models for\nbuilding general-purpose medical AI, where we thoroughly investigate three\nmachine-learning paradigms, i.e., domain/task-specialized learning, joint\nlearning, and continual learning, for training the VLMs and evaluate their\ngeneralization performance on cross-domain and cross-task test sets. To\nalleviate the catastrophic forgetting during sequential training, we employ\nrehearsal learning and receive a sharp boost in terms of generalization\ncapability. In a nutshell, our empirical evidence suggests that continual\nlearning may be a practical and efficient learning paradigm for the medical\nfoundation model. And we hope researchers can use our empirical evidence as\nbasement to further explore the path toward medical foundation model.\n","authors":["Huahui Yi","Ziyuan Qin","Qicheng Lao","Wei Xu","Zekun Jiang","Dequan Wang","Shaoting Zhang","Kang Li"],"pdf_url":"https://arxiv.org/pdf/2303.06580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06574v1","updated":"2023-03-12T05:11:09Z","published":"2023-03-12T05:11:09Z","title":"Diffusion Models for Non-autoregressive Text Generation: A Survey","summary":"  Non-autoregressive (NAR) text generation has attracted much attention in the\nfield of natural language processing, which greatly reduces the inference\nlatency but has to sacrifice the generation accuracy. Recently, diffusion\nmodels, a class of latent variable generative models, have been introduced into\nNAR text generation, showing improved generation quality. In this survey, we\nreview the recent progress in diffusion models for NAR text generation. As the\nbackground, we first present the general definition of diffusion models and the\ntext diffusion models, and then discuss their merits for NAR generation. As the\ncore content, we further introduce two mainstream diffusion models in existing\ntext diffusion works, and review the key designs of the diffusion process.\nMoreover, we discuss the utilization of pre-trained language models (PLMs) for\ntext diffusion models and introduce optimization techniques for text data.\nFinally, we discuss several promising directions and conclude this paper. Our\nsurvey aims to provide researchers with a systematic reference of related\nresearch on text diffusion models for NAR generation.\n","authors":["Yifan Li","Kun Zhou","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2303.06574v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2209.05735v3","updated":"2023-03-12T04:46:00Z","published":"2022-09-13T05:14:08Z","title":"Learning ASR pathways: A sparse multilingual ASR model","summary":"  Neural network pruning compresses automatic speech recognition (ASR) models\neffectively. However, in multilingual ASR, language-agnostic pruning may lead\nto severe performance drops on some languages because language-agnostic pruning\nmasks may not fit all languages and discard important language-specific\nparameters. In this work, we present ASR pathways, a sparse multilingual ASR\nmodel that activates language-specific sub-networks (\"pathways\"), such that the\nparameters for each language are learned explicitly. With the overlapping\nsub-networks, the shared parameters can also enable knowledge transfer for\nlower-resource languages via joint multilingual training. We propose a novel\nalgorithm to learn ASR pathways, and evaluate the proposed method on 4\nlanguages with a streaming RNN-T model. Our proposed ASR pathways outperform\nboth dense models and a language-agnostically pruned model, and provide better\nperformance on low-resource languages compared to the monolingual sparse\nmodels.\n","authors":["Mu Yang","Andros Tjandra","Chunxi Liu","David Zhang","Duc Le","Ozlem Kalinli"],"pdf_url":"https://arxiv.org/pdf/2209.05735v3.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.06565v1","updated":"2023-03-12T04:23:54Z","published":"2023-03-12T04:23:54Z","title":"Compressed Heterogeneous Graph for Abstractive Multi-Document\n  Summarization","summary":"  Multi-document summarization (MDS) aims to generate a summary for a number of\nrelated documents. We propose HGSUM, an MDS model that extends an\nencoder-decoder architecture, to incorporate a heterogeneous graph to represent\ndifferent semantic units (e.g., words and sentences) of the documents. This\ncontrasts with existing MDS models which do not consider different edge types\nof graphs and as such do not capture the diversity of relationships in the\ndocuments. To preserve only key information and relationships of the documents\nin the heterogeneous graph, HGSUM uses graph pooling to compress the input\ngraph. And to guide HGSUM to learn compression, we introduce an additional\nobjective that maximizes the similarity between the compressed graph and the\ngraph constructed from the ground-truth summary during training. HGSUM is\ntrained end-to-end with graph similarity and standard cross-entropy objectives.\nExperimental results over MULTI-NEWS, WCEP-100, and ARXIV show that HGSUM\noutperforms state-of-the-art MDS models. The code for our model and experiments\nis available at: https://github.com/oaimli/HGSum.\n","authors":["Miao Li","Jianzhong Qi","Jey Han Lau"],"pdf_url":"https://arxiv.org/pdf/2303.06565v1.pdf","comment":"AAAI 2023"},{"id":"http://arxiv.org/abs/2211.00171v2","updated":"2023-03-12T00:12:42Z","published":"2022-10-31T22:32:36Z","title":"Using Emotion Embeddings to Transfer Knowledge Between Emotions,\n  Languages, and Annotation Formats","summary":"  The need for emotional inference from text continues to diversify as more and\nmore disciplines integrate emotions into their theories and applications. These\nneeds include inferring different emotion types, handling multiple languages,\nand different annotation formats. A shared model between different\nconfigurations would enable the sharing of knowledge and a decrease in training\ncosts, and would simplify the process of deploying emotion recognition models\nin novel environments. In this work, we study how we can build a single model\nthat can transition between these different configurations by leveraging\nmultilingual models and Demux, a transformer-based model whose input includes\nthe emotions of interest, enabling us to dynamically change the emotions\npredicted by the model. Demux also produces emotion embeddings, and performing\noperations on them allows us to transition to clusters of emotions by pooling\nthe embeddings of each cluster. We show that Demux can simultaneously transfer\nknowledge in a zero-shot manner to a new language, to a novel annotation format\nand to unseen emotions. Code is available at\nhttps://github.com/gchochla/Demux-MEmo .\n","authors":["Georgios Chochlakis","Gireesh Mahajan","Sabyasachee Baruah","Keith Burghardt","Kristina Lerman","Shrikanth Narayanan"],"pdf_url":"https://arxiv.org/pdf/2211.00171v2.pdf","comment":"Accepted at ICASSP'23, 5 pages"},{"id":"http://arxiv.org/abs/2210.15842v2","updated":"2023-03-12T00:10:51Z","published":"2022-10-28T02:27:18Z","title":"Leveraging Label Correlations in a Multi-label Setting: A Case Study in\n  Emotion","summary":"  Detecting emotions expressed in text has become critical to a range of\nfields. In this work, we investigate ways to exploit label correlations in\nmulti-label emotion recognition models to improve emotion detection. First, we\ndevelop two modeling approaches to the problem in order to capture word\nassociations of the emotion words themselves, by either including the emotions\nin the input, or by leveraging Masked Language Modeling (MLM). Second, we\nintegrate pairwise constraints of emotion representations as regularization\nterms alongside the classification loss of the models. We split these terms\ninto two categories, local and global. The former dynamically change based on\nthe gold labels, while the latter remain static during training. We demonstrate\nstate-of-the-art performance across Spanish, English, and Arabic in SemEval\n2018 Task 1 E-c using monolingual BERT-based models. On top of better\nperformance, we also demonstrate improved robustness. Code is available at\nhttps://github.com/gchochla/Demux-MEmo.\n","authors":["Georgios Chochlakis","Gireesh Mahajan","Sabyasachee Baruah","Keith Burghardt","Kristina Lerman","Shrikanth Narayanan"],"pdf_url":"https://arxiv.org/pdf/2210.15842v2.pdf","comment":"Accepted at ICASSP'23, 5 pages, 1 figure"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.06783v1","updated":"2023-03-12T23:51:51Z","published":"2023-03-12T23:51:51Z","title":"Asynchronous Decentralized Federated Lifelong Learning for Landmark\n  Localization in Medical Imaging","summary":"  Federated learning is a recent development in the machine learning area that\nallows a system of devices to train on one or more tasks without sharing their\ndata to a single location or device. However, this framework still requires a\ncentralized global model to consolidate individual models into one, and the\ndevices train synchronously, which both can be potential bottlenecks for using\nfederated learning. In this paper, we propose a novel method of asynchronous\ndecentralized federated lifelong learning (ADFLL) method that inherits the\nmerits of federated learning and can train on multiple tasks simultaneously\nwithout the need for a central node or synchronous training. Thus, overcoming\nthe potential drawbacks of conventional federated learning. We demonstrate\nexcellent performance on the brain tumor segmentation (BRATS) dataset for\nlocalizing the left ventricle on multiple image sequences and image\norientation. Our framework allows agents to achieve the best performance with a\nmean distance error of 7.81, better than the conventional all-knowing agent's\nmean distance error of 11.78, and significantly (p=0.01) better than a\nconventional lifelong learning agent with a distance error of 15.17 after eight\nrounds of training. In addition, all ADFLL agents have comparable or better\nperformance than a conventional LL agent. In conclusion, we developed an ADFLL\nframework with excellent performance and speed-up compared to conventional RL\nagents.\n","authors":["Guangyao Zheng","Michael A. Jacobs","Vladimir Braverman","Vishwa S. Parekh"],"pdf_url":"https://arxiv.org/pdf/2303.06783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06782v1","updated":"2023-03-12T23:46:04Z","published":"2023-03-12T23:46:04Z","title":"AidUI: Toward Automated Recognition of Dark Patterns in User Interfaces","summary":"  Past studies have illustrated the prevalence of UI dark patterns, or user\ninterfaces that can lead end-users toward (unknowingly) taking actions that\nthey may not have intended. Such deceptive UI designs can result in adverse\neffects on end users, such as oversharing personal information or financial\nloss. While significant research progress has been made toward the development\nof dark pattern taxonomies, developers and users currently lack guidance to\nhelp recognize, avoid, and navigate these often subtle design motifs. However,\nautomated recognition of dark patterns is a challenging task, as the\ninstantiation of a single type of pattern can take many forms, leading to\nsignificant variability.\n  In this paper, we take the first step toward understanding the extent to\nwhich common UI dark patterns can be automatically recognized in modern\nsoftware applications. To do this, we introduce AidUI, a novel automated\napproach that uses computer vision and natural language processing techniques\nto recognize a set of visual and textual cues in application screenshots that\nsignify the presence of ten unique UI dark patterns, allowing for their\ndetection, classification, and localization. To evaluate our approach, we have\nconstructed ContextDP, the current largest dataset of fully-localized UI dark\npatterns that spans 175 mobile and 83 web UI screenshots containing 301 dark\npattern instances. The results of our evaluation illustrate that \\AidUI\nachieves an overall precision of 0.66, recall of 0.67, F1-score of 0.65 in\ndetecting dark pattern instances, reports few false positives, and is able to\nlocalize detected patterns with an IoU score of ~0.84. Furthermore, a\nsignificant subset of our studied dark patterns can be detected quite reliably\n(F1 score of over 0.82), and future research directions may allow for improved\ndetection of additional patterns.\n","authors":["SM Hasan Mansur","Sabiha Salma","Damilola Awofisayo","Kevin Moran"],"pdf_url":"https://arxiv.org/pdf/2303.06782v1.pdf","comment":"13 pages, Accepted at The 45th IEEE/ACM International Conference on\n  Software Engineering (ICSE 2023), Melbourne, Australia, May 14th-20th, 2023"},{"id":"http://arxiv.org/abs/2211.08253v2","updated":"2023-03-12T22:00:41Z","published":"2022-11-15T15:59:43Z","title":"HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization","summary":"  Due to domain shift, machine learning systems typically fail to generalize\nwell to domains different from those of training data, which is what domain\ngeneralization (DG) aims to address. Although various DG methods have been\ndeveloped, most of them lack interpretability and require domain labels that\nare not available in many real-world scenarios. This paper presents a novel DG\nmethod, called HMOE: Hypernetwork-based Mixture of Experts (MoE), which does\nnot rely on domain labels and is more interpretable. MoE proves effective in\nidentifying heterogeneous patterns in data. For the DG problem, heterogeneity\narises exactly from domain shift. HMOE uses hypernetworks taking vectors as\ninput to generate experts' weights, which allows experts to share useful\nmeta-knowledge and enables exploring experts' similarities in a low-dimensional\nvector space. We compare HMOE with other DG algorithms under a fair and unified\nbenchmark-DomainBed. Our extensive experiments show that HMOE can divide\nmixed-domain data into distinct clusters that are surprisingly more consistent\nwith human intuition than original domain labels. Compared to other DG methods,\nHMOE shows competitive performance and achieves SOTA results in some cases.\n","authors":["Jingang Qu","Thibault Faney","Ze Wang","Patrick Gallinari","Soleiman Yousef","Jean-Charles de Hemptinne"],"pdf_url":"https://arxiv.org/pdf/2211.08253v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.08767v2","updated":"2023-03-12T21:34:05Z","published":"2022-08-18T11:05:55Z","title":"Evaluating Continual Test-Time Adaptation for Contextual and Semantic\n  Domain Shifts","summary":"  In this paper, our goal is to adapt a pre-trained convolutional neural\nnetwork to domain shifts at test time. We do so continually with the incoming\nstream of test batches, without labels. The existing literature mostly operates\non artificial shifts obtained via adversarial perturbations of a test image.\nMotivated by this, we evaluate the state of the art on two realistic and\nchallenging sources of domain shifts, namely contextual and semantic shifts.\nContextual shifts correspond to the environment types, for example, a model\npre-trained on indoor context has to adapt to the outdoor context on CORe-50.\nSemantic shifts correspond to the capture types, for example a model\npre-trained on natural images has to adapt to cliparts, sketches, and paintings\non DomainNet. We include in our analysis recent techniques such as\nPrediction-Time Batch Normalization (BN), Test Entropy Minimization (TENT) and\nContinual Test-Time Adaptation (CoTTA). Our findings are three-fold: i)\nTest-time adaptation methods perform better and forget less on contextual\nshifts compared to semantic shifts, ii) TENT outperforms other methods on\nshort-term adaptation, whereas CoTTA outpeforms other methods on long-term\nadaptation, iii) BN is most reliable and robust. Our code is available at\nhttps://github.com/tommiekerssies/Evaluating-Continual-Test-Time-Adaptation-for-Contextual-and-Semantic-Domain-Shifts.\n","authors":["Tommie Kerssies","Mert Kılıçkaya","Joaquin Vanschoren"],"pdf_url":"https://arxiv.org/pdf/2208.08767v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12353v2","updated":"2023-03-12T21:15:22Z","published":"2022-11-22T15:43:19Z","title":"U-Flow: A U-shaped Normalizing Flow for Anomaly Detection with\n  Unsupervised Threshold","summary":"  In this work we propose a non-contrastive method for anomaly detection and\nsegmentation in images, that benefits both from a modern machine learning\napproach and a more classic statistical detection theory. The method consists\nof three phases. First, features are extracted using a multi-scale image\nTransformer architecture. Then, these features are fed into a U-shaped\nNormalizing Flow that lays the theoretical foundations for the last phase,\nwhich computes a pixel-level anomaly map, and performs a segmentation based on\nthe a contrario framework. This multiple-hypothesis testing strategy permits to\nderive robust automatic detection thresholds, which are crucial in many\nreal-world applications, where an operational point is needed. The segmentation\nresults are evaluated using the Intersection over Union (IoU) metric; and for\nassessing the generated anomaly maps we report the area under the Receiver\nOperating Characteristic curve (AUROC), and the area under the\nper-region-overlap curve (AUPRO). Extensive experimentation in various datasets\nshows that the proposed approach produces state-of-the-art results for all\nmetrics and all datasets, ranking first in most MvTec-AD categories, with a\nmean pixel-level AUROC of 98.74%. Code and trained models are available at\nhttps:// github.com/mtailanian/uflow.\n","authors":["Matías Tailanian","Álvaro Pardo","Pablo Musé"],"pdf_url":"https://arxiv.org/pdf/2211.12353v2.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2303.06753v1","updated":"2023-03-12T21:01:54Z","published":"2023-03-12T21:01:54Z","title":"Module-Wise Network Quantization for 6D Object Pose Estimation","summary":"  Many edge applications, such as collaborative robotics and spacecraft\nrendezvous, can benefit from 6D object pose estimation, but must do so on\nembedded platforms. Unfortunately, existing 6D pose estimation networks are\ntypically too large for deployment in such situations and must therefore be\ncompressed, while maintaining reliable performance. In this work, we present an\napproach to doing so by quantizing such networks. More precisely, we introduce\na module-wise quantization strategy that, in contrast to uniform and\nmixed-precision quantization, accounts for the modular structure of typical 6D\npose estimation frameworks. We demonstrate that uniquely compressing these\nmodules outperforms uniform and mixed-precision quantization techniques.\nMoreover, our experiments evidence that module-wise quantization can lead to a\nsignificant accuracy boost. We showcase the generality of our approach using\ndifferent datasets, quantization methodologies, and network architectures,\nincluding the recent ZebraPose.\n","authors":["Saqib Javed","Andrew Price","Yinlin Hu","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2303.06753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06747v1","updated":"2023-03-12T20:49:07Z","published":"2023-03-12T20:49:07Z","title":"Raising The Limit Of Image Rescaling Using Auxiliary Encoding","summary":"  Normalizing flow models using invertible neural networks (INN) have been\nwidely investigated for successful generative image super-resolution (SR) by\nlearning the transformation between the normal distribution of latent variable\n$z$ and the conditional distribution of high-resolution (HR) images gave a\nlow-resolution (LR) input. Recently, image rescaling models like IRN utilize\nthe bidirectional nature of INN to push the performance limit of image\nupscaling by optimizing the downscaling and upscaling steps jointly. While the\nrandom sampling of latent variable $z$ is useful in generating diverse\nphoto-realistic images, it is not desirable for image rescaling when accurate\nrestoration of the HR image is more important. Hence, in places of random\nsampling of $z$, we propose auxiliary encoding modules to further push the\nlimit of image rescaling performance. Two options to store the encoded latent\nvariables in downscaled LR images, both readily supported in existing image\nfile format, are proposed. One is saved as the alpha-channel, the other is\nsaved as meta-data in the image header, and the corresponding modules are\ndenoted as suffixes -A and -M respectively. Optimal network architectural\nchanges are investigated for both options to demonstrate their effectiveness in\nraising the rescaling performance limit on different baseline models including\nIRN and DLV-IRN.\n","authors":["Chenzhong Yin","Zhihong Pan","Xin Zhou","Le Kang","Paul Bogdan"],"pdf_url":"https://arxiv.org/pdf/2303.06747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06744v1","updated":"2023-03-12T20:16:14Z","published":"2023-03-12T20:16:14Z","title":"Ensemble Learning of Myocardial Displacements for Myocardial Infarction\n  Detection in Echocardiography","summary":"  Early detection and localization of myocardial infarction (MI) can reduce the\nseverity of cardiac damage through timely treatment interventions. In recent\nyears, deep learning techniques have shown promise for detecting MI in\nechocardiographic images. However, there has been no examination of how\nsegmentation accuracy affects MI classification performance and the potential\nbenefits of using ensemble learning approaches. Our study investigates this\nrelationship and introduces a robust method that combines features from\nmultiple segmentation models to improve MI classification performance by\nleveraging ensemble learning. Our method combines myocardial segment\ndisplacement features from multiple segmentation models, which are then input\ninto a typical classifier to estimate the risk of MI. We validated the proposed\napproach on two datasets: the public HMC-QU dataset (109 echocardiograms) for\ntraining and validation, and an E-Hospital dataset (60 echocardiograms) from a\nlocal clinical site in Vietnam for independent testing. Model performance was\nevaluated based on accuracy, sensitivity, and specificity. The proposed\napproach demonstrated excellent performance in detecting MI. The results showed\nthat the proposed approach outperformed the state-of-the-art feature-based\nmethod. Further research is necessary to determine its potential use in\nclinical settings as a tool to assist cardiologists and technicians with\nobjective assessments and reduce dependence on operator subjectivity. Our\nresearch codes are available on GitHub at\nhttps://github.com/vinuni-vishc/mi-detection-echo.\n","authors":["Nguyen Tuan","Phi Nguyen","Dai Tran","Hung Pham","Quang Nguyen","Thanh Le","Hanh Van","Bach Do","Phuong Tran","Vinh Le","Thuy Nguyen","Long Tran","Hieu Pham"],"pdf_url":"https://arxiv.org/pdf/2303.06744v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.02665v2","updated":"2023-03-12T10:26:48Z","published":"2023-03-05T13:06:53Z","title":"Heterogeneous Graph Learning for Acoustic Event Classification","summary":"  Heterogeneous graphs provide a compact, efficient, and scalable way to model\ndata involving multiple disparate modalities. This makes modeling audiovisual\ndata using heterogeneous graphs an attractive option. However, graph structure\ndoes not appear naturally in audiovisual data. Graphs for audiovisual data are\nconstructed manually which is both difficult and sub-optimal. In this work, we\naddress this problem by (i) proposing a parametric graph construction strategy\nfor the intra-modal edges, and (ii) learning the crossmodal edges. To this end,\nwe develop a new model, heterogeneous graph crossmodal network (HGCN) that\nlearns the crossmodal edges. Our proposed model can adapt to various spatial\nand temporal scales owing to its parametric construction, while the learnable\ncrossmodal edges effectively connect the relevant nodes across modalities.\nExperiments on a large benchmark dataset (AudioSet) show that our model is\nstate-of-the-art (0.53 mean average precision), outperforming transformer-based\nmodels and other graph-based models.\n","authors":["Amir Shirian","Mona Ahmadian","Krishna Somandepalli","Tanaya Guha"],"pdf_url":"https://arxiv.org/pdf/2303.02665v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2207.07935"},{"id":"http://arxiv.org/abs/2302.08774v2","updated":"2023-03-12T07:53:23Z","published":"2023-02-17T09:20:51Z","title":"Vision, Deduction and Alignment: An Empirical Study on Multi-modal\n  Knowledge Graph Alignment","summary":"  Entity alignment (EA) for knowledge graphs (KGs) plays a critical role in\nknowledge engineering. Existing EA methods mostly focus on utilizing the graph\nstructures and entity attributes (including literals), but ignore images that\nare common in modern multi-modal KGs. In this study we first constructed\nMulti-OpenEA -- eight large-scale, image-equipped EA benchmarks, and then\nevaluated some existing embedding-based methods for utilizing images. In view\nof the complementary nature of visual modal information and logical deduction,\nwe further developed a new multi-modal EA method named LODEME using logical\ndeduction and multi-modal KG embedding, with state-of-the-art performance\nachieved on Multi-OpenEA and other existing multi-modal EA benchmarks.\n","authors":["Yangning Li","Jiaoyan Chen","Yinghui Li","Yuejia Xiang","Xi Chen","Hai-Tao Zheng"],"pdf_url":"https://arxiv.org/pdf/2302.08774v2.pdf","comment":"Accepted by ICASSP2023"}]},"2023-03-11T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.06356v1","updated":"2023-03-11T09:25:51Z","published":"2023-03-11T09:25:51Z","title":"PowerMat: context-aware recommender system without user item rating\n  values that solves the cold-start problem","summary":"  Recommender systems serves as an important technical asset in many modern\ncompanies. With the increasing demand for higher precision of the technology,\nmore and more research and investment has been allocated to the field. One\nimportant sub-field of recommender systems that has been stagnating is\ncontext-aware recommender systems. Due to the difficulty of collecting input\ndataset, the amount of research on context-aware recommender systems is much\nless than other sub-fields of recommender systems. In this paper, we propose a\nnew algorithm named PowerMat to tackle the context-aware recommendation\nproblem. We build our theory on matrix factorization and Zipf's law, and also\nmore recent research work such as DotMat. We prove by experiments that our\nmethod achieves superior results to the classic matrix factorization algorithm\nand other context-aware recommender systems such as MovieMat+. In addition, by\ntheoretical analysis, we show that our algorithm solves the cold-start problem\nfor context-aware recommendation.\n","authors":["Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2303.06356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06354v1","updated":"2023-03-11T09:11:27Z","published":"2023-03-11T09:11:27Z","title":"Betti Number for Point Sets","summary":"  Topology is the foundation for many industrial applications ranging from CAD\nto simulation analysis. Computational topology mostly focuses on structured\ndata such as mesh, however unstructured dataset such as point set remains a\nvirgin land for topology scientists. The significance of point-based topology\ncan never be overemphasized, especially in the area of reverse engineering,\ngeometric modeling and algorithmic analysis. In this paper, we propose a novel\napproach to compute the Betti number for point set data and illustrate its\nusefulness in real world examples. To the best of our knowledge, our work is\npioneering and first of its kind in the fields of computational topology.\n","authors":["Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2303.06354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06347v1","updated":"2023-03-11T08:44:14Z","published":"2023-03-11T08:44:14Z","title":"User Retention-oriented Recommendation with Decision Transformer","summary":"  Improving user retention with reinforcement learning~(RL) has attracted\nincreasing attention due to its significant importance in boosting user\nengagement. However, training the RL policy from scratch without hurting users'\nexperience is unavoidable due to the requirement of trial-and-error searches.\nFurthermore, the offline methods, which aim to optimize the policy without\nonline interactions, suffer from the notorious stability problem in value\nestimation or unbounded variance in counterfactual policy evaluation. To this\nend, we propose optimizing user retention with Decision Transformer~(DT), which\navoids the offline difficulty by translating the RL as an autoregressive\nproblem. However, deploying the DT in recommendation is a non-trivial problem\nbecause of the following challenges: (1) deficiency in modeling the numerical\nreward value; (2) data discrepancy between the policy learning and\nrecommendation generation; (3) unreliable offline performance evaluation. In\nthis work, we, therefore, contribute a series of strategies for tackling the\nexposed issues. We first articulate an efficient reward prompt by weighted\naggregation of meta embeddings for informative reward embedding. Then, we endow\na weighted contrastive learning method to solve the discrepancy between\ntraining and inference. Furthermore, we design two robust offline metrics to\nmeasure user retention. Finally, the significant improvement in the benchmark\ndatasets demonstrates the superiority of the proposed method.\n","authors":["Kesen Zhao","Lixin Zou","Xiangyu Zhao","Maolin Wang","Dawei yin"],"pdf_url":"https://arxiv.org/pdf/2303.06347v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.06337v1","updated":"2023-03-11T07:50:49Z","published":"2023-03-11T07:50:49Z","title":"AutoMLP: Automated MLP for Sequential Recommendations","summary":"  Sequential recommender systems aim to predict users' next interested item\ngiven their historical interactions. However, a long-standing issue is how to\ndistinguish between users' long/short-term interests, which may be\nheterogeneous and contribute differently to the next recommendation. Existing\napproaches usually set pre-defined short-term interest length by exhaustive\nsearch or empirical experience, which is either highly inefficient or yields\nsubpar results. The recent advanced transformer-based models can achieve\nstate-of-the-art performances despite the aforementioned issue, but they have a\nquadratic computational complexity to the length of the input sequence. To this\nend, this paper proposes a novel sequential recommender system, AutoMLP, aiming\nfor better modeling users' long/short-term interests from their historical\ninteractions. In addition, we design an automated and adaptive search algorithm\nfor preferable short-term interest length via end-to-end optimization. Through\nextensive experiments, we show that AutoMLP has competitive performance against\nstate-of-the-art methods, while maintaining linear computational complexity.\n","authors":["Muyang Li","Zijian Zhang","Xiangyu Zhao","Wanyu Wang","Minghao Zhao","Runze Wu","Ruocheng Guo"],"pdf_url":"https://arxiv.org/pdf/2303.06337v1.pdf","comment":"Accepted by WWW'23"}],"Computation and Language":[{"id":"http://arxiv.org/abs/2211.15544v2","updated":"2023-03-11T23:13:21Z","published":"2022-11-28T16:49:13Z","title":"Automatically Extracting Information in Medical Dialogue: Expert System\n  And Attention for Labelling","summary":"  Medical dialogue information extraction is becoming an increasingly\nsignificant problem in modern medical care. It is difficult to extract key\ninformation from electronic medical records (EMRs) due to their large numbers.\nPreviously, researchers proposed attention-based models for retrieving features\nfrom EMRs, but their limitations were reflected in their inability to recognize\ndifferent categories in medical dialogues. In this paper, we propose a novel\nmodel, Expert System and Attention for Labelling (ESAL). We use mixture of\nexperts and pre-trained BERT to retrieve the semantics of different categories,\nenabling the model to fuse the differences between them. In our experiment,\nESAL was applied to a public dataset and the experimental results indicated\nthat ESAL significantly improved the performance of Medical Information\nClassification.\n","authors":["Xinshi Wang","Daniel Tang"],"pdf_url":"https://arxiv.org/pdf/2211.15544v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06475v1","updated":"2023-03-11T18:17:03Z","published":"2023-03-11T18:17:03Z","title":"Transcription free filler word detection with Neural semi-CRFs","summary":"  Non-linguistic filler words, such as \"uh\" or \"um\", are prevalent in\nspontaneous speech and serve as indicators for expressing hesitation or\nuncertainty. Previous works for detecting certain non-linguistic filler words\nare highly dependent on transcriptions from a well-established commercial\nautomatic speech recognition (ASR) system. However, certain ASR systems are not\nuniversally accessible from many aspects, e.g., budget, target languages, and\ncomputational power. In this work, we investigate filler word detection system\nthat does not depend on ASR systems. We show that, by using the structured\nstate space sequence model (S4) and neural semi-Markov conditional random\nfields (semi-CRFs), we achieve an absolute F1 improvement of 6.4% (segment\nlevel) and 3.1% (event level) on the PodcastFillers dataset. We also conduct a\nqualitative analysis on the detected results to analyze the limitations of our\nproposed system.\n","authors":["Ge Zhu","Yujia Yan","Juan-Pablo Caceres","Zhiyao Duan"],"pdf_url":"https://arxiv.org/pdf/2303.06475v1.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.06458v1","updated":"2023-03-11T17:14:33Z","published":"2023-03-11T17:14:33Z","title":"ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and\n  Multilingual Natural Language Generation","summary":"  Natural Language Generation (NLG) accepts input data in the form of images,\nvideos, or text and generates corresponding natural language text as output.\nExisting NLG methods mainly adopt a supervised approach and rely heavily on\ncoupled data-to-text pairs. However, for many targeted scenarios and for\nnon-English languages, sufficient quantities of labeled data are often not\navailable. To relax the dependency on labeled data of downstream tasks, we\npropose an intuitive and effective zero-shot learning framework, ZeroNLG, which\ncan deal with multiple NLG tasks, including image-to-text (image captioning),\nvideo-to-text (video captioning), and text-to-text (neural machine\ntranslation), across English, Chinese, German, and French within a unified\nframework. ZeroNLG does not require any labeled downstream pairs for training.\nDuring training, ZeroNLG (i) projects different domains (across modalities and\nlanguages) to corresponding coordinates in a shared common latent space; (ii)\nbridges different domains by aligning their corresponding coordinates in this\nspace; and (iii) builds an unsupervised multilingual auto-encoder to learn to\ngenerate text by reconstructing the input text given its coordinate in shared\nlatent space. Consequently, during inference, based on the data-to-text\npipeline, ZeroNLG can generate target sentences across different languages\ngiven the coordinate of input data in the common space. Within this unified\nframework, given visual (imaging or video) data as input, ZeroNLG can perform\nzero-shot visual captioning; given textual sentences as input, ZeroNLG can\nperform zero-shot machine translation. We present the results of extensive\nexperiments on twelve NLG tasks, showing that, without using any labeled\ndownstream pairs for training, ZeroNLG generates high-quality and believable\noutputs and significantly outperforms existing zero-shot methods.\n","authors":["Bang Yang","Fenglin Liu","Yuexian Zou","Xian Wu","Yaowei Wang","David A. Clifton"],"pdf_url":"https://arxiv.org/pdf/2303.06458v1.pdf","comment":"We will release the codes and models at\n  https://github.com/yangbang18/ZeroNLG soon. Without any labeled downstream\n  pairs for training, the ZeroNLG can deal with multiple NLG tasks, including\n  image-to-text, video-to-text, and text-to-text, across English, Chinese,\n  German, and French within a unified framework"},{"id":"http://arxiv.org/abs/2210.00465v3","updated":"2023-03-11T16:04:32Z","published":"2022-10-02T09:04:47Z","title":"Assessing the impact of contextual information in hate speech detection","summary":"  In recent years, hate speech has gained great relevance in social networks\nand other virtual media because of its intensity and its relationship with\nviolent acts against members of protected groups. Due to the great amount of\ncontent generated by users, great effort has been made in the research and\ndevelopment of automatic tools to aid the analysis and moderation of this\nspeech, at least in its most threatening forms. One of the limitations of\ncurrent approaches to automatic hate speech detection is the lack of context.\nMost studies and resources are performed on data without context; that is,\nisolated messages without any type of conversational context or the topic being\ndiscussed. This restricts the available information to define if a post on a\nsocial network is hateful or not. In this work, we provide a novel corpus for\ncontextualized hate speech detection based on user responses to news posts from\nmedia outlets on Twitter. This corpus was collected in the Rioplatense\ndialectal variety of Spanish and focuses on hate speech associated with the\nCOVID-19 pandemic. Classification experiments using state-of-the-art techniques\nshow evidence that adding contextual information improves hate speech detection\nperformance for two proposed tasks (binary and multi-label prediction). We make\nour code, models, and corpus available for further research.\n","authors":["Juan Manuel Pérez","Franco Luque","Demian Zayat","Martín Kondratzky","Agustín Moro","Pablo Serrati","Joaquín Zajac","Paula Miguel","Natalia Debandi","Agustín Gravano","Viviana Cotik"],"pdf_url":"https://arxiv.org/pdf/2210.00465v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.09231v4","updated":"2023-03-11T15:12:50Z","published":"2021-12-16T22:36:17Z","title":"Two-view Graph Neural Networks for Knowledge Graph Completion","summary":"  We present an effective graph neural network (GNN)-based knowledge graph\nembedding model, which we name WGE, to capture entity- and relation-focused\ngraph structures. Given a knowledge graph, WGE builds a single undirected\nentity-focused graph that views entities as nodes. WGE also constructs another\nsingle undirected graph from relation-focused constraints, which views entities\nand relations as nodes. WGE then proposes a GNN-based architecture to better\nlearn vector representations of entities and relations from these two single\nentity- and relation-focused graphs. WGE feeds the learned entity and relation\nrepresentations into a weighted score function to return the triple scores for\nknowledge graph completion. Experimental results show that WGE outperforms\nstrong baselines on seven benchmark datasets for knowledge graph completion.\n","authors":["Vinh Tong","Dai Quoc Nguyen","Dinh Phung","Dat Quoc Nguyen"],"pdf_url":"https://arxiv.org/pdf/2112.09231v4.pdf","comment":"To appear in Proceedings of ESWC 2023; 17 pages; 4 tables; 4 figures"},{"id":"http://arxiv.org/abs/2204.04916v2","updated":"2023-03-11T15:09:08Z","published":"2022-04-11T07:33:26Z","title":"A Token-level Contrastive Framework for Sign Language Translation","summary":"  Sign Language Translation (SLT) is a promising technology to bridge the\ncommunication gap between the deaf and the hearing people. Recently,\nresearchers have adopted Neural Machine Translation (NMT) methods, which\nusually require large-scale corpus for training, to achieve SLT. However, the\npublicly available SLT corpus is very limited, which causes the collapse of the\ntoken representations and the inaccuracy of the generated tokens. To alleviate\nthis issue, we propose ConSLT, a novel token-level \\textbf{Con}trastive\nlearning framework for \\textbf{S}ign \\textbf{L}anguage \\textbf{T}ranslation ,\nwhich learns effective token representations by incorporating token-level\ncontrastive learning into the SLT decoding process. Concretely, ConSLT treats\neach token and its counterpart generated by different dropout masks as positive\npairs during decoding, and then randomly samples $K$ tokens in the vocabulary\nthat are not in the current sentence to construct negative examples. We conduct\ncomprehensive experiments on two benchmarks (PHOENIX14T and CSL-Daily) for both\nend-to-end and cascaded settings. The experimental results demonstrate that\nConSLT can achieve better translation quality than the strong baselines.\n","authors":["Biao Fu","Peigen Ye","Liang Zhang","Pei Yu","Cong Hu","Yidong Chen","Xiaodong Shi"],"pdf_url":"https://arxiv.org/pdf/2204.04916v2.pdf","comment":"Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.02563v2","updated":"2023-03-11T13:51:21Z","published":"2023-03-05T03:18:56Z","title":"FinXABSA: Explainable Finance through Aspect-Based Sentiment Analysis","summary":"  This paper presents a novel approach for explainability in financial analysis\nby utilizing the Pearson correlation coefficient to establish a relationship\nbetween aspect-based sentiment analysis and stock prices. The proposed\nmethodology involves constructing an aspect list from financial news articles\nand analyzing sentiment intensity scores for each aspect. These scores are then\ncompared to the stock prices for the relevant companies using the Pearson\ncoefficient to determine any significant correlations. The results indicate\nthat the proposed approach provides a more detailed and accurate understanding\nof the relationship between sentiment analysis and stock prices, which can be\nuseful for investors and financial analysts in making informed decisions.\nAdditionally, this methodology offers a transparent and interpretable way to\nexplain the sentiment analysis results and their impact on stock prices.\nOverall, the findings of this paper demonstrate the importance of\nexplainability in financial analysis and highlight the potential benefits of\nutilizing the Pearson coefficient for analyzing aspect-based sentiment analysis\nand stock prices. The proposed approach offers a valuable tool for\nunderstanding the complex relationships between financial news sentiment and\nstock prices, providing a new perspective on the financial market and aiding in\nmaking informed investment decisions.\n","authors":["Keane Ong","Wihan van der Heever","Ranjan Satapathy","Gianmarco Mengaldo","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2303.02563v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.11981v2","updated":"2023-03-11T11:31:12Z","published":"2022-10-21T14:16:51Z","title":"Named Entity Detection and Injection for Direct Speech Translation","summary":"  In a sentence, certain words are critical for its semantic. Among them, named\nentities (NEs) are notoriously challenging for neural models. Despite their\nimportance, their accurate handling has been neglected in speech-to-text (S2T)\ntranslation research, and recent work has shown that S2T models perform poorly\nfor locations and notably person names, whose spelling is challenging unless\nknown in advance. In this work, we explore how to leverage dictionaries of NEs\nknown to likely appear in a given context to improve S2T model outputs. Our\nexperiments show that we can reliably detect NEs likely present in an utterance\nstarting from S2T encoder outputs. Indeed, we demonstrate that the current\ndetection quality is sufficient to improve NE accuracy in the translation with\na 31% reduction in person name errors.\n","authors":["Marco Gaido","Yun Tang","Ilia Kulikov","Rongqing Huang","Hongyu Gong","Hirofumi Inaguma"],"pdf_url":"https://arxiv.org/pdf/2210.11981v2.pdf","comment":"\\c{opyright} 2022 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2303.06333v1","updated":"2023-03-11T07:30:52Z","published":"2023-03-11T07:30:52Z","title":"Parachute: Evaluating Interactive Human-LM Co-writing Systems","summary":"  A surge of advances in language models (LMs) has led to significant interest\nin using LMs to build co-writing systems, in which humans and LMs interactively\ncontribute to a shared writing artifact. However, there is a lack of studies\nassessing co-writing systems in interactive settings. We propose a\nhuman-centered evaluation framework, Parachute, for interactive co-writing\nsystems. Parachute showcases an integrative view of interaction evaluation,\nwhere each evaluation aspect consists of categorized practical metrics.\nFurthermore, we present Parachute with a use case to demonstrate how to\nevaluate and compare co-writing systems using Parachute.\n","authors":["Hua Shen","Tongshuang Wu"],"pdf_url":"https://arxiv.org/pdf/2303.06333v1.pdf","comment":"Accepted by CHI'23 In2Writing Workshop"},{"id":"http://arxiv.org/abs/2303.06296v1","updated":"2023-03-11T03:30:47Z","published":"2023-03-11T03:30:47Z","title":"Stabilizing Transformer Training by Preventing Attention Entropy\n  Collapse","summary":"  Training stability is of great importance to Transformers. In this work, we\ninvestigate the training dynamics of Transformers by examining the evolution of\nthe attention layers. In particular, we track the attention entropy for each\nattention head during the course of training, which is a proxy for model\nsharpness. We identify a common pattern across different architectures and\ntasks, where low attention entropy is accompanied by high training instability,\nwhich can take the form of oscillating loss or divergence. We denote the\npathologically low attention entropy, corresponding to highly concentrated\nattention scores, as $\\textit{entropy collapse}$. As a remedy, we propose\n$\\sigma$Reparam, a simple and efficient solution where we reparametrize all\nlinear layers with spectral normalization and an additional learned scalar. We\ndemonstrate that the proposed reparameterization successfully prevents entropy\ncollapse in the attention layers, promoting more stable training. Additionally,\nwe prove a tight lower bound of the attention entropy, which decreases\nexponentially fast with the spectral norm of the attention logits, providing\nadditional motivation for our approach. We conduct experiments with\n$\\sigma$Reparam on image classification, image self-supervised learning,\nmachine translation, automatic speech recognition, and language modeling tasks,\nacross Transformer architectures. We show that $\\sigma$Reparam provides\nstability and robustness with respect to the choice of hyperparameters, going\nso far as enabling training (a) a Vision Transformer to competitive performance\nwithout warmup, weight decay, layer normalization or adaptive optimizers; (b)\ndeep architectures in machine translation and (c) speech recognition to\ncompetitive performance without warmup and adaptive optimizers.\n","authors":["Shuangfei Zhai","Tatiana Likhomanenko","Etai Littwin","Dan Busbridge","Jason Ramapuram","Yizhe Zhang","Jiatao Gu","Josh Susskind"],"pdf_url":"https://arxiv.org/pdf/2303.06296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.11768v2","updated":"2023-03-11T02:45:13Z","published":"2022-10-21T07:08:31Z","title":"Augmentation with Projection: Towards an Effective and Efficient Data\n  Augmentation Paradigm for Distillation","summary":"  Knowledge distillation is one of the primary methods of transferring\nknowledge from large to small models. However, it requires massive\ntask-specific data, which may not be plausible in many real-world applications.\nData augmentation methods such as representation interpolation, token\nreplacement, or augmentation with models are applied to tackle this problem.\nHowever, these data augmentation methods either potentially cause shifts in\ndecision boundaries (representation interpolation), are not expressive enough\n(token replacement), or introduce too much computational overhead (augmentation\nwith models). To this end, we propose AugPro (Augmentation with Projection), an\neffective and efficient data augmentation method for distillation. Our method\nbuilds on top of representation interpolation augmentation methods to maintain\nthe diversity of expressions and converts the augmented data to tokens to avoid\nshifting decision boundaries. It uses simple operations that come with little\ncomputational overhead. The results on multiple GLUE tasks show that our\nmethods can improve distillation performance by a large margin at a low time\ncost. Codes are available at\nhttps://github.com/google-research/google-research/tree/master/augpro.\n","authors":["Ziqi Wang","Yuexin Wu","Frederick Liu","Daogao Liu","Le Hou","Hongkun Yu","Jing Li","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2210.11768v2.pdf","comment":"20 pages, 5 figures. Accepted by ICLR 2023"},{"id":"http://arxiv.org/abs/2303.06273v1","updated":"2023-03-11T01:19:01Z","published":"2023-03-11T01:19:01Z","title":"Consistency Analysis of ChatGPT","summary":"  ChatGPT, a question-and-answer dialogue system based on a large language\nmodel, has gained huge popularity since its introduction. Its positive aspects\nhave been reported through many media platforms, and some analyses even showed\nthat ChatGPT achieved a decent grade in professional exams, including the law,\nmedical, and finance domains, adding extra support to the claim that AI now can\nassist and, even, replace humans in industrial fields. Others, however, doubt\nits reliability and trustworthiness. In this paper, we investigate ChatGPT's\ntrustworthiness regarding logically consistent behaviours. Our findings suggest\nthat, although ChatGPT seems to achieve an improved language understanding\nability, it still fails to generate logically correct predictions frequently.\nHence, while it is true that ChatGPT is an impressive and promising new\ntechnique, we conclude that its usage in real-world applications without\nthorough human inspection requires further consideration, especially for\nrisk-sensitive areas.\n","authors":["Myeongjun Jang","Thomas Lukasiewicz"],"pdf_url":"https://arxiv.org/pdf/2303.06273v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2303.06264v1","updated":"2023-03-11T01:04:25Z","published":"2023-03-11T01:04:25Z","title":"An Interactive UI to Support Sensemaking over Collections of Parallel\n  Texts","summary":"  Scientists and science journalists, among others, often need to make sense of\na large number of papers and how they compare with each other in scope, focus,\nfindings, or any other important factors. However, with a large corpus of\npapers, it's cognitively demanding to pairwise compare and contrast them all\nwith each other. Fully automating this review process would be infeasible,\nbecause it often requires domain-specific knowledge, as well as understanding\nwhat the context and motivations for the review are. While there are existing\ntools to help with the process of organizing and annotating papers for\nliterature reviews, at the core they still rely on people to serially read\nthrough papers and manually make sense of relevant information.\n  We present AVTALER, which combines peoples' unique skills, contextual\nawareness, and knowledge, together with the strength of automation. Given a set\nof comparable text excerpts from a paper corpus, it supports users in\nsensemaking and contrasting paper attributes by interactively aligning text\nexcerpts in a table so that comparable details are presented in a shared\ncolumn. AVTALER is based on a core alignment algorithm that makes use of modern\nNLP tools. Furthermore, AVTALER is a mixed-initiative system: users can\ninteractively give the system constraints which are integrated into the\nalignment construction process.\n","authors":["Joyce Zhou","Elena Glassman","Daniel S. Weld"],"pdf_url":"https://arxiv.org/pdf/2303.06264v1.pdf","comment":"13 pages, 12 figures"},{"id":"http://arxiv.org/abs/2210.01343v3","updated":"2023-03-11T00:11:03Z","published":"2022-10-04T03:18:19Z","title":"The Surprising Computational Power of Nondeterministic Stack RNNs","summary":"  Traditional recurrent neural networks (RNNs) have a fixed, finite number of\nmemory cells. In theory (assuming bounded range and precision), this limits\ntheir formal language recognition power to regular languages, and in practice,\nRNNs have been shown to be unable to learn many context-free languages (CFLs).\nIn order to expand the class of languages RNNs recognize, prior work has\naugmented RNNs with a nondeterministic stack data structure, putting them on\npar with pushdown automata and increasing their language recognition power to\nCFLs. Nondeterminism is needed for recognizing all CFLs (not just deterministic\nCFLs), but in this paper, we show that nondeterminism and the neural controller\ninteract to produce two more unexpected abilities. First, the nondeterministic\nstack RNN can recognize not only CFLs, but also many non-context-free\nlanguages. Second, it can recognize languages with much larger alphabet sizes\nthan one might expect given the size of its stack alphabet. Finally, to\nincrease the information capacity in the stack and allow it to solve more\ncomplicated tasks with large alphabet sizes, we propose a new version of the\nnondeterministic stack that simulates stacks of vectors rather than discrete\nsymbols. We demonstrate perplexity improvements with this new model on the Penn\nTreebank language modeling benchmark.\n","authors":["Brian DuSell","David Chiang"],"pdf_url":"https://arxiv.org/pdf/2210.01343v3.pdf","comment":"21 pages, 8 figures. Published at ICLR 2023"}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.02353v2","updated":"2023-03-11T19:53:30Z","published":"2023-03-04T08:33:46Z","title":"Self-Asymmetric Invertible Network for Compression-Aware Image Rescaling","summary":"  High-resolution (HR) images are usually downscaled to low-resolution (LR)\nones for better display and afterward upscaled back to the original size to\nrecover details. Recent work in image rescaling formulates downscaling and\nupscaling as a unified task and learns a bijective mapping between HR and LR\nvia invertible networks. However, in real-world applications (e.g., social\nmedia), most images are compressed for transmission. Lossy compression will\nlead to irreversible information loss on LR images, hence damaging the inverse\nupscaling procedure and degrading the reconstruction accuracy. In this paper,\nwe propose the Self-Asymmetric Invertible Network (SAIN) for compression-aware\nimage rescaling. To tackle the distribution shift, we first develop an\nend-to-end asymmetric framework with two separate bijective mappings for\nhigh-quality and compressed LR images, respectively. Then, based on empirical\nanalysis of this framework, we model the distribution of the lost information\n(including downscaling and compression) using isotropic Gaussian mixtures and\npropose the Enhanced Invertible Block to derive high-quality/compressed LR\nimages in one forward pass. Besides, we design a set of losses to regularize\nthe learned LR images and enhance the invertibility. Extensive experiments\ndemonstrate the consistent improvements of SAIN across various image rescaling\ndatasets in terms of both quantitative and qualitative evaluation under\nstandard image compression formats (i.e., JPEG and WebP).\n","authors":["Jinhai Yang","Mengxi Guo","Shijie Zhao","Junlin Li","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.02353v2.pdf","comment":"Accepted by AAAI 2023. Code is available at\n  https://github.com/yang-jin-hai/SAIN"},{"id":"http://arxiv.org/abs/2303.05338v2","updated":"2023-03-11T16:11:24Z","published":"2023-03-09T15:34:36Z","title":"MMCosine: Multi-Modal Cosine Loss Towards Balanced Audio-Visual\n  Fine-Grained Learning","summary":"  Audio-visual learning helps to comprehensively understand the world by fusing\npractical information from multiple modalities. However, recent studies show\nthat the imbalanced optimization of uni-modal encoders in a joint-learning\nmodel is a bottleneck to enhancing the model's performance. We further find\nthat the up-to-date imbalance-mitigating methods fail on some audio-visual\nfine-grained tasks, which have a higher demand for distinguishable feature\ndistribution. Fueled by the success of cosine loss that builds hyperspherical\nfeature spaces and achieves lower intra-class angular variability, this paper\nproposes Multi-Modal Cosine loss, MMCosine. It performs a modality-wise $L_2$\nnormalization to features and weights towards balanced and better multi-modal\nfine-grained learning. We demonstrate that our method can alleviate the\nimbalanced optimization from the perspective of weight norm and fully exploit\nthe discriminability of the cosine metric. Extensive experiments prove the\neffectiveness of our method and the versatility with advanced multi-modal\nfusion strategies and up-to-date imbalance-mitigating methods.\n","authors":["Ruize Xu","Ruoxuan Feng","Shi-Xiong Zhang","Di Hu"],"pdf_url":"https://arxiv.org/pdf/2303.05338v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.04216v4","updated":"2023-03-11T14:49:56Z","published":"2022-10-09T10:10:13Z","title":"AMPose: Alternatively Mixed Global-Local Attention Model for 3D Human\n  Pose Estimation","summary":"  The graph convolutional networks (GCNs) have been applied to model the\nphysically connected and non-local relations among human joints for 3D human\npose estimation (HPE). In addition, the purely Transformer-based models\nrecently show promising results in video-based 3D HPE. However, the\nsingle-frame method still needs to model the physically connected relations\namong joints because the feature representations transformed only by global\nrelations via the Transformer neglect information on the human skeleton. To\ndeal with this problem, we propose a novel method in which the Transformer\nencoder and GCN blocks are alternately stacked, namely AMPose, to combine the\nglobal and physically connected relations among joints towards HPE. In the\nAMPose, the Transformer encoder is applied to connect each joint with all the\nother joints, while GCNs are applied to capture information on physically\nconnected relations. The effectiveness of our proposed method is evaluated on\nthe Human3.6M dataset. Our model also shows better generalization ability by\ntesting on the MPI-INF-3DHP dataset. Code can be retrieved at\nhttps://github.com/erikervalid/AMPose.\n","authors":["Hongxin Lin","Yunwei Chiu","Peiyuan Wu"],"pdf_url":"https://arxiv.org/pdf/2210.04216v4.pdf","comment":"ICASSP 2023 Accepted Paper"},{"id":"http://arxiv.org/abs/2303.03857v2","updated":"2023-03-11T09:22:02Z","published":"2023-03-07T12:49:45Z","title":"Leveraging Pre-trained AudioLDM for Text to Sound Generation: A\n  Benchmark Study","summary":"  Deep neural networks have recently achieved breakthroughs in sound generation\nwith text prompts. Despite their promising performance, current text-to-sound\ngeneration models face issues on small-scale datasets (e.g., overfitting),\nsignificantly limiting their performance. In this paper, we investigate the use\nof pre-trained AudioLDM, the state-of-the-art model for text-to-audio\ngeneration, as the backbone for sound generation. Our study demonstrates the\nadvantages of using pre-trained models for text-to-sound generation, especially\nin data-scarcity scenarios. In addition, experiments show that different\ntraining strategies (e.g., training conditions) may affect the performance of\nAudioLDM on datasets of different scales. To facilitate future studies, we also\nevaluate various text-to-sound generation systems on several frequently used\ndatasets under the same evaluation protocols, which allow fair comparisons and\nbenchmarking of these methods on the common ground.\n","authors":["Yi Yuan","Haohe Liu","Jinhua Liang","Xubo Liu","Mark D. Plumbley","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2303.03857v2.pdf","comment":"EUSIPCO 2023"},{"id":"http://arxiv.org/abs/2301.09799v2","updated":"2023-03-11T08:29:51Z","published":"2023-01-24T03:47:37Z","title":"LDMIC: Learning-based Distributed Multi-view Image Coding","summary":"  Multi-view image compression plays a critical role in 3D-related\napplications. Existing methods adopt a predictive coding architecture, which\nrequires joint encoding to compress the corresponding disparity as well as\nresidual information. This demands collaboration among cameras and enforces the\nepipolar geometric constraint between different views, which makes it\nchallenging to deploy these methods in distributed camera systems with randomly\noverlapping fields of view. Meanwhile, distributed source coding theory\nindicates that efficient data compression of correlated sources can be achieved\nby independent encoding and joint decoding, which motivates us to design a\nlearning-based distributed multi-view image coding (LDMIC) framework. With\nindependent encoders, LDMIC introduces a simple yet effective joint context\ntransfer module based on the cross-attention mechanism at the decoder to\neffectively capture the global inter-view correlations, which is insensitive to\nthe geometric relationships between images. Experimental results show that\nLDMIC significantly outperforms both traditional and learning-based MIC methods\nwhile enjoying fast encoding speed. Code will be released at\nhttps://github.com/Xinjie-Q/LDMIC.\n","authors":["Xinjie Zhang","Jiawei Shao","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2301.09799v2.pdf","comment":"Accepted by ICLR 2023"},{"id":"http://arxiv.org/abs/2303.06326v1","updated":"2023-03-11T06:56:10Z","published":"2023-03-11T06:56:10Z","title":"The Multimodal Information based Speech Processing (MISP) 2022\n  Challenge: Audio-Visual Diarization and Recognition","summary":"  The Multi-modal Information based Speech Processing (MISP) challenge aims to\nextend the application of signal processing technology in specific scenarios by\npromoting the research into wake-up words, speaker diarization, speech\nrecognition, and other technologies. The MISP2022 challenge has two tracks: 1)\naudio-visual speaker diarization (AVSD), aiming to solve ``who spoken when''\nusing both audio and visual data; 2) a novel audio-visual diarization and\nrecognition (AVDR) task that focuses on addressing ``who spoken what when''\nwith audio-visual speaker diarization results. Both tracks focus on the Chinese\nlanguage, and use far-field audio and video in real home-tv scenarios: 2-6\npeople communicating each other with TV noise in the background. This paper\nintroduces the dataset, track settings, and baselines of the MISP2022\nchallenge. Our analyses of experiments and examples indicate the good\nperformance of AVDR baseline system, and the potential difficulties in this\nchallenge due to, e.g., the far-field video quality, the presence of TV noise\nin the background, and the indistinguishable speakers.\n","authors":["Zhe Wang","Shilong Wu","Hang Chen","Mao-Kui He","Jun Du","Chin-Hui Lee","Jingdong Chen","Shinji Watanabe","Sabato Siniscalchi","Odette Scharenborg","Diyuan Liu","Baocai Yin","Jia Pan","Jianqing Gao","Cong Liu"],"pdf_url":"https://arxiv.org/pdf/2303.06326v1.pdf","comment":"5 pages, 4 figures, to be published in ICASSP2023"}]},"2023-03-10T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2302.08351v2","updated":"2023-03-10T20:39:14Z","published":"2023-02-16T15:11:53Z","title":"A Survey on Event-based News Narrative Extraction","summary":"  Narratives are fundamental to our understanding of the world, providing us\nwith a natural structure for knowledge representation over time. Computational\nnarrative extraction is a subfield of artificial intelligence that makes heavy\nuse of information retrieval and natural language processing techniques.\nDespite the importance of computational narrative extraction, relatively little\nscholarly work exists on synthesizing previous research and strategizing future\nresearch in the area. In particular, this article focuses on extracting news\nnarratives from an event-centric perspective. Extracting narratives from news\ndata has multiple applications in understanding the evolving information\nlandscape. This survey presents an extensive study of research in the area of\nevent-based news narrative extraction. In particular, we screened over 900\narticles that yielded 54 relevant articles. These articles are synthesized and\norganized by representation model, extraction criteria, and evaluation\napproaches. Based on the reviewed studies, we identify recent trends, open\nchallenges, and potential research lines.\n","authors":["Brian Keith Norambuena","Tanushree Mitra","Chris North"],"pdf_url":"https://arxiv.org/pdf/2302.08351v2.pdf","comment":"37 pages, 3 figures, to be published in the journal ACM CSUR"},{"id":"http://arxiv.org/abs/2303.06095v1","updated":"2023-03-10T17:24:41Z","published":"2023-03-10T17:24:41Z","title":"HiNet: A Novel Multi-Scenario & Multi-Task Learning Approach with\n  Hierarchical Information Extraction","summary":"  Multi-scenario & multi-task learning has been widely applied to many\nrecommendation systems in industrial applications, wherein an effective and\npractical approach is to carry out multi-scenario transfer learning on the\nbasis of the Mixture-of-Expert (MoE) architecture. However, the MoE-based\nmethod, which aims to project all information in the same feature space, cannot\neffectively deal with the complex relationships inherent among various\nscenarios and tasks, resulting in unsatisfactory performance. To tackle the\nproblem, we propose a Hierarchical information extraction Network (HiNet) for\nmulti-scenario and multi-task recommendation, which achieves hierarchical\nextraction based on coarse-to-fine knowledge transfer scheme. The multiple\nextraction layers of the hierarchical network enable the model to enhance the\ncapability of transferring valuable information across scenarios while\npreserving specific features of scenarios and tasks. Furthermore, a novel\nscenario-aware attentive network module is proposed to model correlations\nbetween scenarios explicitly. Comprehensive experiments conducted on real-world\nindustrial datasets from Meituan Meishi platform demonstrate that HiNet\nachieves a new state-of-the-art performance and significantly outperforms\nexisting solutions. HiNet is currently fully deployed in two scenarios and has\nachieved 2.87% and 1.75% order quantity gain respectively.\n","authors":["Jie Zhou","Xianshuai Cao","Wenhao Li","Kun Zhang","Chuan Luo","Qian Yu"],"pdf_url":"https://arxiv.org/pdf/2303.06095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05847v1","updated":"2023-03-10T10:42:21Z","published":"2023-03-10T10:42:21Z","title":"Gradient Coordination for Quantifying and Maximizing Knowledge\n  Transference in Multi-Task Learning","summary":"  Multi-task learning (MTL) has been widely applied in online advertising and\nrecommender systems. To address the negative transfer issue, recent studies\nhave proposed optimization methods that thoroughly focus on the gradient\nalignment of directions or magnitudes. However, since prior study has proven\nthat both general and specific knowledge exist in the limited shared capacity,\noveremphasizing on gradient alignment may crowd out task-specific knowledge,\nand vice versa. In this paper, we propose a transference-driven approach CoGrad\nthat adaptively maximizes knowledge transference via Coordinated Gradient\nmodification. We explicitly quantify the transference as loss reduction from\none task to another, and then derive an auxiliary gradient from optimizing it.\nWe perform the optimization by incorporating this gradient into original task\ngradients, making the model automatically maximize inter-task transfer and\nminimize individual losses. Thus, CoGrad can harmonize between general and\nspecific knowledge to boost overall performance. Besides, we introduce an\nefficient approximation of the Hessian matrix, making CoGrad computationally\nefficient and simple to implement. Both offline and online experiments verify\nthat CoGrad significantly outperforms previous methods.\n","authors":["Xuanhua Yang","Jianxin Zhao","Shaoguo Liu","Liang Wang","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2303.05847v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.05812v1","updated":"2023-03-10T09:39:18Z","published":"2023-03-10T09:39:18Z","title":"Semi-supervised Adversarial Learning for Complementary Item\n  Recommendation","summary":"  Complementary item recommendations are a ubiquitous feature of modern\ne-commerce sites. Such recommendations are highly effective when they are based\non collaborative signals like co-purchase statistics. In certain online\nmarketplaces, however, e.g., on online auction sites, constantly new items are\nadded to the catalog. In such cases, complementary item recommendations are\noften based on item side-information due to a lack of interaction data. In this\nwork, we propose a novel approach that can leverage both item side-information\nand labeled complementary item pairs to generate effective complementary\nrecommendations for cold items, i.e., for items for which no co-purchase\nstatistics yet exist. Given that complementary items typically have to be of a\ndifferent category than the seed item, we technically maintain a latent space\nfor each item category. Simultaneously, we learn to project distributed item\nrepresentations into these category spaces to determine suitable\nrecommendations. The main learning process in our architecture utilizes labeled\npairs of complementary items. In addition, we adopt ideas from Cycle Generative\nAdversarial Networks (CycleGAN) to leverage available item information even in\ncase no labeled data exists for a given item and category. Experiments on three\ne-commerce datasets show that our method is highly effective.\n","authors":["Koby Bibas","Oren Sar Shalom","Dietmar Jannach"],"pdf_url":"https://arxiv.org/pdf/2303.05812v1.pdf","comment":"ACM Web Conference 2023"},{"id":"http://arxiv.org/abs/2302.03328v2","updated":"2023-03-10T03:36:18Z","published":"2023-02-07T09:11:17Z","title":"Multi-Task Recommendations with Reinforcement Learning","summary":"  In recent years, Multi-task Learning (MTL) has yielded immense success in\nRecommender System (RS) applications. However, current MTL-based recommendation\nmodels tend to disregard the session-wise patterns of user-item interactions\nbecause they are predominantly constructed based on item-wise datasets.\nMoreover, balancing multiple objectives has always been a challenge in this\nfield, which is typically avoided via linear estimations in existing works. To\naddress these issues, in this paper, we propose a Reinforcement Learning (RL)\nenhanced MTL framework, namely RMTL, to combine the losses of different\nrecommendation tasks using dynamic weights. To be specific, the RMTL structure\ncan address the two aforementioned issues by (i) constructing an MTL\nenvironment from session-wise interactions and (ii) training multi-task\nactor-critic network structure, which is compatible with most existing\nMTL-based recommendation models, and (iii) optimizing and fine-tuning the MTL\nloss function using the weights generated by critic networks. Experiments on\ntwo real-world public datasets demonstrate the effectiveness of RMTL with a\nhigher AUC against state-of-the-art MTL-based recommendation models.\nAdditionally, we evaluate and validate RMTL's compatibility and transferability\nacross various MTL models.\n","authors":["Ziru Liu","Jiejie Tian","Qingpeng Cai","Xiangyu Zhao","Jingtong Gao","Shuchang Liu","Dayou Chen","Tonghao He","Dong Zheng","Peng Jiang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2302.03328v2.pdf","comment":"TheWebConf2023"},{"id":"http://arxiv.org/abs/2303.05648v1","updated":"2023-03-10T01:49:56Z","published":"2023-03-10T01:49:56Z","title":"Pacos: Modeling Users' Interpretable and Context-Dependent Choices in\n  Preference Reversals","summary":"  Choice problems refer to selecting the best choices from several items, and\nlearning users' preferences in choice problems is of great significance in\nunderstanding the decision making mechanisms and providing personalized\nservices. Existing works typically assume that people evaluate items\nindependently. In practice, however, users' preferences depend on the market in\nwhich items are placed, which is known as context effects; and the order of\nusers' preferences for two items may even be reversed, which is referred to\npreference reversals. In this work, we identify three factors contributing to\ncontext effects: users' adaptive weights, the inter-item comparison, and\ndisplay positions. We propose a context-dependent preference model named Pacos\nas a unified framework for addressing three factors simultaneously, and\nconsider two design methods including an additive method with high\ninterpretability and an ANN-based method with high accuracy. We study the\nconditions for preference reversals to occur and provide an theoretical proof\nof the effectiveness of Pacos in addressing preference reversals. Experimental\nresults show that the proposed method has better performance than prior works\nin predicting users' choices, and has great interpretability to help understand\nthe cause of preference reversals.\n","authors":["Qingming Li","H. Vicky Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.05648v1.pdf","comment":"29 pages, 12 figures"},{"id":"http://arxiv.org/abs/2303.04392v2","updated":"2023-03-10T01:31:11Z","published":"2023-03-08T05:53:33Z","title":"Achievable Rates and Low-Complexity Encoding of Posterior Matching for\n  the BSC","summary":"  Horstein, Burnashev, Shayevitz and Feder, Naghshvar et al. and others have\nstudied sequential transmission of a K-bit message over the binary symmetric\nchannel (BSC) with full, noiseless feedback using posterior matching. Yang et\nal. provide an improved lower bound on the achievable rate using martingale\nanalysis that relies on the small-enough difference (SED) partitioning\nintroduced by Naghshvar et al. SED requires a relatively complex encoder and\ndecoder. To reduce complexity, this paper replaces SED with relaxed constraints\nthat admit the small enough absolute difference (SEAD) partitioning rule. The\nmain analytical results show that achievable-rate bounds higher than those\nfound by Yang et al. are possible even under the new constraints, which are\nless restrictive than SED. The new analysis does not use martingale theory for\nthe confirmation phase and applies a surrogate channel technique to tighten the\nresults. An initial systematic transmission further increases the achievable\nrate bound. The simplified encoder associated with SEAD has a complexity below\norder O(K^2) and allows simulations for message sizes of at least 1000 bits.\nFor example, simulations achieve 99% of of the channel's 0.50-bit capacity with\nan average block size of 200 bits for a target codeword error rate of 10^(-3).\n","authors":["Amaael Antonini","Rita Gimelshein","Richard Wesel"],"pdf_url":"https://arxiv.org/pdf/2303.04392v2.pdf","comment":"This paper consists of 26 pages and contains 6 figures. An earlier\n  version of the algorithm included in this paper was published at the 2020\n  IEEE International Symposium on Information Theory (ISIT), (DOI:\n  10.1109/ISIT44484.2020.9174232)"}],"Computation and Language":[{"id":"http://arxiv.org/abs/2303.06245v1","updated":"2023-03-10T23:34:14Z","published":"2023-03-10T23:34:14Z","title":"AUTODIAL: Efficient Asynchronous Task-Oriented Dialogue Model","summary":"  As large dialogue models become commonplace in practice, the problems\nsurrounding high compute requirements for training, inference and larger memory\nfootprint still persists. In this work, we present AUTODIAL, a multi-task\ndialogue model that addresses the challenges of deploying dialogue model.\nAUTODIAL utilizes parallel decoders to perform tasks such as dialogue act\nprediction, domain prediction, intent prediction, and dialogue state tracking.\nUsing classification decoders over generative decoders allows AUTODIAL to\nsignificantly reduce memory footprint and achieve faster inference times\ncompared to existing generative approach namely SimpleTOD. We demonstrate that\nAUTODIAL provides 3-6x speedups during inference while having 11x fewer\nparameters on three dialogue tasks compared to SimpleTOD. Our results show that\nextending current dialogue models to have parallel decoders can be a viable\nalternative for deploying them in resource-constrained environments.\n","authors":["Prajjwal Bhargava","Pooyan Amini","Shahin Shayandeh","Chinnadhurai Sankar"],"pdf_url":"https://arxiv.org/pdf/2303.06245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06230v1","updated":"2023-03-10T22:40:15Z","published":"2023-03-10T22:40:15Z","title":"Generating Query Focused Summaries without Fine-tuning the\n  Transformer-based Pre-trained Models","summary":"  Fine-tuning the Natural Language Processing (NLP) models for each new data\nset requires higher computational time associated with increased carbon\nfootprint and cost. However, fine-tuning helps the pre-trained models adapt to\nthe latest data sets; what if we avoid the fine-tuning steps and attempt to\ngenerate summaries using just the pre-trained models to reduce computational\ntime and cost. In this paper, we tried to omit the fine-tuning steps and\ninvestigate whether the Marginal Maximum Relevance (MMR)-based approach can\nhelp the pre-trained models to obtain query-focused summaries directly from a\nnew data set that was not used to pre-train the models. First, we used topic\nmodelling on Wikipedia Current Events Portal (WCEP) and Debatepedia datasets to\ngenerate queries for summarization tasks. Then, using MMR, we ranked the\nsentences of the documents according to the queries. Next, we passed the ranked\nsentences to seven transformer-based pre-trained models to perform the\nsummarization tasks. Finally, we used the MMR approach again to select the\nquery relevant sentences from the generated summaries of individual pre-trained\nmodels and constructed the final summary. As indicated by the experimental\nresults, our MMR-based approach successfully ranked and selected the most\nrelevant sentences as summaries and showed better performance than the\nindividual pre-trained models.\n","authors":["Deen Abdullah","Shamanth Nayak","Gandharv Suri","Yllias Chali"],"pdf_url":"https://arxiv.org/pdf/2303.06230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.08351v2","updated":"2023-03-10T20:39:14Z","published":"2023-02-16T15:11:53Z","title":"A Survey on Event-based News Narrative Extraction","summary":"  Narratives are fundamental to our understanding of the world, providing us\nwith a natural structure for knowledge representation over time. Computational\nnarrative extraction is a subfield of artificial intelligence that makes heavy\nuse of information retrieval and natural language processing techniques.\nDespite the importance of computational narrative extraction, relatively little\nscholarly work exists on synthesizing previous research and strategizing future\nresearch in the area. In particular, this article focuses on extracting news\nnarratives from an event-centric perspective. Extracting narratives from news\ndata has multiple applications in understanding the evolving information\nlandscape. This survey presents an extensive study of research in the area of\nevent-based news narrative extraction. In particular, we screened over 900\narticles that yielded 54 relevant articles. These articles are synthesized and\norganized by representation model, extraction criteria, and evaluation\napproaches. Based on the reviewed studies, we identify recent trends, open\nchallenges, and potential research lines.\n","authors":["Brian Keith Norambuena","Tanushree Mitra","Chris North"],"pdf_url":"https://arxiv.org/pdf/2302.08351v2.pdf","comment":"37 pages, 3 figures, to be published in the journal ACM CSUR"},{"id":"http://arxiv.org/abs/2303.05077v2","updated":"2023-03-10T19:54:39Z","published":"2023-03-09T07:22:07Z","title":"Learning the Legibility of Visual Text Perturbations","summary":"  Many adversarial attacks in NLP perturb inputs to produce visually similar\nstrings ('ergo' $\\rightarrow$ '$\\epsilon$rgo') which are legible to humans but\ndegrade model performance. Although preserving legibility is a necessary\ncondition for text perturbation, little work has been done to systematically\ncharacterize it; instead, legibility is typically loosely enforced via\nintuitions around the nature and extent of perturbations. Particularly, it is\nunclear to what extent can inputs be perturbed while preserving legibility, or\nhow to quantify the legibility of a perturbed string. In this work, we address\nthis gap by learning models that predict the legibility of a perturbed string,\nand rank candidate perturbations based on their legibility. To do so, we\ncollect and release LEGIT, a human-annotated dataset comprising the legibility\nof visually perturbed text. Using this dataset, we build both text- and\nvision-based models which achieve up to $0.91$ F1 score in predicting whether\nan input is legible, and an accuracy of $0.86$ in predicting which of two given\nperturbations is more legible. Additionally, we discover that legible\nperturbations from the LEGIT dataset are more effective at lowering the\nperformance of NLP models than best-known attack strategies, suggesting that\ncurrent models may be vulnerable to a broad range of perturbations beyond what\nis captured by existing visual attacks. Data, code, and models are available at\nhttps://github.com/dvsth/learning-legibility-2023.\n","authors":["Dev Seth","Rickard Stureborg","Danish Pruthi","Bhuwan Dhingra"],"pdf_url":"https://arxiv.org/pdf/2303.05077v2.pdf","comment":"14 pages, 7 figures. Accepted at EACL 2023 (main, long)"},{"id":"http://arxiv.org/abs/2303.06182v1","updated":"2023-03-10T19:30:15Z","published":"2023-03-10T19:30:15Z","title":"Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert\n  (MoE) Inference","summary":"  Mixture-of-Experts (MoE) models have recently gained steam in achieving the\nstate-of-the-art performance in a wide range of tasks in computer vision and\nnatural language processing. They effectively expand the model capacity while\nincurring a minimal increase in computation cost during training. However,\ndeploying such models for inference is difficult due to their large model size\nand complex communication pattern. In this work, we provide a characterization\nof two MoE workloads, namely Language Modeling (LM) and Machine Translation\n(MT) and identify their sources of inefficiencies at deployment.\n  We propose three optimization techniques to mitigate sources of\ninefficiencies, namely (1) Dynamic gating, (2) Expert Buffering, and (3) Expert\nload balancing. We show that dynamic gating improves execution time by\n1.25-4$\\times$ for LM, 2-5$\\times$ for MT Encoder and 1.09-1.5$\\times$ for MT\nDecoder. It also reduces memory usage by up to 1.36$\\times$ for LM and up to\n1.1$\\times$ for MT. We further propose Expert Buffering, a new caching\nmechanism that only keeps hot, active experts in GPU memory while buffering the\nrest in CPU memory. This reduces static memory allocation by 1.47$\\times$. We\nfinally propose a load balancing methodology that provides additional\nrobustness to the workload. The code will be open-sourced upon acceptance.\n","authors":["Haiyang Huang","Newsha Ardalani","Anna Sun","Liu Ke","Hsien-Hsin S. Lee","Anjali Sridhar","Shruti Bhosale","Carole-Jean Wu","Benjamin Lee"],"pdf_url":"https://arxiv.org/pdf/2303.06182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.13290v4","updated":"2023-03-10T18:59:02Z","published":"2021-07-28T11:34:00Z","title":"Arabic aspect sentiment polarity classification using BERT","summary":"  Aspect-based sentiment analysis(ABSA) is a textual analysis methodology that\ndefines the polarity of opinions on certain aspects related to specific\ntargets. The majority of research on ABSA is in English, with a small amount of\nwork available in Arabic. Most previous Arabic research has relied on deep\nlearning models that depend primarily on context-independent word embeddings\n(e.g.word2vec), where each word has a fixed representation independent of its\ncontext. This article explores the modeling capabilities of contextual\nembeddings from pre-trained language models, such as BERT, and making use of\nsentence pair input on Arabic aspect sentiment polarity classification task. In\nparticular, we develop a simple but effective BERT-based neural baseline to\nhandle this task. Our BERT architecture with a simple linear classification\nlayer surpassed the state-of-the-art works, according to the experimental\nresults on three different Arabic datasets. Achieving an accuracy of 89.51% on\nthe Arabic hotel reviews dataset, 73% on the Human annotated book reviews\ndataset, and 85.73% on the Arabic news dataset.\n","authors":["Mohammed M. Abdelgwad","Taysir Hassan A Soliman","Ahmed I. Taloba"],"pdf_url":"https://arxiv.org/pdf/2107.13290v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06135v1","updated":"2023-03-10T18:53:52Z","published":"2023-03-10T18:53:52Z","title":"Rewarding Chatbots for Real-World Engagement with Millions of Users","summary":"  The emergence of pretrained large language models has led to the deployment\nof a range of social chatbots for chitchat. Although these chatbots demonstrate\nlanguage ability and fluency, they are not guaranteed to be engaging and can\nstruggle to retain users. This work investigates the development of social\nchatbots that prioritize user engagement to enhance retention, specifically\nexamining the use of human feedback to efficiently develop highly engaging\nchatbots. The proposed approach uses automatic pseudo-labels collected from\nuser interactions to train a reward model that can be used to reject\nlow-scoring sample responses generated by the chatbot model at inference time.\nIntuitive evaluation metrics, such as mean conversation length (MCL), are\nintroduced as proxies to measure the level of engagement of deployed chatbots.\nA/B testing on groups of 10,000 new daily chatbot users on the Chai Research\nplatform shows that this approach increases the MCL by up to 70%, which\ntranslates to a more than 30% increase in user retention for a GPT-J 6B model.\nFuture work aims to use the reward model to realise a data fly-wheel, where the\nlatest user conversations can be used to alternately fine-tune the language\nmodel and the reward model.\n","authors":["Robert Irvine","Douglas Boubert","Vyas Raina","Adian Liusie","Vineet Mudupalli","Aliaksei Korshuk","Zongyi Liu","Fritz Cremer","Valentin Assassi","Christie-Carol Beauchamp","Xiaoding Lu","Thomas Rialan","William Beauchamp"],"pdf_url":"https://arxiv.org/pdf/2303.06135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14228v4","updated":"2023-03-10T17:24:26Z","published":"2022-11-25T16:41:59Z","title":"GPT-3-driven pedagogical agents for training children's curious\n  question-asking skills","summary":"  In order to train children's ability to ask curiosity-driven questions,\nprevious research has explored designing specific exercises relying on\nproviding semantic and linguistic cues to help formulate such questions. But\ndespite showing pedagogical efficiency, this method is still limited as it\nrelies on generating the said cues by hand, which can be a very costly process.\nIn this context, we propose to leverage advances in the natural language\nprocessing field (NLP) and investigate the efficiency of using a large language\nmodel (LLM) for automating the production of the pedagogical content of a\ncurious question-asking (QA) training. We study generating the said content\nusing the \"prompt-based\" method that consists of explaining the task to the LLM\nin natural text. We evaluate the output using human experts annotations and\ncomparisons with hand-generated content. Results suggested indeed the relevance\nand usefulness of this content. We also conduct a field study in primary school\n(75 children aged 9-10), where we evaluate children's QA performance when\nhaving this training. We compare 3 types of content : 1) hand-generated content\nthat proposes \"closed\" cues leading to predefined questions; 2) GPT-3-generated\ncontent that proposes the same type of cues; 3) GPT-3-generated content that\nproposes \"open\" cues leading to several possible questions. We see a similar QA\nperformance between the two \"closed\" trainings (showing the scalability of the\napproach using GPT-3), and a better one for participants with the \"open\"\ntraining. These results suggest the efficiency of using LLMs to support\nchildren in generating more curious questions, using a natural language\nprompting approach that affords usability by teachers and other users not\nspecialists of AI techniques. Furthermore, results also show that open-ended\ncontent may be more suitable for training curious question-asking skills.\n","authors":["Rania Abdelghani","Yen-Hsiang Wang","Xingdi Yuan","Tong Wang","Pauline Lucas","Hélène Sauzéon","Pierre-Yves Oudeyer"],"pdf_url":"https://arxiv.org/pdf/2211.14228v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.01910v2","updated":"2023-03-10T17:20:17Z","published":"2022-11-03T15:43:03Z","title":"Large Language Models Are Human-Level Prompt Engineers","summary":"  By conditioning on natural language instructions, large language models\n(LLMs) have displayed impressive capabilities as general-purpose computers.\nHowever, task performance depends significantly on the quality of the prompt\nused to steer the model, and most effective prompts have been handcrafted by\nhumans. Inspired by classical program synthesis and the human approach to\nprompt engineering, we propose Automatic Prompt Engineer (APE) for automatic\ninstruction generation and selection. In our method, we treat the instruction\nas the \"program,\" optimized by searching over a pool of instruction candidates\nproposed by an LLM in order to maximize a chosen score function. To evaluate\nthe quality of the selected instruction, we evaluate the zero-shot performance\nof another LLM following the selected instruction. Experiments on 24 NLP tasks\nshow that our automatically generated instructions outperform the prior LLM\nbaseline by a large margin and achieve better or comparable performance to the\ninstructions generated by human annotators on 19/24 tasks. We conduct extensive\nqualitative and quantitative analyses to explore the performance of APE. We\nshow that APE-engineered prompts can be applied to steer models toward\ntruthfulness and/or informativeness, as well as to improve few-shot learning\nperformance by simply prepending them to standard in-context learning prompts.\nPlease check out our webpage at\nhttps://sites.google.com/view/automatic-prompt-engineer.\n","authors":["Yongchao Zhou","Andrei Ioan Muresanu","Ziwen Han","Keiran Paster","Silviu Pitis","Harris Chan","Jimmy Ba"],"pdf_url":"https://arxiv.org/pdf/2211.01910v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06074v1","updated":"2023-03-10T16:53:30Z","published":"2023-03-10T16:53:30Z","title":"Susceptibility to Influence of Large Language Models","summary":"  Two studies tested the hypothesis that a Large Language Model (LLM) can be\nused to model psychological change following exposure to influential input. The\nfirst study tested a generic mode of influence - the Illusory Truth Effect\n(ITE) - where earlier exposure to a statement (through, for example, rating its\ninterest) boosts a later truthfulness test rating. Data was collected from 1000\nhuman participants using an online experiment, and 1000 simulated participants\nusing engineered prompts and LLM completion. 64 ratings per participant were\ncollected, using all exposure-test combinations of the attributes: truth,\ninterest, sentiment and importance. The results for human participants\nreconfirmed the ITE, and demonstrated an absence of effect for attributes other\nthan truth, and when the same attribute is used for exposure and test. The same\npattern of effects was found for LLM-simulated participants. The second study\nconcerns a specific mode of influence - populist framing of news to increase\nits persuasion and political mobilization. Data from LLM-simulated participants\nwas collected and compared to previously published data from a 15-country\nexperiment on 7286 human participants. Several effects previously demonstrated\nfrom the human study were replicated by the simulated study, including effects\nthat surprised the authors of the human study by contradicting their\ntheoretical expectations (anti-immigrant framing of news decreases its\npersuasion and mobilization); but some significant relationships found in human\ndata (modulation of the effectiveness of populist framing according to relative\ndeprivation of the participant) were not present in the LLM data. Together the\ntwo studies support the view that LLMs have potential to act as models of the\neffect of influence.\n","authors":["Lewis D Griffin","Bennett Kleinberg","Maximilian Mozes","Kimberly T Mai","Maria Vau","Matthew Caldwell","Augustine Marvor-Parker"],"pdf_url":"https://arxiv.org/pdf/2303.06074v1.pdf","comment":"24 pages, 6 figures, 7 tables, 53 references"},{"id":"http://arxiv.org/abs/2303.06002v1","updated":"2023-03-10T16:03:19Z","published":"2023-03-10T16:03:19Z","title":"Is In-hospital Meta-information Useful for Abstractive Discharge Summary\n  Generation?","summary":"  During the patient's hospitalization, the physician must record daily\nobservations of the patient and summarize them into a brief document called\n\"discharge summary\" when the patient is discharged. Automated generation of\ndischarge summary can greatly relieve the physicians' burden, and has been\naddressed recently in the research community. Most previous studies of\ndischarge summary generation using the sequence-to-sequence architecture focus\non only inpatient notes for input. However, electric health records (EHR) also\nhave rich structured metadata (e.g., hospital, physician, disease, length of\nstay, etc.) that might be useful. This paper investigates the effectiveness of\nmedical meta-information for summarization tasks. We obtain four types of\nmeta-information from the EHR systems and encode each meta-information into a\nsequence-to-sequence model. Using Japanese EHRs, meta-information encoded\nmodels increased ROUGE-1 by up to 4.45 points and BERTScore by 3.77 points over\nthe vanilla Longformer. Also, we found that the encoded meta-information\nimproves the precisions of its related terms in the outputs. Our results showed\nthe benefit of the use of medical meta-information.\n","authors":["Kenichiro Ando","Mamoru Komachi","Takashi Okumura","Hiromasa Horiguchi","Yuji Matsumoto"],"pdf_url":"https://arxiv.org/pdf/2303.06002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05958v1","updated":"2023-03-10T14:46:23Z","published":"2023-03-10T14:46:23Z","title":"Robust Knowledge Distillation from RNN-T Models With Noisy Training\n  Labels Using Full-Sum Loss","summary":"  This work studies knowledge distillation (KD) and addresses its constraints\nfor recurrent neural network transducer (RNN-T) models. In hard distillation, a\nteacher model transcribes large amounts of unlabelled speech to train a student\nmodel. Soft distillation is another popular KD method that distills the output\nlogits of the teacher model. Due to the nature of RNN-T alignments, applying\nsoft distillation between RNN-T architectures having different posterior\ndistributions is challenging. In addition, bad teachers having high\nword-error-rate (WER) reduce the efficacy of KD. We investigate how to\neffectively distill knowledge from variable quality ASR teachers, which has not\nbeen studied before to the best of our knowledge. We show that a sequence-level\nKD, full-sum distillation, outperforms other distillation methods for RNN-T\nmodels, especially for bad teachers. We also propose a variant of full-sum\ndistillation that distills the sequence discriminative knowledge of the teacher\nleading to further improvement in WER. We conduct experiments on public\ndatasets namely SpeechStew and LibriSpeech, and on in-house production data.\n","authors":["Mohammad Zeineldeen","Kartik Audhkhasi","Murali Karthick Baskar","Bhuvana Ramabhadran"],"pdf_url":"https://arxiv.org/pdf/2303.05958v1.pdf","comment":"Accepted at ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07304v1","updated":"2023-03-10T14:25:29Z","published":"2023-03-10T14:25:29Z","title":"Algorithmic Ghost in the Research Shell: Large Language Models and\n  Academic Knowledge Creation in Management Research","summary":"  The paper looks at the role of large language models in academic knowledge\ncreation based on a scoping review (2018 to January 2023) of how researchers\nhave previously used the language model GPT to assist in the performance of\nacademic knowledge creation tasks beyond data analysis. These tasks include\nwriting, editing, reviewing, dataset creation and curation, which have been\ndifficult to perform using earlier ML tools. Based on a synthesis of these\npapers, this study identifies pathways for a future academic research landscape\nthat incorporates wider usage of large language models based on the current\nmodes of adoption in published articles as a Co-Writer, Research Assistant and\nRespondent.\n","authors":["Nigel Williams","Stanislav Ivanov","Dimitrios Buhalis"],"pdf_url":"https://arxiv.org/pdf/2303.07304v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2303.05891v1","updated":"2023-03-10T12:58:34Z","published":"2023-03-10T12:58:34Z","title":"Creation and evaluation of timelines for longitudinal user posts","summary":"  There is increasing interest to work with user generated content in social\nmedia, especially textual posts over time. Currently there is no consistent way\nof segmenting user posts into timelines in a meaningful way that improves the\nquality and cost of manual annotation. Here we propose a set of methods for\nsegmenting longitudinal user posts into timelines likely to contain interesting\nmoments of change in a user's behaviour, based on their online posting\nactivity. We also propose a novel framework for evaluating timelines and show\nits applicability in the context of two different social media datasets.\nFinally, we present a discussion of the linguistic content of highly ranked\ntimelines.\n","authors":["Anthony Hills","Adam Tsakalidis","Federico Nanni","Ioannis Zachos","Maria Liakata"],"pdf_url":"https://arxiv.org/pdf/2303.05891v1.pdf","comment":"Accepted at EACL 2023 (main, long); camera-ready version"},{"id":"http://arxiv.org/abs/2208.09012v2","updated":"2023-03-10T12:32:27Z","published":"2022-08-18T18:22:42Z","title":"A Kind Introduction to Lexical and Grammatical Aspect, with a Survey of\n  Computational Approaches","summary":"  Aspectual meaning refers to how the internal temporal structure of situations\nis presented. This includes whether a situation is described as a state or as\nan event, whether the situation is finished or ongoing, and whether it is\nviewed as a whole or with a focus on a particular phase. This survey gives an\noverview of computational approaches to modeling lexical and grammatical aspect\nalong with intuitive explanations of the necessary linguistic concepts and\nterminology. In particular, we describe the concepts of stativity, telicity,\nhabituality, perfective and imperfective, as well as influential inventories of\neventuality and situation types. We argue that because aspect is a crucial\ncomponent of semantics, especially when it comes to reporting the temporal\nstructure of situations in a precise way, future NLP approaches need to be able\nto handle and evaluate it systematically in order to achieve human-level\nlanguage understanding.\n","authors":["Annemarie Friedrich","Nianwen Xue","Alexis Palmer"],"pdf_url":"https://arxiv.org/pdf/2208.09012v2.pdf","comment":"Accepted at EACL 2023, camera ready version"},{"id":"http://arxiv.org/abs/2210.07523v2","updated":"2023-03-10T12:32:16Z","published":"2022-10-14T05:10:53Z","title":"Self-Adaptive Named Entity Recognition by Retrieving Unstructured\n  Knowledge","summary":"  Although named entity recognition (NER) helps us to extract domain-specific\nentities from text (e.g., artists in the music domain), it is costly to create\na large amount of training data or a structured knowledge base to perform\naccurate NER in the target domain. Here, we propose self-adaptive NER, which\nretrieves external knowledge from unstructured text to learn the usages of\nentities that have not been learned well. To retrieve useful knowledge for NER,\nwe design an effective two-stage model that retrieves unstructured knowledge\nusing uncertain entities as queries. Our model predicts the entities in the\ninput and then finds those of which the prediction is not confident. Then, it\nretrieves knowledge by using these uncertain entities as queries and\nconcatenates the retrieved text to the original input to revise the prediction.\nExperiments on CrossNER datasets demonstrated that our model outperforms strong\nbaselines by 2.35 points in F1 metric.\n","authors":["Kosuke Nishida","Naoki Yoshinaga","Kyosuke Nishida"],"pdf_url":"https://arxiv.org/pdf/2210.07523v2.pdf","comment":"EACL2023 (long)"},{"id":"http://arxiv.org/abs/2301.03029v5","updated":"2023-03-10T11:38:05Z","published":"2023-01-08T12:33:58Z","title":"Topic Modelling of Swedish Newspaper Articles about Coronavirus: a Case\n  Study using Latent Dirichlet Allocation Method","summary":"  Topic Modelling (TM) is from the research branches of natural language\nunderstanding (NLU) and natural language processing (NLP) that is to facilitate\ninsightful analysis from large documents and datasets, such as a summarisation\nof main topics and the topic changes. This kind of discovery is getting more\npopular in real-life applications due to its impact on big data analytics. In\nthis study, from the social-media and healthcare domain, we apply popular\nLatent Dirichlet Allocation (LDA) methods to model the topic changes in Swedish\nnewspaper articles about Coronavirus. We describe the corpus we created\nincluding 6515 articles, methods applied, and statistics on topic changes over\napproximately 1 year and two months period of time from 17th January 2020 to\n13th March 2021. We hope this work can be an asset for grounding applications\nof topic modelling and can be inspiring for similar case studies in an era with\npandemics, to support socio-economic impact research as well as clinical and\nhealthcare analytics. Our data and source code are openly available at\nhttps://github. com/poethan/Swed_Covid_TM Keywords: Latent Dirichlet Allocation\n(LDA); Topic Modelling; Coronavirus; Pandemics; Natural Language Understanding;\nBERT-topic\n","authors":["Bernadeta Griciūtė","Lifeng Han","Hao Li","Goran Nenadic"],"pdf_url":"https://arxiv.org/pdf/2301.03029v5.pdf","comment":"14 pages, 14 figures"},{"id":"http://arxiv.org/abs/2301.10761v3","updated":"2023-03-10T11:04:26Z","published":"2023-01-25T18:55:05Z","title":"Fillers in Spoken Language Understanding: Computational and\n  Psycholinguistic Perspectives","summary":"  Disfluencies (i.e. interruptions in the regular flow of speech), are\nubiquitous to spoken discourse. Fillers (\"uh\", \"um\") are disfluencies that\noccur the most frequently compared to other kinds of disfluencies. Yet, to the\nbest of our knowledge, there isn't a resource that brings together the research\nperspectives influencing Spoken Language Understanding (SLU) on these speech\nevents. This aim of this article is to survey a breadth of perspectives in a\nholistic way; i.e. from considering underlying (psycho)linguistic theory, to\ntheir annotation and consideration in Automatic Speech Recognition (ASR) and\nSLU systems, to lastly, their study from a generation standpoint. This article\naims to present the perspectives in an approachable way to the SLU and\nConversational AI community, and discuss moving forward, what we believe are\nthe trends and challenges in each area.\n","authors":["Tanvi Dinkar","Chloé Clavel","Ioana Vasilescu"],"pdf_url":"https://arxiv.org/pdf/2301.10761v3.pdf","comment":"To appear in TAL Journal"},{"id":"http://arxiv.org/abs/2301.04312v3","updated":"2023-03-10T11:03:10Z","published":"2023-01-11T05:21:00Z","title":"Word-Graph2vec: An efficient word embedding approach on word\n  co-occurrence graph using random walk sampling","summary":"  Word embedding has become ubiquitous and is widely used in various text\nmining and natural language processing (NLP) tasks, such as information\nretrieval, semantic analysis, and machine translation, among many others.\nUnfortunately, it is prohibitively expensive to train the word embedding in a\nrelatively large corpus. We propose a graph-based word embedding algorithm,\ncalled Word-Graph2vec, which converts the large corpus into a word\nco-occurrence graph, then takes the word sequence samples from this graph by\nrandomly traveling and trains the word embedding on this sampling corpus in the\nend. We posit that because of the stable vocabulary, relative idioms, and fixed\nexpressions in English, the size and density of the word co-occurrence graph\nchange slightly with the increase in the training corpus. So that\nWord-Graph2vec has stable runtime on the large scale data set, and its\nperformance advantage becomes more and more obvious with the growth of the\ntraining corpus. Extensive experiments conducted on real-world datasets show\nthat the proposed algorithm outperforms traditional Skip-Gram by four-five\ntimes in terms of efficiency, while the error generated by the random walk\nsampling is small.\n","authors":["Wenting Li","Yuanzhe Cai","Jiahong Xue","Xi Zhang","Huacan Chen","Zeyu Chen"],"pdf_url":"https://arxiv.org/pdf/2301.04312v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05834v1","updated":"2023-03-10T10:21:20Z","published":"2023-03-10T10:21:20Z","title":"An algebraic approach to translating Japanese","summary":"  We use Lambek's pregroups and the framework of compositional distributional\nmodels of language (\"DisCoCat\") to study translations from Japanese to English\nas pairs of functors. Adding decorations to pregroups we show how to handle\nword order changes between languages.\n","authors":["Valentin Boboc"],"pdf_url":"https://arxiv.org/pdf/2303.05834v1.pdf","comment":"20 pages, multiple diagrams and glosses"},{"id":"http://arxiv.org/abs/2211.09733v2","updated":"2023-03-10T08:06:57Z","published":"2022-11-04T14:35:56Z","title":"BERT-Deep CNN: State-of-the-Art for Sentiment Analysis of COVID-19\n  Tweets","summary":"  The free flow of information has been accelerated by the rapid development of\nsocial media technology. There has been a significant social and psychological\nimpact on the population due to the outbreak of Coronavirus disease (COVID-19).\nThe COVID-19 pandemic is one of the current events being discussed on social\nmedia platforms. In order to safeguard societies from this pandemic, studying\npeople's emotions on social media is crucial. As a result of their particular\ncharacteristics, sentiment analysis of texts like tweets remains challenging.\nSentiment analysis is a powerful text analysis tool. It automatically detects\nand analyzes opinions and emotions from unstructured data. Texts from a wide\nrange of sources are examined by a sentiment analysis tool, which extracts\nmeaning from them, including emails, surveys, reviews, social media posts, and\nweb articles. To evaluate sentiments, natural language processing (NLP) and\nmachine learning techniques are used, which assign weights to entities, topics,\nthemes, and categories in sentences or phrases. Machine learning tools learn\nhow to detect sentiment without human intervention by examining examples of\nemotions in text. In a pandemic situation, analyzing social media texts to\nuncover sentimental trends can be very helpful in gaining a better\nunderstanding of society's needs and predicting future trends. We intend to\nstudy society's perception of the COVID-19 pandemic through social media using\nstate-of-the-art BERT and Deep CNN models. The superiority of BERT models over\nother deep models in sentiment analysis is evident and can be concluded from\nthe comparison of the various research studies mentioned in this article.\n","authors":["Javad Hassannataj Joloudari","Sadiq Hussain","Mohammad Ali Nematollahi","Rouhollah Bagheri","Fatemeh Fazl","Roohallah Alizadehsani","Reza Lashgari","Ashis Talukder"],"pdf_url":"https://arxiv.org/pdf/2211.09733v2.pdf","comment":"20 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.05759v1","updated":"2023-03-10T07:55:00Z","published":"2023-03-10T07:55:00Z","title":"An Overview on Language Models: Recent Developments and Outlook","summary":"  Language modeling studies the probability distributions over strings of\ntexts. It is one of the most fundamental tasks in natural language processing\n(NLP). It has been widely used in text generation, speech recognition, machine\ntranslation, etc. Conventional language models (CLMs) aim to predict the\nprobability of linguistic sequences in a causal manner. In contrast,\npre-trained language models (PLMs) cover broader concepts and can be used in\nboth causal sequential modeling and fine-tuning for downstream applications.\nPLMs have their own training paradigms (usually self-supervised) and serve as\nfoundation models in modern NLP systems. This overview paper provides an\nintroduction to both CLMs and PLMs from five aspects, i.e., linguistic units,\nstructures, training methods, evaluation methods, and applications.\nFurthermore, we discuss the relationship between CLMs and PLMs and shed light\non the future directions of language modeling in the pre-trained era.\n","authors":["Chengwei Wei","Yun-Cheng Wang","Bin Wang","C. -C. Jay Kuo"],"pdf_url":"https://arxiv.org/pdf/2303.05759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05758v1","updated":"2023-03-10T07:52:28Z","published":"2023-03-10T07:52:28Z","title":"MIXPGD: Hybrid Adversarial Training for Speech Recognition Systems","summary":"  Automatic speech recognition (ASR) systems based on deep neural networks are\nweak against adversarial perturbations. We propose mixPGD adversarial training\nmethod to improve the robustness of the model for ASR systems. In standard\nadversarial training, adversarial samples are generated by leveraging\nsupervised or unsupervised methods. We merge the capabilities of both\nsupervised and unsupervised approaches in our method to generate new\nadversarial samples which aid in improving model robustness. Extensive\nexperiments and comparison across various state-of-the-art defense methods and\nadversarial attacks have been performed to show that mixPGD gains 4.1% WER of\nbetter performance than previous best performing models under white-box\nadversarial attack setting. We tested our proposed defense method against both\nwhite-box and transfer based black-box attack settings to ensure that our\ndefense strategy is robust against various types of attacks. Empirical results\non several adversarial attacks validate the effectiveness of our proposed\napproach.\n","authors":["Aminul Huq","Weiyi Zhang","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2303.05758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05707v1","updated":"2023-03-10T05:22:39Z","published":"2023-03-10T05:22:39Z","title":"MuLTI: Efficient Video-and-Language Understanding with MultiWay-Sampler\n  and Multiple Choice Modeling","summary":"  Video-and-language understanding has a variety of applications in the\nindustry, such as video question answering, text-video retrieval and\nmulti-label classification. Existing video-and-language understanding methods\ngenerally adopt heavy multi-modal encoders and feature fusion modules, which\nconsume large amounts of GPU memory. Especially, they have difficulty dealing\nwith dense video frames or long text that are prevalent in industrial\napplications. In this paper, we propose MuLTI, a highly accurate and\nmemory-efficient video-and-language understanding model that achieves efficient\nand effective feature fusion through feature sampling and attention modules.\nTherefore, MuLTI can handle longer sequences with limited GPU memory. Then, we\nintroduce an attention-based adapter to the encoders, which finetunes the\nshallow features to improve the model's performance with low GPU memory\nconsumption. Finally, to further improve the model's performance, we introduce\na new pretraining task named Multiple Choice Modeling to bridge the task gap\nbetween pretraining and downstream tasks and enhance the model's ability to\nalign the video and the text. Benefiting from the efficient feature fusion\nmodule, the attention-based adapter and the new pretraining task, MuLTI\nachieves state-of-the-art performance on multiple datasets. Implementation and\npretrained models will be released.\n","authors":["Jiaqi Xu","Bo Liu","Yunkuo Chen","Mengli Cheng","Xing Shi"],"pdf_url":"https://arxiv.org/pdf/2303.05707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.08233v2","updated":"2023-03-10T04:11:56Z","published":"2022-11-14T13:35:01Z","title":"Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach\n  for Speech Emotion Recognition","summary":"  Speech emotion recognition (SER) plays a vital role in improving the\ninteractions between humans and machines by inferring human emotion and\naffective states from speech signals. Whereas recent works primarily focus on\nmining spatiotemporal information from hand-crafted features, we explore how to\nmodel the temporal patterns of speech emotions from dynamic temporal scales.\nTowards that goal, we introduce a novel temporal emotional modeling approach\nfor SER, termed Temporal-aware bI-direction Multi-scale Network (TIM-Net),\nwhich learns multi-scale contextual affective representations from various time\nscales. Specifically, TIM-Net first employs temporal-aware blocks to learn\ntemporal affective representation, then integrates complementary information\nfrom the past and the future to enrich contextual representations, and finally,\nfuses multiple time scale features for better adaptation to the emotional\nvariation. Extensive experimental results on six benchmark SER datasets\ndemonstrate the superior performance of TIM-Net, gaining 2.34% and 2.61%\nimprovements of the average UAR and WAR over the second-best on each corpus.\nThe source code is available at https://github.com/Jiaxin-Ye/TIM-Net_SER.\n","authors":["Jiaxin Ye","Xin-cheng Wen","Yujie Wei","Yong Xu","Kunhong Liu","Hongming Shan"],"pdf_url":"https://arxiv.org/pdf/2211.08233v2.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.05670v1","updated":"2023-03-10T02:52:13Z","published":"2023-03-10T02:52:13Z","title":"Logic Against Bias: Textual Entailment Mitigates Stereotypical Sentence\n  Reasoning","summary":"  Due to their similarity-based learning objectives, pretrained sentence\nencoders often internalize stereotypical assumptions that reflect the social\nbiases that exist within their training corpora. In this paper, we describe\nseveral kinds of stereotypes concerning different communities that are present\nin popular sentence representation models, including pretrained next sentence\nprediction and contrastive sentence representation models. We compare such\nmodels to textual entailment models that learn language logic for a variety of\ndownstream language understanding tasks. By comparing strong pretrained models\nbased on text similarity with textual entailment learning, we conclude that the\nexplicit logic learning with textual entailment can significantly reduce bias\nand improve the recognition of social communities, without an explicit\nde-biasing process\n","authors":["Hongyin Luo","James Glass"],"pdf_url":"https://arxiv.org/pdf/2303.05670v1.pdf","comment":"Accepted by EACL 2023"},{"id":"http://arxiv.org/abs/2303.03387v2","updated":"2023-03-10T02:09:29Z","published":"2023-03-02T17:30:43Z","title":"CoSyn: Detecting Implicit Hate Speech in Online Conversations Using a\n  Context Synergized Hyperbolic Network","summary":"  The tremendous growth of social media users interacting in online\nconversations has also led to significant growth in hate speech. Most of the\nprior works focus on detecting explicit hate speech, which is overt and\nleverages hateful phrases, with very little work focusing on detecting hate\nspeech that is implicit or denotes hatred through indirect or coded language.\nIn this paper, we present CoSyn, a user- and conversational-context synergized\nnetwork for detecting implicit hate speech in online conversation trees. CoSyn\nfirst models the user's personal historical and social context using a novel\nhyperbolic Fourier attention mechanism and hyperbolic graph convolution\nnetwork. Next, we jointly model the user's personal context and the\nconversational context using a novel context interaction mechanism in the\nhyperbolic space that clearly captures the interplay between the two and makes\nindependent assessments on the amounts of information to be retrieved from both\ncontexts. CoSyn performs all operations in the hyperbolic space to account for\nthe scale-free dynamics of social media. We demonstrate the effectiveness of\nCoSyn both qualitatively and quantitatively on an open-source hate speech\ndataset with Twitter conversations and show that CoSyn outperforms all our\nbaselines in detecting implicit hate speech with absolute improvements in the\nrange of 8.15% - 19.50%.\n","authors":["Sreyan Ghosh","Manan Suri","Purva Chiniya","Utkarsh Tyagi","Sonal Kumar","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2303.03387v2.pdf","comment":"Under review at IJCAI 2023"},{"id":"http://arxiv.org/abs/2210.03629v3","updated":"2023-03-10T01:00:17Z","published":"2022-10-06T01:00:32Z","title":"ReAct: Synergizing Reasoning and Acting in Language Models","summary":"  While large language models (LLMs) have demonstrated impressive capabilities\nacross tasks in language understanding and interactive decision making, their\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\naction plan generation) have primarily been studied as separate topics. In this\npaper, we explore the use of LLMs to generate both reasoning traces and\ntask-specific actions in an interleaved manner, allowing for greater synergy\nbetween the two: reasoning traces help the model induce, track, and update\naction plans as well as handle exceptions, while actions allow it to interface\nwith external sources, such as knowledge bases or environments, to gather\nadditional information. We apply our approach, named ReAct, to a diverse set of\nlanguage and decision making tasks and demonstrate its effectiveness over\nstate-of-the-art baselines, as well as improved human interpretability and\ntrustworthiness over methods without reasoning or acting components.\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\nReAct overcomes issues of hallucination and error propagation prevalent in\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\ngenerates human-like task-solving trajectories that are more interpretable than\nbaselines without reasoning traces. On two interactive decision making\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\nreinforcement learning methods by an absolute success rate of 34% and 10%\nrespectively, while being prompted with only one or two in-context examples.\nProject site with code: https://react-lm.github.io\n","authors":["Shunyu Yao","Jeffrey Zhao","Dian Yu","Nan Du","Izhak Shafran","Karthik Narasimhan","Yuan Cao"],"pdf_url":"https://arxiv.org/pdf/2210.03629v3.pdf","comment":"v3 is the ICLR camera ready version with some typos fixed. Project\n  site with code: https://react-lm.github.io"}],"Multimedia":[{"id":"http://arxiv.org/abs/2302.14402v2","updated":"2023-03-10T07:56:58Z","published":"2023-02-28T08:35:50Z","title":"Neural Video Compression with Diverse Contexts","summary":"  For any video codecs, the coding efficiency highly relies on whether the\ncurrent signal to be encoded can find the relevant contexts from the previous\nreconstructed signals. Traditional codec has verified more contexts bring\nsubstantial coding gain, but in a time-consuming manner. However, for the\nemerging neural video codec (NVC), its contexts are still limited, leading to\nlow compression ratio. To boost NVC, this paper proposes increasing the context\ndiversity in both temporal and spatial dimensions. First, we guide the model to\nlearn hierarchical quality patterns across frames, which enriches long-term and\nyet high-quality temporal contexts. Furthermore, to tap the potential of\noptical flow-based coding framework, we introduce a group-based offset\ndiversity where the cross-group interaction is proposed for better context\nmining. In addition, this paper also adopts a quadtree-based partition to\nincrease spatial context diversity when encoding the latent representation in\nparallel. Experiments show that our codec obtains 23.5% bitrate saving over\nprevious SOTA NVC. Better yet, our codec has surpassed the under-developing\nnext generation traditional codec/ECM in both RGB and YUV420 colorspaces, in\nterms of PSNR. The codes are at https://github.com/microsoft/DCVC.\n","authors":["Jiahao Li","Bin Li","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2302.14402v2.pdf","comment":"Accepted by CVPR 2023. Codes are at https://github.com/microsoft/DCVC"},{"id":"http://arxiv.org/abs/2210.07839v3","updated":"2023-03-10T07:37:34Z","published":"2022-10-02T07:29:57Z","title":"Contrastive Audio-Visual Masked Autoencoder","summary":"  In this paper, we first extend the recent Masked Auto-Encoder (MAE) model\nfrom a single modality to audio-visual multi-modalities. Subsequently, we\npropose the Contrastive Audio-Visual Masked Auto-Encoder (CAV-MAE) by combining\ncontrastive learning and masked data modeling, two major self-supervised\nlearning frameworks, to learn a joint and coordinated audio-visual\nrepresentation. Our experiments show that the contrastive audio-visual\ncorrespondence learning objective not only enables the model to perform\naudio-visual retrieval tasks, but also helps the model learn a better joint\nrepresentation. As a result, our fully self-supervised pretrained CAV-MAE\nachieves a new SOTA accuracy of 65.9% on VGGSound, and is comparable with the\nprevious best supervised pretrained model on AudioSet in the audio-visual event\nclassification task. Code and pretrained models are at\nhttps://github.com/yuangongnd/cav-mae.\n","authors":["Yuan Gong","Andrew Rouditchenko","Alexander H. Liu","David Harwath","Leonid Karlinsky","Hilde Kuehne","James Glass"],"pdf_url":"https://arxiv.org/pdf/2210.07839v3.pdf","comment":"Accepted at ICLR 2023 as a notable top 25% paper. Code and pretrained\n  models are at https://github.com/yuangongnd/cav-mae"},{"id":"http://arxiv.org/abs/2303.05744v1","updated":"2023-03-10T07:08:24Z","published":"2023-03-10T07:08:24Z","title":"QVRF: A Quantization-error-aware Variable Rate Framework for Learned\n  Image Compression","summary":"  Learned image compression has exhibited promising compression performance,\nbut variable bitrates over a wide range remain a challenge. State-of-the-art\nvariable rate methods compromise the loss of model performance and require\nnumerous additional parameters. In this paper, we present a\nQuantization-error-aware Variable Rate Framework (QVRF) that utilizes a\nunivariate quantization regulator a to achieve wide-range variable rates within\na single model. Specifically, QVRF defines a quantization regulator vector\ncoupled with predefined Lagrange multipliers to control quantization error of\nall latent representation for discrete variable rates. Additionally, the\nreparameterization method makes QVRF compatible with a round quantizer.\nExhaustive experiments demonstrate that existing fixed-rate VAE-based methods\nequipped with QVRF can achieve wide-range continuous variable rates within a\nsingle model without significant performance degradation. Furthermore, QVRF\noutperforms contemporary variable-rate methods in rate-distortion performance\nwith minimal additional parameters.\n","authors":["Kedeng Tong","Yaojun Wu","Yue Li","Kai Zhang","Li Zhang","Xin Jin"],"pdf_url":"https://arxiv.org/pdf/2303.05744v1.pdf","comment":"7 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.05707v1","updated":"2023-03-10T05:22:39Z","published":"2023-03-10T05:22:39Z","title":"MuLTI: Efficient Video-and-Language Understanding with MultiWay-Sampler\n  and Multiple Choice Modeling","summary":"  Video-and-language understanding has a variety of applications in the\nindustry, such as video question answering, text-video retrieval and\nmulti-label classification. Existing video-and-language understanding methods\ngenerally adopt heavy multi-modal encoders and feature fusion modules, which\nconsume large amounts of GPU memory. Especially, they have difficulty dealing\nwith dense video frames or long text that are prevalent in industrial\napplications. In this paper, we propose MuLTI, a highly accurate and\nmemory-efficient video-and-language understanding model that achieves efficient\nand effective feature fusion through feature sampling and attention modules.\nTherefore, MuLTI can handle longer sequences with limited GPU memory. Then, we\nintroduce an attention-based adapter to the encoders, which finetunes the\nshallow features to improve the model's performance with low GPU memory\nconsumption. Finally, to further improve the model's performance, we introduce\na new pretraining task named Multiple Choice Modeling to bridge the task gap\nbetween pretraining and downstream tasks and enhance the model's ability to\nalign the video and the text. Benefiting from the efficient feature fusion\nmodule, the attention-based adapter and the new pretraining task, MuLTI\nachieves state-of-the-art performance on multiple datasets. Implementation and\npretrained models will be released.\n","authors":["Jiaqi Xu","Bo Liu","Yunkuo Chen","Mengli Cheng","Xing Shi"],"pdf_url":"https://arxiv.org/pdf/2303.05707v1.pdf","comment":null}]},"2023-03-09T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.05582v1","updated":"2023-03-09T21:13:32Z","published":"2023-03-09T21:13:32Z","title":"Generalization analysis of an unfolding network for analysis-based\n  Compressed Sensing","summary":"  Unfolding networks have shown promising results in the Compressed Sensing\n(CS) field. Yet, the investigation of their generalization ability is still in\nits infancy. In this paper, we perform generalization analysis of a\nstate-of-the-art ADMM-based unfolding network, which jointly learns a decoder\nfor CS and a sparsifying redundant analysis operator. To this end, we first\nimpose a structural constraint on the learnable sparsifier, which parametrizes\nthe network's hypothesis class. For the latter, we estimate its Rademacher\ncomplexity. With this estimate in hand, we deliver generalization error bounds\nfor the examined network. Finally, the validity of our theory is assessed and\nnumerical comparisons to a state-of-the-art unfolding network are made, on\nsynthetic and real-world datasets. Our experimental results demonstrate that\nour proposed framework complies with our theoretical findings and outperforms\nthe baseline, consistently for all datasets.\n","authors":["Vicky Kouni","Yannis Panagakis"],"pdf_url":"https://arxiv.org/pdf/2303.05582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05575v1","updated":"2023-03-09T20:51:18Z","published":"2023-03-09T20:51:18Z","title":"Evaluating the Robustness of Conversational Recommender Systems by\n  Adversarial Examples","summary":"  Conversational recommender systems (CRSs) are improving rapidly, according to\nthe standard recommendation accuracy metrics. However, it is essential to make\nsure that these systems are robust in interacting with users including regular\nand malicious users who want to attack the system by feeding the system\nmodified input data. In this paper, we propose an adversarial evaluation scheme\nincluding four scenarios in two categories and automatically generate\nadversarial examples to evaluate the robustness of these systems in the face of\ndifferent input data. By executing these adversarial examples we can compare\nthe ability of different conversational recommender systems to satisfy the\nuser's preferences. We evaluate three CRSs by the proposed adversarial examples\non two datasets. Our results show that none of these systems are robust and\nreliable to the adversarial examples.\n","authors":["Ali Montazeralghaem","James Allan"],"pdf_url":"https://arxiv.org/pdf/2303.05575v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2302.09051v3","updated":"2023-03-09T17:25:10Z","published":"2023-02-17T18:31:31Z","title":"Complex QA and language models hybrid architectures, Survey","summary":"  This paper reviews the state-of-the-art of hybrid language models\narchitectures and strategies for \"complex\" question-answering (QA, CQA, CPS).\nLarge Language Models (LLM) are good at leveraging public data on standard\nproblems but once you want to tackle more specific complex questions or\nproblems you may need specific architecture, knowledge, skills, methods,\nsensitive data protection, explainability, human approval and versatile\nfeedback... We identify key elements augmenting LLM to solve complex questions\nor problems. We extend findings from the robust community edited research\npapers BIG, BLOOM and HELM which open source, benchmark and analyze limits and\nchallenges of LLM in terms of tasks complexity and strict evaluation on\naccuracy (e.g. fairness, robustness, toxicity, ...). Recent projects like\nChatGPT and GALACTICA have allowed non-specialists to grasp the great potential\nas well as the equally strong limitations of language models in complex QA.\nHybridizing these models with different components could allow to overcome\nthese different limits and go much further. We discuss some challenges\nassociated with complex QA, including domain adaptation, decomposition and\nefficient multi-step QA, long form and non-factoid QA, safety and\nmulti-sensitivity data protection, multimodal search, hallucinations,\nexplainability and truthfulness, temproal reasoning. Therefore, we analyze\ncurrent solutions and promising research trends, using elements such as: hybrid\nLLM architectures, active human reinforcement learning supervised with AI,\nprompting adaptation, neuro-symbolic and structured knowledge grounding,\nprogram synthesis, iterated decomposition and others.\n","authors":["Xavier Daull","Patrice Bellot","Emmanuel Bruno","Vincent Martin","Elisabeth Murisasco"],"pdf_url":"https://arxiv.org/pdf/2302.09051v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06016v1","updated":"2023-03-09T12:13:46Z","published":"2023-03-09T12:13:46Z","title":"Modelling Projection Bias in Intertemporal Choices: A Prospect Theory\n  Based Approach","summary":"  Users often face bundle promotions when purchasing, where they have to select\nbetween two options: buy the single item at full price, or buy the bundle at a\ndiscount. In this scenario, users' preferences are usually influenced by the\nprojection bias, that is, users often believe that their future preferences are\nsimilar to their current preferences, causing them to make irrational and\nshort-sighted decisions. It is of great significance to analyze the effect of\nthe projection bias on users' preferences, and this study may help understand\nusers' decision-making process and provide bundling and pricing strategies for\nsellers. Prior works typically use a linear bias model for qualitative\nanalysis, and they cannot quantitatively calculate users' nonlinear and\npersonalized bias. In this work, we propose Pobe, a projection bias-embedded\npreference model to accurately predict users' choices. The proposed Pobe\nintroduces the prospect theory to analyze users' irrational decisions, and\nutilizes the weight function to handle users' nonlinear and personalized bias.\nBased on the proposed Pobe, we also study the impact of items' correlations or\ndiscount prices on users' choices, and provide four bundling strategies.\nExperimental results show that the proposed method can achieve better\nperformance than prior works, especially when only small data is available.\n","authors":["Qingming Li","H. Vicky Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.06016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.14233v2","updated":"2023-03-09T10:56:28Z","published":"2021-09-29T07:14:22Z","title":"A Next Basket Recommendation Reality Check","summary":"  The goal of a next basket recommendation (NBR) system is to recommend items\nfor the next basket for a user, based on the sequence of their prior baskets.\nRecently, a number of methods with complex modules have been proposed that\nclaim state-of-the-art performance. They rarely look into the predicted basket\nand just provide intuitive reasons for the observed improvements, e.g., better\nrepresentation, capturing intentions or relations, etc. We provide a novel\nangle on the evaluation of next basket recommendation methods, centered on the\ndistinction between repetition and exploration: the next basket is typically\ncomposed of previously consumed items (i.e., repeat items) and new items (i.e,\nexplore items). We propose a set of metrics that measure the repeat/explore\nratio and performance of NBR models. Using these new metrics, we analyze\nstate-of-the-art NBR models. The results of our analysis help to clarify the\nextent of the actual progress achieved by existing NBR methods as well as the\nunderlying reasons for the improvements. Overall, our work sheds light on the\nevaluation problem of NBR and provides useful insights into the model design\nfor this task.\n","authors":["Ming Li","Sami Jullien","Mozhdeh Ariannezhad","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2109.14233v2.pdf","comment":"This paper has been accepted to ACM TOIS"},{"id":"http://arxiv.org/abs/2303.05103v1","updated":"2023-03-09T08:23:56Z","published":"2023-03-09T08:23:56Z","title":"Algorithmic neutrality","summary":"  Bias infects the algorithms that wield increasing control over our lives.\nPredictive policing systems overestimate crime in communities of color; hiring\nalgorithms dock qualified female candidates; and facial recognition software\nstruggles to recognize dark-skinned faces. Algorithmic bias has received\nsignificant attention. Algorithmic neutrality, in contrast, has been largely\nneglected. Algorithmic neutrality is my topic. I take up three questions. What\nis algorithmic neutrality? Is algorithmic neutrality possible? When we have an\neye to algorithmic neutrality, what can we learn about algorithmic bias? To\nanswer these questions in concrete terms, I work with a case study: search\nengines. Drawing on work about neutrality in science, I say that a search\nengine is neutral only if certain values, like political ideologies or the\nfinancial interests of the search engine operator, play no role in how the\nsearch engine ranks pages. Search neutrality, I argue, is impossible. Its\nimpossibility seems to threaten the significance of search bias: if no search\nengine is neutral, then every search engine is biased. To defuse this threat, I\ndistinguish two forms of bias, failing-on-its-own-terms bias and other-values\nbias. This distinction allows us to make sense of search bias, and capture its\nnormative complexion, despite the impossibility of neutrality.\n","authors":["Milo Phillips-Brown"],"pdf_url":"https://arxiv.org/pdf/2303.05103v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2303.05039v1","updated":"2023-03-09T05:20:17Z","published":"2023-03-09T05:20:17Z","title":"Improving Recommendation Systems with User Personality Inferred from\n  Product Reviews","summary":"  Personality is a psychological factor that reflects people's preferences,\nwhich in turn influences their decision-making. We hypothesize that accurate\nmodeling of users' personalities improves recommendation systems' performance.\nHowever, acquiring such personality profiles is both sensitive and expensive.\nWe address this problem by introducing a novel method to automatically extract\npersonality profiles from public product review text. We then design and assess\nthree context-aware recommendation architectures that leverage the profiles to\ntest our hypothesis.\n  Experiments on our two newly contributed personality datasets --\nAmazon-beauty and Amazon-music -- validate our hypothesis, showing performance\nboosts of 3--28%.Our analysis uncovers that varying personality types\ncontribute differently to recommendation performance: open and extroverted\npersonalities are most helpful in music recommendation, while a conscientious\npersonality is most helpful in beauty product recommendation. The dataset is\navailable at https://github.com/XinyuanLu00/IRS-WSDM2023-personality-dataset.\n","authors":["Xinyuan Lu","Min-Yen Kan"],"pdf_url":"https://arxiv.org/pdf/2303.05039v1.pdf","comment":"Accepted by IRS@WSDM'23"},{"id":"http://arxiv.org/abs/2301.00767v2","updated":"2023-03-09T04:40:23Z","published":"2022-12-27T08:09:45Z","title":"A Survey on Federated Recommendation Systems","summary":"  Federated learning has recently been applied to recommendation systems to\nprotect user privacy. In federated learning settings, recommendation systems\ncan train recommendation models only collecting the intermediate parameters\ninstead of the real user data, which greatly enhances the user privacy. Beside,\nfederated recommendation systems enable to collaborate with other data\nplatforms to improve recommended model performance while meeting the regulation\nand privacy constraints. However, federated recommendation systems faces many\nnew challenges such as privacy, security, heterogeneity and communication\ncosts. While significant research has been conducted in these areas, gaps in\nthe surveying literature still exist. In this survey, we-(1) summarize some\ncommon privacy mechanisms used in federated recommendation systems and discuss\nthe advantages and limitations of each mechanism; (2) review some robust\naggregation strategies and several novel attacks against security; (3)\nsummarize some approaches to address heterogeneity and communication costs\nproblems; (4)introduce some open source platforms that can be used to build\nfederated recommendation systems; (5) present some prospective research\ndirections in the future. This survey can guide researchers and practitioners\nunderstand the research progress in these areas.\n","authors":["Zehua Sun","Yonghui Xu","Yong Liu","Wei He","Lanju Kong","Fangzhao Wu","Yali Jiang","Lizhen Cui"],"pdf_url":"https://arxiv.org/pdf/2301.00767v2.pdf","comment":null}],"Computation and Language":[{"id":"http://arxiv.org/abs/2303.05581v1","updated":"2023-03-09T21:12:46Z","published":"2023-03-09T21:12:46Z","title":"Open World Classification with Adaptive Negative Samples","summary":"  Open world classification is a task in natural language processing with key\npractical relevance and impact. Since the open or {\\em unknown} category data\nonly manifests in the inference phase, finding a model with a suitable decision\nboundary accommodating for the identification of known classes and\ndiscrimination of the open category is challenging. The performance of existing\nmodels is limited by the lack of effective open category data during the\ntraining stage or the lack of a good mechanism to learn appropriate decision\nboundaries. We propose an approach based on \\underline{a}daptive\n\\underline{n}egative \\underline{s}amples (ANS) designed to generate effective\nsynthetic open category samples in the training stage and without requiring any\nprior knowledge or external datasets. Empirically, we find a significant\nadvantage in using auxiliary one-versus-rest binary classifiers, which\neffectively utilize the generated negative samples and avoid the complex\nthreshold-seeking stage in previous works. Extensive experiments on three\nbenchmark datasets show that ANS achieves significant improvements over\nstate-of-the-art methods.\n","authors":["Ke Bai","Guoyin Wang","Jiwei Li","Sunghyun Park","Sungjin Lee","Puyang Xu","Ricardo Henao","Lawrence Carin"],"pdf_url":"https://arxiv.org/pdf/2303.05581v1.pdf","comment":"Accepted by EMNLP 2021 (Main Track, Long Paper)"},{"id":"http://arxiv.org/abs/2303.05510v1","updated":"2023-03-09T18:59:47Z","published":"2023-03-09T18:59:47Z","title":"Planning with Large Language Models for Code Generation","summary":"  Existing large language model-based code generation pipelines typically use\nbeam search or sampling algorithms during the decoding process. Although the\nprograms they generate achieve high token-matching-based scores, they often\nfail to compile or generate incorrect outputs. The main reason is that\nconventional Transformer decoding algorithms may not be the best choice for\ncode generation. In this work, we propose a novel Transformer decoding\nalgorithm, Planning-Guided Transformer Decoding (PG-TD), that uses a planning\nalgorithm to do lookahead search and guide the Transformer to generate better\nprograms. Specifically, instead of simply optimizing the likelihood of the\ngenerated sequences, the Transformer makes use of a planner to generate\ncandidate programs and test them on public test cases. The Transformer can\ntherefore make more informed decisions and generate tokens that will eventually\nlead to higher-quality programs. We also design a mechanism that shares\ninformation between the Transformer and the planner to make our algorithm\ncomputationally efficient. We empirically evaluate our framework with several\nlarge language models as backbones on public coding challenge benchmarks,\nshowing that 1) it can generate programs that consistently achieve higher\nperformance compared with competing baseline methods; 2) it enables\ncontrollable code generation, such as concise codes and highly-commented codes\nby optimizing modified objective.\n","authors":["Shun Zhang","Zhenfang Chen","Yikang Shen","Mingyu Ding","Joshua B. Tenenbaum","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2303.05510v1.pdf","comment":"ICLR 2023. Project page:https://codeaimcts.github.io"},{"id":"http://arxiv.org/abs/2303.05453v1","updated":"2023-03-09T17:52:07Z","published":"2023-03-09T17:52:07Z","title":"Personalisation within bounds: A risk taxonomy and policy framework for\n  the alignment of large language models with personalised feedback","summary":"  Large language models (LLMs) are used to generate content for a wide range of\ntasks, and are set to reach a growing audience in coming years due to\nintegration in product interfaces like ChatGPT or search engines like Bing.\nThis intensifies the need to ensure that models are aligned with human\npreferences and do not produce unsafe, inaccurate or toxic outputs. While\nalignment techniques like reinforcement learning with human feedback (RLHF) and\nred-teaming can mitigate some safety concerns and improve model capabilities,\nit is unlikely that an aggregate fine-tuning process can adequately represent\nthe full range of users' preferences and values. Different people may\nlegitimately disagree on their preferences for language and conversational\nnorms, as well as on values or ideologies which guide their communication.\nPersonalising LLMs through micro-level preference learning processes may result\nin models that are better aligned with each user. However, there are several\nnormative challenges in defining the bounds of a societally-acceptable and safe\ndegree of personalisation. In this paper, we ask how, and in what ways, LLMs\nshould be personalised. First, we review literature on current paradigms for\naligning LLMs with human feedback, and identify issues including (i) a lack of\nclarity regarding what alignment means; (ii) a tendency of technology providers\nto prescribe definitions of inherently subjective preferences and values; and\n(iii) a 'tyranny of the crowdworker', exacerbated by a lack of documentation in\nwho we are really aligning to. Second, we present a taxonomy of benefits and\nrisks associated with personalised LLMs, for individuals and society at large.\nFinally, we propose a three-tiered policy framework that allows users to\nexperience the benefits of personalised alignment, while restraining unsafe and\nundesirable LLM-behaviours within (supra-)national and organisational bounds.\n","authors":["Hannah Rose Kirk","Bertie Vidgen","Paul Röttger","Scott A. Hale"],"pdf_url":"https://arxiv.org/pdf/2303.05453v1.pdf","comment":"19 pages, 1 table"},{"id":"http://arxiv.org/abs/2303.03836v2","updated":"2023-03-09T17:33:31Z","published":"2023-03-07T12:03:58Z","title":"Exploring the Feasibility of ChatGPT for Event Extraction","summary":"  Event extraction is a fundamental task in natural language processing that\ninvolves identifying and extracting information about events mentioned in text.\nHowever, it is a challenging task due to the lack of annotated data, which is\nexpensive and time-consuming to obtain. The emergence of large language models\n(LLMs) such as ChatGPT provides an opportunity to solve language tasks with\nsimple prompts without the need for task-specific datasets and fine-tuning.\nWhile ChatGPT has demonstrated impressive results in tasks like machine\ntranslation, text summarization, and question answering, it presents challenges\nwhen used for complex tasks like event extraction. Unlike other tasks, event\nextraction requires the model to be provided with a complex set of instructions\ndefining all event types and their schemas. To explore the feasibility of\nChatGPT for event extraction and the challenges it poses, we conducted a series\nof experiments. Our results show that ChatGPT has, on average, only 51.04% of\nthe performance of a task-specific model such as EEQA in long-tail and complex\nscenarios. Our usability testing experiments indicate that ChatGPT is not\nrobust enough, and continuous refinement of the prompt does not lead to stable\nperformance improvements, which can result in a poor user experience. Besides,\nChatGPT is highly sensitive to different prompt styles.\n","authors":["Jun Gao","Huan Zhao","Changlong Yu","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2303.03836v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.09051v3","updated":"2023-03-09T17:25:10Z","published":"2023-02-17T18:31:31Z","title":"Complex QA and language models hybrid architectures, Survey","summary":"  This paper reviews the state-of-the-art of hybrid language models\narchitectures and strategies for \"complex\" question-answering (QA, CQA, CPS).\nLarge Language Models (LLM) are good at leveraging public data on standard\nproblems but once you want to tackle more specific complex questions or\nproblems you may need specific architecture, knowledge, skills, methods,\nsensitive data protection, explainability, human approval and versatile\nfeedback... We identify key elements augmenting LLM to solve complex questions\nor problems. We extend findings from the robust community edited research\npapers BIG, BLOOM and HELM which open source, benchmark and analyze limits and\nchallenges of LLM in terms of tasks complexity and strict evaluation on\naccuracy (e.g. fairness, robustness, toxicity, ...). Recent projects like\nChatGPT and GALACTICA have allowed non-specialists to grasp the great potential\nas well as the equally strong limitations of language models in complex QA.\nHybridizing these models with different components could allow to overcome\nthese different limits and go much further. We discuss some challenges\nassociated with complex QA, including domain adaptation, decomposition and\nefficient multi-step QA, long form and non-factoid QA, safety and\nmulti-sensitivity data protection, multimodal search, hallucinations,\nexplainability and truthfulness, temproal reasoning. Therefore, we analyze\ncurrent solutions and promising research trends, using elements such as: hybrid\nLLM architectures, active human reinforcement learning supervised with AI,\nprompting adaptation, neuro-symbolic and structured knowledge grounding,\nprogram synthesis, iterated decomposition and others.\n","authors":["Xavier Daull","Patrice Bellot","Emmanuel Bruno","Vincent Martin","Elisabeth Murisasco"],"pdf_url":"https://arxiv.org/pdf/2302.09051v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04001v2","updated":"2023-03-09T17:10:27Z","published":"2023-03-07T16:00:26Z","title":"ELODIN: Naming Concepts in Embedding Spaces","summary":"  Despite recent advancements, the field of text-to-image synthesis still\nsuffers from lack of fine-grained control. Using only text, it remains\nchallenging to deal with issues such as concept coherence and concept\ncontamination. We propose a method to enhance control by generating specific\nconcepts that can be reused throughout multiple images, effectively expanding\nnatural language with new words that can be combined much like a painter's\npalette. Unlike previous contributions, our method does not copy visuals from\ninput data and can generate concepts through text alone. We perform a set of\ncomparisons that finds our method to be a significant improvement over\ntext-only prompts.\n","authors":["Rodrigo Mello","Filipe Calegario","Geber Ramalho"],"pdf_url":"https://arxiv.org/pdf/2303.04001v2.pdf","comment":"Added quantitative data, fixed formatting issues"},{"id":"http://arxiv.org/abs/2303.07203v1","updated":"2023-03-09T16:37:37Z","published":"2023-03-09T16:37:37Z","title":"On the Robustness of Text Vectorizers","summary":"  A fundamental issue in natural language processing is the robustness of the\nmodels with respect to changes in the input. One critical step in this process\nis the embedding of documents, which transforms sequences of words or tokens\ninto vector representations. Our work formally proves that popular embedding\nschemes, such as concatenation, TF-IDF, and Paragraph Vector (a.k.a. doc2vec),\nexhibit robustness in the H\\\"older or Lipschitz sense with respect to the\nHamming distance. We provide quantitative bounds for these schemes and\ndemonstrate how the constants involved are affected by the length of the\ndocument. These findings are exemplified through a series of numerical\nexamples.\n","authors":["Rémi Catellier","Samuel Vaiter","Damien Garreau"],"pdf_url":"https://arxiv.org/pdf/2303.07203v1.pdf","comment":"33 pages, 10 figures"},{"id":"http://arxiv.org/abs/2303.05349v1","updated":"2023-03-09T15:46:54Z","published":"2023-03-09T15:46:54Z","title":"Seeing ChatGPT Through Students' Eyes: An Analysis of TikTok Data","summary":"  Advanced large language models like ChatGPT have gained considerable\nattention recently, including among students. However, while the debate on\nChatGPT in academia is making waves, more understanding is needed among\nlecturers and teachers on how students use and perceive ChatGPT. To address\nthis gap, we analyzed the content on ChatGPT available on TikTok in February\n2023. TikTok is a rapidly growing social media platform popular among\nindividuals under 30. Specifically, we analyzed the content of the 100 most\npopular videos in English tagged with #chatgpt, which collectively garnered\nover 250 million views. Most of the videos we studied promoted the use of\nChatGPT for tasks like writing essays or code. In addition, many videos\ndiscussed AI detectors, with a focus on how other tools can help to transform\nChatGPT output to fool these detectors. This also mirrors the discussion among\neducators on how to treat ChatGPT as lecturers and teachers in teaching and\ngrading. What is, however, missing from the analyzed clips on TikTok are videos\nthat discuss ChatGPT producing content that is nonsensical or unfaithful to the\ntraining data.\n","authors":["Anna-Carolina Haensch","Sarah Ball","Markus Herklotz","Frauke Kreuter"],"pdf_url":"https://arxiv.org/pdf/2303.05349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.15162v3","updated":"2023-03-09T15:02:07Z","published":"2022-09-30T01:17:18Z","title":"Linearly Mapping from Image to Text Space","summary":"  The extent to which text-only language models (LMs) learn to represent\nfeatures of the non-linguistic world is an open question. Prior work has shown\nthat pretrained LMs can be taught to caption images when a vision model's\nparameters are optimized to encode images in the language space. We test a\nstronger hypothesis: that the conceptual representations learned by frozen\ntext-only models and vision-only models are similar enough that this can be\nachieved with a linear map. We show that the image representations from vision\nmodels can be transferred as continuous prompts to frozen LMs by training only\na single linear projection. Using these to prompt the LM achieves competitive\nperformance on captioning and visual question answering tasks compared to\nmodels that tune both the image encoder and text decoder (such as the MAGMA\nmodel). We compare three image encoders with increasing amounts of linguistic\nsupervision seen during pretraining: BEIT (no linguistic information),\nNF-ResNET (lexical category information), and CLIP (full natural language\ndescriptions). We find that all three encoders perform equally well at\ntransferring visual property information to the language model (e.g., whether\nan animal is large or small), but that image encoders pretrained with\nlinguistic supervision more saliently encode category information (e.g.,\ndistinguishing hippo vs. elephant) and thus perform significantly better on\nbenchmark language-and-vision tasks. Our results indicate that LMs encode\nconceptual information structurally similarly to vision-based models, even\nthose that are solely trained on images. Code is available here:\nhttps://github.com/jmerullo/limber\n","authors":["Jack Merullo","Louis Castricato","Carsten Eickhoff","Ellie Pavlick"],"pdf_url":"https://arxiv.org/pdf/2209.15162v3.pdf","comment":"Accepted at ICLR 2023"},{"id":"http://arxiv.org/abs/2303.05313v1","updated":"2023-03-09T15:01:12Z","published":"2023-03-09T15:01:12Z","title":"Replacement as a Self-supervision for Fine-grained Vision-language\n  Pre-training","summary":"  Fine-grained supervision based on object annotations has been widely used for\nvision and language pre-training (VLP). However, in real-world application\nscenarios, aligned multi-modal data is usually in the image-caption format,\nwhich only provides coarse-grained supervision. It is cost-expensive to collect\nobject annotations and build object annotation pre-extractor for different\nscenarios. In this paper, we propose a fine-grained self-supervision signal\nwithout object annotations from a replacement perspective. First, we propose a\nhomonym sentence rewriting (HSR) algorithm to provide token-level supervision.\nThe algorithm replaces a verb/noun/adjective/quantifier word of the caption\nwith its homonyms from WordNet. Correspondingly, we propose a replacement\nvision-language modeling (RVLM) framework to exploit the token-level\nsupervision. Two replaced modeling tasks, i.e., replaced language contrastive\n(RLC) and replaced language modeling (RLM), are proposed to learn the\nfine-grained alignment. Extensive experiments on several downstream tasks\ndemonstrate the superior performance of the proposed method.\n","authors":["Lisai Zhang","Qingcai Chen","Zhijian Chen","Yunpeng Han","Zhonghua Li","Zhao Cao"],"pdf_url":"https://arxiv.org/pdf/2303.05313v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2303.05309v1","updated":"2023-03-09T14:58:29Z","published":"2023-03-09T14:58:29Z","title":"MixSpeech: Cross-Modality Self-Learning with Audio-Visual Stream Mixup\n  for Visual Speech Translation and Recognition","summary":"  Multi-media communications facilitate global interaction among people.\nHowever, despite researchers exploring cross-lingual translation techniques\nsuch as machine translation and audio speech translation to overcome language\nbarriers, there is still a shortage of cross-lingual studies on visual speech.\nThis lack of research is mainly due to the absence of datasets containing\nvisual speech and translated text pairs. In this paper, we present\n\\textbf{AVMuST-TED}, the first dataset for \\textbf{A}udio-\\textbf{V}isual\n\\textbf{Mu}ltilingual \\textbf{S}peech \\textbf{T}ranslation, derived from\n\\textbf{TED} talks. Nonetheless, visual speech is not as distinguishable as\naudio speech, making it difficult to develop a mapping from source speech\nphonemes to the target language text. To address this issue, we propose\nMixSpeech, a cross-modality self-learning framework that utilizes audio speech\nto regularize the training of visual speech tasks. To further minimize the\ncross-modality gap and its impact on knowledge transfer, we suggest adopting\nmixed speech, which is created by interpolating audio and visual streams, along\nwith a curriculum learning strategy to adjust the mixing ratio as needed.\nMixSpeech enhances speech translation in noisy environments, improving BLEU\nscores for four languages on AVMuST-TED by +1.4 to +4.2. Moreover, it achieves\nstate-of-the-art performance in lip reading on CMLR (11.1\\%), LRS2 (25.5\\%),\nand LRS3 (28.0\\%).\n","authors":["Xize Cheng","Linjun Li","Tao Jin","Rongjie Huang","Wang Lin","Zehan Wang","Huangdai Liu","Ye Wang","Aoxiong Yin","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.05309v1.pdf","comment":"https://github.com/Exgc/AVMuST-TED"},{"id":"http://arxiv.org/abs/2303.05295v1","updated":"2023-03-09T14:44:31Z","published":"2023-03-09T14:44:31Z","title":"Dynamic Stashing Quantization for Efficient Transformer Training","summary":"  Large Language Models (LLMs) have demonstrated impressive performance on a\nrange of Natural Language Processing (NLP) tasks. Unfortunately, the immense\namount of computations and memory accesses required for LLM training makes them\nprohibitively expensive in terms of hardware cost, and thus challenging to\ndeploy in use cases such as on-device learning. In this paper, motivated by the\nobservation that LLM training is memory-bound, we propose a novel dynamic\nquantization strategy, termed Dynamic Stashing Quantization (DSQ), that puts a\nspecial focus on reducing the memory operations, but also enjoys the other\nbenefits of low precision training, such as the reduced arithmetic cost. We\nconduct a thorough study on two translation tasks (trained-from-scratch) and\nthree classification tasks (fine-tuning). DSQ reduces the amount of arithmetic\noperations by $20.95\\times$ and the number of DRAM operations by $2.55\\times$\non IWSLT17 compared to the standard 16-bit fixed-point, which is widely used in\non-device learning.\n","authors":["Guo Yang","Daniel Lo","Robert Mullins","Yiren Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.05295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05221v1","updated":"2023-03-09T12:50:34Z","published":"2023-03-09T12:50:34Z","title":"SEAM: An Integrated Activation-Coupled Model of Sentence Processing and\n  Eye Movements in Reading","summary":"  Models of eye-movement control during reading, developed largely within\npsychology, usually focus on visual, attentional, and motor processes but\nneglect post-lexical language processing; by contrast, models of sentence\ncomprehension processes, developed largely within psycholinguistics, generally\nfocus only on post-lexical language processes. We present a model that combines\nthese two research threads, by integrating eye-movement control and sentence\nprocessing. Developing such an integrated model is extremely challenging and\ncomputationally demanding, but such an integration is an important step toward\ncomplete mathematical models of natural language comprehension in reading. We\ncombine the SWIFT model of eye-movement control (Engbert et al., Psychological\nReview, 112, 2005, pp. 777-813) with key components of the Lewis and Vasishth\nsentence processing model (Lewis and Vasishth, Cognitive Science, 29, 2005, pp.\n375-419). This integration becomes possible, for the first time, due in part to\nrecent advances in successful parameter identification in dynamical models,\nwhich allows us to investigate profile log-likelihoods for individual model\nparameters. We present a fully implemented proof-of-concept model demonstrating\nhow such an integrated model can be achieved; our approach includes Bayesian\nmodel inference with Markov Chain Monte Carlo (MCMC) sampling as a key\ncomputational tool. The integrated model, SEAM, can successfully reproduce eye\nmovement patterns that arise due to similarity-based interference in reading.\nTo our knowledge, this is the first-ever integration of a complete process\nmodel of eye-movement control with linguistic dependency completion processes\nin sentence comprehension. In future work, this proof of concept model will\nneed to be evaluated using a comprehensive set of benchmark data.\n","authors":["Maximilian M. Rabe","Dario Paape","Daniela Mertzen","Shravan Vasishth","Ralf Engbert"],"pdf_url":"https://arxiv.org/pdf/2303.05221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.02228v2","updated":"2023-03-09T12:45:10Z","published":"2023-01-05T18:55:09Z","title":"MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training","summary":"  In this paper, we consider enhancing medical visual-language pre-training\n(VLP) with domain-specific knowledge, by exploiting the paired image-text\nreports from the radiological daily practice. In particular, we make the\nfollowing contributions: First, unlike existing works that directly process the\nraw reports, we adopt a novel triplet extraction module to extract the\nmedical-related information, avoiding unnecessary complexity from language\ngrammar and enhancing the supervision signals; Second, we propose a novel\ntriplet encoding module with entity translation by querying a knowledge base,\nto exploit the rich domain knowledge in medical field, and implicitly build\nrelationships between medical entities in the language embedding space; Third,\nwe propose to use a Transformer-based fusion model for spatially aligning the\nentity description with visual signals at the image patch level, enabling the\nability for medical diagnosis; Fourth, we conduct thorough experiments to\nvalidate the effectiveness of our architecture, and benchmark on numerous\npublic benchmarks, e.g., ChestX-ray14, RSNA Pneumonia, SIIM-ACR Pneumothorax,\nCOVIDx CXR-2, COVID Rural, and EdemaSeverity. In both zero-shot and fine-tuning\nsettings, our model has demonstrated strong performance compared with the\nformer methods on disease classification and grounding.\n","authors":["Chaoyi Wu","Xiaoman Zhang","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2301.02228v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05208v1","updated":"2023-03-09T12:22:28Z","published":"2023-03-09T12:22:28Z","title":"Geometry of Language","summary":"  In this article, we present a fresh perspective on language, combining ideas\nfrom various sources, but mixed in a new synthesis. As in the minimalist\nprogram, the question is whether we can formulate an elegant formalism, a\nuniversal grammar or a mechanism which explains significant aspects of the\nhuman faculty of language, which in turn can be considered a natural\ndisposition for the evolution and deployment of the diverse human languages. We\ndescribe such a mechanism, which differs from existing logical and grammatical\napproaches by its geometric nature. Our main contribution is to explore the\nassumption that sentence recognition takes place by forming chains of tokens\nrepresenting words, followed by matching these chains with pre-existing chains\nrepresenting grammatical word orders. The aligned chains of tokens give rise to\ntwo- and three-dimensional complexes. The resulting model gives an alternative\npresentation for subtle rules, traditionally formalized using categorial\ngrammar.\n","authors":["Loe Feijs"],"pdf_url":"https://arxiv.org/pdf/2303.05208v1.pdf","comment":"17 pages, 24 figures"},{"id":"http://arxiv.org/abs/2210.15282v2","updated":"2023-03-09T11:41:34Z","published":"2022-10-27T09:31:37Z","title":"Weight Averaging: A Simple Yet Effective Method to Overcome Catastrophic\n  Forgetting in Automatic Speech Recognition","summary":"  Adapting a trained Automatic Speech Recognition (ASR) model to new tasks\nresults in catastrophic forgetting of old tasks, limiting the model's ability\nto learn continually and to be extended to new speakers, dialects, languages,\netc. Focusing on End-to-End ASR, in this paper, we propose a simple yet\neffective method to overcome catastrophic forgetting: weight averaging. By\nsimply taking the average of the previous and the adapted model, our method\nachieves high performance on both the old and new tasks. It can be further\nimproved by introducing a knowledge distillation loss during the adaptation. We\nillustrate the effectiveness of our method on both monolingual and multilingual\nASR. In both cases, our method strongly outperforms all baselines, even in its\nsimplest form.\n","authors":["Steven Vander Eeckt","Hugo Van hamme"],"pdf_url":"https://arxiv.org/pdf/2210.15282v2.pdf","comment":"Accepted at ICASSP 2023. 5 pages"},{"id":"http://arxiv.org/abs/2112.09427v4","updated":"2023-03-09T11:34:24Z","published":"2021-12-17T10:47:17Z","title":"Continual Learning for Monolingual End-to-End Automatic Speech\n  Recognition","summary":"  Adapting Automatic Speech Recognition (ASR) models to new domains results in\na deterioration of performance on the original domain(s), a phenomenon called\nCatastrophic Forgetting (CF). Even monolingual ASR models cannot be extended to\nnew accents, dialects, topics, etc. without suffering from CF, making them\nunable to be continually enhanced without storing all past data. Fortunately,\nContinual Learning (CL) methods, which aim to enable continual adaptation while\novercoming CF, can be used. In this paper, we implement an extensive number of\nCL methods for End-to-End ASR and test and compare their ability to extend a\nmonolingual Hybrid CTC-Transformer model across four new tasks. We find that\nthe best performing CL method closes the gap between the fine-tuned model\n(lower bound) and the model trained jointly on all tasks (upper bound) by more\nthan 40%, while requiring access to only 0.6% of the original data.\n","authors":["Steven Vander Eeckt","Hugo Van hamme"],"pdf_url":"https://arxiv.org/pdf/2112.09427v4.pdf","comment":"Published at EUSIPCO 2022. 5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2203.10854v2","updated":"2023-03-09T11:08:39Z","published":"2022-03-21T10:20:30Z","title":"Paraphrasing Techniques for Maritime QA system","summary":"  There has been an increasing interest in incorporating Artificial\nIntelligence (AI) into Defence and military systems to complement and augment\nhuman intelligence and capabilities. However, much work still needs to be done\ntoward achieving an effective human-machine partnership. This work is aimed at\nenhancing human-machine communications by developing a capability for\nautomatically translating human natural language into a machine-understandable\nlanguage (e.g., SQL queries). Techniques toward achieving this goal typically\ninvolve building a semantic parser trained on a very large amount of\nhigh-quality manually-annotated data. However, in many real-world Defence\nscenarios, it is not feasible to obtain such a large amount of training data.\nTo the best of our knowledge, there are few works trying to explore the\npossibility of training a semantic parser with limited manually-paraphrased\ndata, in other words, zero-shot. In this paper, we investigate how to exploit\nparaphrasing methods for the automated generation of large-scale training\ndatasets (in the form of paraphrased utterances and their corresponding logical\nforms in SQL format) and present our experimental results using real-world data\nin the maritime domain.\n","authors":["Fatemeh Shiri","Terry Yue Zhuo","Zhuang Li","Van Nguyen","Shirui Pan","Weiqing Wang","Reza Haffari","Yuan-Fang Li"],"pdf_url":"https://arxiv.org/pdf/2203.10854v2.pdf","comment":"8 pages. The first three authors contribute equally"},{"id":"http://arxiv.org/abs/2210.05556v4","updated":"2023-03-09T11:04:07Z","published":"2022-10-11T15:50:51Z","title":"ViLPAct: A Benchmark for Compositional Generalization on Multimodal\n  Human Activities","summary":"  We introduce ViLPAct, a novel vision-language benchmark for human activity\nplanning. It is designed for a task where embodied AI agents can reason and\nforecast future actions of humans based on video clips about their initial\nactivities and intents in text. The dataset consists of 2.9k videos from\n\\charades extended with intents via crowdsourcing, a multi-choice question test\nset, and four strong baselines. One of the baselines implements a neurosymbolic\napproach based on a multi-modal knowledge base (MKB), while the other ones are\ndeep generative models adapted from recent state-of-the-art (SOTA) methods.\nAccording to our extensive experiments, the key challenges are compositional\ngeneralization and effective use of information from both modalities.\n","authors":["Terry Yue Zhuo","Yaqing Liao","Yuecheng Lei","Lizhen Qu","Gerard de Melo","Xiaojun Chang","Yazhou Ren","Zenglin Xu"],"pdf_url":"https://arxiv.org/pdf/2210.05556v4.pdf","comment":"Accepted at EACL2023 (Findings)"},{"id":"http://arxiv.org/abs/2301.12868v3","updated":"2023-03-09T11:01:02Z","published":"2023-01-30T13:21:00Z","title":"On Robustness of Prompt-based Semantic Parsing with Large Pre-trained\n  Language Model: An Empirical Study on Codex","summary":"  Semantic parsing is a technique aimed at constructing a structured\nrepresentation of the meaning of a natural-language question. Recent\nadvancements in few-shot language models trained on code have demonstrated\nsuperior performance in generating these representations compared to\ntraditional unimodal language models, which are trained on downstream tasks.\nDespite these advancements, existing fine-tuned neural semantic parsers are\nsusceptible to adversarial attacks on natural-language inputs. While it has\nbeen established that the robustness of smaller semantic parsers can be\nenhanced through adversarial training, this approach is not feasible for large\nlanguage models in real-world scenarios, as it requires both substantial\ncomputational resources and expensive human annotation on in-domain semantic\nparsing data. This paper presents the first empirical study on the adversarial\nrobustness of a large prompt-based language model of code, \\codex. Our results\ndemonstrate that the state-of-the-art (SOTA) code-language models are\nvulnerable to carefully crafted adversarial examples. To address this\nchallenge, we propose methods for improving robustness without the need for\nsignificant amounts of labeled data or heavy computational resources.\n","authors":["Terry Yue Zhuo","Zhuang Li","Yujin Huang","Fatemeh Shiri","Weiqing Wang","Gholamreza Haffari","Yuan-Fang Li"],"pdf_url":"https://arxiv.org/pdf/2301.12868v3.pdf","comment":"Accepted at EACL2023 (main)"},{"id":"http://arxiv.org/abs/2303.05160v1","updated":"2023-03-09T10:33:31Z","published":"2023-03-09T10:33:31Z","title":"$π$-augmented pregroups and applications to linguistics","summary":"  We enrich pregroups with a mapping which allows us to locally apply precyclic\npermutations to designated substrings. We prove a normalisation theorem for\nsuch algebraic structures and briefly formalise some known applications of\npregroups to the analysis of clitic pronouns in certain natural languages.\n","authors":["Valentin Boboc"],"pdf_url":"https://arxiv.org/pdf/2303.05160v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2303.05153v1","updated":"2023-03-09T10:12:18Z","published":"2023-03-09T10:12:18Z","title":"Can a Frozen Pretrained Language Model be used for Zero-shot Neural\n  Retrieval on Entity-centric Questions?","summary":"  Neural document retrievers, including dense passage retrieval (DPR), have\noutperformed classical lexical-matching retrievers, such as BM25, when\nfine-tuned and tested on specific question-answering datasets. However, it has\nbeen shown that the existing dense retrievers do not generalize well not only\nout of domain but even in domain such as Wikipedia, especially when a named\nentity in a question is a dominant clue for retrieval. In this paper, we\npropose an approach toward in-domain generalization using the embeddings\ngenerated by the frozen language model trained with the entities in the domain.\nBy not fine-tuning, we explore the possibility that the rich knowledge\ncontained in a pretrained language model can be used for retrieval tasks. The\nproposed method outperforms conventional DPRs on entity-centric questions in\nWikipedia domain and achieves almost comparable performance to BM25 and\nstate-of-the-art SPAR model. We also show that the contextualized keys lead to\nstrong improvements compared to BM25 when the entity names consist of common\nwords. Our results demonstrate the feasibility of the zero-shot retrieval\nmethod for entity-centric questions of Wikipedia domain, where DPR has\nstruggled to perform.\n","authors":["Yasuto Hoshi","Daisuke Miyashita","Yasuhiro Morioka","Youyang Ng","Osamu Torii","Jun Deguchi"],"pdf_url":"https://arxiv.org/pdf/2303.05153v1.pdf","comment":"Accepted to Workshop on Knowledge Augmented Methods for Natural\n  Language Processing, in conjunction with AAAI 2023"},{"id":"http://arxiv.org/abs/2303.05143v1","updated":"2023-03-09T09:52:28Z","published":"2023-03-09T09:52:28Z","title":"ESCL: Equivariant Self-Contrastive Learning for Sentence Representations","summary":"  Previous contrastive learning methods for sentence representations often\nfocus on insensitive transformations to produce positive pairs, but neglect the\nrole of sensitive transformations that are harmful to semantic representations.\nTherefore, we propose an Equivariant Self-Contrastive Learning (ESCL) method to\nmake full use of sensitive transformations, which encourages the learned\nrepresentations to be sensitive to certain types of transformations with an\nadditional equivariant learning task. Meanwhile, in order to improve\npracticability and generality, ESCL simplifies the implementations of\ntraditional equivariant contrastive methods to share model parameters from the\nperspective of multi-task learning. We evaluate our ESCL on semantic textual\nsimilarity tasks. The proposed method achieves better results while using fewer\nlearning parameters compared to previous methods.\n","authors":["Jie Liu","Yixuan Liu","Xue Han","Chao Deng","Junlan Feng"],"pdf_url":"https://arxiv.org/pdf/2303.05143v1.pdf","comment":"accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2112.03073v2","updated":"2023-03-09T09:22:29Z","published":"2021-11-26T07:58:11Z","title":"Active Learning for Event Extraction with Memory-based Loss Prediction\n  Model","summary":"  Event extraction (EE) plays an important role in many industrial application\nscenarios, and high-quality EE methods require a large amount of manual\nannotation data to train supervised learning models. However, the cost of\nobtaining annotation data is very high, especially for annotation of domain\nevents, which requires the participation of experts from corresponding domain.\nSo we introduce active learning (AL) technology to reduce the cost of event\nannotation. But the existing AL methods have two main problems, which make them\nnot well used for event extraction. Firstly, the existing pool-based selection\nstrategies have limitations in terms of computational cost and sample validity.\nSecondly, the existing evaluation of sample importance lacks the use of local\nsample information. In this paper, we present a novel deep AL method for EE. We\npropose a batch-based selection strategy and a Memory-Based Loss Prediction\nmodel (MBLP) to select unlabeled samples efficiently. During the selection\nprocess, we use an internal-external sample loss ranking method to evaluate the\nsample importance by using local information. Finally, we propose a delayed\ntraining strategy to train the MBLP model. Extensive experiments are performed\non three domain datasets, and our method outperforms other state-of-the-art\nmethods.\n","authors":["Shirong Shen","Zhen Li","Guilin Qi"],"pdf_url":"https://arxiv.org/pdf/2112.03073v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.07126v2","updated":"2023-03-09T09:17:25Z","published":"2022-10-13T16:06:59Z","title":"Challenges in Explanation Quality Evaluation","summary":"  While much research focused on producing explanations, it is still unclear\nhow the produced explanations' quality can be evaluated in a meaningful way.\nToday's predominant approach is to quantify explanations using proxy scores\nwhich compare explanations to (human-annotated) gold explanations. This\napproach assumes that explanations which reach higher proxy scores will also\nprovide a greater benefit to human users. In this paper, we present problems of\nthis approach. Concretely, we (i) formulate desired characteristics of\nexplanation quality, (ii) describe how current evaluation practices violate\nthem, and (iii) support our argumentation with initial evidence from a\ncrowdsourcing case study in which we investigate the explanation quality of\nstate-of-the-art explainable question answering systems. We find that proxy\nscores correlate poorly with human quality ratings and, additionally, become\nless expressive the more often they are used (i.e. following Goodhart's law).\nFinally, we propose guidelines to enable a meaningful evaluation of\nexplanations to drive the development of systems that provide tangible benefits\nto human users.\n","authors":["Hendrik Schuff","Heike Adel","Peng Qi","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2210.07126v2.pdf","comment":"41 pages, 11 figures"},{"id":"http://arxiv.org/abs/2303.05093v1","updated":"2023-03-09T08:07:38Z","published":"2023-03-09T08:07:38Z","title":"Improving Video Retrieval by Adaptive Margin","summary":"  Video retrieval is becoming increasingly important owing to the rapid\nemergence of videos on the Internet. The dominant paradigm for video retrieval\nlearns video-text representations by pushing the distance between the\nsimilarity of positive pairs and that of negative pairs apart from a fixed\nmargin. However, negative pairs used for training are sampled randomly, which\nindicates that the semantics between negative pairs may be related or even\nequivalent, while most methods still enforce dissimilar representations to\ndecrease their similarity. This phenomenon leads to inaccurate supervision and\npoor performance in learning video-text representations.\n  While most video retrieval methods overlook that phenomenon, we propose an\nadaptive margin changed with the distance between positive and negative pairs\nto solve the aforementioned issue. First, we design the calculation framework\nof the adaptive margin, including the method of distance measurement and the\nfunction between the distance and the margin. Then, we explore a novel\nimplementation called \"Cross-Modal Generalized Self-Distillation\" (CMGSD),\nwhich can be built on the top of most video retrieval models with few\nmodifications. Notably, CMGSD adds few computational overheads at train time\nand adds no computational overhead at test time. Experimental results on three\nwidely used datasets demonstrate that the proposed method can yield\nsignificantly better performance than the corresponding backbone model, and it\noutperforms state-of-the-art methods by a large margin.\n","authors":["Feng He","Qi Wang","Zhifan Feng","Wenbin Jiang","Yajuan Lv","Yong zhu","Xiao Tan"],"pdf_url":"https://arxiv.org/pdf/2303.05093v1.pdf","comment":"Accepted by SIGIR 2021"},{"id":"http://arxiv.org/abs/2303.05082v1","updated":"2023-03-09T07:35:31Z","published":"2023-03-09T07:35:31Z","title":"Dynamic Multi-View Fusion Mechanism For Chinese Relation Extraction","summary":"  Recently, many studies incorporate external knowledge into character-level\nfeature based models to improve the performance of Chinese relation extraction.\nHowever, these methods tend to ignore the internal information of the Chinese\ncharacter and cannot filter out the noisy information of external knowledge. To\naddress these issues, we propose a mixture-of-view-experts framework (MoVE) to\ndynamically learn multi-view features for Chinese relation extraction. With\nboth the internal and external knowledge of Chinese characters, our framework\ncan better capture the semantic information of Chinese characters. To\ndemonstrate the effectiveness of the proposed framework, we conduct extensive\nexperiments on three real-world datasets in distinct domains. Experimental\nresults show consistent and significant superiority and robustness of our\nproposed framework. Our code and dataset will be released at:\nhttps://gitee.com/tmg-nudt/multi-view-of-expert-for-chineserelation-extraction\n","authors":["Jing Yang","Bin Ji","Shasha Li","Jun Ma","Long Peng","Jie Yu"],"pdf_url":"https://arxiv.org/pdf/2303.05082v1.pdf","comment":"This paper has been accepted by PAKDD 2023"},{"id":"http://arxiv.org/abs/2303.05080v1","updated":"2023-03-09T07:31:56Z","published":"2023-03-09T07:31:56Z","title":"Revisiting the relevance of traditional genres: a network analysis of\n  fiction readers' preferences","summary":"  We investigate how well traditional fiction genres like Fantasy, Thriller,\nand Literature represent readers' preferences. Using user data from Goodreads\nwe construct a book network where two books are strongly linked if the same\npeople tend to read or enjoy them both. We then partition this network into\ncommunities of similar books and assign each a list of subjects from The Open\nLibrary to serve as a proxy for traditional genres. Our analysis reveals that\nthe network communities correspond to existing combinations of traditional\ngenres, but that the exact communities differ depending on whether we consider\nbooks that people read or books that people enjoy.\n  In addition, we apply principal component analysis to the data and find that\nthe variance in the book communities is best explained by two factors: the\nmaturity/childishness and realism/fantastical nature of the books. We propose\nusing this maturity-realism plane as a coarse classification tool for stories.\n","authors":["Taom Sakal","Stephen Proulx"],"pdf_url":"https://arxiv.org/pdf/2303.05080v1.pdf","comment":"Supplementary materials at https://github.com/taomsakal/book-networks"},{"id":"http://arxiv.org/abs/2303.03840v2","updated":"2023-03-09T06:28:38Z","published":"2023-03-07T12:10:47Z","title":"A Challenging Benchmark for Low-Resource Learning","summary":"  With promising yet saturated results in high-resource settings, low-resource\ndatasets have gradually become popular benchmarks for evaluating the learning\nability of advanced neural networks (e.g., BigBench, superGLUE). Some models\neven surpass humans according to benchmark test results. However, we find that\nthere exists a set of hard examples in low-resource settings that challenge\nneural networks but are not well evaluated, which causes over-estimated\nperformance. We first give a theoretical analysis on which factors bring the\ndifficulty of low-resource learning. It then motivate us to propose a\nchallenging benchmark hardBench to better evaluate the learning ability, which\ncovers 11 datasets, including 3 computer vision (CV) datasets and 8 natural\nlanguage process (NLP) datasets. Experiments on a wide range of models show\nthat neural networks, even pre-trained language models, have sharp performance\ndrops on our benchmark, demonstrating the effectiveness on evaluating the\nweaknesses of neural networks. On NLP tasks, we surprisingly find that despite\nbetter results on traditional low-resource benchmarks, pre-trained networks,\ndoes not show performance improvements on our benchmarks. These results\ndemonstrate that there are still a large robustness gap between existing models\nand human-level performance.\n","authors":["Yudong Wang","Chang Ma","Qingxiu Dong","Lingpeng Kong","Jingjing Xu"],"pdf_url":"https://arxiv.org/pdf/2303.03840v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05063v1","updated":"2023-03-09T06:24:50Z","published":"2023-03-09T06:24:50Z","title":"ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for\n  Document Information Extraction","summary":"  Large language models (LLMs), such as GPT-3 and ChatGPT, have demonstrated\nremarkable results in various natural language processing (NLP) tasks with\nin-context learning, which involves inference based on a few demonstration\nexamples. Despite their successes in NLP tasks, no investigation has been\nconducted to assess the ability of LLMs to perform document information\nextraction (DIE) using in-context learning. Applying LLMs to DIE poses two\nchallenges: the modality and task gap. To this end, we propose a simple but\neffective in-context learning framework called ICL-D3IE, which enables LLMs to\nperform DIE with different types of demonstration examples. Specifically, we\nextract the most difficult and distinct segments from hard training documents\nas hard demonstrations for benefiting all test instances. We design\ndemonstrations describing relationships that enable LLMs to understand\npositional relationships. We introduce formatting demonstrations for easy\nanswer extraction. Additionally, the framework improves diverse demonstrations\nby updating them iteratively. Our experiments on three widely used benchmark\ndatasets demonstrate that the ICL-D3IE framework enables GPT-3/ChatGPT to\nachieve superior performance when compared to previous pre-trained methods\nfine-tuned with full training in both the in-distribution (ID) setting and in\nthe out-of-distribution (OOD) setting.\n","authors":["Jiabang He","Lei Wang","Yi Hu","Ning Liu","Hui Liu","Xing Xu","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2303.05063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05046v1","updated":"2023-03-09T05:50:54Z","published":"2023-03-09T05:50:54Z","title":"Unsupervised Language agnostic WER Standardization","summary":"  Word error rate (WER) is a standard metric for the evaluation of Automated\nSpeech Recognition (ASR) systems. However, WER fails to provide a fair\nevaluation of human perceived quality in presence of spelling variations,\nabbreviations, or compound words arising out of agglutination. Multiple\nspelling variations might be acceptable based on locale/geography, alternative\nabbreviations, borrowed words, and transliteration of code-mixed words from a\nforeign language to the target language script. Similarly, in case of\nagglutination, often times the agglutinated, as well as the split forms, are\nacceptable. Previous work handled this problem by using manually identified\nnormalization pairs and applying them to both the transcription and the\nhypothesis before computing WER. In this paper, we propose an automatic WER\nnormalization system consisting of two modules: spelling normalization and\nsegmentation normalization. The proposed system is unsupervised and language\nagnostic, and therefore scalable. Experiments with ASR on 35K utterances across\nfour languages yielded an average WER reduction of 13.28%. Human judgements of\nthese automatically identified normalization pairs show that our WER-normalized\nevaluation is highly consistent with the perceived quality of ASR output.\n","authors":["Satarupa Guha","Rahul Ambavat","Ankur Gupta","Manish Gupta","Rupeshkumar Mehta"],"pdf_url":"https://arxiv.org/pdf/2303.05046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.11039v2","updated":"2023-03-09T05:48:21Z","published":"2022-08-23T15:25:44Z","title":"Flat Multi-modal Interaction Transformer for Named Entity Recognition","summary":"  Multi-modal named entity recognition (MNER) aims at identifying entity spans\nand recognizing their categories in social media posts with the aid of images.\nHowever, in dominant MNER approaches, the interaction of different modalities\nis usually carried out through the alternation of self-attention and\ncross-attention or over-reliance on the gating machine, which results in\nimprecise and biased correspondence between fine-grained semantic units of text\nand image. To address this issue, we propose a Flat Multi-modal Interaction\nTransformer (FMIT) for MNER. Specifically, we first utilize noun phrases in\nsentences and general domain words to obtain visual cues. Then, we transform\nthe fine-grained semantic representation of the vision and text into a unified\nlattice structure and design a novel relative position encoding to match\ndifferent modalities in Transformer. Meanwhile, we propose to leverage entity\nboundary detection as an auxiliary task to alleviate visual bias. Experiments\nshow that our methods achieve the new state-of-the-art performance on two\nbenchmark datasets.\n","authors":["Junyu Lu","Dixiang Zhang","Pingjian Zhang"],"pdf_url":"https://arxiv.org/pdf/2208.11039v2.pdf","comment":"Accepted by COLING 2022, oral paper"},{"id":"http://arxiv.org/abs/2303.05034v1","updated":"2023-03-09T04:51:27Z","published":"2023-03-09T04:51:27Z","title":"Multi-Stage Coarse-to-Fine Contrastive Learning for Conversation Intent\n  Induction","summary":"  Intent recognition is critical for task-oriented dialogue systems. However,\nfor emerging domains and new services, it is difficult to accurately identify\nthe key intent of a conversation due to time-consuming data annotation and\ncomparatively poor model transferability. Therefore, the automatic induction of\ndialogue intention is very important for intelligent dialogue systems. This\npaper presents our solution to Track 2 of Intent Induction from Conversations\nfor Task-Oriented Dialogue at the Eleventh Dialogue System Technology Challenge\n(DSTC11). The essence of intention clustering lies in distinguishing the\nrepresentation of different dialogue utterances. The key to automatic intention\ninduction is that, for any given set of new data, the sentence representation\nobtained by the model can be well distinguished from different labels.\nTherefore, we propose a multi-stage coarse-to-fine contrastive learning model\ntraining scheme including unsupervised contrastive learning pre-training,\nsupervised contrastive learning pre-training, and fine-tuning with joint\ncontrastive learning and clustering to obtain a better dialogue utterance\nrepresentation model for the clustering task. In the released DSTC11 Track 2\nevaluation results, our proposed system ranked first on both of the two\nsubtasks of this Track.\n","authors":["Caiyuan Chu","Ya Li","Yifan Liu","Jia-Chen Gu","Quan Liu","Yongxin Ge","Guoping Hu"],"pdf_url":"https://arxiv.org/pdf/2303.05034v1.pdf","comment":"Ranked 1st on Track 2 at DSTC 11, Accepted by DSTC 11 Workshop"},{"id":"http://arxiv.org/abs/2303.07199v1","updated":"2023-03-09T03:30:52Z","published":"2023-03-09T03:30:52Z","title":"BeamAttack: Generating High-quality Textual Adversarial Examples through\n  Beam Search and Mixed Semantic Spaces","summary":"  Natural language processing models based on neural networks are vulnerable to\nadversarial examples. These adversarial examples are imperceptible to human\nreaders but can mislead models to make the wrong predictions. In a black-box\nsetting, attacker can fool the model without knowing model's parameters and\narchitecture. Previous works on word-level attacks widely use single semantic\nspace and greedy search as a search strategy. However, these methods fail to\nbalance the attack success rate, quality of adversarial examples and time\nconsumption. In this paper, we propose BeamAttack, a textual attack algorithm\nthat makes use of mixed semantic spaces and improved beam search to craft\nhigh-quality adversarial examples. Extensive experiments demonstrate that\nBeamAttack can improve attack success rate while saving numerous queries and\ntime, e.g., improving at most 7\\% attack success rate than greedy search when\nattacking the examples from MR dataset. Compared with heuristic search,\nBeamAttack can save at most 85\\% model queries and achieve a competitive attack\nsuccess rate. The adversarial examples crafted by BeamAttack are highly\ntransferable and can effectively improve model's robustness during adversarial\ntraining. Code is available at\nhttps://github.com/zhuhai-ustc/beamattack/tree/master\n","authors":["Hai Zhu","Qingyang Zhao","Yuren Wu"],"pdf_url":"https://arxiv.org/pdf/2303.07199v1.pdf","comment":"PAKDD2023"},{"id":"http://arxiv.org/abs/2303.04729v2","updated":"2023-03-09T02:40:44Z","published":"2023-03-08T17:15:58Z","title":"On the Risks of Stealing the Decoding Algorithms of Language Models","summary":"  A key component of generating text from modern language models (LM) is the\nselection and tuning of decoding algorithms. These algorithms determine how to\ngenerate text from the internal probability distribution generated by the LM.\nThe process of choosing a decoding algorithm and tuning its hyperparameters\ntakes significant time, manual effort, and computation, and it also requires\nextensive human evaluation. Therefore, the identity and hyperparameters of such\ndecoding algorithms are considered to be extremely valuable to their owners. In\nthis work, we show, for the first time, that an adversary with typical API\naccess to an LM can steal the type and hyperparameters of its decoding\nalgorithms at very low monetary costs. Our attack is effective against popular\nLMs used in text generation APIs, including GPT-2 and GPT-3. We demonstrate the\nfeasibility of stealing such information with only a few dollars, e.g.,\n$\\$0.8$, $\\$1$, $\\$4$, and $\\$40$ for the four versions of GPT-3.\n","authors":["Ali Naseh","Kalpesh Krishna","Mohit Iyyer","Amir Houmansadr"],"pdf_url":"https://arxiv.org/pdf/2303.04729v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10447v3","updated":"2023-03-09T01:38:38Z","published":"2023-02-21T05:24:00Z","title":"Mask-guided BERT for Few Shot Text Classification","summary":"  Transformer-based language models have achieved significant success in\nvarious domains. However, the data-intensive nature of the transformer\narchitecture requires much labeled data, which is challenging in low-resource\nscenarios (i.e., few-shot learning (FSL)). The main challenge of FSL is the\ndifficulty of training robust models on small amounts of samples, which\nfrequently leads to overfitting. Here we present Mask-BERT, a simple and\nmodular framework to help BERT-based architectures tackle FSL. The proposed\napproach fundamentally differs from existing FSL strategies such as prompt\ntuning and meta-learning. The core idea is to selectively apply masks on text\ninputs and filter out irrelevant information, which guides the model to focus\non discriminative tokens that influence prediction results. In addition, to\nmake the text representations from different categories more separable and the\ntext representations from the same category more compact, we introduce a\ncontrastive learning loss function. Experimental results on public-domain\nbenchmark datasets demonstrate the effectiveness of Mask-BERT.\n","authors":["Wenxiong Liao","Zhengliang Liu","Haixing Dai","Zihao Wu","Yiyang Zhang","Xiaoke Huang","Yuzhong Chen","Xi Jiang","Wei Liu","Dajiang Zhu","Tianming Liu","Sheng Li","Xiang Li","Hongmin Cai"],"pdf_url":"https://arxiv.org/pdf/2302.10447v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04953v1","updated":"2023-03-09T00:10:29Z","published":"2023-03-09T00:10:29Z","title":"Let's Get Personal: Personal Questions Improve SocialBot Performance in\n  the Alexa Prize","summary":"  There has been an increased focus on creating conversational open-domain\ndialogue systems in the spoken dialogue community. Unlike traditional dialogue\nsystems, these conversational systems cannot assume any specific information\nneed or domain restrictions, i.e., the only inherent goal is to converse with\nthe user on an unknown set of topics. While massive improvements in Natural\nLanguage Understanding (NLU) and the growth of available knowledge resources\ncan partially support a robust conversation, these conversations generally lack\nthe rapport between two humans that know each other. We developed a robust\nopen-domain conversational system, Athena, that real Amazon Echo users access\nand evaluate at scale in the context of the Alexa Prize competition. We\nexperiment with methods intended to increase intimacy between Athena and the\nuser by heuristically developing a rule-based user model that personalizes both\nthe current and subsequent conversations and evaluating specific personal\nopinion question strategies in A/B studies. Our results show a statistically\nsignificant positive impact on perceived conversation quality and length when\nemploying these strategies.\n","authors":["Kevin K. Bowden","Marilyn Walker"],"pdf_url":"https://arxiv.org/pdf/2303.04953v1.pdf","comment":"Won Best Paper at IWSDS '23"}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.05322v1","updated":"2023-03-09T15:13:46Z","published":"2023-03-09T15:13:46Z","title":"Improving Few-Shot Learning for Talking Face System with TTS Data\n  Augmentation","summary":"  Audio-driven talking face has attracted broad interest from academia and\nindustry recently. However, data acquisition and labeling in audio-driven\ntalking face are labor-intensive and costly. The lack of data resource results\nin poor synthesis effect. To alleviate this issue, we propose to use TTS\n(Text-To-Speech) for data augmentation to improve few-shot ability of the\ntalking face system. The misalignment problem brought by the TTS audio is\nsolved with the introduction of soft-DTW, which is first adopted in the talking\nface task. Moreover, features extracted by HuBERT are explored to utilize\nunderlying information of audio, and found to be superior over other features.\nThe proposed method achieves 17%, 14%, 38% dominance on MSE score, DTW score\nand user study preference repectively over the baseline model, which shows the\neffectiveness of improving few-shot learning for talking face system with TTS\naugmentation.\n","authors":["Qi Chen","Ziyang Ma","Tao Liu","Xu Tan","Qu Lu","Xie Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2303.05322v1.pdf","comment":"4 pages. Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.05007v1","updated":"2023-03-09T03:16:04Z","published":"2023-03-09T03:16:04Z","title":"Towards Robust Image-in-Audio Deep Steganography","summary":"  The field of steganography has experienced a surge of interest due to the\nrecent advancements in AI-powered techniques, particularly in the context of\nmultimodal setups that enable the concealment of signals within signals of a\ndifferent nature. The primary objectives of all steganographic methods are to\nachieve perceptual transparency, robustness, and large embedding capacity -\nwhich often present conflicting goals that classical methods have struggled to\nreconcile. This paper extends and enhances an existing image-in-audio deep\nsteganography method by focusing on improving its robustness. The proposed\nenhancements include modifications to the loss function, utilization of the\nShort-Time Fourier Transform (STFT), introduction of redundancy in the encoding\nprocess for error correction, and buffering of additional information in the\npixel subconvolution operation. The results demonstrate that our approach\noutperforms the existing method in terms of robustness and perceptual\ntransparency.\n","authors":["Jaume Ros Alonso","Margarita Geleta","Jordi Pons","Xavier Giro-i-Nieto"],"pdf_url":"https://arxiv.org/pdf/2303.05007v1.pdf","comment":"8 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2303.04027v2","updated":"2023-03-09T02:58:24Z","published":"2023-03-07T16:39:09Z","title":"BIRD-PCC: Bi-directional Range Image-based Deep LiDAR Point Cloud\n  Compression","summary":"  The large amount of data collected by LiDAR sensors brings the issue of LiDAR\npoint cloud compression (PCC). Previous works on LiDAR PCC have used range\nimage representations and followed the predictive coding paradigm to create a\nbasic prototype of a coding framework. However, their prediction methods give\nan inaccurate result due to the negligence of invalid pixels in range images\nand the omission of future frames in the time step. Moreover, their handcrafted\ndesign of residual coding methods could not fully exploit spatial redundancy.\nTo remedy this, we propose a coding framework BIRD-PCC. Our prediction module\nis aware of the coordinates of invalid pixels in range images and takes a\nbidirectional scheme. Also, we introduce a deep-learned residual coding module\nthat can further exploit spatial redundancy within a residual frame.\nExperiments conducted on SemanticKITTI and KITTI-360 datasets show that\nBIRD-PCC outperforms other methods in most bitrate conditions and generalizes\nwell to unseen environments.\n","authors":["Chia-Sheng Liu","Jia-Fong Yeh","Hao Hsu","Hung-Ting Su","Ming-Sui Lee","Winston H. Hsu"],"pdf_url":"https://arxiv.org/pdf/2303.04027v2.pdf","comment":"Accepted to ICASSP 2023"}]},"2023-03-08T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.04766v1","updated":"2023-03-08T18:03:51Z","published":"2023-03-08T18:03:51Z","title":"FastFill: Efficient Compatible Model Update","summary":"  In many retrieval systems the original high dimensional data (e.g., images)\nis mapped to a lower dimensional feature through a learned embedding model. The\ntask of retrieving the most similar data from a gallery set to a given query\ndata is performed through a similarity comparison on features. When the\nembedding model is updated, it might produce features that are not\ncomparable/compatible with features already in the gallery computed with the\nold model. Subsequently, all features in the gallery need to be re-computed\nusing the new embedding model -- a computationally expensive process called\nbackfilling. Recently, compatible representation learning methods have been\nproposed to avoid backfilling. Despite their relative success, there is an\ninherent trade-off between the new model performance and its compatibility with\nthe old model. In this work, we introduce FastFill: a compatible model update\nprocess using feature alignment and policy based partial backfilling to\npromptly elevate retrieval performance. We show that previous backfilling\nstrategies suffer from decreased performance and demonstrate the importance of\nboth the training objective and the ordering in online partial backfilling. We\npropose a new training method for feature alignment between old and new\nembedding models using uncertainty estimation. Compared to previous works, we\nobtain significantly improved backfilling results on a variety of datasets: mAP\non ImageNet (+4.4\\%), Places-365 (+2.7\\%), and VGG-Face2 (+1.3\\%). Further, we\ndemonstrate that when updating a biased model with FastFill, the minority\nsubgroup accuracy gap promptly vanishes with a small fraction of partial\nbackfilling.\n","authors":["Florian Jaeckle","Fartash Faghri","Ali Farhadi","Oncel Tuzel","Hadi Pouransari"],"pdf_url":"https://arxiv.org/pdf/2303.04766v1.pdf","comment":"To appear in The Eleventh International Conference on Learning\n  Representations"},{"id":"http://arxiv.org/abs/2207.08087v4","updated":"2023-03-08T15:06:11Z","published":"2022-07-17T06:50:35Z","title":"Automatic Context Pattern Generation for Entity Set Expansion","summary":"  Entity Set Expansion (ESE) is a valuable task that aims to find entities of\nthe target semantic class described by given seed entities. Various Natural\nLanguage Processing (NLP) and Information Retrieval (IR) downstream\napplications have benefited from ESE due to its ability to discover knowledge.\nAlthough existing corpus-based ESE methods have achieved great progress, they\nstill rely on corpora with high-quality entity information annotated, because\nmost of them need to obtain the context patterns through the position of the\nentity in a sentence. Therefore, the quality of the given corpora and their\nentity annotation has become the bottleneck that limits the performance of such\nmethods. To overcome this dilemma and make the ESE models free from the\ndependence on entity annotation, our work aims to explore a new ESE paradigm,\nnamely corpus-independent ESE. Specifically, we devise a context pattern\ngeneration module that utilizes autoregressive language models (e.g., GPT-2) to\nautomatically generate high-quality context patterns for entities. In addition,\nwe propose the GAPA, a novel ESE framework that leverages the aforementioned\nGenerAted PAtterns to expand target entities. Extensive experiments and\ndetailed analyses on three widely used datasets demonstrate the effectiveness\nof our method. All the codes of our experiments are available at\nhttps://github.com/geekjuruo/GAPA.\n","authors":["Yinghui Li","Shulin Huang","Xinwei Zhang","Qingyu Zhou","Yangning Li","Ruiyang Liu","Yunbo Cao","Hai-Tao Zheng","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2207.08087v4.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2303.04587v1","updated":"2023-03-08T13:59:41Z","published":"2023-03-08T13:59:41Z","title":"A Prompt Log Analysis of Text-to-Image Generation Systems","summary":"  Recent developments in diffusion models have unleashed the astonishing\ncapabilities of text-to-image generation systems to synthesize high-quality\nimages that are faithful to a given reference text, known as a \"prompt.\" These\nsystems, once released to the public, have immediately received tons of\nattention from researchers, creators, and common users. Despite the plenty of\nefforts to improve the underneath generative models, there is limited work on\nunderstanding the information needs of the real users of these systems, e.g.,\nby investigating the prompts the users input at scale. In this paper, we take\nthe initiative to conduct a comprehensive analysis of large-scale prompt logs\ncollected from multiple text-to-image generation systems. Our work is analogous\nto analyzing the query log of Web search engines, a line of work that has made\ncritical contributions to the glory of the Web search industry and research. We\nanalyze over two million user-input prompts submitted to three popular\ntext-to-image systems at scale. Compared to Web search queries, text-to-image\nprompts are significantly longer, often organized into unique structures, and\npresent different categories of information needs. Users tend to make more\nedits within creation sessions, showing remarkable exploratory patterns. Our\nfindings provide concrete implications on how to improve text-to-image\ngeneration systems for creation purposes.\n","authors":["Yutong Xie","Zhaoying Pan","Jinge Ma","Jie Luo","Qiaozhu Mei"],"pdf_url":"https://arxiv.org/pdf/2303.04587v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.11879v4","updated":"2023-03-08T13:45:55Z","published":"2022-10-21T11:08:10Z","title":"GLCC: A General Framework for Graph-Level Clustering","summary":"  This paper studies the problem of graph-level clustering, which is a novel\nyet challenging task. This problem is critical in a variety of real-world\napplications such as protein clustering and genome analysis in bioinformatics.\nRecent years have witnessed the success of deep clustering coupled with graph\nneural networks (GNNs). However, existing methods focus on clustering among\nnodes given a single graph, while exploring clustering on multiple graphs is\nstill under-explored. In this paper, we propose a general graph-level\nclustering framework named Graph-Level Contrastive Clustering (GLCC) given\nmultiple graphs. Specifically, GLCC first constructs an adaptive affinity graph\nto explore instance- and cluster-level contrastive learning (CL).\nInstance-level CL leverages graph Laplacian based contrastive loss to learn\nclustering-friendly representations while cluster-level CL captures\ndiscriminative cluster representations incorporating neighbor information of\neach sample. Moreover, we utilize neighbor-aware pseudo-labels to reward the\noptimization of representation learning. The two steps can be alternatively\ntrained to collaborate and benefit each other. Experiments on a range of\nwell-known datasets demonstrate the superiority of our proposed GLCC over\ncompetitive baselines.\n","authors":["Wei Ju","Yiyang Gu","Binqi Chen","Gongbo Sun","Yifang Qin","Xingyuming Liu","Xiao Luo","Ming Zhang"],"pdf_url":"https://arxiv.org/pdf/2210.11879v4.pdf","comment":"Accepted by Proceedings of the AAAI Conference on Artificial\n  Intelligence (AAAI 2023)"},{"id":"http://arxiv.org/abs/2303.04561v1","updated":"2023-03-08T13:20:53Z","published":"2023-03-08T13:20:53Z","title":"Kernel-CF: Collaborative filtering done right with social network\n  analysis and kernel smoothing","summary":"  Collaborative filtering is the simplest but oldest machine learning algorithm\nin the field of recommender systems. In spite of its long history, it remains a\ndiscussion topic in research venues. Usually people use users/items whose\nsimilarity scores with the target customer greater than 0 to compute the\nalgorithms. However, this might not be the optimal solution after careful\nscrutiny. In this paper, we transform the recommender system input data into a\n2-D social network, and apply kernel smoothing to compute preferences for\nunknown values in the user item rating matrix. We unifies the theoretical\nframework of recommender system and non-parametric statistics and provides an\nalgorithmic procedure with optimal parameter selection method to achieve the\ngoal.\n","authors":["Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2303.04561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04532v1","updated":"2023-03-08T12:04:16Z","published":"2023-03-08T12:04:16Z","title":"Class Cardinality Comparison as a Fermi Problem","summary":"  Questions on class cardinality comparisons are quite tricky to answer and\ncome with its own challenges. They require some kind of reasoning since web\ndocuments and knowledge bases, indispensable sources of information, rarely\nstore direct answers to questions, such as, ``Are there more astronauts or\nPhysics Nobel Laureates?'' We tackle questions on class cardinality comparison\nby tapping into three sources for absolute cardinalities as well as the\ncardinalities of orthogonal subgroups of the classes. We propose novel\ntechniques for aggregating signals with partial coverage for more reliable\nestimates and evaluate them on a dataset of 4005 class pairs, achieving an\naccuracy of 83.7%.\n","authors":["Shrestha Ghosh","Simon Razniewski","Gerhard Weikum"],"pdf_url":"https://arxiv.org/pdf/2303.04532v1.pdf","comment":"Accepted to the Web Conference 2023"},{"id":"http://arxiv.org/abs/2302.13053v2","updated":"2023-03-08T04:13:48Z","published":"2023-02-25T10:42:34Z","title":"RETEXO: Scalable Neural Network Training over Distributed Graphs","summary":"  Graph neural networks offer a promising approach to supervised learning over\ngraph data. Graph data, especially when it is privacy-sensitive or too large to\ntrain on centrally, is often stored partitioned across disparate processing\nunits (clients) which want to minimize the communication costs during\ncollaborative training. The fully-distributed setup takes such partitioning to\nits extreme, wherein features of only a single node and its adjacent edges are\nkept locally with one client processor. Existing GNNs are not architected for\ntraining in such setups and incur prohibitive costs therein. We propose RETEXO,\na novel transformation of existing GNNs that improves the communication\nefficiency during training in the fully-distributed setup. We experimentally\nconfirm that RETEXO offers up to 6 orders of magnitude better communication\nefficiency even when training shallow GNNs, with a minimal trade-off in\naccuracy for supervised node classification tasks.\n","authors":["Aashish Kolluri","Sarthak Choudhary","Bryan Hooi","Prateek Saxena"],"pdf_url":"https://arxiv.org/pdf/2302.13053v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.13795v2","updated":"2023-03-08T02:51:32Z","published":"2022-10-25T06:57:00Z","title":"Line Graph Contrastive Learning for Link Prediction","summary":"  Link prediction tasks focus on predicting possible future connections. Most\nexisting researches measure the likelihood of links by different similarity\nscores on node pairs and predict links between nodes. However, the\nsimilarity-based approaches have some challenges in information loss on nodes\nand generalization ability on similarity indexes. To address the above issues,\nwe propose a Line Graph Contrastive Learning(LGCL) method to obtain rich\ninformation with multiple perspectives. LGCL obtains a subgraph view by h-hop\nsubgraph sampling with target node pairs. After transforming the sampled\nsubgraph into a line graph, the link prediction task is converted into a node\nclassification task, which graph convolution progress can learn edge embeddings\nfrom graphs more effectively. Then we design a novel cross-scale contrastive\nlearning framework on the line graph and the subgraph to maximize the mutual\ninformation of them, so that fuses the structure and feature information. The\nexperimental results demonstrate that the proposed LGCL outperforms the\nstate-of-the-art methods and has better performance on generalization and\nrobustness.\n","authors":["Zehua Zhang","Shilin Sun","Guixiang Ma","Caiming Zhong"],"pdf_url":"https://arxiv.org/pdf/2210.13795v2.pdf","comment":"37 pages"},{"id":"http://arxiv.org/abs/2303.04335v1","updated":"2023-03-08T02:14:08Z","published":"2023-03-08T02:14:08Z","title":"Unbiased Learning to Rank with Biased Continuous Feedback","summary":"  It is a well-known challenge to learn an unbiased ranker with biased\nfeedback. Unbiased learning-to-rank(LTR) algorithms, which are verified to\nmodel the relative relevance accurately based on noisy feedback, are appealing\ncandidates and have already been applied in many applications with single\ncategorical labels, such as user click signals. Nevertheless, the existing\nunbiased LTR methods cannot properly handle continuous feedback, which are\nessential for many industrial applications, such as content recommender\nsystems.\n  To provide personalized high-quality recommendation results, recommender\nsystems need model both categorical and continuous biased feedback, such as\nclick and dwell time. Accordingly, we design a novel unbiased LTR algorithm to\ntackle the challenges, which innovatively models position bias in the pairwise\nfashion and introduces the pairwise trust bias to separate the position bias,\ntrust bias, and user relevance explicitly and can work for both continuous and\ncategorical feedback. Experiment results on public benchmark datasets and\ninternal live traffic of a large-scale recommender system at Tencent News show\nsuperior results for continuous labels and also competitive performance for\ncategorical labels of the proposed method.\n","authors":["Yi Ren","Hongyan Tang","Siwen Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.04335v1.pdf","comment":"10 pages. arXiv admin note: substantial text overlap with\n  arXiv:2111.12929"}],"Computation and Language":[{"id":"http://arxiv.org/abs/2302.12813v3","updated":"2023-03-08T23:41:49Z","published":"2023-02-24T18:48:43Z","title":"Check Your Facts and Try Again: Improving Large Language Models with\n  External Knowledge and Automated Feedback","summary":"  Large language models (LLMs), such as ChatGPT, are able to generate\nhuman-like, fluent responses for many downstream tasks, e.g., task-oriented\ndialog and question answering. However, applying LLMs to real-world,\nmission-critical applications remains challenging mainly due to their tendency\nto generate hallucinations and their inability to use external knowledge. This\npaper proposes a LLM-Augmenter system, which augments a black-box LLM with a\nset of plug-and-play modules. Our system makes the LLM generate responses\ngrounded in external knowledge, e.g., stored in task-specific databases. It\nalso iteratively revises LLM prompts to improve model responses using feedback\ngenerated by utility functions, e.g., the factuality score of a LLM-generated\nresponse. The effectiveness of LLM-Augmenter is empirically validated on two\ntypes of scenarios, task-oriented dialog and open-domain question answering.\nLLM-Augmenter significantly reduces ChatGPT's hallucinations without\nsacrificing the fluency and informativeness of its responses. We make the\nsource code and models publicly available.\n","authors":["Baolin Peng","Michel Galley","Pengcheng He","Hao Cheng","Yujia Xie","Yu Hu","Qiuyuan Huang","Lars Liden","Zhou Yu","Weizhu Chen","Jianfeng Gao"],"pdf_url":"https://arxiv.org/pdf/2302.12813v3.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2211.08451v3","updated":"2023-03-08T20:50:27Z","published":"2022-11-15T19:04:13Z","title":"kogito: A Commonsense Knowledge Inference Toolkit","summary":"  In this paper, we present kogito, an open-source tool for generating\ncommonsense inferences about situations described in text. kogito provides an\nintuitive and extensible interface to interact with natural language generation\nmodels that can be used for hypothesizing commonsense knowledge inference from\na textual input. In particular, kogito offers several features for targeted,\nmulti-granularity knowledge generation. These include a standardized API for\ntraining and evaluating knowledge models, and generating and filtering\ninferences from them. We also include helper functions for converting natural\nlanguage texts into a format ingestible by knowledge models - intermediate\npipeline stages such as knowledge head extraction from text, heuristic and\nmodel-based knowledge head-relation matching, and an ability to define and use\ncustom knowledge relations. We make the code for kogito available at\nhttps://github.com/epfl-nlp/kogito along with thorough documentation at\nhttps://kogito.readthedocs.io.\n","authors":["Mete Ismayilzada","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2211.08451v3.pdf","comment":"EACL 2023 Camera ready, 9 pages"},{"id":"http://arxiv.org/abs/2303.07316v1","updated":"2023-03-08T20:45:37Z","published":"2023-03-08T20:45:37Z","title":"FaceChat: An Emotion-Aware Face-to-face Dialogue Framework","summary":"  While current dialogue systems like ChatGPT have made significant\nadvancements in text-based interactions, they often overlook the potential of\nother modalities in enhancing the overall user experience. We present FaceChat,\na web-based dialogue framework that enables emotionally-sensitive and\nface-to-face conversations. By seamlessly integrating cutting-edge technologies\nin natural language processing, computer vision, and speech processing,\nFaceChat delivers a highly immersive and engaging user experience. FaceChat\nframework has a wide range of potential applications, including counseling,\nemotional support, and personalized customer service. The system is designed to\nbe simple and flexible as a platform for future researchers to advance the\nfield of multimodal dialogue systems. The code is publicly available at\nhttps://github.com/qywu/FaceChat.\n","authors":["Deema Alnuhait","Qingyang Wu","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2303.07316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.06024v2","updated":"2023-03-08T20:31:58Z","published":"2023-01-15T06:25:50Z","title":"A data science and machine learning approach to continuous analysis of\n  Shakespeare's plays","summary":"  The availability of quantitative methods that can analyze text has provided\nnew ways of examining literature in a manner that was not available in the\npre-information era. Here we apply comprehensive machine learning analysis to\nthe work of William Shakespeare. The analysis shows clear change in style of\nwriting over time, with the most significant changes in the sentence length,\nfrequency of adjectives and adverbs, and the sentiments expressed in the text.\nApplying machine learning to make a stylometric prediction of the year of the\nplay shows a Pearson correlation of 0.71 between the actual and predicted year,\nindicating that Shakespeare's writing style as reflected by the quantitative\nmeasurements changed over time. Additionally, it shows that the stylometrics of\nsome of the plays is more similar to plays written either before or after the\nyear they were written. For instance, Romeo and Juliet is dated 1596, but is\nmore similar in stylometrics to plays written by Shakespeare after 1600. The\nsource code for the analysis is available for free download.\n","authors":["Charles Swisher","Lior Shamir"],"pdf_url":"https://arxiv.org/pdf/2301.06024v2.pdf","comment":"Journal of Data Mining and Digital Humanities, accepted"},{"id":"http://arxiv.org/abs/2303.04851v1","updated":"2023-03-08T19:35:08Z","published":"2023-03-08T19:35:08Z","title":"Lexical Complexity Prediction: An Overview","summary":"  The occurrence of unknown words in texts significantly hinders reading\ncomprehension. To improve accessibility for specific target populations,\ncomputational modelling has been applied to identify complex words in texts and\nsubstitute them for simpler alternatives. In this paper, we present an overview\nof computational approaches to lexical complexity prediction focusing on the\nwork carried out on English data. We survey relevant approaches to this problem\nwhich include traditional machine learning classifiers (e.g. SVMs, logistic\nregression) and deep neural networks as well as a variety of features, such as\nthose inspired by literature in psycholinguistics as well as word frequency,\nword length, and many others. Furthermore, we introduce readers to past\ncompetitions and available datasets created on this topic. Finally, we include\nbrief sections on applications of lexical complexity prediction, such as\nreadability and text simplification, together with related studies on languages\nother than English.\n","authors":["Kai North","Marcos Zampieri","Matthew Shardlow"],"pdf_url":"https://arxiv.org/pdf/2303.04851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04838v1","updated":"2023-03-08T19:17:05Z","published":"2023-03-08T19:17:05Z","title":"The Casual Conversations v2 Dataset","summary":"  This paper introduces a new large consent-driven dataset aimed at assisting\nin the evaluation of algorithmic bias and robustness of computer vision and\naudio speech models in regards to 11 attributes that are self-provided or\nlabeled by trained annotators. The dataset includes 26,467 videos of 5,567\nunique paid participants, with an average of almost 5 videos per person,\nrecorded in Brazil, India, Indonesia, Mexico, Vietnam, Philippines, and the\nUSA, representing diverse demographic characteristics. The participants agreed\nfor their data to be used in assessing fairness of AI models and provided\nself-reported age, gender, language/dialect, disability status, physical\nadornments, physical attributes and geo-location information, while trained\nannotators labeled apparent skin tone using the Fitzpatrick Skin Type and Monk\nSkin Tone scales, and voice timbre. Annotators also labeled for different\nrecording setups and per-second activity annotations.\n","authors":["Bilal Porgali","Vítor Albiero","Jordan Ryda","Cristian Canton Ferrer","Caner Hazirbas"],"pdf_url":"https://arxiv.org/pdf/2303.04838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05431v1","updated":"2023-03-08T18:58:52Z","published":"2023-03-08T18:58:52Z","title":"disco: a toolkit for Distributional Control of Generative Models","summary":"  Pre-trained language models and other generative models have revolutionized\nNLP and beyond. However, these models tend to reproduce undesirable biases\npresent in their training data. Also, they may overlook patterns that are\nimportant but challenging to capture. To address these limitations, researchers\nhave introduced distributional control techniques. These techniques, not\nlimited to language, allow controlling the prevalence (i.e., expectations) of\nany features of interest in the model's outputs. Despite their potential, the\nwidespread adoption of these techniques has been hindered by the difficulty in\nadapting complex, disconnected code. Here, we present disco, an open-source\nPython library that brings these techniques to the broader public.\n","authors":["Germán Kruszewski","Jos Rozen","Marc Dymetman"],"pdf_url":"https://arxiv.org/pdf/2303.05431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04794v1","updated":"2023-03-08T18:43:39Z","published":"2023-03-08T18:43:39Z","title":"Comprehensive Event Representations using Event Knowledge Graphs and\n  Natural Language Processing","summary":"  Recent work has utilised knowledge-aware approaches to natural language\nunderstanding, question answering, recommendation systems, and other tasks.\nThese approaches rely on well-constructed and large-scale knowledge graphs that\ncan be useful for many downstream applications and empower knowledge-aware\nmodels with commonsense reasoning. Such knowledge graphs are constructed\nthrough knowledge acquisition tasks such as relation extraction and knowledge\ngraph completion. This work seeks to utilise and build on the growing body of\nwork that uses findings from the field of natural language processing (NLP) to\nextract knowledge from text and build knowledge graphs. The focus of this\nresearch project is on how we can use transformer-based approaches to extract\nand contextualise event information, matching it to existing ontologies, to\nbuild a comprehensive knowledge of graph-based event representations.\nSpecifically, sub-event extraction is used as a way of creating sub-event-aware\nevent representations. These event representations are then further enriched\nthrough fine-grained location extraction and contextualised through the\nalignment of historically relevant quotes.\n","authors":["Tin Kuculo"],"pdf_url":"https://arxiv.org/pdf/2303.04794v1.pdf","comment":"This is the author's version of the work. It is posted here for your\n  personal use. Not for redistribution. The definitive Version of Record was\n  published in Companion Proceedings of the Web Conference 2022"},{"id":"http://arxiv.org/abs/2303.04715v1","updated":"2023-03-08T16:53:19Z","published":"2023-03-08T16:53:19Z","title":"Extending the Pre-Training of BLOOM for Improved Support of Traditional\n  Chinese: Models, Methods and Results","summary":"  In this paper we present the multilingual language model BLOOM-zh that\nfeatures enhanced support for Traditional Chinese. BLOOM-zh has its origins in\nthe open-source BLOOM models presented by BigScience in 2022. Starting from\nreleased models, we extended the pre-training of BLOOM by additional 7.4\nbillion tokens in Traditional Chinese and English covering a variety of domains\nsuch as news articles, books, encyclopedias, educational materials as well as\nspoken language. In order to show the properties of BLOOM-zh, both existing and\nnewly created benchmark scenarios are used for evaluating the performance.\nBLOOM-zh outperforms its predecessor on most Traditional Chinese benchmarks\nwhile maintaining its English capability. We release all our models to the\nresearch community.\n","authors":["Philipp Ennen","Po-Chun Hsu","Chan-Jan Hsu","Chang-Le Liu","Yen-Chen Wu","Yin-Hsiang Liao","Chin-Tung Lin","Da-Shan Shiu","Wei-Yun Ma"],"pdf_url":"https://arxiv.org/pdf/2303.04715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.17517v2","updated":"2023-03-08T16:47:46Z","published":"2022-10-31T17:41:26Z","title":"Lila: A Unified Benchmark for Mathematical Reasoning","summary":"  Mathematical reasoning skills are essential for general-purpose intelligent\nsystems to perform tasks from grocery shopping to climate modeling. Towards\nevaluating and improving AI systems in this domain, we propose LILA, a unified\nmathematical reasoning benchmark consisting of 23 diverse tasks along four\ndimensions: (i) mathematical abilities e.g., arithmetic, calculus (ii) language\nformat e.g., question-answering, fill-in-the-blanks (iii) language diversity\ne.g., no language, simple language (iv) external knowledge e.g., commonsense,\nphysics. We construct our benchmark by extending 20 datasets benchmark by\ncollecting task instructions and solutions in the form of Python programs,\nthereby obtaining explainable solutions in addition to the correct answer. We\nadditionally introduce two evaluation datasets to measure out-of-distribution\nperformance and robustness to language perturbation. Finally, we introduce\nBHASKARA, a general-purpose mathematical reasoning model trained on LILA.\nImportantly, we find that multi-tasking leads to significant improvements\n(average relative improvement of 21.83% F1 score vs. single-task models), while\nthe best performing model only obtains 60.40%, indicating the room for\nimprovement in general mathematical reasoning and understanding.\n","authors":["Swaroop Mishra","Matthew Finlayson","Pan Lu","Leonard Tang","Sean Welleck","Chitta Baral","Tanmay Rajpurohit","Oyvind Tafjord","Ashish Sabharwal","Peter Clark","Ashwin Kalyan"],"pdf_url":"https://arxiv.org/pdf/2210.17517v2.pdf","comment":"EMNLP 2022"},{"id":"http://arxiv.org/abs/2303.04691v1","updated":"2023-03-08T16:32:10Z","published":"2023-03-08T16:32:10Z","title":"Self-contained Beta-with-Spikes Approximation for Inference Under a\n  Wright-Fisher Model","summary":"  We construct a reliable estimation of evolutionary parameters within the\nWright-Fisher model, which describes changes in allele frequencies due to\nselection and genetic drift, from time-series data. Such data exists for\nbiological populations, for example via artificial evolution experiments, and\nfor the cultural evolution of behavior, such as linguistic corpora that\ndocument historical usage of different words with similar meanings. Our method\nof analysis builds on a Beta-with-Spikes approximation to the distribution of\nallele frequencies predicted by the Wright-Fisher model. We introduce a\nself-contained scheme for estimating the parameters in the approximation, and\ndemonstrate its robustness with synthetic data, especially in the\nstrong-selection and near-extinction regimes where previous approaches fail. We\nfurther apply to allele frequency data for baker's yeast (Saccharomyces\ncerevisiae), finding a significant signal of selection in cases where\nindependent evidence supports such a conclusion. We further demonstrate the\npossibility of detecting time-points at which evolutionary parameters change in\nthe context of a historical spelling reform in the Spanish language.\n","authors":["Juan Guerrero Montero","Richard A. Blythe"],"pdf_url":"https://arxiv.org/pdf/2303.04691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04673v1","updated":"2023-03-08T15:52:14Z","published":"2023-03-08T15:52:14Z","title":"Cost-Effective Hyperparameter Optimization for Large Language Model\n  Generation Inference","summary":"  Large Language Models (LLMs) like GPT-3 have sparked significant interest in\ntheir generative capabilities, leading to the development of various commercial\napplications. The high cost of using the models drives application builders to\nmaximize the value of generation under a limited inference budget. This paper\npresents a study of optimizing inference hyperparameters like the number of\nresponses, temperature and max tokens, which significantly affects the\nutility/cost of text generation. We design a framework named EcoOptiGen which\nleverages economical hyperparameter optimization and cost-based pruning.\nExperiments with the latest GPT-3.5 models on a variety of tasks verify its\neffectiveness. EcoOptiGen is implemented in the FLAML library:\nhttps://github.com/microsoft/FLAML, and we provide one example of using it at:\nhttps://microsoft.github.io/FLAML/docs/Examples/Integrate%20-%20OpenAI.\n","authors":["Chi Wang","Susan Xueqing Liu","Ahmed H. Awadallah"],"pdf_url":"https://arxiv.org/pdf/2303.04673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08087v4","updated":"2023-03-08T15:06:11Z","published":"2022-07-17T06:50:35Z","title":"Automatic Context Pattern Generation for Entity Set Expansion","summary":"  Entity Set Expansion (ESE) is a valuable task that aims to find entities of\nthe target semantic class described by given seed entities. Various Natural\nLanguage Processing (NLP) and Information Retrieval (IR) downstream\napplications have benefited from ESE due to its ability to discover knowledge.\nAlthough existing corpus-based ESE methods have achieved great progress, they\nstill rely on corpora with high-quality entity information annotated, because\nmost of them need to obtain the context patterns through the position of the\nentity in a sentence. Therefore, the quality of the given corpora and their\nentity annotation has become the bottleneck that limits the performance of such\nmethods. To overcome this dilemma and make the ESE models free from the\ndependence on entity annotation, our work aims to explore a new ESE paradigm,\nnamely corpus-independent ESE. Specifically, we devise a context pattern\ngeneration module that utilizes autoregressive language models (e.g., GPT-2) to\nautomatically generate high-quality context patterns for entities. In addition,\nwe propose the GAPA, a novel ESE framework that leverages the aforementioned\nGenerAted PAtterns to expand target entities. Extensive experiments and\ndetailed analyses on three widely used datasets demonstrate the effectiveness\nof our method. All the codes of our experiments are available at\nhttps://github.com/geekjuruo/GAPA.\n","authors":["Yinghui Li","Shulin Huang","Xinwei Zhang","Qingyu Zhou","Yangning Li","Ruiyang Liu","Yunbo Cao","Hai-Tao Zheng","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2207.08087v4.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2110.15720v3","updated":"2023-03-08T13:35:04Z","published":"2021-10-08T20:17:10Z","title":"Weakly Supervised Concept Map Generation through Task-Guided Graph\n  Translation","summary":"  Recent years have witnessed the rapid development of concept map generation\ntechniques due to their advantages in providing well-structured summarization\nof knowledge from free texts. Traditional unsupervised methods do not generate\ntask-oriented concept maps, whereas deep generative models require large\namounts of training data. In this work, we present GT-D2G (Graph\nTranslation-based Document To Graph), an automatic concept map generation\nframework that leverages generalized NLP pipelines to derive semantic-rich\ninitial graphs, and translates them into more concise structures under the weak\nsupervision of downstream task labels. The concept maps generated by GT-D2G can\nprovide interpretable summarization of structured knowledge for the input\ntexts, which are demonstrated through human evaluation and case studies on\nthree real-world corpora. Further experiments on the downstream task of\ndocument classification show that GT-D2G beats other concept map generation\nmethods. Moreover, we specifically validate the labeling efficiency of GT-D2G\nin the label-efficient learning setting and the flexibility of generated graph\nsizes in controlled hyper-parameter studies.\n","authors":["Jiaying Lu","Xiangjue Dong","Carl Yang"],"pdf_url":"https://arxiv.org/pdf/2110.15720v3.pdf","comment":"Accepted by IEEE TKDE. All code and data available at\n  https://github.com/lujiaying/GT-doc2graph"},{"id":"http://arxiv.org/abs/2303.04562v1","updated":"2023-03-08T13:21:27Z","published":"2023-03-08T13:21:27Z","title":"Extrapolative Controlled Sequence Generation via Iterative Refinement","summary":"  We study the problem of extrapolative controlled generation, i.e., generating\nsequences with attribute values beyond the range seen in training. This task is\nof significant importance in automated design, especially drug discovery, where\nthe goal is to design novel proteins that are \\textit{better} (e.g., more\nstable) than existing sequences. Thus, by definition, the target sequences and\ntheir attribute values are out of the training distribution, posing challenges\nto existing methods that aim to directly generate the target sequence. Instead,\nin this work, we propose Iterative Controlled Extrapolation (ICE) which\niteratively makes local edits to a sequence to enable extrapolation. We train\nthe model on synthetically generated sequence pairs that demonstrate small\nimprovement in the attribute value. Results on one natural language task\n(sentiment analysis) and two protein engineering tasks (ACE2 stability and AAV\nfitness) show that ICE considerably outperforms state-of-the-art approaches\ndespite its simplicity. Our code and models are available at:\nhttps://github.com/vishakhpk/iter-extrapolation.\n","authors":["Vishakh Padmakumar","Richard Yuanzhe Pang","He He","Ankur P. Parikh"],"pdf_url":"https://arxiv.org/pdf/2303.04562v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2303.04544v1","updated":"2023-03-08T12:53:03Z","published":"2023-03-08T12:53:03Z","title":"Models of symbol emergence in communication: a conceptual review and a\n  guide for avoiding local minima","summary":"  Computational simulations are a popular method for testing hypotheses about\nthe emergence of communication. This kind of research is performed in a variety\nof traditions including language evolution, developmental psychology, cognitive\nscience, machine learning, robotics, etc. The motivations for the models are\ndifferent, but the operationalizations and methods used are often similar. We\nidentify the assumptions and explanatory targets of several most representative\nmodels and summarise the known results. We claim that some of the assumptions\n-- such as portraying meaning in terms of mapping, focusing on the descriptive\nfunction of communication, modelling signals with amodal tokens -- may hinder\nthe success of modelling. Relaxing these assumptions and foregrounding the\ninteractions of embodied and situated agents allows one to systematise the\nmultiplicity of pressures under which symbolic systems evolve. In line with\nthis perspective, we sketch the road towards modelling the emergence of\nmeaningful symbolic communication, where symbols are simultaneously grounded in\naction and perception and form an abstract system.\n","authors":["Julian Zubek","Tomasz Korbak","Joanna Rączaszek-Leonardi"],"pdf_url":"https://arxiv.org/pdf/2303.04544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05387v1","updated":"2023-03-08T12:41:56Z","published":"2023-03-08T12:41:56Z","title":"Automatic Detection of Industry Sectors in Legal Articles Using Machine\n  Learning Approaches","summary":"  The ability to automatically identify industry sector coverage in articles on\nlegal developments, or any kind of news articles for that matter, can bring\nplentiful of benefits both to the readers and the content creators themselves.\nBy having articles tagged based on industry coverage, readers from all around\nthe world would be able to get to legal news that are specific to their region\nand professional industry. Simultaneously, writers would benefit from\nunderstanding which industries potentially lack coverage or which industries\nreaders are currently mostly interested in and thus, they would focus their\nwriting efforts towards more inclusive and relevant legal news coverage. In\nthis paper, a Machine Learning-powered industry analysis approach which\ncombined Natural Language Processing (NLP) with Statistical and Machine\nLearning (ML) techniques was investigated. A dataset consisting of over 1,700\nannotated legal articles was created for the identification of six industry\nsectors. Text and legal based features were extracted from the text. Both\ntraditional ML methods (e.g. gradient boosting machine algorithms, and\ndecision-tree based algorithms) and deep neural network (e.g. transformer\nmodels) were applied for performance comparison of predictive models. The\nsystem achieved promising results with area under the receiver operating\ncharacteristic curve scores above 0.90 and F-scores above 0.81 with respect to\nthe six industry sectors. The experimental results show that the suggested\nautomated industry analysis which employs ML techniques allows the processing\nof large collections of text data in an easy, efficient, and scalable way.\nTraditional ML methods perform better than deep neural networks when only a\nsmall and domain-specific training data is available for the study.\n","authors":["Hui Yang","Stella Hadjiantoni","Yunfei Long","Ruta Petraityte","Berthold Lausen"],"pdf_url":"https://arxiv.org/pdf/2303.05387v1.pdf","comment":"26 pages, 5 figures, 3 tables. Paper was presented at 'Classification\n  and Data Science in the Digital Age', 17th conference of the International\n  Federation of Classification Societies (IFCS2022), Porto, Portugal,\n  https://ifcs2022.fep.up.pt/"},{"id":"http://arxiv.org/abs/2212.05764v2","updated":"2023-03-08T12:29:29Z","published":"2022-12-12T08:32:28Z","title":"Domain Adaptation of Transformer-Based Models using Unlabeled Data for\n  Relevance and Polarity Classification of German Customer Feedback","summary":"  Understanding customer feedback is becoming a necessity for companies to\nidentify problems and improve their products and services. Text classification\nand sentiment analysis can play a major role in analyzing this data by using a\nvariety of machine and deep learning approaches. In this work, different\ntransformer-based models are utilized to explore how efficient these models are\nwhen working with a German customer feedback dataset. In addition, these\npre-trained models are further analyzed to determine if adapting them to a\nspecific domain using unlabeled data can yield better results than\noff-the-shelf pre-trained models. To evaluate the models, two downstream tasks\nfrom the GermEval 2017 are considered. The experimental results show that\ntransformer-based models can reach significant improvements compared to a\nfastText baseline and outperform the published scores and previous models. For\nthe subtask Relevance Classification, the best models achieve a micro-averaged\n$F1$-Score of 96.1 % on the first test set and 95.9 % on the second one, and a\nscore of 85.1 % and 85.3 % for the subtask Polarity Classification.\n","authors":["Ahmad Idrissi-Yaghir","Henning Schäfer","Nadja Bauer","Christoph M. Friedrich"],"pdf_url":"https://arxiv.org/pdf/2212.05764v2.pdf","comment":"Complete"},{"id":"http://arxiv.org/abs/2303.04526v1","updated":"2023-03-08T11:51:26Z","published":"2023-03-08T11:51:26Z","title":"Student's t-Distribution: On Measuring the Inter-Rater Reliability When\n  the Observations are Scarce","summary":"  In natural language processing (NLP) we always rely on human judgement as the\ngolden quality evaluation method. However, there has been an ongoing debate on\nhow to better evaluate inter-rater reliability (IRR) levels for certain\nevaluation tasks, such as translation quality evaluation (TQE), especially when\nthe data samples (observations) are very scarce. In this work, we first\nintroduce the study on how to estimate the confidence interval for the\nmeasurement value when only one data (evaluation) point is available. Then,\nthis leads to our example with two human-generated observational scores, for\nwhich, we introduce ``Student's \\textit{t}-Distribution'' method and explain\nhow to use it to measure the IRR score using only these two data points, as\nwell as the confidence intervals (CIs) of the quality evaluation. We give\nquantitative analysis on how the evaluation confidence can be greatly improved\nby introducing more observations, even if only one extra observation. We\nencourage researchers to report their IRR scores in all possible means, e.g.\nusing Student's \\textit{t}-Distribution method whenever possible; thus making\nthe NLP evaluation more meaningful, transparent, and trustworthy. This\n\\textit{t}-Distribution method can be also used outside of NLP fields to\nmeasure IRR level for trustworthy evaluation of experimental investigations,\nwhenever the observational data is scarce.\n  Keywords: Inter-Rater Reliability (IRR); Scarce Observations; Confidence\nIntervals (CIs); Natural Language Processing (NLP); Translation Quality\nEvaluation (TQE); Student's \\textit{t}-Distribution\n","authors":["Serge Gladkoff","Lifeng Han","Goran Nenadic"],"pdf_url":"https://arxiv.org/pdf/2303.04526v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2302.04054v4","updated":"2023-03-08T11:37:27Z","published":"2023-02-08T13:47:00Z","title":"Towards Inferential Reproducibility of Machine Learning Research","summary":"  Reliability of machine learning evaluation -- the consistency of observed\nevaluation scores across replicated model training runs -- is affected by\nseveral sources of nondeterminism which can be regarded as measurement noise.\nCurrent tendencies to remove noise in order to enforce reproducibility of\nresearch results neglect inherent nondeterminism at the implementation level\nand disregard crucial interaction effects between algorithmic noise factors and\ndata properties. This limits the scope of conclusions that can be drawn from\nsuch experiments. Instead of removing noise, we propose to incorporate several\nsources of variance, including their interaction with data properties, into an\nanalysis of significance and reliability of machine learning evaluation, with\nthe aim to draw inferences beyond particular instances of trained models. We\nshow how to use linear mixed effects models (LMEMs) to analyze performance\nevaluation scores, and to conduct statistical inference with a generalized\nlikelihood ratio test (GLRT). This allows us to incorporate arbitrary sources\nof noise like meta-parameter variations into statistical significance testing,\nand to assess performance differences conditional on data properties.\nFurthermore, a variance component analysis (VCA) enables the analysis of the\ncontribution of noise sources to overall variance and the computation of a\nreliability coefficient by the ratio of substantial to total variance.\n","authors":["Michael Hagmann","Philipp Meier","Stefan Riezler"],"pdf_url":"https://arxiv.org/pdf/2302.04054v4.pdf","comment":"Published at ICLR 2023 (see https://openreview.net/pdf?id=li4GQCQWkv)"},{"id":"http://arxiv.org/abs/2210.01911v3","updated":"2023-03-08T11:00:55Z","published":"2022-10-04T21:16:48Z","title":"Grounding Language with Visual Affordances over Unstructured Data","summary":"  Recent works have shown that Large Language Models (LLMs) can be applied to\nground natural language to a wide variety of robot skills. However, in\npractice, learning multi-task, language-conditioned robotic skills typically\nrequires large-scale data collection and frequent human intervention to reset\nthe environment or help correcting the current policies. In this work, we\npropose a novel approach to efficiently learn general-purpose\nlanguage-conditioned robot skills from unstructured, offline and reset-free\ndata in the real world by exploiting a self-supervised visuo-lingual affordance\nmodel, which requires annotating as little as 1% of the total data with\nlanguage. We evaluate our method in extensive experiments both in simulated and\nreal-world robotic tasks, achieving state-of-the-art performance on the\nchallenging CALVIN benchmark and learning over 25 distinct visuomotor\nmanipulation tasks with a single policy in the real world. We find that when\npaired with LLMs to break down abstract natural language instructions into\nsubgoals via few-shot prompting, our method is capable of completing\nlong-horizon, multi-tier tasks in the real world, while requiring an order of\nmagnitude less data than previous approaches. Code and videos are available at\nhttp://hulc2.cs.uni-freiburg.de\n","authors":["Oier Mees","Jessica Borja-Diaz","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2210.01911v3.pdf","comment":"Accepted at the 2023 IEEE International Conference on Robotics and\n  Automation (ICRA). Project website: http://hulc2.cs.uni-freiburg.de"}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.04626v1","updated":"2023-03-08T14:45:51Z","published":"2023-03-08T14:45:51Z","title":"Multi-MEC Cooperation Based VR Video Transmission and Cache using\n  K-Shortest Paths Optimization","summary":"  In recent network architectures, multi-MEC cooperative caching has been\nintroduced to reduce the transmission latency of VR videos, in which MEC\nservers' computing and caching capability are utilized to optimize the\ntransmission process. However, many solutions that use the computing capability\nof MEC servers ignore the additional arithmetic power consumed by the codec\nprocess, thus making them infeasible. Besides, the minimum cache unit is\nusually the entire VR video, which makes caching inefficient.\n  To address these challenges, we split VR videos into tile files for caching\nbased on the current popular network architecture and provide a reliable\ntransmission mechanism and an effective caching strategy. Since the number of\ndifferent tile files N is too large, the current cooperative caching algorithms\ndo not cope with such large-scale input data. We further analyze the problem\nand propose an optimized k-shortest paths (OKSP) algorithm with an upper bound\ntime complexity of O((K * M + N) * M * logN)), and suitable for shortest paths\nwith restricted number of edges, where K is the total number of tiles that all\nM MEC servers can cache in the collaboration domain. And we prove the OKSP\nalgorithm can compute the caching scheme with the lowest average latency in any\ncase, which means the solution given is the exact solution. The simulation\nresults show that the OKSP algorithm has excellent speed for solving\nlarge-scale data and consistently outperforms other caching algorithms in the\nexperiments.\n","authors":["Jingwen Xia","Luyao Chen","Yong Tang","Ting Yang","Wenyong Wang"],"pdf_url":"https://arxiv.org/pdf/2303.04626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.08428v2","updated":"2023-03-08T14:25:10Z","published":"2022-11-15T05:14:48Z","title":"CaDM: Codec-aware Diffusion Modeling for Neural-enhanced Video Streaming","summary":"  Recent years have witnessed the dramatic growth of Internet video traffic,\nwhere the video bitstreams are often compressed and delivered in low quality to\nfit the streamer's uplink bandwidth. To alleviate the quality degradation, it\ncomes the rise of Neural-enhanced Video Streaming (NVS), which shows great\nprospects for recovering low-quality videos by mostly deploying neural\nsuper-resolution (SR) on the media server. Despite its benefit, we reveal that\ncurrent mainstream works with SR enhancement have not achieved the desired\nrate-distortion trade-off between bitrate saving and quality restoration, due\nto: (1) overemphasizing the enhancement on the decoder side while omitting the\nco-design of encoder, (2) limited generative capacity to recover high-fidelity\nperceptual details, and (3) optimizing the compression-and-restoration pipeline\nfrom the resolution perspective solely, without considering color bit-depth.\nAiming at overcoming these limitations, we are the first to conduct an\nencoder-decoder (i.e., codec) synergy by leveraging the inherent\nvisual-generative property of diffusion models. Specifically, we present the\nCodec-aware Diffusion Modeling (CaDM), a novel NVS paradigm to significantly\nreduce streaming delivery bitrates while holding pretty higher restoration\ncapacity over existing methods. First, CaDM improves the encoder's compression\nefficiency by simultaneously reducing resolution and color bit-depth of video\nframes. Second, CaDM empowers the decoder with high-quality enhancement by\nmaking the denoising diffusion restoration aware of encoder's resolution-color\nconditions. Evaluation on public cloud services with OpenMMLab benchmarks shows\nthat CaDM effectively saves up to 5.12 - 21.44 times bitrates based on common\nvideo standards and achieves much better recovery quality (e.g., FID of 0.61)\nover state-of-the-art neural-enhancing methods.\n","authors":["Qihua Zhou","Ruibin Li","Song Guo","Peiran Dong","Yi Liu","Jingcai Guo","Zhenda Xu"],"pdf_url":"https://arxiv.org/pdf/2211.08428v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03131v2","updated":"2023-03-08T11:35:51Z","published":"2023-03-06T13:49:15Z","title":"Video Question Answering Using CLIP-Guided Visual-Text Attention","summary":"  Cross-modal learning of video and text plays a key role in Video Question\nAnswering (VideoQA). In this paper, we propose a visual-text attention\nmechanism to utilize the Contrastive Language-Image Pre-training (CLIP) trained\non lots of general domain language-image pairs to guide the cross-modal\nlearning for VideoQA. Specifically, we first extract video features using a\nTimeSformer and text features using a BERT from the target application domain,\nand utilize CLIP to extract a pair of visual-text features from the\ngeneral-knowledge domain through the domain-specific learning. We then propose\na Cross-domain Learning to extract the attention information between visual and\nlinguistic features across the target domain and general domain. The set of\nCLIP-guided visual-text features are integrated to predict the answer. The\nproposed method is evaluated on MSVD-QA and MSRVTT-QA datasets, and outperforms\nstate-of-the-art methods.\n","authors":["Shuhong Ye","Weikai Kong","Chenglin Yao","Jianfeng Ren","Xudong Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.03131v2.pdf","comment":"Submitted to the 2023 IEEE International Conference on Image\n  Processing (ICIP 2023)"}]},"2023-03-07T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2212.08841v2","updated":"2023-03-07T20:51:26Z","published":"2022-12-17T10:43:25Z","title":"AugTriever: Unsupervised Dense Retrieval by Scalable Data Augmentation","summary":"  Dense retrievers have made significant strides in text retrieval and\nopen-domain question answering, even though most achievements were made\npossible only with large amounts of human supervision. In this work, we aim to\ndevelop unsupervised methods by proposing two methods that create pseudo\nquery-document pairs and train dense retrieval models in an annotation-free and\nscalable manner: query extraction and transferred query generation. The former\nmethod produces pseudo queries by selecting salient spans from the original\ndocument. The latter utilizes generation models trained for other NLP tasks\n(e.g., summarization) to produce pseudo queries. Extensive experiments show\nthat models trained with the proposed augmentation methods can perform\ncomparably well (or better) to multiple strong baselines. Combining those\nstrategies leads to further improvements, achieving the state-of-the-art\nperformance of unsupervised dense retrieval on both BEIR and ODQA datasets.\n","authors":["Rui Meng","Ye Liu","Semih Yavuz","Divyansh Agarwal","Lifu Tu","Ning Yu","Jianguo Zhang","Meghana Bhat","Yingbo Zhou"],"pdf_url":"https://arxiv.org/pdf/2212.08841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04188v1","updated":"2023-03-07T19:23:33Z","published":"2023-03-07T19:23:33Z","title":"Clustering large 3D volumes: A sampling-based approach","summary":"  In many applications of X-ray computed tomography, an unsupervised\nsegmentation of the reconstructed 3D volumes forms an important step in the\nimage processing chain for further investigation of the digitized object.\nTherefore, the goal is to train a clustering algorithm on the volume, which\nproduces a voxelwise classification by assigning a cluster index to each voxel.\nHowever, clustering methods, e.g., K-Means, typically have an asymptotic\npolynomial runtime with respect to the dataset size, and thus, these techniques\nare rarely applicable to large volumes. In this work, we introduce a novel\nclustering technique based on random sampling, which allows for the voxelwise\nclassification of arbitrarily large volumes. The presented method conducts\nefficient linear passes over the data to extract a representative random sample\nof a fixed size on which the classifier can be trained. Then, a final linear\npass performs the segmentation and assigns a cluster index to each individual\nvoxel. Quantitative and qualitative evaluations show that excellent results can\nbe achieved even with a very small sample size. Consequently, the unsupervised\nsegmentation by means of clustering becomes feasible for arbitrarily large\nvolumes.\n","authors":["Thomas Lang"],"pdf_url":"https://arxiv.org/pdf/2303.04188v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.05392v1","updated":"2023-03-07T17:30:48Z","published":"2023-03-07T17:30:48Z","title":"Automatically Summarizing Evidence from Clinical Trials: A Prototype\n  Highlighting Current Challenges","summary":"  We present TrialsSummarizer, a system that aims to automatically summarize\nevidence presented in the set of randomized controlled trials most relevant to\na given query. Building on prior work, the system retrieves trial publications\nmatching a query specifying a combination of condition, intervention(s), and\noutcome(s), and ranks these according to sample size and estimated study\nquality. The top-k such studies are passed through a neural multi-document\nsummarization system, yielding a synopsis of these trials. We consider two\narchitectures: A standard sequence-to-sequence model based on BART and a\nmulti-headed architecture intended to provide greater transparency to\nend-users. Both models produce fluent and relevant summaries of evidence\nretrieved for queries, but their tendency to introduce unsupported statements\nrender them inappropriate for use in this domain at present. The proposed\narchitecture may help users verify outputs allowing users to trace generated\ntokens back to inputs.\n","authors":["Sanjana Ramprasad","Denis Jered McInerney","Iain J. Marshal","Byron C. Wallace"],"pdf_url":"https://arxiv.org/pdf/2303.05392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04689v1","updated":"2023-03-07T17:22:38Z","published":"2023-03-07T17:22:38Z","title":"A Privacy Preserving System for Movie Recommendations using Federated\n  Learning","summary":"  Recommender systems have become ubiquitous in the past years. They solve the\ntyranny of choice problem faced by many users, and are employed by many online\nbusinesses to drive engagement and sales. Besides other criticisms, like\ncreating filter bubbles within social networks, recommender systems are often\nreproved for collecting considerable amounts of personal data. However, to\npersonalize recommendations, personal information is fundamentally required. A\nrecent distributed learning scheme called federated learning has made it\npossible to learn from personal user data without its central collection.\nAccordingly, we present a complete recommender system for movie\nrecommendations, which provides privacy and thus trustworthiness on two levels:\nFirst, it is trained using federated learning and thus is, by its very nature,\nprivacy-preserving, while still enabling individual users to benefit from\nglobal insights. And second, a novel federated learning scheme, FedQ, is\nemployed, which not only addresses the problem of non-i.i.d. and small local\ndatasets, but also prevents input data reconstruction attacks by aggregating\nclient models early. To reduce the communication overhead, compression is\napplied, which significantly reduces the exchanged neural network updates to a\nfraction of their original data. We conjecture that it may also improve data\nprivacy through its lossy quantization stage.\n","authors":["David Neumann","Andreas Lutz","Karsten Müller","Wojciech Samek"],"pdf_url":"https://arxiv.org/pdf/2303.04689v1.pdf","comment":"Submitted to the ACM TORS Special Issue on Trustworthy Recommender\n  Systems"},{"id":"http://arxiv.org/abs/2102.13392v3","updated":"2023-03-07T14:56:18Z","published":"2021-02-26T11:01:30Z","title":"Unifying Remote Sensing Image Retrieval and Classification with Robust\n  Fine-tuning","summary":"  Advances in high resolution remote sensing image analysis are currently\nhampered by the difficulty of gathering enough annotated data for training deep\nlearning methods, giving rise to a variety of small datasets and associated\ndataset-specific methods. Moreover, typical tasks such as classification and\nretrieval lack a systematic evaluation on standard benchmarks and training\ndatasets, which make it hard to identify durable and generalizable scientific\ncontributions. We aim at unifying remote sensing image retrieval and\nclassification with a new large-scale training and testing dataset, SF300,\nincluding both vertical and oblique aerial images and made available to the\nresearch community, and an associated fine-tuning method. We additionally\npropose a new adversarial fine-tuning method for global descriptors. We show\nthat our framework systematically achieves a boost of retrieval and\nclassification performance on nine different datasets compared to an ImageNet\npretrained baseline, with currently no other method to compare to.\n","authors":["Dimitri Gominski","Valérie Gouet-Brunet","Liming Chen"],"pdf_url":"https://arxiv.org/pdf/2102.13392v3.pdf","comment":"Performance margin with the proposed method is not statistically\n  significant. Please refer to http://alegoria.ign.fr/en/SF300_dataset if you\n  are interested in the dataset"},{"id":"http://arxiv.org/abs/2210.00305v2","updated":"2023-03-07T14:35:33Z","published":"2022-10-01T16:01:53Z","title":"LambdaKG: A Library for Pre-trained Language Model-Based Knowledge Graph\n  Embeddings","summary":"  Knowledge Graphs (KGs) often have two characteristics: heterogeneous graph\nstructure and text-rich entity/relation information. Text-based KG embeddings\ncan represent entities by encoding descriptions with pre-trained language\nmodels, but no open-sourced library is specifically designed for KGs with PLMs\nat present. In this paper, we present LambdaKG, a library for KGE that equips\nwith many pre-trained language models (e.g., BERT, BART, T5, GPT-3), and\nsupports various tasks (e.g., knowledge graph completion, question answering,\nrecommendation, and knowledge probing). LambdaKG is publicly open-sourced at\nhttps://github.com/zjunlp/PromptKG/tree/main/lambdaKG, with a demo video at\nhttp://deepke.zjukg.cn/lambdakg.mp4 and long-term maintenance.\n","authors":["Xin Xie","Zhoubo Li","Xiaohan Wang","Yuqi Zhu","Ningyu Zhang","Jintian Zhang","Siyuan Cheng","Bozhong Tian","Shumin Deng","Feiyu Xiong","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2210.00305v2.pdf","comment":"Work in progress and the project website is\n  https://zjunlp.github.io/project/promptkg/"},{"id":"http://arxiv.org/abs/2202.12524v4","updated":"2023-03-07T03:21:01Z","published":"2022-02-25T06:58:28Z","title":"MAMDR: A Model Agnostic Learning Method for Multi-Domain Recommendation","summary":"  Large-scale e-commercial platforms in the real-world usually contain various\nrecommendation scenarios (domains) to meet demands of diverse customer groups.\nMulti-Domain Recommendation (MDR), which aims to jointly improve\nrecommendations on all domains and easily scales to thousands of domains, has\nattracted increasing attention from practitioners and researchers. Existing MDR\nmethods usually employ a shared structure and several specific components to\nrespectively leverage reusable features and domain-specific information.\nHowever, data distribution differs across domains, making it challenging to\ndevelop a general model that can be applied to all circumstances. Additionally,\nduring training, shared parameters often suffer from the domain conflict while\nspecific parameters are inclined to overfitting on data sparsity domains. we\nfirst present a scalable MDR platform served in Taobao that enables to provide\nservices for thousands of domains without specialists involved. To address the\nproblems of MDR methods, we propose a novel model agnostic learning framework,\nnamely MAMDR, for the multi-domain recommendation. Specifically, we first\npropose a Domain Negotiation (DN) strategy to alleviate the conflict between\ndomains. Then, we develop a Domain Regularization (DR) to improve the\ngeneralizability of specific parameters by learning from other domains. We\nintegrate these components into a unified framework and present MAMDR, which\ncan be applied to any model structure to perform multi-domain recommendation.\nFinally, we present a large-scale implementation of MAMDR in the Taobao\napplication and construct various public MDR benchmark datasets which can be\nused for following studies. Extensive experiments on both benchmark datasets\nand industry datasets demonstrate the effectiveness and generalizability of\nMAMDR.\n","authors":["Linhao Luo","Yumeng Li","Buyu Gao","Shuai Tang","Sinan Wang","Jiancheng Li","Tanchao Zhu","Jiancai Liu","Zhao Li","Shirui Pan"],"pdf_url":"https://arxiv.org/pdf/2202.12524v4.pdf","comment":"This paper has been accepted by ICDE 2023"}],"Multimedia":[{"id":"http://arxiv.org/abs/2209.08212v4","updated":"2023-03-07T14:19:17Z","published":"2022-09-17T01:20:59Z","title":"Compose & Embellish: Well-Structured Piano Performance Generation via A\n  Two-Stage Approach","summary":"  Even with strong sequence models like Transformers, generating expressive\npiano performances with long-range musical structures remains challenging.\nMeanwhile, methods to compose well-structured melodies or lead sheets (melody +\nchords), i.e., simpler forms of music, gained more success. Observing the\nabove, we devise a two-stage Transformer-based framework that Composes a lead\nsheet first, and then Embellishes it with accompaniment and expressive touches.\nSuch a factorization also enables pretraining on non-piano data. Our objective\nand subjective experiments show that Compose & Embellish shrinks the gap in\nstructureness between a current state of the art and real performances by half,\nand improves other musical aspects such as richness and coherence as well.\n","authors":["Shih-Lun Wu","Yi-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2209.08212v4.pdf","comment":"Accepted to International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP) 2023"},{"id":"http://arxiv.org/abs/2106.06924v3","updated":"2023-03-07T14:05:05Z","published":"2021-06-13T05:32:17Z","title":"Deep Learning for Predictive Analytics in Reversible Steganography","summary":"  Deep learning is regarded as a promising solution for reversible\nsteganography. There is an accelerating trend of representing a reversible\nsteo-system by monolithic neural networks, which bypass intermediate operations\nin traditional pipelines of reversible steganography. This end-to-end paradigm,\nhowever, suffers from imperfect reversibility. By contrast, the modular\nparadigm that incorporates neural networks into modules of traditional\npipelines can stably guarantee reversibility with mathematical explainability.\nPrediction-error modulation is a well-established reversible steganography\npipeline for digital images. It consists of a predictive analytics module and a\nreversible coding module. Given that reversibility is governed independently by\nthe coding module, we narrow our focus to the incorporation of neural networks\ninto the analytics module, which serves the purpose of predicting pixel\nintensities and a pivotal role in determining capacity and imperceptibility.\nThe objective of this study is to evaluate the impacts of different training\nconfigurations upon predictive accuracy of neural networks and provide\npractical insights. In particular, we investigate how different initialisation\nstrategies for input images may affect the learning process and how different\ntraining strategies for dual-layer prediction respond to the problem of\ndistributional shift. Furthermore, we compare steganographic performance of\nvarious model architectures with different loss functions.\n","authors":["Ching-Chun Chang","Xu Wang","Sisheng Chen","Isao Echizen","Victor Sanchez","Chang-Tsun Li"],"pdf_url":"https://arxiv.org/pdf/2106.06924v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.02478v2","updated":"2023-03-07T13:11:04Z","published":"2022-01-07T14:56:33Z","title":"Bayesian Neural Networks for Reversible Steganography","summary":"  Recent advances in deep learning have led to a paradigm shift in the field of\nreversible steganography. A fundamental pillar of reversible steganography is\npredictive modelling which can be realised via deep neural networks. However,\nnon-trivial errors exist in inferences about some out-of-distribution and noisy\ndata. In view of this issue, we propose to consider uncertainty in predictive\nmodels based upon a theoretical framework of Bayesian deep learning, thereby\ncreating an adaptive steganographic system. Most modern deep-learning models\nare regarded as deterministic because they only offer predictions while failing\nto provide uncertainty measurement. Bayesian neural networks bring a\nprobabilistic perspective to deep learning and can be regarded as self-aware\nintelligent machinery; that is, a machine that knows its own limitations. To\nquantify uncertainty, we apply Bayesian statistics to model the predictive\ndistribution and approximate it through Monte Carlo sampling with stochastic\nforward passes. We further show that predictive uncertainty can be disentangled\ninto aleatoric and epistemic uncertainties and these quantities can be learnt\nunsupervised. Experimental results demonstrate an improvement delivered by\nBayesian uncertainty analysis upon steganographic rate-distortion performance.\n","authors":["Ching-Chun Chang"],"pdf_url":"https://arxiv.org/pdf/2201.02478v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.02518v2","updated":"2023-03-07T12:55:55Z","published":"2022-02-05T09:04:50Z","title":"On the predictability in reversible steganography","summary":"  Artificial neural networks have advanced the frontiers of reversible\nsteganography. The core strength of neural networks is the ability to render\naccurate predictions for a bewildering variety of data. Residual modulation is\nrecognised as the most advanced reversible steganographic algorithm for digital\nimages. The pivot of this algorithm is predictive analytics in which pixel\nintensities are predicted given some pixel-wise contextual information. This\ntask can be perceived as a low-level vision problem and hence neural networks\nfor addressing a similar class of problems can be deployed. On top of the prior\nart, this paper investigates predictability of pixel intensities based on\nsupervised and unsupervised learning frameworks. Predictability analysis\nenables adaptive data embedding, which in turn leads to a better trade-off\nbetween capacity and imperceptibility. While conventional methods estimate\npredictability by the statistics of local image patterns, learning-based\nframeworks consider further the degree to which correct predictions can be made\nby a designated predictor. Not only should the image patterns be taken into\naccount but also the predictor in use. Experimental results show that\nsteganographic performance can be significantly improved by incorporating the\nlearning-based predictability analysers into a reversible steganographic\nsystem.\n","authors":["Ching-Chun Chang","Xu Wang","Sisheng Chen","Hitoshi Kiya","Isao Echizen"],"pdf_url":"https://arxiv.org/pdf/2202.02518v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.04894v3","updated":"2023-03-07T08:25:24Z","published":"2022-11-09T13:55:50Z","title":"Exploring Video Quality Assessment on User Generated Contents from\n  Aesthetic and Technical Perspectives","summary":"  The rapid increase in user-generated-content (UGC) videos calls for the\ndevelopment of effective video quality assessment (VQA) algorithms. However,\nthe objective of the UGC-VQA problem is still ambiguous and can be viewed from\ntwo perspectives: the technical perspective, measuring the perception of\ndistortions; and the aesthetic perspective, which relates to preference and\nrecommendation on contents. To understand how these two perspectives affect\noverall subjective opinions in UGC-VQA, we conduct a large-scale subjective\nstudy to collect human quality opinions on overall quality of videos as well as\nperceptions from aesthetic and technical perspectives. The collected\nDisentangled Video Quality Database (DIVIDE-3k) confirms that human quality\nopinions on UGC videos are universally and inevitably affected by both\naesthetic and technical perspectives. In light of this, we propose the\nDisentangled Objective Video Quality Evaluator (DOVER) to learn the quality of\nUGC videos based on the two perspectives. The DOVER proves state-of-the-art\nperformance in UGC-VQA under very high efficiency. With perspective opinions in\nDIVIDE-3k, we further propose DOVER++, the first approach to provide reliable\nclear-cut quality evaluations from a single aesthetic or technical perspective.\nCode at https://github.com/VQAssessment/DOVER.\n","authors":["Haoning Wu","Erli Zhang","Liang Liao","Chaofeng Chen","Jingwen Hou","Annan Wang","Wenxiu Sun","Qiong Yan","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2211.04894v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.15603v3","updated":"2023-03-07T06:14:56Z","published":"2022-11-28T17:57:48Z","title":"Action-GPT: Leveraging Large-scale Language Models for Improved and\n  Generalized Action Generation","summary":"  We introduce Action-GPT, a plug-and-play framework for incorporating Large\nLanguage Models (LLMs) into text-based action generation models. Action phrases\nin current motion capture datasets contain minimal and to-the-point\ninformation. By carefully crafting prompts for LLMs, we generate richer and\nfine-grained descriptions of the action. We show that utilizing these detailed\ndescriptions instead of the original action phrases leads to better alignment\nof text and motion spaces. We introduce a generic approach compatible with\nstochastic (e.g. VAE-based) and deterministic (e.g. MotionCLIP) text-to-motion\nmodels. In addition, the approach enables multiple text descriptions to be\nutilized. Our experiments show (i) noticeable qualitative and quantitative\nimprovement in the quality of synthesized motions, (ii) benefits of utilizing\nmultiple LLM-generated descriptions, (iii) suitability of the prompt function,\nand (iv) zero-shot generation capabilities of the proposed approach. Project\npage: https://actiongpt.github.io\n","authors":["Sai Shashank Kalakonda","Shubh Maheshwari","Ravi Kiran Sarvadevabhatla"],"pdf_url":"https://arxiv.org/pdf/2211.15603v3.pdf","comment":"Code, pretrained models and sample videos will be made available at\n  \\url{https://actiongpt.github.io}"},{"id":"http://arxiv.org/abs/2303.03105v2","updated":"2023-03-07T05:39:39Z","published":"2023-03-06T13:16:17Z","title":"Confidence-based Event-centric Online Video Question Answering on a\n  Newly Constructed ATBS Dataset","summary":"  Deep neural networks facilitate video question answering (VideoQA), but the\nreal-world applications on video streams such as CCTV and live cast place\nhigher demands on the solver. To address the challenges of VideoQA on long\nvideos of unknown length, we define a new set of problems called Online\nOpen-ended Video Question Answering (O^2VQA). It requires an online\nstate-updating mechanism for the solver to decide if the collected information\nis sufficient to conclude an answer. We then propose a Confidence-based\nEvent-centric Online Video Question Answering (CEO-VQA) model to solve this\nproblem. Furthermore, a dataset called Answer Target in Background Stream\n(ATBS) is constructed to evaluate this newly developed online VideoQA\napplication. Compared to the baseline VideoQA method that watches the whole\nvideo, the experimental results show that the proposed method achieves a\nsignificant performance gain.\n","authors":["Weikai Kong","Shuhong Ye","Chenglin Yao","Jianfeng Ren"],"pdf_url":"https://arxiv.org/pdf/2303.03105v2.pdf","comment":"Accepted for publication at the 2023 IEEE International Conference on\n  Acoustics, Speech, and Signal Processing (ICASSP 2023)"},{"id":"http://arxiv.org/abs/2303.02673v2","updated":"2023-03-07T02:38:16Z","published":"2023-03-05T13:48:47Z","title":"Time-frequency Network for Robust Speaker Recognition","summary":"  The wide deployment of speech-based biometric systems usually demands\nhigh-performance speaker recognition algorithms. However, most of the prior\nworks for speaker recognition either process the speech in the frequency domain\nor time domain, which may produce suboptimal results because both time and\nfrequency domains are important for speaker recognition. In this paper, we\nattempt to analyze the speech signal in both time and frequency domains and\npropose the time-frequency network~(TFN) for speaker recognition by extracting\nand fusing the features in the two domains. Based on the recent advance of deep\nneural networks, we propose a convolution neural network to encode the raw\nspeech waveform and the frequency spectrum into domain-specific features, which\nare then fused and transformed into a classification feature space for speaker\nrecognition. Experimental results on the publicly available datasets TIMIT and\nLibriSpeech show that our framework is effective to combine the information in\nthe two domains and performs better than the state-of-the-art methods for\nspeaker recognition.\n","authors":["Jiguo Li","Tianzi Zhang","Xiaobin Liu","Lirong Zheng"],"pdf_url":"https://arxiv.org/pdf/2303.02673v2.pdf","comment":"5pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.03599v1","updated":"2023-03-07T02:31:08Z","published":"2023-03-07T02:31:08Z","title":"FSVVD: A Dataset of Full Scene Volumetric Video","summary":"  Recent years have witnessed a rapid development of immersive multimedia which\nbridges the gap between the real world and virtual space. Volumetric videos, as\nan emerging representative 3D video paradigm that empowers extended reality,\nstand out to provide unprecedented immersive and interactive video watching\nexperience. Despite the tremendous potential, the research towards 3D\nvolumetric video is still in its infancy, relying on sufficient and complete\ndatasets for further exploration. However, existing related volumetric video\ndatasets mostly only include a single object, lacking details about the scene\nand the interaction between them. In this paper, we focus on the current most\nwidely used data format, point cloud, and for the first time release a\nfull-scene volumetric video dataset that includes multiple people and their\ndaily activities interacting with the external environments. Comprehensive\ndataset description and analysis are conducted, with potential usage of this\ndataset. The dataset and additional tools can be accessed via the following\nwebsite: https://cuhksz-inml.github.io/full_scene_volumetric_video_dataset/.\n","authors":["Kaiyuan Hu","Yili Jin","Haowen Yang","Junhua Liu","Fangxin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.03599v1.pdf","comment":"Accepted by MMSys'23 Open Dataset and Software Track, A preliminary\n  version. The dataset and additional tools can be accessed via\n  https://cuhksz-inml.github.io/full_scene_volumetric_video_dataset/"},{"id":"http://arxiv.org/abs/2303.03591v1","updated":"2023-03-07T01:54:24Z","published":"2023-03-07T01:54:24Z","title":"Approach to Learning Generalized Audio Representation Through Batch\n  Embedding Covariance Regularization and Constant-Q Transforms","summary":"  General-purpose embedding is highly desirable for few-shot even zero-shot\nlearning in many application scenarios, including audio tasks. In order to\nunderstand representations better, we conducted a thorough error analysis and\nvisualization of HEAR 2021 submission results. Inspired by the analysis, this\nwork experiments with different front-end audio preprocessing methods,\nincluding Constant-Q Transform (CQT) and Short-time Fourier transform (STFT),\nand proposes a Batch Embedding Covariance Regularization (BECR) term to uncover\na more holistic simulation of the frequency information received by the human\nauditory system. We tested the models on the suite of HEAR 2021 tasks, which\nencompass a broad category of tasks. Preliminary results show (1) the proposed\nBECR can incur a more dispersed embedding on the test set, (2) BECR improves\nthe PaSST model without extra computation complexity, and (3) STFT\npreprocessing outperforms CQT in all tasks we tested.\nGithub:https://github.com/ankitshah009/general_audio_embedding_hear_2021\n","authors":["Ankit Shah","Shuyi Chen","Kejun Zhou","Yue Chen","Bhiksha Raj"],"pdf_url":"https://arxiv.org/pdf/2303.03591v1.pdf","comment":"Technical report, 10 pages"}]},"2023-03-06T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2110.11248v2","updated":"2023-03-06T21:47:50Z","published":"2021-10-21T16:17:40Z","title":"Learning to Recommend Using Non-Uniform Data","summary":"  Learning user preferences for products based on their past purchases or\nreviews is at the cornerstone of modern recommendation engines. One\ncomplication in this learning task is that some users are more likely to\npurchase products or review them, and some products are more likely to be\npurchased or reviewed by the users. This non-uniform pattern degrades the power\nof many existing recommendation algorithms, as they assume that the observed\ndata are sampled uniformly at random among user-product pairs. In addition,\nexisting literature on modeling non-uniformity either assume user interests are\nindependent of the products, or lack theoretical understanding. In this paper,\nwe first model the user-product preferences as a partially observed matrix with\nnon-uniform observation pattern. Next, building on the literature about\nlow-rank matrix estimation, we introduce a new weighted trace-norm penalized\nregression to predict unobserved values of the matrix. We then prove an upper\nbound for the prediction error of our proposed approach. Our upper bound is a\nfunction of a number of parameters that are based on a certain weight matrix\nthat depends on the joint distribution of users and products. Utilizing this\nobservation, we introduce a new optimization problem to select a weight matrix\nthat minimizes the upper bound on the prediction error. The final product is a\nnew estimator, NU-Recommend, that outperforms existing methods in both\nsynthetic and real datasets. Our approach aims at accurate predictions for all\nusers while prioritizing fairness. To achieve this, we employ a bias-variance\ntradeoff mechanism that ensures good overall prediction performance without\ncompromising the predictive accuracy for less active users.\n","authors":["Wanning Chen","Mohsen Bayati"],"pdf_url":"https://arxiv.org/pdf/2110.11248v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03445v1","updated":"2023-03-06T19:08:51Z","published":"2023-03-06T19:08:51Z","title":"How Auditing Methodologies Can Impact Our Understanding of YouTube's\n  Recommendation Systems","summary":"  Data generated by audits of social media websites have formed the basis of\nour understanding of the biases presented in algorithmic content recommendation\nsystems. As legislators around the world are beginning to consider regulating\nthe algorithmic systems that drive online platforms, it is critical to ensure\nthe correctness of these inferred biases. However, as we will show in this\npaper, doing so is a challenging task for a variety of reasons related to the\ncomplexity of configuration parameters associated with the audits that gather\ndata from a specific platform.\n  Focusing specifically on YouTube, we show that conducting audits to make\ninferences about YouTube's recommendation systems is more methodologically\nchallenging than one might expect. There are many methodological decisions that\nneed to be considered in order to obtain scientifically valid results, and each\nof these decisions incur costs. For example, should an auditor use (expensive\nto obtain) logged-in YouTube accounts while gathering recommendations from the\nalgorithm to obtain more accurate inferences? We explore the impact of this and\nmany other decisions and make some startling discoveries about the\nmethodological choices that impact YouTube's recommendations. Taken all\ntogether, our research suggests auditing configuration compromises that YouTube\nauditors and researchers can use to reduce audit overhead, both economically\nand computationally, without sacrificing accuracy of their inferences.\nSimilarly, we also identify several configuration parameters that have a\nsignificant impact on the accuracy of measured inferences and should be\ncarefully considered.\n","authors":["Sarmad Chandio","Daniyal Pirwani Dar","Rishab Nithyanand"],"pdf_url":"https://arxiv.org/pdf/2303.03445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03321v1","updated":"2023-03-06T17:48:27Z","published":"2023-03-06T17:48:27Z","title":"Implementation of a noisy hyperlink removal system: A semantic and\n  relatedness approach","summary":"  As the volume of data on the web grows, the web structure graph, which is a\ngraph representation of the web, continues to evolve. The structure of this\ngraph has gradually shifted from content-based to non-content-based.\nFurthermore, spam data, such as noisy hyperlinks, in the web structure graph\nadversely affect the speed and efficiency of information retrieval and link\nmining algorithms. Previous works in this area have focused on removing noisy\nhyperlinks using structural and string approaches. However, these approaches\nmay incorrectly remove useful links or be unable to detect noisy hyperlinks in\ncertain circumstances. In this paper, a data collection of hyperlinks is\ninitially constructed using an interactive crawler. The semantic and\nrelatedness structure of the hyperlinks is then studied through semantic web\napproaches and tools such as the DBpedia ontology. Finally, the removal process\nof noisy hyperlinks is carried out using a reasoner on the DBpedia ontology.\nOur experiments demonstrate the accuracy and ability of semantic web\ntechnologies to remove noisy hyperlinks\n","authors":["Kazem Taghandiki","Elnaz Rezaei Ehsan"],"pdf_url":"https://arxiv.org/pdf/2303.03321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03290v1","updated":"2023-03-06T17:06:50Z","published":"2023-03-06T17:06:50Z","title":"AmQA: Amharic Question Answering Dataset","summary":"  Question Answering (QA) returns concise answers or answer lists from natural\nlanguage text given a context document. Many resources go into curating QA\ndatasets to advance robust models' development. There is a surge of QA datasets\nfor languages like English, however, this is not true for Amharic. Amharic, the\nofficial language of Ethiopia, is the second most spoken Semitic language in\nthe world. There is no published or publicly available Amharic QA dataset.\nHence, to foster the research in Amharic QA, we present the first Amharic QA\n(AmQA) dataset. We crowdsourced 2628 question-answer pairs over 378 Wikipedia\narticles. Additionally, we run an XLMR Large-based baseline model to spark\nopen-domain QA research interest. The best-performing baseline achieves an\nF-score of 69.58 and 71.74 in reader-retriever QA and reading comprehension\nsettings respectively.\n","authors":["Tilahun Abedissa","Ricardo Usbeck","Yaregal Assabie"],"pdf_url":"https://arxiv.org/pdf/2303.03290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03229v1","updated":"2023-03-06T15:47:45Z","published":"2023-03-06T15:47:45Z","title":"LongEval-Retrieval: French-English Dynamic Test Collection for\n  Continuous Web Search Evaluation","summary":"  LongEval-Retrieval is a Web document retrieval benchmark that focuses on\ncontinuous retrieval evaluation. This test collection is intended to be used to\nstudy the temporal persistence of Information Retrieval systems and will be\nused as the test collection in the Longitudinal Evaluation of Model Performance\nTrack (LongEval) at CLEF 2023. This benchmark simulates an evolving information\nsystem environment - such as the one a Web search engine operates in - where\nthe document collection, the query distribution, and relevance all move\ncontinuously, while following the Cranfield paradigm for offline evaluation. To\ndo that, we introduce the concept of a dynamic test collection that is composed\nof successive sub-collections each representing the state of an information\nsystem at a given time step. In LongEval-Retrieval, each sub-collection\ncontains a set of queries, documents, and soft relevance assessments built from\nclick models. The data comes from Qwant, a privacy-preserving Web search engine\nthat primarily focuses on the French market. LongEval-Retrieval also provides a\n'mirror' collection: it is initially constructed in the French language to\nbenefit from the majority of Qwant's traffic, before being translated to\nEnglish. This paper presents the creation process of LongEval-Retrieval and\nprovides baseline runs and analysis.\n","authors":["Petra Galuščáková","Romain Deveaud","Gabriela Gonzalez-Saez","Philippe Mulhem","Lorraine Goeuriot","Florina Piroi","Martin Popel"],"pdf_url":"https://arxiv.org/pdf/2303.03229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03050v1","updated":"2023-03-06T11:46:58Z","published":"2023-03-06T11:46:58Z","title":"MABNet: Master Assistant Buddy Network with Hybrid Learning for Image\n  Retrieval","summary":"  Image retrieval has garnered growing interest in recent times. The current\napproaches are either supervised or self-supervised. These methods do not\nexploit the benefits of hybrid learning using both supervision and\nself-supervision. We present a novel Master Assistant Buddy Network (MABNet)\nfor image retrieval which incorporates both learning mechanisms. MABNet\nconsists of master and assistant blocks, both learning independently through\nsupervision and collectively via self-supervision. The master guides the\nassistant by providing its knowledge base as a reference for self-supervision\nand the assistant reports its knowledge back to the master by weight transfer.\nWe perform extensive experiments on public datasets with and without\npost-processing.\n","authors":["Rohit Agarwal","Gyanendra Das","Saksham Aggarwal","Alexander Horsch","Dilip K. Prasad"],"pdf_url":"https://arxiv.org/pdf/2303.03050v1.pdf","comment":"Accepted at International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP) 2023"},{"id":"http://arxiv.org/abs/2303.02916v1","updated":"2023-03-06T06:21:20Z","published":"2023-03-06T06:21:20Z","title":"Privacy-Preserving Fair Item Ranking","summary":"  Users worldwide access massive amounts of curated data in the form of\nrankings on a daily basis. The societal impact of this ease of access has been\nstudied and work has been done to propose and enforce various notions of\nfairness in rankings. Current computational methods for fair item ranking rely\non disclosing user data to a centralized server, which gives rise to privacy\nconcerns for the users. This work is the first to advance research at the\nconjunction of producer (item) fairness and consumer (user) privacy in rankings\nby exploring the incorporation of privacy-preserving techniques; specifically,\ndifferential privacy and secure multi-party computation. Our work extends the\nequity of amortized attention ranking mechanism to be privacy-preserving, and\nwe evaluate its effects with respect to privacy, fairness, and ranking quality.\nOur results using real-world datasets show that we are able to effectively\npreserve the privacy of users and mitigate unfairness of items without making\nadditional sacrifices to the quality of rankings in comparison to the ranking\nmechanism in the clear.\n","authors":["Jia Ao Sun","Sikha Pentyala","Martine De Cock","Golnoosh Farnadi"],"pdf_url":"https://arxiv.org/pdf/2303.02916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.02867v1","updated":"2023-03-06T03:36:06Z","published":"2023-03-06T03:36:06Z","title":"Dual Feedback Attention Framework via Boundary-Aware Auxiliary and\n  Progressive Semantic Optimization for Salient Object Detection in Optical\n  Remote Sensing Imagery","summary":"  Salient object detection in optical remote sensing image (ORSI-SOD) has\ngradually attracted attention thanks to the development of deep learning (DL)\nand salient object detection in natural scene image (NSI-SOD). However, NSI and\nORSI are different in many aspects, such as large coverage, complex background,\nand large differences in target types and scales. Therefore, a new dedicated\nmethod is needed for ORSI-SOD. In addition, existing methods do not pay\nsufficient attention to the boundary of the object, and the completeness of the\nfinal saliency map still needs improvement. To address these issues, we propose\na novel method called Dual Feedback Attention Framework via Boundary-Aware\nAuxiliary and Progressive Semantic Optimization (DFA-BASO). First, Boundary\nProtection Calibration (BPC) module is proposed to reduce the loss of edge\nposition information during forward propagation and suppress noise in low-level\nfeatures. Second, a Dual Feature Feedback Complementary (DFFC) module is\nproposed based on BPC module. It aggregates boundary-semantic dual features and\nprovides effective feedback to coordinate features across different layers.\nFinally, a Strong Semantic Feedback Refinement (SSFR) module is proposed to\nobtain more complete saliency maps. This module further refines feature\nrepresentation and eliminates feature differences through a unique feedback\nmechanism. Extensive experiments on two public datasets show that DFA-BASO\noutperforms 15 state-of-the-art methods. Furthermore, this paper strongly\ndemonstrates the true contribution of DFA-BASO to ORSI-SOD by in-depth analysis\nof the visualization figure. All codes can be found at\nhttps://github.com/YUHsss/DFA-BASO.\n","authors":["Dejun Feng","Hongyu Chen","Suning Liu","Xingyu Shen","Ziyang Liao","Yakun Xie","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.02867v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.02851v1","updated":"2023-03-06T03:10:38Z","published":"2023-03-06T03:10:38Z","title":"A Survey on Incremental Update for Neural Recommender Systems","summary":"  Recommender Systems (RS) aim to provide personalized suggestions of items for\nusers against consumer over-choice. Although extensive research has been\nconducted to address different aspects and challenges of RS, there still exists\na gap between academic research and industrial applications. Specifically, most\nof the existing models still work in an offline manner, in which the\nrecommender is trained on a large static training set and evaluated on a very\nrestrictive testing set in a one-time process. RS will stay unchanged until the\nnext batch retrain is performed. We frame such RS as Batch Update Recommender\nSystems (BURS). In reality, they have to face the challenges where RS are\nexpected to be instantly updated with new data streaming in, and generate\nupdated recommendations for current user activities based on the newly arrived\ndata. We frame such RS as Incremental Update Recommender Systems (IURS).\n  In this article, we offer a systematic survey of incremental update for\nneural recommender systems. We begin the survey by introducing key concepts and\nformulating the task of IURS. We then illustrate the challenges in IURS\ncompared with traditional BURS. Afterwards, we detail the introduction of\nexisting literature and evaluation issues. We conclude the survey by outlining\nsome prominent open research issues in this area.\n","authors":["Peiyan Zhang","Sunghun Kim"],"pdf_url":"https://arxiv.org/pdf/2303.02851v1.pdf","comment":"18 pages"}],"Multimedia":[{"id":"http://arxiv.org/abs/2207.00056v3","updated":"2023-03-06T19:39:18Z","published":"2022-06-30T18:42:06Z","title":"MultiViz: Towards Visualizing and Understanding Multimodal Models","summary":"  The promise of multimodal models for real-world applications has inspired\nresearch in visualizing and understanding their internal mechanics with the end\ngoal of empowering stakeholders to visualize model behavior, perform model\ndebugging, and promote trust in machine learning models. However, modern\nmultimodal models are typically black-box neural networks, which makes it\nchallenging to understand their internal mechanics. How can we visualize the\ninternal modeling of multimodal interactions in these models? Our paper aims to\nfill this gap by proposing MultiViz, a method for analyzing the behavior of\nmultimodal models by scaffolding the problem of interpretability into 4 stages:\n(1) unimodal importance: how each modality contributes towards downstream\nmodeling and prediction, (2) cross-modal interactions: how different modalities\nrelate with each other, (3) multimodal representations: how unimodal and\ncross-modal interactions are represented in decision-level features, and (4)\nmultimodal prediction: how decision-level features are composed to make a\nprediction. MultiViz is designed to operate on diverse modalities, models,\ntasks, and research areas. Through experiments on 8 trained models across 6\nreal-world tasks, we show that the complementary stages in MultiViz together\nenable users to (1) simulate model predictions, (2) assign interpretable\nconcepts to features, (3) perform error analysis on model misclassifications,\nand (4) use insights from error analysis to debug models. MultiViz is publicly\navailable, will be regularly updated with new interpretation tools and metrics,\nand welcomes inputs from the community.\n","authors":["Paul Pu Liang","Yiwei Lyu","Gunjan Chhablani","Nihal Jain","Zihao Deng","Xingbo Wang","Louis-Philippe Morency","Ruslan Salakhutdinov"],"pdf_url":"https://arxiv.org/pdf/2207.00056v3.pdf","comment":"ICLR 2023. Code available at: https://github.com/pliang279/MultiViz"},{"id":"http://arxiv.org/abs/2303.03171v1","updated":"2023-03-06T14:39:54Z","published":"2023-03-06T14:39:54Z","title":"Neighborhood Contrastive Transformer for Change Captioning","summary":"  Change captioning is to describe the semantic change between a pair of\nsimilar images in natural language. It is more challenging than general image\ncaptioning, because it requires capturing fine-grained change information while\nbeing immune to irrelevant viewpoint changes, and solving syntax ambiguity in\nchange descriptions. In this paper, we propose a neighborhood contrastive\ntransformer to improve the model's perceiving ability for various changes under\ndifferent scenes and cognition ability for complex syntax structure.\nConcretely, we first design a neighboring feature aggregating to integrate\nneighboring context into each feature, which helps quickly locate the\ninconspicuous changes under the guidance of conspicuous referents. Then, we\ndevise a common feature distilling to compare two images at neighborhood level\nand extract common properties from each image, so as to learn effective\ncontrastive information between them. Finally, we introduce the explicit\ndependencies between words to calibrate the transformer decoder, which helps\nbetter understand complex syntax structure during training. Extensive\nexperimental results demonstrate that the proposed method achieves the\nstate-of-the-art performance on three public datasets with different change\nscenarios. The code is available at https://github.com/tuyunbin/NCT.\n","authors":["Yunbin Tu","Liang Li","Li Su","Ke Lu","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2303.03171v1.pdf","comment":"Accepted by IEEE TMM"},{"id":"http://arxiv.org/abs/2303.03144v1","updated":"2023-03-06T13:59:37Z","published":"2023-03-06T13:59:37Z","title":"IPA-CLIP: Integrating Phonetic Priors into Vision and Language\n  Pretraining","summary":"  Recently, large-scale Vision and Language (V\\&L) pretraining has become the\nstandard backbone of many multimedia systems. While it has shown remarkable\nperformance even in unseen situations, it often performs in ways not intuitive\nto humans. Particularly, they usually do not consider the pronunciation of the\ninput, which humans would utilize to understand language, especially when it\ncomes to unknown words. Thus, this paper inserts phonetic prior into\nContrastive Language-Image Pretraining (CLIP), one of the V\\&L pretrained\nmodels, to make it consider the pronunciation similarity among its\npronunciation inputs. To achieve this, we first propose a phoneme embedding\nthat utilizes the phoneme relationships provided by the International Phonetic\nAlphabet (IPA) chart as a phonetic prior. Next, by distilling the frozen CLIP\ntext encoder, we train a pronunciation encoder employing the IPA-based\nembedding. The proposed model named IPA-CLIP comprises this pronunciation\nencoder and the original CLIP encoders (image and text). Quantitative\nevaluation reveals that the phoneme distribution on the embedding space\nrepresents phonetic relationships more accurately when using the proposed\nphoneme embedding. Furthermore, in some multimodal retrieval tasks, we confirm\nthat the proposed pronunciation encoder enhances the performance of the text\nencoder and that the pronunciation encoder handles nonsense words in a more\nphonetic manner than the text encoder. Finally, qualitative evaluation verifies\nthe correlation between the pronunciation encoder and human perception\nregarding pronunciation similarity.\n","authors":["Chihaya Matsuhira","Marc A. Kastner","Takahiro Komamizu","Takatsugu Hirayama","Keisuke Doman","Yasutomo Kawanishi","Ichiro Ide"],"pdf_url":"https://arxiv.org/pdf/2303.03144v1.pdf","comment":"11 pages, 8 figures, 5 Tables"},{"id":"http://arxiv.org/abs/2210.14889v2","updated":"2023-03-06T11:50:26Z","published":"2022-10-24T17:40:07Z","title":"Perfectly Secure Steganography Using Minimum Entropy Coupling","summary":"  Steganography is the practice of encoding secret information into innocuous\ncontent in such a manner that an adversarial third party would not realize that\nthere is hidden meaning. While this problem has classically been studied in\nsecurity literature, recent advances in generative models have led to a shared\ninterest among security and machine learning researchers in developing scalable\nsteganography techniques. In this work, we show that a steganography procedure\nis perfectly secure under \\citet{cachin_perfect}'s information theoretic-model\nof steganography if and only if it is induced by a coupling. Furthermore, we\nshow that, among perfectly secure procedures, a procedure is maximally\nefficient if and only if it is induced by a minimum entropy coupling. These\ninsights yield what are, to the best of our knowledge, the first steganography\nalgorithms to achieve perfect security guarantees with non-trivial efficiency;\nadditionally, these algorithms are highly scalable. To provide empirical\nvalidation, we compare a minimum entropy coupling-based approach to three\nmodern baselines -- arithmetic coding, Meteor, and adaptive dynamic grouping --\nusing GPT-2 and WaveRNN as communication channels. We find that the minimum\nentropy coupling-based approach yields superior encoding efficiency, despite\nits stronger security constraints. In aggregate, these results suggest that it\nmay be natural to view information-theoretic steganography through the lens of\nminimum entropy coupling.\n","authors":["Christian Schroeder de Witt","Samuel Sokota","J. Zico Kolter","Jakob Foerster","Martin Strohmeier"],"pdf_url":"https://arxiv.org/pdf/2210.14889v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.02959v1","updated":"2023-03-06T08:19:15Z","published":"2023-03-06T08:19:15Z","title":"Butterfly: Multiple Reference Frames Feature Propagation Mechanism for\n  Neural Video Compression","summary":"  Using more reference frames can significantly improve the compression\nefficiency in neural video compression. However, in low-latency scenarios, most\nexisting neural video compression frameworks usually use the previous one frame\nas reference. Or a few frameworks which use the previous multiple frames as\nreference only adopt a simple multi-reference frames propagation mechanism. In\nthis paper, we present a more reasonable multi-reference frames propagation\nmechanism for neural video compression, called butterfly multi-reference frame\npropagation mechanism (Butterfly), which allows a more effective feature fusion\nof multi-reference frames. By this, we can generate more accurate temporal\ncontext conditional prior for Contextual Coding Module. Besides, when the\nnumber of decoded frames does not meet the required number of reference frames,\nwe duplicate the nearest reference frame to achieve the requirement, which is\nbetter than duplicating the furthest one. Experiment results show that our\nmethod can significantly outperform the previous state-of-the-art (SOTA), and\nour neural codec can achieve -7.6% bitrate save on HEVC Class D dataset when\ncompares with our base single-reference frame model with the same compression\nconfiguration.\n","authors":["Feng Wang","Haihang Ruan","Fei Xiong","Jiayu Yang","Litian Li","Ronggang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.02959v1.pdf","comment":"Accepted by DCC 2023"}]}}