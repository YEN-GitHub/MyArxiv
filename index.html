<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-13T00:00:00Z">2023-03-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Group Recommendation Based on a Probabilistic Semantic
  Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorge Dueñas-Lerín, Raúl Lara-Cabrera, Fernando Ortega, Jesús Bobadilla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation to groups of users is a challenging subfield of recommendation
systems. Its key concept is how and where to make the aggregation of each set
of user information into an individual entity, such as a ranked recommendation
list, a virtual user, or a multi-hot input vector encoding. This paper proposes
an innovative strategy where aggregation is made in the multi-hot vector that
feeds the neural network model. The aggregation provides a probabilistic
semantic, and the resulting input vectors feed a model that is able to
conveniently generalize the group recommendation from the individual
predictions. Furthermore, using the proposed architecture, group
recommendations can be obtained by simply feedforwarding the pre-trained model
with individual ratings; that is, without the need to obtain datasets
containing group of user information, and without the need of running two
separate trainings (individual and group). This approach also avoids
maintaining two different models to support both individual and group learning.
Experiments have tested the proposed architecture using three representative
collaborative filtering datasets and a series of baselines; results show
suitable accuracy improvements compared to the state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Single Items: Exploring User Preferences in Item Sets with the
  Conversational Playlist Curation <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arun Tejasvi Chaganty, Megan Leszczynski, Shu Zhang, Ravi Ganti, Krisztian Balog, Filip Radlinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Users in consumption domains, like music, are often able to more efficiently
provide preferences over a set of items (e.g. a playlist or radio) than over
single items (e.g. songs). Unfortunately, this is an underexplored area of
research, with most existing recommendation systems limited to understanding
preferences over single items. Curating an item set exponentiates the search
space that recommender systems must consider (all subsets of items!): this
motivates conversational approaches-where users explicitly state or refine
their preferences and systems elicit preferences in natural language-as an
efficient way to understand user needs. We call this task conversational item
set curation and present a novel data collection methodology that efficiently
collects realistic preferences about item sets in a conversational setting by
observing both item-level and set-level feedback. We apply this methodology to
music recommendation to build the Conversational Playlist Curation Dataset
(CPCD), where we show that it leads raters to express preferences that would
not be otherwise expressed. Finally, we propose a wide range of conversational
retrieval models as baselines for this task and evaluate them on the dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NASTyLinker: NIL-Aware Scalable <span class="highlight-title">Transformer</span>-based Entity Linker <span class="chip">ESWC'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Heist, Heiko Paulheim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity Linking (EL) is the task of detecting mentions of entities in text and
disambiguating them to a reference knowledge base. Most prevalent EL approaches
assume that the reference knowledge base is complete. In practice, however, it
is necessary to deal with the case of linking to an entity that is not
contained in the knowledge base (NIL entity). Recent works have shown that,
instead of focusing only on affinities between mentions and entities,
considering inter-mention affinities can be used to represent NIL entities by
producing clusters of mentions. At the same time, inter-mention affinities can
help to substantially improve linking performance for known entities. With
NASTyLinker, we introduce an EL approach that is aware of NIL entities and
produces corresponding mention clusters while maintaining high linking
performance for known entities. The approach clusters mentions and entities
based on dense representations from Transformers and resolves conflicts (if
more than one entity is assigned to a cluster) by computing transitive
mention-entity affinities. We show the effectiveness and scalability of
NASTyLinker on NILK, a dataset that is explicitly constructed to evaluate EL
with respect to NIL entities. Further, we apply the presented approach to an
actual EL task, namely to knowledge graph population by linking entities in
Wikipedia listings, and provide an analysis of the outcome.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of a paper in the research track of the 20th Extended
  Semantic Web Conference (ESWC'23)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Class-Incremental Learning with <span class="highlight-title">Pre-Train</span>ed Models:
  Generalizability and Adaptivity are All You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Da-Wei Zhou, Han-Jia Ye, De-Chuan Zhan, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning (CIL) aims to adapt to emerging new classes
without forgetting old ones. Traditional CIL models are trained from scratch to
continually acquire knowledge as data evolves. Recently, pre-training has
achieved substantial progress, making vast pre-trained models (PTMs) accessible
for CIL. Contrary to traditional methods, PTMs possess generalizable
embeddings, which can be easily transferred. In this work, we revisit CIL with
PTMs and argue that the core factors in CIL are adaptivity for model updating
and generalizability for knowledge transferring. 1) We first reveal that frozen
PTM can already provide generalizable embeddings for CIL. Surprisingly, a
simple baseline (SimpleCIL) which continually sets the classifiers of PTM to
prototype features can beat state-of-the-art even without training on the
downstream task. 2) Due to the distribution gap between pre-trained and
downstream datasets, PTM can be further cultivated with adaptivity via model
adapting. We propose ADapt And Merge (ADAM), which aggregates the embeddings of
PTM and adapted models for classifier construction. ADAM is a general framework
that can be orthogonally combined with any parameter-efficient tuning method,
which holds the advantages of PTM's generalizability and adapted model's
adaptivity. 3) Additionally, we find previous benchmarks are unsuitable in the
era of PTM due to data overlapping and propose four new benchmarks for
assessment, namely ImageNet-A, ObjectNet, OmniBenchmark, and VTAB. Extensive
experiments validate the effectiveness of ADAM with a unified and concise
framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at: https://github.com/zhoudw-zdw/RevisitingCIL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collision Cross-entropy and EM Algorithm for Self-labeled Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongwen Zhang, Yuri Boykov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose "collision cross-entropy" as a robust alternative to the Shannon's
cross-entropy in the context of self-labeled classification with posterior
models. Assuming unlabeled data, self-labeling works by estimating latent
pseudo-labels, categorical distributions y, that optimize some discriminative
clustering criteria, e.g. "decisiveness" and "fairness". All existing
self-labeled losses incorporate Shannon's cross-entropy term targeting the
model prediction, softmax, at the estimated distribution y. In fact, softmax is
trained to mimic the uncertainty in y exactly. Instead, we propose the negative
log-likelihood of "collision" to maximize the probability of equality between
two random variables represented by distributions softmax and y. We show that
our loss satisfies some properties of a generalized cross-entropy.
Interestingly, it agrees with the Shannon's cross-entropy for one-hot
pseudo-labels y, but the training from softer labels weakens. For example, if y
is a uniform distribution at some data point, it has zero contribution to the
training. Our self-labeling loss combining collision cross entropy with basic
clustering criteria is convex w.r.t. pseudo-labels, but non-trivial to optimize
over the probability simplex. We derive a practical EM algorithm optimizing
pseudo-labels y significantly faster than generic methods, e.g. the projectile
gradient descent. The collision cross-entropy consistently improves the results
on multiple self-labeled clustering examples using different DNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model-tuning Via <span class="highlight-title">Prompt</span>s Makes NLP Models Adversarially Robust 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mrigank Raman, Pratyush Maini, J. Zico Kolter, Zachary C. Lipton, Danish Pruthi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, NLP practitioners have converged on the following practice:
(i) import an off-the-shelf pretrained (masked) language model; (ii) append a
multilayer perceptron atop the CLS token's hidden representation (with randomly
initialized weights); and (iii) fine-tune the entire model on a downstream task
(MLP). This procedure has produced massive gains on standard NLP benchmarks,
but these models remain brittle, even to mild adversarial perturbations, such
as word-level synonym substitutions. In this work, we demonstrate surprising
gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an
alternative method of adapting to downstream tasks. Rather than modifying the
model (by appending an MLP head), MVP instead modifies the input (by appending
a prompt template). Across three classification datasets, MVP improves
performance against adversarial word-level synonym substitutions by an average
of 8% over standard methods and even outperforms adversarial training-based
state-of-art defenses by 3.5%. By combining MVP with adversarial training, we
achieve further improvements in robust accuracy while maintaining clean
accuracy. Finally, we conduct ablations to investigate the mechanism underlying
these gains. Notably, we find that the main causes of vulnerability of MLP can
be attributed to the misalignment between pre-training and fine-tuning tasks,
and the randomly initialized MLP parameters. Code is available at
https://github.com/acmi-lab/mvp
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Reduced-Order Models for Cardiovascular Simulations with Graph
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Pegolotti, Martin R. Pfaller, Natalia L. Rubio, Ke Ding, Rita Brugarolas Brufau, Eric Darve, Alison L. Marsden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reduced-order models based on physics are a popular choice in cardiovascular
modeling due to their efficiency, but they may experience reduced accuracy when
working with anatomies that contain numerous junctions or pathological
conditions. We develop one-dimensional reduced-order models that simulate blood
flow dynamics using a graph neural network trained on three-dimensional
hemodynamic simulation data. Given the initial condition of the system, the
network iteratively predicts the pressure and flow rate at the vessel
centerline nodes. Our numerical results demonstrate the accuracy and
generalizability of our method in physiological geometries comprising a variety
of anatomies and boundary conditions. Our findings demonstrate that our
approach can achieve errors below 2% and 3% for pressure and flow rate,
respectively, provided there is adequate training data. As a result, our method
exhibits superior performance compared to physics-based one-dimensional models,
while maintaining high efficiency at inference time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial
  Understanding with Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahui Fu, Yilun Du, Kurran Singh, Joshua B. Tenenbaum, John J. Leonard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present NeuSE, a novel Neural SE(3)-Equivariant Embedding for objects, and
illustrate how it supports object SLAM for consistent spatial understanding
with long-term scene changes. NeuSE is a set of latent object embeddings
created from partial object observations. It serves as a compact point cloud
surrogate for complete object models, encoding full shape information while
transforming SE(3)-equivariantly in tandem with the object in the physical
world. With NeuSE, relative frame transforms can be directly derived from
inferred latent codes. Our proposed SLAM paradigm, using NeuSE for object shape
and pose characterization, can operate independently or in conjunction with
typical SLAM systems. It directly infers SE(3) camera pose constraints that are
compatible with general SLAM pose graph optimization, while also maintaining a
lightweight object-centric map that adapts to real-world changes. Our approach
is evaluated on synthetic and real-world sequences featuring changed objects
and shows improved localization accuracy and change-aware mapping capability,
when working either standalone or jointly with a common SLAM pipeline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://neuse-slam.github.io/neuse/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span> Models for Acute Brain Dysfunction Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon Silva, Miguel Contreras, Tezcan Ozrazgat Baslanti, Yuanfang Ren, Guan Ziyuan, Kia Khezeli, Azra Bihorac, Parisa Rashidi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acute brain dysfunctions (ABD), which include coma and delirium, are
prevalent in the ICU, especially among older patients. The current approach in
manual assessment of ABD by care providers may be sporadic and subjective.
Hence, there exists a need for a data-driven robust system automating the
assessment and prediction of ABD. In this work, we develop a machine learning
system for real-time prediction of ADB using Electronic Health Record (HER)
data. Our data processing pipeline enables integration of static and temporal
data, and extraction of features relevant to ABD. We train several
state-of-the-art transformer models and baseline machine learning models
including CatBoost and XGB on the data that was collected from patients
admitted to the ICU at UF Shands Hospital. We demonstrate the efficacy of our
system for tasks related to acute brain dysfunction including binary
classification of brain acuity and multi-class classification (i.e., coma,
delirium, death, or normal), achieving a mean AUROC of 0.953 on our Long-former
implementation. Our system can then be deployed for real-time prediction of ADB
in ICUs to reduce the number of incidents caused by ABD. Moreover, the
real-time system has the potential to reduce costs, duration of patients stays
in the ICU, and mortality among those afflicted.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meet in the Middle: A New <span class="highlight-title">Pre-train</span>ing Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anh Nguyen, Nikos Karampatziakis, Weizhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most language models (LMs) are trained and applied in an autoregressive
left-to-right fashion, assuming that the next token only depends on the
preceding ones. However, this assumption ignores the potential benefits of
using the full sequence information during training, and the possibility of
having context from both sides during inference. In this paper, we propose a
new pre-training paradigm with techniques that jointly improve the training
data efficiency and the capabilities of the LMs in the infilling task. The
first is a training objective that aligns the predictions of a left-to-right LM
with those of a right-to-left LM, trained on the same data but in reverse
order. The second is a bidirectional inference procedure that enables both LMs
to meet in the middle. We show the effectiveness of our pre-training paradigm
with extensive experiments on both programming and natural language models,
outperforming strong baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tight Non-asymptotic Inference via Sub-Gaussian Intrinsic Moment Norm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiming Zhang, Haoyu Wei, Guang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In non-asymptotic statistical inferences, variance-type parameters of
sub-Gaussian distributions play a crucial role. However, direct estimation of
these parameters based on the empirical moment generating function (MGF) is
infeasible. To this end, we recommend using a sub-Gaussian intrinsic moment
norm [Buldygin and Kozachenko (2000), Theorem 1.3] through maximizing a series
of normalized moments. Importantly, the recommended norm can not only recover
the exponential moment bounds for the corresponding MGFs, but also lead to
tighter Hoeffding's sub-Gaussian concentration inequalities. In practice,
{\color{black} we propose an intuitive way of checking sub-Gaussian data with a
finite sample size by the sub-Gaussian plot}. Intrinsic moment norm can be
robustly estimated via a simple plug-in approach. Our theoretical results are
applied to non-asymptotic analysis, including the multi-armed bandit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision-Language Models as Success Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqing Du, Ksenia Konyushkova, Misha Denil, Akhil Raju, Jessica Landon, Felix Hill, Nando de Freitas, Serkan Cabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting successful behaviour is crucial for training intelligent agents. As
such, generalisable reward models are a prerequisite for agents that can learn
to generalise their behaviour. In this work we focus on developing robust
success detectors that leverage large, pretrained vision-language models
(Flamingo, Alayrac et al. (2022)) and human reward annotations. Concretely, we
treat success detection as a visual question answering (VQA) problem, denoted
SuccessVQA. We study success detection across three vastly different domains:
(i) interactive language-conditioned agents in a simulated household, (ii) real
world robotic manipulation, and (iii) "in-the-wild" human egocentric videos. We
investigate the generalisation properties of a Flamingo-based success detection
model across unseen language and visual changes in the first two domains, and
find that the proposed method is able to outperform bespoke reward models in
out-of-distribution test scenarios with either variation. In the last domain of
"in-the-wild" human videos, we show that success detection on unseen real
videos presents an even more challenging generalisation task warranting future
work. We hope our initial results encourage further work in real world success
detection and reward modelling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Graph <span class="highlight-title">Prompt</span>ing Methods: Techniques, Applications, and
  Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuansheng Wu, Kaixiong Zhou, Mingchen Sun, Xin Wang, Ninghao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning has achieved great success on various tasks, the
task-specific model training notoriously relies on a large volume of labeled
data. Recently, a new training paradigm of ``pre-train, prompt, predict'' has
been proposed to improve model generalization ability with limited labeled
data. The main idea is that, based on a pre-trained model, the prompting
function uses a template to augment input samples with indicative context and
reformalizes the target task to one of the pre-training tasks. In this survey,
we provide a unique review of prompting methods from the graph perspective.
Graph data has served as structured knowledge repositories in various systems
by explicitly modeling the interaction between entities. Compared with
traditional methods, graph prompting functions could induce task-related
context and apply templates with structured knowledge. The pre-trained model is
then adaptively generalized for future samples. In particular, we introduce the
basic concepts of graph prompt learning, organize the existing work of
designing graph prompting functions, and describe their applications and
challenges to a variety of machine learning problems. This survey attempts to
bridge the gap between structured graphs and prompt design to facilitate future
methodology development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Surface-normal Based Neural Framework for Colonoscopy Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuxian Wang, Yubo Zhang, Sarah K. McGill, Julian G. Rosenman, Jan-Michael Frahm, Soumyadip Sengupta, Stephen M. Pizer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing a 3D surface from colonoscopy video is challenging due to
illumination and reflectivity variation in the video frame that can cause
defective shape predictions. Aiming to overcome this challenge, we utilize the
characteristics of surface normal vectors and develop a two-step neural
framework that significantly improves the colonoscopy reconstruction quality.
The normal-based depth initialization network trained with self-supervised
normal consistency loss provides depth map initialization to the normal-depth
refinement module, which utilizes the relationship between illumination and
surface normals to refine the frame-wise normal and depth predictions
recursively. Our framework's depth accuracy performance on phantom colonoscopy
data demonstrates the value of exploiting the surface normals in colonoscopy
reconstruction, especially on en face views. Due to its low depth error, the
prediction result from our framework will require limited post-processing to be
clinically applicable for real-time colonoscopy reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IPMI 2023; first two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Channel Estimation for Underwater Visible Light Communication: A Sparse
  Learning Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Younan Mou, Sicong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The underwater propagation environment for visible light signals is affected
by complex factors such as absorption, shadowing, and reflection, making it
very challengeable to achieve effective underwater visible light communication
(UVLC) channel estimation. It is difficult for the UVLC channel to be sparse
represented in the time and frequency domains, which limits the chance of using
sparse signal processing techniques to achieve better performance of channel
estimation. To this end, a compressed sensing (CS) based framework is
established in this paper by fully exploiting the sparsity of the underwater
visible light channel in the distance domain of the propagation links. In order
to solve the sparse recovery problem and achieve more accurate UVLC channel
estimation, a sparse learning based underwater visible light channel estimation
(SL-UVCE) scheme is proposed. Specifically, a deep-unfolding neural network
mimicking the classical iterative sparse recovery algorithm of approximate
message passing (AMP) is employed, which decomposes the iterations of AMP into
a series of layers with different learnable parameters. Compared with the
existing non-CS-based and CS-based schemes, the proposed scheme shows better
performance of accuracy in channel estimation, especially in severe conditions
such as insufficient measurement pilots and large number of multipath
components.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by and is to appear in Proc. 2023 IEEE
  International Conference on Communications (ICC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PMC-CLIP: Contrastive Language-Image <span class="highlight-title">Pre-train</span>ing using Biomedical
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models trained on large-scale dataset gain a recent surge in CV
and NLP. In contrast, development in biomedical domain lags far behind due to
data scarcity. To address this issue, we build and release PMC-OA, a biomedical
dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess
subset, which is 8 times larger than before. PMC-OA covers diverse modalities
or diseases, with majority of the image-caption samples aligned at
finer-grained level, i.e., subfigure and subcaption. While pretraining a
CLIP-style model on PMC-OA, our model named PMC-CLIP achieves state-of-the-art
results on various downstream tasks, including image-text retrieval on ROCO,
MedMNIST image classification, Medical VQA, i.e. +8.1% R@10 on image-text
retrieval, +3.9% accuracy on image classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Convolutional Neural Networks for Chronic Obstructive
  Pulmonary Disease Detection in Clinical Computed Tomography Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tina Dorosti, Manuel Schultheiss, Felix Hofmann, Luisa Kirchner, Theresa Urban, Franz Pfeiffer, Johannes Thalhammer, Florian Schaff, Tobias Lasser, Daniela Pfeiffer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chronic Obstructive Pulmonary Disease (COPD) is a leading cause of death
worldwide, yet early detection and treatment can prevent the progression of the
disease. In contrast to the conventional method of detecting COPD with
spirometry tests, X-ray Computed Tomography (CT) scans of the chest provide a
measure of morphological changes in the lung. It has been shown that automated
detection of COPD can be performed with deep learning models. However, the
potential of incorporating optimal window setting selection, typically carried
out by clinicians during examination of CT scans for COPD, is generally
overlooked in deep learning approaches. We aim to optimize the binary
classification of COPD with densely connected convolutional neural networks
(DenseNets) through implementation of manual and automated Window-Setting
Optimization (WSO) steps. Our dataset consisted of 78 CT scans from the
Klinikum rechts der Isar research hospital. Repeated inference on the test set
showed that without WSO, the plain DenseNet resulted in a mean slice-level AUC
of 0.80$\pm$0.05. With input images manually adjusted to the emphysema window
setting, the plain DenseNet model predicted COPD with a mean AUC of
0.86$\pm$0.04. By automating the WSO through addition of a customized layer to
the DenseNet, an optimal window setting in the proximity of the emphysema
window setting was learned and a mean AUC of 0.82$\pm$0.04 was achieved.
Detection of COPD with DenseNet models was optimized by WSO of CT data to the
emphysema window setting range, demonstrating the importance of implementing
optimal window setting selection in the deep learning pipeline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Traffic Prediction with Transfer Learning: A Mutual Information-based
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunjie Huang, Xiaozhuang Song, Yuanshao Zhu, Shiyao Zhang, James J. Q. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In modern traffic management, one of the most essential yet challenging tasks
is accurately and timely predicting traffic. It has been well investigated and
examined that deep learning-based Spatio-temporal models have an edge when
exploiting Spatio-temporal relationships in traffic data. Typically,
data-driven models require vast volumes of data, but gathering data in small
cities can be difficult owing to constraints such as equipment deployment and
maintenance costs. To resolve this problem, we propose TrafficTL, a cross-city
traffic prediction approach that uses big data from other cities to aid
data-scarce cities in traffic prediction. Utilizing a periodicity-based
transfer paradigm, it identifies data similarity and reduces negative transfer
caused by the disparity between two data distributions from distant cities. In
addition, the suggested method employs graph reconstruction techniques to
rectify defects in data from small data cities. TrafficTL is evaluated by
comprehensive case studies on three real-world datasets and outperforms the
state-of-the-art baseline by around 8 to 25 percent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submited to T-ITS, 16 pages, 13 figures in color</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incomplete Multi-View Multi-Label Learning via Label-Guided Masked View-
  and Category-Aware <span class="highlight-title">Transformer</span>s <span class="chip">AAAI-23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengliang Liu, Jie Wen, Xiaoling Luo, Yong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As we all know, multi-view data is more expressive than single-view data and
multi-label annotation enjoys richer supervision information than single-label,
which makes multi-view multi-label learning widely applicable for various
pattern recognition tasks. In this complex representation learning problem,
three main challenges can be characterized as follows: i) How to learn
consistent representations of samples across all views? ii) How to exploit and
utilize category correlations of multi-label to guide inference? iii) How to
avoid the negative impact resulting from the incompleteness of views or labels?
To cope with these problems, we propose a general multi-view multi-label
learning framework named label-guided masked view- and category-aware
transformers in this paper. First, we design two transformer-style based
modules for cross-view features aggregation and multi-label classification,
respectively. The former aggregates information from different views in the
process of extracting view-specific features, and the latter learns subcategory
embedding to improve classification performance. Second, considering the
imbalance of expressive power among views, an adaptively weighted view fusion
module is proposed to obtain view-consistent embedding features. Third, we
impose a label manifold constraint in sample-level representation learning to
maximize the utilization of supervised information. Last but not least, all the
modules are designed under the premise of incomplete views and labels, which
makes our method adaptable to arbitrary multi-view and multi-label data.
Extensive experiments on five datasets confirm that our method has clear
advantages over other state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI-23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Visual Number Discrimination in Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivana Kajić, Aida Nematzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to discriminate between large and small quantities is a core
aspect of basic numerical competence in both humans and animals. In this work,
we examine the extent to which the state-of-the-art neural networks designed
for vision exhibit this basic ability. Motivated by studies in animal and
infant numerical cognition, we use the numerical bisection procedure to test
number discrimination in different families of neural architectures. Our
results suggest that vision-specific inductive biases are helpful in numerosity
discrimination, as models with such biases have lowest test errors on the task,
and often have psychometric curves that qualitatively resemble those of humans
and animals performing the task. However, even the strongest models, as
measured on standard metrics of performance, fail to discriminate quantities in
transfer experiments with differing training and testing conditions, indicating
that such inductive biases might not be sufficient.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Tree Search for Automatic Program Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aran Carmon, Lior Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the task of automatic program synthesis, one obtains pairs of matching
inputs and outputs and generates a computer program, in a particular
domain-specific language (DSL), which given each sample input returns the
matching output. A key element is being able to perform an efficient search in
the space of valid programs. Here, we suggest a variant of MCTS that leads to
state of the art results on two vastly different DSLs. The exploration method
we propose includes multiple contributions: a modified visit count, a
preprocessing procedure for the training dataset, and encoding the part of the
program that was already executed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 2nd Exploration in Reinforcement Learning Workshop
  at the 36th International Conference on Machine Learning, 2019</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tighter Lower Bounds for Shuffling SGD: Random Permutations and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeyoung Cha, Jaewook Lee, Chulhee Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study convergence lower bounds of without-replacement stochastic gradient
descent (SGD) for solving smooth (strongly-)convex finite-sum minimization
problems. Unlike most existing results focusing on final iterate lower bounds
in terms of the number of components $n$ and the number of epochs $K$, we seek
bounds for arbitrary weighted average iterates that are tight in all factors
including the condition number $\kappa$. For SGD with Random Reshuffling, we
present lower bounds that have tighter $\kappa$ dependencies than existing
bounds. Our results are the first to perfectly close the gap between lower and
upper bounds for weighted average iterates in both strongly-convex and convex
cases. We also prove weighted average iterate lower bounds for arbitrary
permutation-based SGD, which apply to all variants that carefully choose the
best permutation. Our bounds improve the existing bounds in factors of $n$ and
$\kappa$ and thereby match the upper bounds shown for a recently proposed
algorithm called GraB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>62 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differential Good Arm Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun-Da Tsai, Tzu-Hsien Tsai, Shou-De Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper targets a variant of the stochastic multi-armed bandit problem
called good arm identification (GAI). GAI is a pure-exploration bandit problem
with the goal to output as many good arms using as few samples as possible,
where a good arm is defined as an arm whose expected reward is greater than a
given threshold. In this work, we propose DGAI - a differentiable good arm
identification algorithm to improve the sample complexity of the
state-of-the-art HDoC algorithm in a data-driven fashion. We also showed that
the DGAI can further boost the performance of a general multi-arm bandit (MAB)
problem given a threshold as a prior knowledge to the arm set. Extensive
experiments confirm that our algorithm outperform the baseline algorithms
significantly in both synthetic and real world datasets for both GAI and MAB
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SA-CNN: Application to text categorization issues using simulated
  annealing-based convolutional neural network optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Guo, Yueying Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) are a representative class of deep
learning algorithms including convolutional computation that perform
translation-invariant classification of input data based on their hierarchical
architecture. However, classical convolutional neural network learning methods
use the steepest descent algorithm for training, and the learning performance
is greatly influenced by the initial weight settings of the convolutional and
fully connected layers, requiring re-tuning to achieve better performance under
different model structures and data. Combining the strengths of the simulated
annealing algorithm in global search, we propose applying it to the
hyperparameter search process in order to increase the effectiveness of
convolutional neural networks (CNNs). In this paper, we introduce SA-CNN neural
networks for text classification tasks based on Text-CNN neural networks and
implement the simulated annealing algorithm for hyperparameter search.
Experiments demonstrate that we can achieve greater classification accuracy
than earlier models with manual tuning, and the improvement in time and space
for exploration relative to human tuning is substantial.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM EITCE-2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Score Attack: A Lower Bound Technique for Optimal Differentially Private
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        T. Tony Cai, Yichen Wang, Linjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving optimal statistical performance while ensuring the privacy of
personal data is a challenging yet crucial objective in modern data analysis.
However, characterizing the optimality, particularly the minimax lower bound,
under privacy constraints is technically difficult.
  To address this issue, we propose a novel approach called the score attack,
which provides a lower bound on the differential-privacy-constrained minimax
risk of parameter estimation. The score attack method is based on the tracing
attack concept in differential privacy and can be applied to any statistical
model with a well-defined score statistic. It can optimally lower bound the
minimax risk of estimating unknown model parameters, up to a logarithmic
factor, while ensuring differential privacy for a range of statistical
problems. We demonstrate the effectiveness and optimality of this general
method in various examples, such as the generalized linear model in both
classical and high-dimensional sparse settings, the Bradley-Terry-Luce model
for pairwise comparisons, and nonparametric regression over the Sobolev class.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2011.03900</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi PILOT: Learned Feasible Multiple Acquisition Trajectories for
  Dynamic MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tamir Shor, Tomer Weiss, Dor Noti, Alex Bronstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic Magnetic Resonance Imaging (MRI) is known to be a powerful and
reliable technique for the dynamic imaging of internal organs and tissues,
making it a leading diagnostic tool. A major difficulty in using MRI in this
setting is the relatively long acquisition time (and, hence, increased cost)
required for imaging in high spatio-temporal resolution, leading to the
appearance of related motion artifacts and decrease in resolution. Compressed
Sensing (CS) techniques have become a common tool to reduce MRI acquisition
time by subsampling images in the k-space according to some acquisition
trajectory. Several studies have particularly focused on applying deep learning
techniques to learn these acquisition trajectories in order to attain better
image reconstruction, rather than using some predefined set of trajectories. To
the best of our knowledge, learning acquisition trajectories has been only
explored in the context of static MRI. In this study, we consider acquisition
trajectory learning in the dynamic imaging setting. We design an end-to-end
pipeline for the joint optimization of multiple per-frame acquisition
trajectories along with a reconstruction neural network, and demonstrate
improved image reconstruction quality in shorter acquisition times. The code
for reproducing all experiments is accessible at
https://github.com/tamirshor7/MultiPILOT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Microphone Speaker Separation by Spatial Regions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Wechsler, Srikanth Raj Chetupalli, Wolfgang Mack, Emanuël A. P. Habets
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the task of region-based source separation of reverberant
multi-microphone recordings. We assume pre-defined spatial regions with a
single active source per region. The objective is to estimate the signals from
the individual spatial regions as captured by a reference microphone while
retaining a correspondence between signals and spatial regions. We propose a
data-driven approach using a modified version of a state-of-the-art network,
where different layers model spatial and spectro-temporal information. The
network is trained to enforce a fixed mapping of regions to network outputs.
Using speech from LibriMix, we construct a data set specifically designed to
contain the region information. Additionally, we train the network with
permutation invariant training. We show that both training methods result in a
fixed mapping of regions to network outputs, achieve comparable performance,
and that the networks exploit spatial information. The proposed network
outperforms a baseline network by 1.5 dB in scale-invariant
signal-to-distortion ratio.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the 2023 IEEE International Conference on Acoustics,
  Speech, and Signal Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing statistical and machine learning methods for time series
  forecasting in data-driven logistics -- A simulation study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lena Schmid, Moritz Roidl, Markus Pauly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many planning and decision activities in logistics and supply chain
management are based on forecasts of multiple time dependent factors.
Therefore, the quality of planning depends on the quality of the forecasts. We
compare various forecasting methods in terms of out of the box forecasting
performance on a broad set of simulated time series. We simulate various linear
and non-linear time series and look at the one step forecast performance of
statistical learning methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transferable Deep Learning Power System Short-Term Voltage Stability
  Assessment with Physics-Informed Topological Feature Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Feng, Xin Chen, Zijian Lv, Peiyuan Sun, Kai Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL) algorithms have been widely applied to short-term voltage
stability (STVS) assessment in power systems. However, transferring the
knowledge learned in one power grid to other power grids with topology changes
is still a challenging task. This paper proposed a transferable DL-based model
for STVS assessment by constructing the topology-aware voltage dynamic features
from raw PMU data. Since the reactive power flow and grid topology are
essential to voltage stability, the topology-aware and physics-informed voltage
dynamic features are utilized to effectively represent the topological and
temporal patterns from post-disturbance system dynamic trajectories. The
proposed DL-based STVS assessment model is tested under random operating
conditions on the New England 39-bus system. It has 99.99\% classification
accuracy of the short-term voltage stability status using the topology-aware
and physics-informed voltage dynamic features. In addition to high accuracy,
the experiments show good adaptability to PMU errors. Moreover, The proposed
STVS assessment method has outstanding performance on new grid topologies after
fine-tuning. In particular, the highest accuracy reaches 99.68\% in evaluation,
which demonstrates a good knowledge transfer ability of the proposed model for
power grid topology change.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE Transactions on Power
  Systems for possible publication. Copyright may be transferred without
  notice, after which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evolutionary quantum feature selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton S. Albino, Otto M. Pires, Mauro Q. Nooblath, Erick G. S. Nascimento
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective feature selection is essential for enhancing the performance of
artificial intelligence models. It involves identifying feature combinations
that optimize a given metric, but this is a challenging task due to the
problem's exponential time complexity. In this study, we present an innovative
heuristic called Evolutionary Quantum Feature Selection (EQFS) that employs the
Quantum Circuit Evolution (QCE) algorithm. Our approach harnesses the unique
capabilities of QCE, which utilizes shallow depth circuits to generate sparse
probability distributions. Our computational experiments demonstrate that EQFS
can identify good feature combinations with quadratic scaling in the number of
features. To evaluate EQFS's performance, we counted the number of times a
given classical model assesses the cost function for a specific metric, as a
function of the number of generations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing COVID-19 Severity Analysis through Ensemble Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anand Thyagachandran, Hema A Murthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computed Tomography (CT) scans provide a detailed image of the lungs,
allowing clinicians to observe the extent of damage caused by COVID-19. The CT
severity score (CTSS) of COVID-19 can be categorized based on the extent of
lung involvement observed on a CT scan. This paper proposes a domain
knowledge-based pipeline to extract the infection regions using diverse
image-processing algorithms and a pre-trained UNET model. An ensemble of three
machine-learning models, Random Forest (RF), Extremely Randomized Trees (ERT),
and Support Vector Machine (SVM), is employed to classify the CT scans into
different severity classes. The proposed system achieved a macro F1 score of
57.47% on the validation dataset in the AI-Enabled Medical Image Analysis
Workshop and COVID-19 Diagnosis Competition (AI-MIA-COV19D).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaptiveNet: Post-deployment Neural Architecture Adaptation for Diverse
  Edge Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wen, Yuanchun Li, Zunshuai Zhang, Shiqi Jiang, Xiaozhou Ye, Ye Ouyang, Ya-Qin Zhang, Yunxin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models are increasingly deployed to edge devices for real-time
applications. To ensure stable service quality across diverse edge
environments, it is highly desirable to generate tailored model architectures
for different conditions. However, conventional pre-deployment model generation
approaches are not satisfactory due to the difficulty of handling the diversity
of edge environments and the demand for edge information. In this paper, we
propose to adapt the model architecture after deployment in the target
environment, where the model quality can be precisely measured and private edge
data can be retained. To achieve efficient and effective edge model generation,
we introduce a pretraining-assisted on-cloud model elastification method and an
edge-friendly on-device architecture search method. Model elastification
generates a high-quality search space of model architectures with the guidance
of a developer-specified oracle model. Each subnet in the space is a valid
model with different environment affinity, and each device efficiently finds
and maintains the most suitable subnet based on a series of edge-tailored
optimizations. Extensive experiments on various edge devices demonstrate that
our approach is able to achieve significantly better accuracy-latency tradeoffs
(e.g. 46.74\% higher on average accuracy with a 60\% latency budget) than
strong baselines with minimal overhead (13 GPU hours in the cloud and 2 minutes
on the edge server).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving physics-informed neural networks with meta-learned
  optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Bihlo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that the error achievable using physics-informed neural networks for
solving systems of differential equations can be substantially reduced when
these networks are trained using meta-learned optimization methods rather than
to using fixed, hand-crafted optimizers as traditionally done. We choose a
learnable optimization method based on a shallow multi-layer perceptron that is
meta-trained for specific classes of differential equations. We illustrate
meta-trained optimizers for several equations of practical relevance in
mathematical physics, including the linear advection equation, Poisson's
equation, the Korteweg--de Vries equation and Burgers' equation. We also
illustrate that meta-learned optimizers exhibit transfer learning abilities, in
that a meta-trained optimizer on one differential equation can also be
successfully deployed on another differential equation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't PANIC: Prototypical Additive Neural Network for Interpretable
  Classification of Alzheimer's Disease 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Nuno Wolf, Sebastian Pölster, Christian Wachinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's disease (AD) has a complex and multifactorial etiology, which
requires integrating information about neuroanatomy, genetics, and
cerebrospinal fluid biomarkers for accurate diagnosis. Hence, recent deep
learning approaches combined image and tabular information to improve
diagnostic performance. However, the black-box nature of such neural networks
is still a barrier for clinical applications, in which understanding the
decision of a heterogeneous model is integral. We propose PANIC, a prototypical
additive neural network for interpretable AD classification that integrates 3D
image and tabular data. It is interpretable by design and, thus, avoids the
need for post-hoc explanations that try to approximate the decision of a
network. Our results demonstrate that PANIC achieves state-of-the-art
performance in AD classification, while directly providing local and global
explanations. Finally, we show that PANIC extracts biologically meaningful
signatures of AD, and satisfies a set of desirable desiderata for trustworthy
machine learning. Our implementation is available at
\url{https://github.com/ai-med/PANIC}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in proceedings of Information Processing In Medical
  Imaging 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modality-Agnostic Debiasing for Single Domain Generalization <span class="chip">CVPR-2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanqing Qu, Yingwei Pan, Guang Chen, Ting Yao, Changjun Jiang, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) usually fail to generalize well to outside of
distribution (OOD) data, especially in the extreme case of single domain
generalization (single-DG) that transfers DNNs from single domain to multiple
unseen domains. Existing single-DG techniques commonly devise various
data-augmentation algorithms, and remould the multi-source domain
generalization methodology to learn domain-generalized (semantic) features.
Nevertheless, these methods are typically modality-specific, thereby being only
applicable to one single modality (e.g., image). In contrast, we target a
versatile Modality-Agnostic Debiasing (MAD) framework for single-DG, that
enables generalization for different modalities. Technically, MAD introduces a
novel two-branch classifier: a biased-branch encourages the classifier to
identify the domain-specific (superficial) features, and a general-branch
captures domain-generalized features based on the knowledge from biased-branch.
Our MAD is appealing in view that it is pluggable to most single-DG models. We
validate the superiority of our MAD in a variety of single-DG scenarios with
different modalities, including recognition on 1D texts, 2D images, 3D point
clouds, and semantic segmentation on 2D images. More remarkably, for
recognition on 3D point clouds and semantic segmentation on 2D images, MAD
improves DSU by 2.82\% and 1.5\% in accuracy and mIOU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in CVPR-2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Upcycling Models under Domain and Category Shift <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanqing Qu, Tianpei Zou, Florian Roehrbein, Cewu Lu, Guang Chen, Dacheng Tao, Changjun Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) often perform poorly in the presence of domain
shift and category shift. How to upcycle DNNs and adapt them to the target task
remains an important open problem. Unsupervised Domain Adaptation (UDA),
especially recently proposed Source-free Domain Adaptation (SFDA), has become a
promising technology to address this issue. Nevertheless, existing SFDA methods
require that the source domain and target domain share the same label space,
consequently being only applicable to the vanilla closed-set setting. In this
paper, we take one step further and explore the Source-free Universal Domain
Adaptation (SF-UniDA). The goal is to identify "known" data samples under both
domain and category shift, and reject those "unknown" data samples (not present
in source classes), with only the knowledge from standard pre-trained source
model. To this end, we introduce an innovative global and local clustering
learning technique (GLC). Specifically, we design a novel, adaptive one-vs-all
global clustering algorithm to achieve the distinction across different target
classes and introduce a local k-NN clustering strategy to alleviate negative
transfer. We examine the superiority of our GLC on multiple benchmarks with
different category shift scenarios, including partial-set, open-set, and
open-partial-set DA. Remarkably, in the most challenging open-partial-set DA
scenario, GLC outperforms UMAD by 14.8\% on the VisDA benchmark. The code is
available at https://github.com/ispc-lab/GLC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in CVPR 2023. The code has been made public</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>-based World Models Are Happy With 100k Interactions <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Robine, Marc Höftmann, Tobias Uelwer, Stefan Harmeling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have been successful in many reinforcement learning
settings. However, compared to human learners they are overly data hungry. To
build a sample-efficient world model, we apply a transformer to real-world
episodes in an autoregressive manner: not only the compact latent states and
the taken actions but also the experienced or predicted rewards are fed into
the transformer, so that it can attend flexibly to all three modalities at
different time steps. The transformer allows our world model to access previous
states directly, instead of viewing them through a compressed recurrent state.
By utilizing the Transformer-XL architecture, it is able to learn long-term
dependencies while staying computationally efficient. Our transformer-based
world model (TWM) generates meaningful, new experience, which is used to train
a policy that outperforms previous model-free and model-based reinforcement
learning algorithms on the Atari 100k benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2023. Code is available at
  https://github.com/jrobine/twm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ n-Step Temporal Difference Learning with Optimal n 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lakshmi Mandal, Shalabh Bhatnagar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of finding the optimal value of n in the n-step
temporal difference (TD) algorithm. We find the optimal n by resorting to the
model-free optimization technique of simultaneous perturbation stochastic
approximation (SPSA). We adopt a one-simulation SPSA procedure that is
originally for continuous optimization to the discrete optimization framework
but incorporates a cyclic perturbation sequence. We prove the convergence of
our proposed algorithm, SDPSA, and show that it finds the optimal value of n in
n-step TD. Through experiments, we show that the optimal value of n is achieved
with SDPSA for any arbitrary initial value of the same.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-device Federated Learning for Mobile Health Diagnostics: A First
  Study on COVID-19 Detection <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Xia, Jing Han, Abhirup Ghosh, Cecilia Mascolo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) aided health diagnostic models can incorporate data
from a large number of personal edge devices (e.g., mobile phones) while
keeping the data local to the originating devices, largely ensuring privacy.
However, such a cross-device FL approach for health diagnostics still imposes
many challenges due to both local data imbalance (as extreme as local data
consists of a single disease class) and global data imbalance (the disease
prevalence is generally low in a population). Since the federated server has no
access to data distribution information, it is not trivial to solve the
imbalance issue towards an unbiased model. In this paper, we propose FedLoss, a
novel cross-device FL framework for health diagnostics. Here the federated
server averages the models trained on edge devices according to the predictive
loss on the local data, rather than using only the number of samples as
weights. As the predictive loss better quantifies the data distribution at a
device, FedLoss alleviates the impact of data imbalance. Through a real-world
dataset on respiratory sound and symptom-based COVID-$19$ detection task, we
validate the superiority of FedLoss. It achieves competitive COVID-$19$
detection performance compared to a centralised model with an AUC-ROC of
$79\%$. It also outperforms the state-of-the-art FL baselines in sensitivity
and convergence speed. Our work not only demonstrates the promise of federated
COVID-$19$ detection but also paves the way to a plethora of mobile health
model development in a privacy-preserving fashion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by IEEE ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantile Online Learning for Semiconductor Failure Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bangjian Zhou, Pan Jieming, Maheswari Sivan, Aaron Voon-Yew Thean, J. Senthilnath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With high device integration density and evolving sophisticated device
structures in semiconductor chips, detecting defects becomes elusive and
complex. Conventionally, machine learning (ML)-guided failure analysis is
performed with offline batch mode training. However, the occurrence of new
types of failures or changes in the data distribution demands retraining the
model. During the manufacturing process, detecting defects in a single-pass
online fashion is more challenging and favoured. This paper focuses on novel
quantile online learning for semiconductor failure analysis. The proposed
method is applied to semiconductor device-level defects: FinFET bridge defect,
GAA-FET bridge defect, GAA-FET dislocation defect, and a public database:
SECOM. From the obtained results, we observed that the proposed method is able
to perform better than the existing methods. Our proposed method achieved an
overall accuracy of 86.66% and compared with the second-best existing method it
improves 15.50% on the GAA-FET dislocation defect dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bandit-supported care planning for older people with complex health and
  care needs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gi-Soo Kim, Young Suh Hong, Tae Hoon Lee, Myunghee Cho Paik, Hongsoo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-term care service for old people is in great demand in most of the aging
societies. The number of nursing homes residents is increasing while the number
of care providers is limited. Due to the care worker shortage, care to
vulnerable older residents cannot be fully tailored to the unique needs and
preference of each individual. This may bring negative impacts on health
outcomes and quality of life among institutionalized older people. To improve
care quality through personalized care planning and delivery with limited care
workforce, we propose a new care planning model assisted by artificial
intelligence. We apply bandit algorithms which optimize the clinical decision
for care planning by adapting to the sequential feedback from the past
decisions. We evaluate the proposed model on empirical data acquired from the
Systems for Person-centered Elder Care (SPEC) study, a ICT-enhanced care
management program.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Variational Autoencoder for Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Borui Cai, Shuiqiao Yang, Longxiang Gao, Yong Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational autoencoders (VAE) are powerful generative models that learn the
latent representations of input data as random variables. Recent studies show
that VAE can flexibly learn the complex temporal dynamics of time series and
achieve more promising forecasting results than deterministic models. However,
a major limitation of existing works is that they fail to jointly learn the
local patterns (e.g., seasonality and trend) and temporal dynamics of time
series for forecasting. Accordingly, we propose a novel hybrid variational
autoencoder (HyVAE) to integrate the learning of local patterns and temporal
dynamics by variational inference for time series forecasting. Experimental
results on four real-world datasets show that the proposed HyVAE achieves
better forecasting results than various counterpart methods, as well as two
HyVAE variants that only learn the local patterns or temporal dynamics of time
series, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deploying Offline Reinforcement Learning with Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziniu Li, Ke Xu, Liu Liu, Lanqing Li, Deheng Ye, Peilin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has shown promise for decision-making tasks in
real-world applications. One practical framework involves training
parameterized policy models from an offline dataset and subsequently deploying
them in an online environment. However, this approach can be risky since the
offline training may not be perfect, leading to poor performance of the RL
models that may take dangerous actions. To address this issue, we propose an
alternative framework that involves a human supervising the RL models and
providing additional feedback in the online deployment phase. We formalize this
online deployment problem and develop two approaches. The first approach uses
model selection and the upper confidence bound algorithm to adaptively select a
model to deploy from a candidate set of trained offline RL models. The second
approach involves fine-tuning the model in the online deployment phase when a
supervision signal arrives. We demonstrate the effectiveness of these
approaches for robot locomotion control and traffic light control tasks through
empirical validation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $\nabla$SD: Differentiable Programming for Sparse Tensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Shaikhha, Mathieu Huot, Shideh Hashemian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse tensors are prevalent in many data-intensive applications, yet
existing differentiable programming frameworks are tailored towards dense
tensors. This presents a significant challenge for efficiently computing
gradients through sparse tensor operations, as their irregular sparsity
patterns can result in substantial memory and computational overheads. In this
work, we introduce a novel framework that enables the efficient and automatic
differentiation of sparse tensors, addressing this fundamental issue. Our
experiments demonstrate the effectiveness of the proposed framework in terms of
performance and scalability, outperforming state-of-the-art frameworks across a
range of synthetic and real-world datasets. Our approach offers a promising
direction for enabling efficient and scalable differentiable programming with
sparse tensors, which has significant implications for numerous applications in
machine learning, natural language processing, and scientific computing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symbolic Regression for PDEs using Pruned Differentiable Programs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ritam Majumdar, Vishal Jadhav, Anirudh Deodhar, Shirish Karande, Lovekesh Vig, Venkataramana Runkana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-informed Neural Networks (PINNs) have been widely used to obtain
accurate neural surrogates for a system of Partial Differential Equations
(PDE). One of the major limitations of PINNs is that the neural solutions are
challenging to interpret, and are often treated as black-box solvers. While
Symbolic Regression (SR) has been studied extensively, very few works exist
which generate analytical expressions to directly perform SR for a system of
PDEs. In this work, we introduce an end-to-end framework for obtaining
mathematical expressions for solutions of PDEs. We use a trained PINN to
generate a dataset, upon which we perform SR. We use a Differentiable Program
Architecture (DPA) defined using context-free grammar to describe the space of
symbolic expressions. We improve the interpretability by pruning the DPA in a
depth-first manner using the magnitude of weights as our heuristic. On average,
we observe a 95.3% reduction in parameters of DPA while maintaining accuracy at
par with PINNs. Furthermore, on an average, pruning improves the accuracy of
DPA by 7.81% . We demonstrate our framework outperforms the existing
state-of-the-art SR solvers on systems of complex PDEs like Navier-Stokes:
Kovasznay flow and Taylor-Green Vortex flow. Furthermore, we produce analytical
expressions for a complex industrial use-case of an Air-Preheater, without
suffering from performance loss viz-a-viz PINNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Publication accepted at International Conference for Learning
  Representations 2023: Physics for Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Density of States via Multi-modal <span class="highlight-title">Transformer</span> <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Namkyeong Lee, Heewoong Noh, Sungwon Kim, Dongmin Hyun, Gyoung S. Na, Chanyoung Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The density of states (DOS) is a spectral property of materials, which
provides fundamental insights on various characteristics of materials. In this
paper, we propose a model to predict the DOS by reflecting the nature of DOS:
DOS determines the general distribution of states as a function of energy.
Specifically, we integrate the heterogeneous information obtained from the
crystal structure and the energies via multi-modal transformer, thereby
modeling the complex relationships between the atoms in the crystal structure,
and various energy levels. Extensive experiments on two types of DOS, i.e.,
Phonon DOS and Electron DOS, with various real-world scenarios demonstrate the
superiority of DOSTransformer. The source code for DOSTransformer is available
at https://github.com/HeewoongNoh/DOSTransformer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023 Workshop on Machine Learning for Materials (ML4Materials)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying Label Errors in Object Detection <span class="highlight-title">Dataset</span>s by Loss Inspection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marius Schubert, Tobias Riedlinger, Karsten Kahl, Daniel Kröll, Sebastian Schoenen, Siniša Šegvić, Matthias Rottmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Labeling datasets for supervised object detection is a dull and
time-consuming task. Errors can be easily introduced during annotation and
overlooked during review, yielding inaccurate benchmarks and performance
degradation of deep neural networks trained on noisy labels. In this work, we
for the first time introduce a benchmark for label error detection methods on
object detection datasets as well as a label error detection method and a
number of baselines. We simulate four different types of randomly introduced
label errors on train and test sets of well-labeled object detection datasets.
For our label error detection method we assume a two-stage object detector to
be given and consider the sum of both stages' classification and regression
losses. The losses are computed with respect to the predictions and the noisy
labels including simulated label errors, aiming at detecting the latter. We
compare our method to three baselines: a naive one without deep learning, the
object detector's score and the entropy of the classification softmax
distribution. We outperform all baselines and demonstrate that among the
considered methods, ours is the only one that detects label errors of all four
types efficiently. Furthermore, we detect real label errors a) on commonly used
test datasets in object detection and b) on a proprietary dataset. In both
cases we achieve low false positives rates, i.e., when considering 200
proposals from our method, we detect label errors with a precision for a) of up
to 71.5% and for b) with 97%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Mutual Information Estimation with Annealed and Energy-Based
  Bounds <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rob Brekelmans, Sicong Huang, Marzyeh Ghassemi, Greg Ver Steeg, Roger Grosse, Alireza Makhzani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mutual information (MI) is a fundamental quantity in information theory and
machine learning. However, direct estimation of MI is intractable, even if the
true joint probability density for the variables of interest is known, as it
involves estimating a potentially high-dimensional log partition function. In
this work, we present a unifying view of existing MI bounds from the
perspective of importance sampling, and propose three novel bounds based on
this approach. Since accurate estimation of MI without density information
requires a sample size exponential in the true MI, we assume either a single
marginal or the full joint density information is known. In settings where the
full joint density is available, we propose Multi-Sample Annealed Importance
Sampling (AIS) bounds on MI, which we demonstrate can tightly estimate large
values of MI in our experiments. In settings where only a single marginal
distribution is known, we propose Generalized IWAE (GIWAE) and MINE-AIS bounds.
Our GIWAE bound unifies variational and contrastive bounds in a single
framework that generalizes InfoNCE, IWAE, and Barber-Agakov bounds. Our
MINE-AIS method improves upon existing energy-based methods such as MINE-DV and
MINE-F by directly optimizing a tighter lower bound on MI. MINE-AIS uses MCMC
sampling to estimate gradients for training and Multi-Sample AIS for evaluating
the bound. Our methods are particularly suitable for evaluating MI in deep
generative models, since explicit forms of the marginal or joint densities are
often available. We evaluate our bounds on estimating the MI of VAEs and GANs
trained on the MNIST and CIFAR datasets, and showcase significant gains over
existing bounds in these challenging settings with high ground truth MI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A shorter version appeared in the International Conference on
  Learning Representations (ICLR) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> based general laboratory progress <span class="highlight-title">pretrain</span>ed model for
  cardiovascular event detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li-Chin Chen, Kuo-Hsuan Hung, Yi-Ju Tseng, Hsin-Yao Wang, Tse-Min Lu, Wei-Chieh Huang, Yu Tsao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regular surveillance is an indispensable aspect of managing cardiovascular
disorders. Patient recruitment for rare or specific diseases is often limited
due to their small patient size and episodic observations, whereas prevalent
cases accumulate longitudinal data easily due to regular follow-ups. These
data, however, are notorious for their irregularity, temporality, sparsity, and
absenteeism. In this study, we leveraged self-supervised learning (SSL) and
transfer learning to overcome the above-mentioned barriers, transferring
patient progress trends in cardiovascular laboratory parameters from prevalent
cases to rare or specific cardiovascular events detection. We pretrained a
general laboratory progress (GLP) pretrain model using hypertension patients
(who were yet to be diabetic), and transferred their laboratory progress trend
to assist in detecting target vessel revascularization (TVR) in percutaneous
coronary intervention patients. GLP adopted a two-stage training process that
utilized interpolated data, enhancing the performance of SSL. After pretraining
GLP, we fine-tuned it for TVR prediction. The proposed two-stage training
process outperformed SSL. Upon processing by GLP, the classification
demonstrated a marked improvement, increasing from 0.63 to 0.90 in averaged
accuracy. All metrics were significantly superior (p < 0.01) to the performance
of prior GLP processing. The representation displayed distinct separability
independent of algorithmic mechanisms, and diverse data distribution trend. Our
approach effectively transferred the progression trends of cardiovascular
laboratory parameters from prevalent cases to small-numbered cases, thereby
demonstrating its efficacy in aiding the risk assessment of cardiovascular
events without limiting to episodic observation. The potential for extending
this approach to other laboratory tests and diseases is promising.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Neural Koopman Operators to Learn Continuous Representations
  of Dynamical Systems from Scarce Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Frion, Lucas Drumetz, Mauro Dalla Mura, Guillaume Tochon, Abdeldjalil Aissa El Bey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the last few years, several works have proposed deep learning
architectures to learn dynamical systems from observation data with no or
little knowledge of the underlying physics. A line of work relies on learning
representations where the dynamics of the underlying phenomenon can be
described by a linear operator, based on the Koopman operator theory. However,
despite being able to provide reliable long-term predictions for some dynamical
systems in ideal situations, the methods proposed so far have limitations, such
as requiring to discretize intrinsically continuous dynamical systems, leading
to data loss, especially when handling incomplete or sparsely sampled data.
Here, we propose a new deep Koopman framework that represents dynamics in an
intrinsically continuous way, leading to better performance on limited training
data, as exemplified on several datasets arising from dynamical systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uni-RXN: An Unified Framework that Bridge the Gap between Chemical
  Reaction <span class="highlight-title">Pretrain</span>ing and Conditional Molecule Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Qiang, Yiran Zhou, Yuheng Ding, Ningfeng Liu, Liangren Zhang, Zhenming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chemical reactions are the fundamental building blocks of drug design and
organic chemistry research. Machine learning for chemistry is a rapidly
advancing field with numerous applications. In recent years, there has been a
growing need for a large-scale deep-learning framework that can efficiently
capture the basic rules of chemical reactions. In this paper, we have proposed
a unified framework that addresses both the reaction representation learning
and molecule generation tasks, which allows for a more holistic approach.
Inspired by the organic chemistry mechanism, we develop a novel pretraining
framework that enables us to incorporate inductive biases into the model. Our
framework achieves state-of-the-art results on challenging downstream tasks. By
possessing chemical knowledge, this framework can be applied to reaction-based
generative models, overcoming the limitations of current molecule generation
models that rely on a small number of reaction templates. In the extensive
experiments, our model generates synthesizable drug-like structures of high
quality. Overall, our work presents a significant step toward a large-scale
deep-learning framework for a variety of reaction-based applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Modal Simulation Framework to Enable Digital Twin-based V2X
  Communications in Dynamic Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Cazzella, Francesco Linsalata, Maurizio Magarini, Matteo Matteucci, Umberto Spagnolini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital Twins (DTs) for physical wireless environments have been recently
proposed as accurate virtual representations of the propagation environment
that can enable multi-layer decisions at the physical communication equipment.
At high frequency bands, DTs can help to overcome the challenges emerging in
the high mobility conditions featuring vehicular environments. In this paper,
we propose a novel data-driven workflow for the creation of the DT of a
Vehicle-to-Everything (V2X) communication scenario and a multi-modal simulation
framework for the generation of realistic sensor data and accurate
mmWave/sub-THz wireless channels. The proposed method leverages an automotive
simulation and testing framework based on the Unreal Engine game engine and an
accurate ray-tracing channel simulator. Simulations over an urban scenario show
the achievable realistic sensor and channel modelling both at the
infrastructure and at an ego-vehicle.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-Aware Selective Label Smoothing for Calibrating Sequence
  Recognition Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuangping Huang, Yu Luo, Zhenzhou Zhuang, Jin-Gang Yu, Mengchao He, Yongpan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the success of deep neural network (DNN) on sequential data (i.e.,
scene text and speech) recognition, it suffers from the over-confidence problem
mainly due to overfitting in training with the cross-entropy loss, which may
make the decision-making less reliable. Confidence calibration has been
recently proposed as one effective solution to this problem. Nevertheless, the
majority of existing confidence calibration methods aims at non-sequential
data, which is limited if directly applied to sequential data since the
intrinsic contextual dependency in sequences or the class-specific statistical
prior is seldom exploited. To the end, we propose a Context-Aware Selective
Label Smoothing (CASLS) method for calibrating sequential data. The proposed
CASLS fully leverages the contextual dependency in sequences to construct
confusion matrices of contextual prediction statistics over different classes.
Class-specific error rates are then used to adjust the weights of smoothing
strength in order to achieve adaptive calibration. Experimental results on
sequence recognition tasks, including scene text recognition and speech
recognition, demonstrate that our method can achieve the state-of-the-art
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoGANPPIS: Coevolution-enhanced Global Attention Neural Network for
  Protein-Protein Interaction Site Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxing Guo, Xuening Zhu, Zixin Hu, Xiaoxi Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protein-protein interactions are essential in biochemical processes. Accurate
prediction of the protein-protein interaction sites (PPIs) deepens our
understanding of biological mechanism and is crucial for new drug design.
However, conventional experimental methods for PPIs prediction are costly and
time-consuming so that many computational approaches, especially ML-based
methods, have been developed recently. Although these approaches have achieved
gratifying results, there are still two limitations: (1) Most models have
excavated some useful input features, but failed to take coevolutionary
features into account, which could provide clues for inter-residue
relationships; (2) The attention-based models only allocate attention weights
for neighboring residues, instead of doing it globally, neglecting that some
residues being far away from the target residues might also matter.
  We propose a coevolution-enhanced global attention neural network, a
sequence-based deep learning model for PPIs prediction, called CoGANPPIS. It
utilizes three layers in parallel for feature extraction: (1) Local-level
representation aggregation layer, which aggregates the neighboring residues'
features; (2) Global-level representation learning layer, which employs a novel
coevolution-enhanced global attention mechanism to allocate attention weights
to all the residues on the same protein sequences; (3) Coevolutionary
information learning layer, which applies CNN & pooling to coevolutionary
information to obtain the coevolutionary profile representation. Then, the
three outputs are concatenated and passed into several fully connected layers
for the final prediction. Application on two benchmark datasets demonstrated a
state-of-the-art performance of our model. The source code is publicly
available at https://github.com/Slam1423/CoGANPPIS_source_code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Catastrophic Forgetting in Federated Class-Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Zhang, Chen Chen, Weiming Zhuang, Lingjuan Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on an under-explored yet important problem: Federated
Class-Continual Learning (FCCL), where new classes are dynamically added in
federated learning. Existing FCCL works suffer from various limitations, such
as requiring additional datasets or storing the private data from previous
tasks. In response, we first demonstrate that non-IID data exacerbates
catastrophic forgetting issue in FL. Then we propose a novel method called
TARGET (federat\textbf{T}ed cl\textbf{A}ss-continual lea\textbf{R}nin\textbf{G}
via \textbf{E}xemplar-free dis\textbf{T}illation), which alleviates
catastrophic forgetting in FCCL while preserving client data privacy. Our
proposed method leverages the previously trained global model to transfer
knowledge of old tasks to the current task at the model level. Moreover, a
generator is trained to produce synthetic data to simulate the global
distribution of data on each client at the data level. Compared to previous
FCCL methods, TARGET does not require any additional datasets or storing real
data from previous tasks, which makes it ideal for data-sensitive scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepVigor: Vulnerability Value Ranges and Factors for DNNs' Reliability
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Hasan Ahmadilivani, Mahdi Taheri, Jaan Raik, Masoud Daneshtalab, Maksim Jenihhin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) and their accelerators are being deployed ever
more frequently in safety-critical applications leading to increasing
reliability concerns. A traditional and accurate method for assessing DNNs'
reliability has been resorting to fault injection, which, however, suffers from
prohibitive time complexity. While analytical and hybrid fault
injection-/analytical-based methods have been proposed, they are either
inaccurate or specific to particular accelerator architectures. In this work,
we propose a novel accurate, fine-grain, metric-oriented, and
accelerator-agnostic method called DeepVigor that provides vulnerability value
ranges for DNN neurons' outputs. An outcome of DeepVigor is an analytical model
representing vulnerable and non-vulnerable ranges for each neuron that can be
exploited to develop different techniques for improving DNNs' reliability.
Moreover, DeepVigor provides reliability assessment metrics based on
vulnerability factors for bits, neurons, and layers using the vulnerability
ranges. The proposed method is not only faster than fault injection but also
provides extensive and accurate information about the reliability of DNNs,
independent from the accelerator. The experimental evaluations in the paper
indicate that the proposed vulnerability ranges are 99.9% to 100% accurate even
when evaluated on previously unseen test data. Also, it is shown that the
obtained vulnerability factors represent the criticality of bits, neurons, and
layers proficiently. DeepVigor is implemented in the PyTorch framework and
validated on complex DNN benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures, 2 tables, accepted at ETS 2023
  (cas.polito.it/ETS23/#/program-conference#tab-accepted)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Molecular Property Prediction by Semantic-invariant Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiao Zhang, Ailin Xie, Jihong Guan, Shuigeng Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning have been widely used as pretext tasks for
self-supervised pre-trained molecular representation learning models in
AI-aided drug design and discovery. However, exiting methods that generate
molecular views by noise-adding operations for contrastive learning may face
the semantic inconsistency problem, which leads to false positive pairs and
consequently poor prediction performance. To address this problem, in this
paper we first propose a semantic-invariant view generation method by properly
breaking molecular graphs into fragment pairs. Then, we develop a
Fragment-based Semantic-Invariant Contrastive Learning (FraSICL) model based on
this view generation method for molecular property prediction. The FraSICL
model consists of two branches to generate representations of views for
contrastive learning, meanwhile a multi-view fusion and an auxiliary similarity
loss are introduced to make better use of the information contained in
different fragment-pair views. Extensive experiments on various benchmark
datasets show that with the least number of pre-training samples, FraSICL can
achieve state-of-the-art performance, compared with major existing counterpart
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spacecraft Anomaly Detection with Attention Temporal Convolution Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Liu, Ling Tian, Zhao Kang, Tianqi Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spacecraft faces various situations when carrying out exploration missions in
complex space, thus monitoring the anomaly status of spacecraft is crucial to
the development of \textcolor{blue}{the} aerospace industry. The time series
telemetry data generated by on-orbit spacecraft \textcolor{blue}{contains}
important information about the status of spacecraft. However, traditional
domain knowledge-based spacecraft anomaly detection methods are not effective
due to high dimensionality and complex correlation among variables. In this
work, we propose an anomaly detection framework for spacecraft multivariate
time-series data based on temporal convolution networks (TCNs). First, we
employ dynamic graph attention to model the complex correlation among variables
and time series. Second, temporal convolution networks with parallel processing
ability are used to extract multidimensional \textcolor{blue}{features} for
\textcolor{blue}{the} downstream prediction task. Finally, many potential
anomalies are detected by the best threshold. Experiments on real NASA SMAP/MSL
spacecraft datasets show the superiority of our proposed model with respect to
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-driven machine learning models coupling PyTorch and Firedrake <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06871v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06871v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nacime Bouziani, David A. Ham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial differential equations (PDEs) are central to describing and modelling
complex physical systems that arise in many disciplines across science and
engineering. However, in many realistic applications PDE modelling provides an
incomplete description of the physics of interest. PDE-based machine learning
techniques are designed to address this limitation. In this approach, the PDE
is used as an inductive bias enabling the coupled model to rely on fundamental
physical laws while requiring less training data. The deployment of
high-performance simulations coupling PDEs and machine learning to complex
problems necessitates the composition of capabilities provided by machine
learning and PDE-based frameworks. We present a simple yet effective coupling
between the machine learning framework PyTorch and the PDE system Firedrake
that provides researchers, engineers and domain specialists with a high
productive way of specifying coupled models while only requiring trivial
changes to existing code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the ICLR 2023 Workshop on Physics for Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Three Guidelines You Should Know for Universally Slimmable
  <span class="highlight-title">Self-Supervised</span> Learning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun-Hao Cao, Peiqin Sun, Shuchang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose universally slimmable self-supervised learning (dubbed as US3L) to
achieve better accuracy-efficiency trade-offs for deploying self-supervised
models across different devices. We observe that direct adaptation of
self-supervised learning (SSL) to universally slimmable networks misbehaves as
the training process frequently collapses. We then discover that temporal
consistent guidance is the key to the success of SSL for universally slimmable
networks, and we propose three guidelines for the loss design to ensure this
temporal consistency from a unified gradient perspective. Moreover, we propose
dynamic sampling and group regularization strategies to simultaneously improve
training efficiency and accuracy. Our US3L method has been empirically
validated on both convolutional neural networks and vision transformers. With
only once training and one copy of weights, our method outperforms various
state-of-the-art methods (individually trained or not) on benchmarks including
recognition, object detection and instance segmentation. Our code is available
at https://github.com/megvii-research/US3L-CVPR2023.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-throughput Generative Inference of Large Language Models with a
  Single GPU 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gonzalez, Percy Liang, Christopher Ré, Ion Stoica, Ce Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high computational and memory requirements of large language model (LLM)
inference traditionally make it feasible only with multiple high-end
accelerators. Motivated by the emerging demand for latency-insensitive tasks
with batched processing, this paper initiates the study of high-throughput LLM
inference using limited resources, such as a single commodity GPU. We present
FlexGen, a high-throughput generation engine for running LLMs with limited GPU
memory. FlexGen can be flexibly configured under various hardware resource
constraints by aggregating memory and computation from the GPU, CPU, and disk.
Through a linear programming optimizer, it searches for efficient patterns to
store and access tensors. FlexGen further compresses these weights and the
attention cache to 4 bits with negligible accuracy loss. These techniques
enable FlexGen to have a larger space of batch size choices and thus
significantly increase maximum throughput. As a result, when running OPT-175B
on a single 16GB GPU, FlexGen achieves significantly higher throughput compared
to state-of-the-art offloading systems, reaching a generation throughput of 1
token/s for the first time with an effective batch size of 144. On the HELM
benchmark, FlexGen can benchmark a 30B model with a 16GB GPU on 7
representative sub-scenarios in 21 hours. The code is available at
https://github.com/FMInference/FlexGen
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Neural Network for Multi-Task Learning Searching across Diverse
  Network Topologies <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonhyeok Choi, Sunghoon Im
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a new MTL framework that searches for structures
optimized for multiple tasks with diverse graph topologies and shares features
among tasks. We design a restricted DAG-based central network with
read-in/read-out layers to build topologically diverse task-adaptive structures
while limiting search space and time. We search for a single optimized network
that serves as multiple task adaptive sub-networks using our three-stage
training process. To make the network compact and discretized, we propose a
flow-based reduction algorithm and a squeeze loss used in the training process.
We evaluate our optimized network on various public MTL datasets and show ours
achieves state-of-the-art performance. An extensive ablation study
experimentally validates the effectiveness of the sub-module and schemes in our
framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2023, 13 pages, 10 encapsulated postscript figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Contrastive Language-Image <span class="highlight-title">Pretrain</span>ing against Adversarial
  Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhan Yang, Baharan Mirzasoleiman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive vision-language representation learning has achieved
state-of-the-art performance for zero-shot classification, by learning from
millions of image-caption pairs crawled from the internet. However, the massive
data that powers large multimodal models such as CLIP, makes them extremely
vulnerable to various types of adversarial attacks, including targeted and
backdoor data poisoning attacks. Despite this vulnerability, robust contrastive
vision-language pretraining against adversarial attacks has remained
unaddressed. In this work, we propose RoCLIP, the first effective method for
robust pretraining {and fine-tuning} multimodal vision-language models. RoCLIP
effectively breaks the association between poisoned image-caption pairs by
considering a pool of random examples, and (1) matching every image with the
text that is most similar to its caption in the pool, and (2) matching every
caption with the image that is most similar to its image in the pool. Our
extensive experiments show that our method renders state-of-the-art targeted
data poisoning and backdoor attacks ineffective during pre-training or
fine-tuning of CLIP. In particular, RoCLIP decreases the poison and backdoor
attack success rates down to 0\% during pre-training and 1\%-4\% during
fine-tuning, and effectively improves the model's performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Regret of Online Edge Service Hosting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        R Sri Prakash, Nikhil Karamchandani, Sharayu Moharir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of service hosting where a service provider can
dynamically rent edge resources via short term contracts to ensure better
quality of service to its customers. The service can also be partially hosted
at the edge, in which case, customers' requests can be partially served at the
edge. The total cost incurred by the system is modeled as a combination of the
rent cost, the service cost incurred due to latency in serving customers, and
the fetch cost incurred as a result of the bandwidth used to fetch the
code/databases of the service from the cloud servers to host the service at the
edge. In this paper, we compare multiple hosting policies with regret as a
metric, defined as the difference in the cost incurred by the policy and the
optimal policy over some time horizon $T$. In particular we consider the Retro
Renting (RR) and Follow The Perturbed Leader (FTPL) policies proposed in the
literature and provide performance guarantees on the regret of these policies.
We show that under i.i.d stochastic arrivals, RR policy has linear regret while
FTPL policy has constant regret. Next, we propose a variant of FTPL, namely
Wait then FTPL (W-FTPL), which also has constant regret while demonstrating
much better dependence on the fetch cost. We also show that under adversarial
arrivals, RR policy has linear regret while both FTPL and W-FTPL have regret
$\mathrm{O}(\sqrt{T})$ which is order-optimal.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Label Distribution Learning from Logical Label 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Jia, Jiawei Tang, Jiahao Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Label distribution learning (LDL) is an effective method to predict the label
description degree (a.k.a. label distribution) of a sample. However, annotating
label distribution (LD) for training samples is extremely costly. So recent
studies often first use label enhancement (LE) to generate the estimated label
distribution from the logical label and then apply external LDL algorithms on
the recovered label distribution to predict the label distribution for unseen
samples. But this step-wise manner overlooks the possible connections between
LE and LDL. Moreover, the existing LE approaches may assign some description
degrees to invalid labels. To solve the above problems, we propose a novel
method to learn an LDL model directly from the logical label, which unifies LE
and LDL into a joint model, and avoids the drawbacks of the previous LE
methods. Extensive experiments on various datasets prove that the proposed
approach can construct a reliable LDL model directly from the logical label,
and produce more accurate label distribution than the state-of-the-art LE
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span> Encoder with Multiscale Deep Learning for Pain
  Classification Using Physiological Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyuan Lu, Burcu Ozek, Sagar Kamarthi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pain is a serious worldwide health problem that affects a vast proportion of
the population. For efficient pain management and treatment, accurate
classification and evaluation of pain severity are necessary. However, this can
be challenging as pain is a subjective sensation-driven experience. Traditional
techniques for measuring pain intensity, e.g. self-report scales, are
susceptible to bias and unreliable in some instances. Consequently, there is a
need for more objective and automatic pain intensity assessment strategies. In
this research, we develop PainAttnNet (PAN), a novel transfomer-encoder
deep-learning framework for classifying pain intensities with physiological
signals as input. The proposed approach is comprised of three feature
extraction architectures: multiscale convolutional networks (MSCN), a
squeeze-and-excitation residual network (SEResNet), and a transformer encoder
block. On the basis of pain stimuli, MSCN extracts short- and long-window
information as well as sequential features. SEResNet highlights relevant
extracted features by mapping the interdependencies among features. The third
architecture employs a transformer encoder consisting of three temporal
convolutional networks (TCN) with three multi-head attention (MHA) layers to
extract temporal dependencies from the features. Using the publicly available
BioVid pain dataset, we test the proposed PainAttnNet model and demonstrate
that our outcomes outperform state-of-the-art models. These results confirm
that our approach can be utilized for automated classification of pain
intensity using physiological signals to improve pain management and treatment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Label Information Bottleneck for Label Enhancement <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinghai Zheng, Jihua Zhu, Haoyu Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we focus on the challenging problem of Label Enhancement (LE),
which aims to exactly recover label distributions from logical labels, and
present a novel Label Information Bottleneck (LIB) method for LE. For the
recovery process of label distributions, the label irrelevant information
contained in the dataset may lead to unsatisfactory recovery performance. To
address this limitation, we make efforts to excavate the essential label
relevant information to improve the recovery performance. Our method formulates
the LE problem as the following two joint processes: 1) learning the
representation with the essential label relevant information, 2) recovering
label distributions based on the learned representation. The label relevant
information can be excavated based on the "bottleneck" formed by the learned
representation. Significantly, both the label relevant information about the
label assignments and the label relevant information about the label gaps can
be explored in our method. Evaluation experiments conducted on several
benchmark label distribution learning datasets verify the effectiveness and
competitiveness of LIB. Our source codes are available
"https://github.com/qinghai-zheng/LIBLE"
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023, our source codes are available at
  "https://github.com/qinghai-zheng/LIBLE"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>-based Planning for Symbolic Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parshin Shojaee, Kazem Meidani, Amir Barati Farimani, Chandan K. Reddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symbolic regression (SR) is a challenging task in machine learning that
involves finding a mathematical expression for a function based on its values.
Recent advancements in SR have demonstrated the efficacy of pretrained
transformer-based models for generating equations as sequences, which benefit
from large-scale pretraining on synthetic datasets and offer considerable
advantages over GP-based methods in terms of inference time. However, these
models focus on supervised pretraining goals borrowed from text generation and
ignore equation-specific objectives like accuracy and complexity. To address
this, we propose TPSR, a Transformer-based Planning strategy for Symbolic
Regression that incorporates Monte Carlo Tree Search into the transformer
decoding process. TPSR, as opposed to conventional decoding strategies, allows
for the integration of non-differentiable feedback, such as fitting accuracy
and complexity, as external sources of knowledge into the equation generation
process. Extensive experiments on various datasets show that our approach
outperforms state-of-the-art methods, enhancing the model's fitting-complexity
trade-off, extrapolation abilities, and robustness to noise. We also
demonstrate that the utilization of various caching mechanisms can further
enhance the efficiency of TPSR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Parshin Shojaee and Kazem Meidani contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ODIN: On-demand Data Formulation to Mitigate <span class="highlight-title">Dataset</span> Lock-in 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Spchoi, Jihoon Lee, HyeongSeok Ahn, Sanghee Jung, Bumsoo Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ODIN is an innovative approach that addresses the problem of dataset
constraints by integrating generative AI models. Traditional zero-shot learning
methods are constrained by the training dataset. To fundamentally overcome this
limitation, ODIN attempts to mitigate the dataset constraints by generating
on-demand datasets based on user requirements. ODIN consists of three main
modules: a prompt generator, a text-to-image generator, and an image
post-processor. To generate high-quality prompts and images, we adopted a large
language model (e.g., ChatGPT), and a text-to-image diffusion model (e.g.,
Stable Diffusion), respectively. We evaluated ODIN on various datasets in terms
of model accuracy and data diversity to demonstrate its potential, and
conducted post-experiments for further investigation. Overall, ODIN is a
feasible approach that enables Al to learn unseen knowledge beyond the training
dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kernel Density Bayesian Inverse Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishwarya Mandyam, Didong Li, Diana Cai, Andrew Jones, Barbara E. Engelhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse reinforcement learning~(IRL) is a powerful framework to infer an
agent's reward function by observing its behavior, but IRL algorithms that
learn point estimates of the reward function can be misleading because there
may be several functions that describe an agent's behavior equally well. A
Bayesian approach to IRL models a distribution over candidate reward functions,
alleviating the shortcomings of learning a point estimate. However, several
Bayesian IRL algorithms use a $Q$-value function in place of the likelihood
function. The resulting posterior is computationally intensive to calculate,
has few theoretical guarantees, and the $Q$-value function is often a poor
approximation for the likelihood. We introduce kernel density Bayesian IRL
(KD-BIRL), which uses conditional kernel density estimation to directly
approximate the likelihood, providing an efficient framework that, with a
modified reward function parameterization, is applicable to environments with
complex and infinite state spaces. We demonstrate KD-BIRL's benefits through a
series of experiments in Gridworld environments and a simulated sepsis
treatment task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Best-of-three-worlds Analysis for Linear Bandits with
  Follow-the-regularized-leader Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fang Kong, Canzhe Zhao, Shuai Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The linear bandit problem has been studied for many years in both stochastic
and adversarial settings. Designing an algorithm that can optimize the
environment without knowing the loss type attracts lots of interest.
\citet{LeeLWZ021} propose an algorithm that actively detects the loss type and
then switches between different algorithms specially designed for different
settings. However, such an approach requires meticulous designs to perform well
in all settings. Follow-the-regularized-leader (FTRL) is another popular
algorithm type that can adapt to different environments. This algorithm is of
simple design and the regret bounds are shown to be optimal in traditional
multi-armed bandit problems compared with the detect-switch type algorithms.
Designing an FTRL-type algorithm for linear bandits is an important question
that has been open for a long time. In this paper, we prove that the FTRL-type
algorithm with a negative entropy regularizer can achieve the
best-of-three-world results for the linear bandit problem with the tacit
cooperation between the choice of the learning rate and the specially designed
self-bounding inequality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Backdoor Defense via Deconfounded Representation Learning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06818v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06818v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaixi Zhang, Qi Liu, Zhicai Wang, Zepu Lu, Qingyong Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are recently shown to be vulnerable to backdoor
attacks, where attackers embed hidden backdoors in the DNN model by injecting a
few poisoned examples into the training dataset. While extensive efforts have
been made to detect and remove backdoors from backdoored DNNs, it is still not
clear whether a backdoor-free clean model can be directly obtained from
poisoned datasets. In this paper, we first construct a causal graph to model
the generation process of poisoned data and find that the backdoor attack acts
as the confounder, which brings spurious associations between the input images
and target labels, making the model predictions less reliable. Inspired by the
causal understanding, we propose the Causality-inspired Backdoor Defense (CBD),
to learn deconfounded representations for reliable classification.
Specifically, a backdoored model is intentionally trained to capture the
confounding effects. The other clean model dedicates to capturing the desired
causal effects by minimizing the mutual information with the confounding
representations from the backdoored model and employing a sample-wise
re-weighting scheme. Extensive experiments on multiple benchmark datasets
against 6 state-of-the-art attacks verify that our proposed defense method is
effective in reducing backdoor threats while maintaining high accuracy in
predicting benign samples. Further analysis shows that CBD can also resist
potential adaptive attacks. The code is available at
\url{https://github.com/zaixizhang/CBD}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provable Convergence of Tensor Decomposition-Based Neural Network
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Li, Bo Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advanced tensor decomposition, such as tensor train (TT), has been widely
studied for tensor decomposition-based neural network (NN) training, which is
one of the most common model compression methods. However, training NN with
tensor decomposition always suffers significant accuracy loss and convergence
issues. In this paper, a holistic framework is proposed for tensor
decomposition-based NN training by formulating TT decomposition-based NN
training as a nonconvex optimization problem. This problem can be solved by the
proposed tensor block coordinate descent (tenBCD) method, which is a
gradient-free algorithm. The global convergence of tenBCD to a critical point
at a rate of O(1/k) is established with the Kurdyka {\L}ojasiewicz (K{\L})
property, where k is the number of iterations. The theoretical results can be
extended to the popular residual neural networks (ResNets). The effectiveness
and efficiency of our proposed framework are verified through an image
classification dataset, where our proposed method can converge efficiently in
training and prevent overfitting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2107.12422 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gaussian Process on the Product of Directional Manifolds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Cao, Kailai Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a principled study on establishing Gaussian processes over
variables on the product of directional manifolds. As a basic functional
component, a manifold-adaptive kernel is presented based on the von Mises
distribution for Gaussian process regression on unit circles. Afterward, a
novel hypertoroidal von Mises kernel is introduced to enable topology-aware
Gaussian processes on hypertori with consideration of correlational circular
components. Based thereon, we enable multi-output regression for learning
vector-valued functions on hypertori using intrinsic coregionalization model
and provide analytical derivatives in hyperparameter optimization. The proposed
multi-output hypertoroidal Gaussian process is further embedded to a
data-driven recursive estimation scheme for learning unknown range sensing
models of angle-of-arrival inputs. Evaluations on range-based localization show
that the proposed scheme enables superior tracking accuracy over parametric
modeling and common Gaussian processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimization with access to auxiliary information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.00395v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.00395v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        El Mahdi Chayti, Sai Praneeth Karimireddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the fundamental optimization question of minimizing a target
function $f(x)$ whose gradients are expensive to compute or have limited
availability, given access to some auxiliary side function $h(x)$ whose
gradients are cheap or more available. This formulation captures many settings
of practical relevance such as i) re-using batches in SGD, ii) transfer
learning, iii) federated learning, iv) training with compressed models/dropout,
etc. We propose two generic new algorithms which are applicable in all these
settings and prove using only an assumption on the Hessian similarity between
the target and side information that we can benefit from this framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We corrected a mistake that we had in Lemma 9 in the previous version
  of the paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Random Laplacian Features for Learning with Hyperbolic Space <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06854v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06854v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Yu, Christopher De Sa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to its geometric properties, hyperbolic space can support high-fidelity
embeddings of tree- and graph-structured data, upon which various hyperbolic
networks have been developed. Existing hyperbolic networks encode geometric
priors not only for the input, but also at every layer of the network. This
approach involves repeatedly mapping to and from hyperbolic space, which makes
these networks complicated to implement, computationally expensive to scale,
and numerically unstable to train. In this paper, we propose a simpler
approach: learn a hyperbolic embedding of the input, then map once from it to
Euclidean space using a mapping that encodes geometric priors by respecting the
isometries of hyperbolic space, and finish with a standard Euclidean network.
The key insight is to use a random feature mapping via the eigenfunctions of
the Laplace operator, which we show can approximate any isometry-invariant
kernel on hyperbolic space. Our method can be used together with any graph
neural networks: using even a linear graph model yields significant
improvements in both efficiency and performance over other hyperbolic baselines
in both transductive and inductive tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PØDA: <span class="highlight-title">Prompt</span>-driven Zero-shot Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.03241v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.03241v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick Pérez, Raoul de Charette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain adaptation has been vastly investigated in computer vision but still
requires access to target images at train time, which might be intractable in
some uncommon conditions. In this paper, we propose the task of `Prompt-driven
Zero-shot Domain Adaptation', where we adapt a model trained on a source domain
using only a single general textual description of the target domain, i.e., a
prompt. First, we leverage a pretrained contrastive vision-language model
(CLIP) to optimize affine transformations of source features, steering them
towards target text embeddings, while preserving their content and semantics.
Second, we show that augmented features can be used to perform zero-shot domain
adaptation for semantic segmentation. Experiments demonstrate that our method
significantly outperforms CLIP-based style transfer baselines on several
datasets for the downstream task at hand. Our prompt-driven approach even
outperforms one-shot unsupervised domain adaptation on some datasets, and gives
comparable results on others. Our code is available at
https://github.com/astra-vision/PODA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://astra-vision.github.io/PODA/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unifying Vision, Text, and Layout for Universal Document Processing <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02623v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02623v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Universal Document Processing (UDOP), a foundation Document AI
model which unifies text, image, and layout modalities together with varied
task formats, including document understanding and generation. UDOP leverages
the spatial correlation between textual content and document image to model
image, text, and layout modalities with one uniform representation. With a
novel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain
downstream tasks into a prompt-based sequence generation scheme. UDOP is
pretrained on both large-scale unlabeled document corpora using innovative
self-supervised objectives and diverse labeled data. UDOP also learns to
generate document images from text and layout modalities via masked image
reconstruction. To the best of our knowledge, this is the first time in the
field of document AI that one model simultaneously achieves high-quality neural
document editing and content customization. Our method sets the
state-of-the-art on 8 Document AI tasks, e.g., document understanding and QA,
across diverse data domains like finance reports, academic papers, and
websites. UDOP ranks first on the leaderboard of the Document Understanding
Benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PGMax: Factor Graphs for Discrete Probabilistic Graphical Models and
  Loopy Belief Propagation in JAX 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.04110v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.04110v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyao Zhou, Antoine Dedieu, Nishanth Kumar, Miguel Lázaro-Gredilla, Shrinu Kushagra, Dileep George
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PGMax is an open-source Python package for (a) easily specifying discrete
Probabilistic Graphical Models (PGMs) as factor graphs; and (b) automatically
running efficient and scalable loopy belief propagation (LBP) in JAX. PGMax
supports general factor graphs with tractable factors, and leverages modern
accelerators like GPUs for inference. Compared with existing alternatives,
PGMax obtains higher-quality inference results with up to three
orders-of-magnitude inference time speedups. PGMax additionally interacts
seamlessly with the rapidly growing JAX ecosystem, opening up new research
possibilities. Our source code, examples and documentation are available at
https://github.com/deepmind/PGMax.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update authors list</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perceptual-Neural-Physical Sound Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.02886v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.02886v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Han, Vincent Lostanlen, Mathieu Lagrange
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sound matching algorithms seek to approximate a target waveform by parametric
audio synthesis. Deep neural networks have achieved promising results in
matching sustained harmonic tones. However, the task is more challenging when
targets are nonstationary and inharmonic, e.g., percussion. We attribute this
problem to the inadequacy of loss function. On one hand, mean square error in
the parametric domain, known as "P-loss", is simple and fast but fails to
accommodate the differing perceptual significance of each parameter. On the
other hand, mean square error in the spectrotemporal domain, known as "spectral
loss", is perceptually motivated and serves in differentiable digital signal
processing (DDSP). Yet, spectral loss is a poor predictor of pitch intervals
and its gradient may be computationally expensive; hence a slow convergence.
Against this conundrum, we present Perceptual-Neural-Physical loss (PNP). PNP
is the optimal quadratic approximation of spectral loss while being as fast as
P-loss during training. We instantiate PNP with physical modeling synthesis as
decoder and joint time-frequency scattering transform (JTFS) as spectral
representation. We demonstrate its potential on matching synthetic drum sounds
in comparison with other loss functions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Frequency Joint Community Detection and Phase Synchronization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.12276v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.12276v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingda Wang, Zhizhen Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the joint community detection and phase synchronization
problem on the stochastic block model with relative phase, where each node is
associated with an unknown phase angle. This problem, with a variety of
real-world applications, aims to recover the cluster structure and associated
phase angles simultaneously. We show this problem exhibits a
``multi-frequency'' structure by closely examining its maximum likelihood
estimation (MLE) formulation, whereas existing methods are not originated from
this perspective. To this end, two simple yet efficient algorithms that
leverage the MLE formulation and benefit from the information across multiple
frequencies are proposed. The former is a spectral method based on the novel
multi-frequency column-pivoted QR factorization. The factorization applied to
the top eigenvectors of the observation matrix provides key information about
the cluster structure and associated phase angles. The second approach is an
iterative multi-frequency generalized power method, where each iteration
updates the estimation in a matrix-multiplication-then-projection manner.
Numerical experiments show that our proposed algorithms significantly improve
the ability of exactly recovering the cluster structure and the accuracy of the
estimated phase angles, compared to state-of-the-art algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Signal and Information Processing
  over Networks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LA-VocE: Low-SNR Audio-visual Speech Enhancement using Neural Vocoders <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10999v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10999v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Mira, Buye Xu, Jacob Donley, Anurag Kumar, Stavros Petridis, Vamsi Krishna Ithapu, Maja Pantic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual speech enhancement aims to extract clean speech from a noisy
environment by leveraging not only the audio itself but also the target
speaker's lip movements. This approach has been shown to yield improvements
over audio-only speech enhancement, particularly for the removal of interfering
speech. Despite recent advances in speech synthesis, most audio-visual
approaches continue to use spectral mapping/masking to reproduce the clean
audio, often resulting in visual backbones added to existing speech enhancement
architectures. In this work, we propose LA-VocE, a new two-stage approach that
predicts mel-spectrograms from noisy audio-visual speech via a
transformer-based architecture, and then converts them into waveform audio
using a neural vocoder (HiFi-GAN). We train and evaluate our framework on
thousands of speakers and 11+ different languages, and study our model's
ability to adapt to different levels of background noise and speech
interference. Our experiments show that LA-VocE outperforms existing methods
according to multiple metrics, particularly under very noisy scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ State-Conditioned Adversarial Subgoal Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.09635v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.09635v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivienne Huiling Wang, Joni Pajarinen, Tinghuai Wang, Joni-Kristian Kämäräinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hierarchical reinforcement learning (HRL) proposes to solve difficult tasks
by performing decision-making and control at successively higher levels of
temporal abstraction. However, off-policy HRL often suffers from the problem of
a non-stationary high-level policy since the low-level policy is constantly
changing. In this paper, we propose a novel HRL approach for mitigating the
non-stationarity by adversarially enforcing the high-level policy to generate
subgoals compatible with the current instantiation of the low-level policy. In
practice, the adversarial learning is implemented by training a simple
state-conditioned discriminator network concurrently with the high-level policy
which determines the compatibility level of subgoals. Comparison to
state-of-the-art algorithms shows that our approach improves both learning
efficiency and performance in challenging continuous control tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Audio Features with Metadata and Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.16192v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.16192v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilyass Moummad, Nicolas Farrugia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Methods based on supervised learning using annotations in an end-to-end
fashion have been the state-of-the-art for classification problems. However,
they may be limited in their generalization capability, especially in the low
data regime. In this study, we address this issue using supervised contrastive
learning combined with available metadata to solve multiple pretext tasks that
learn a good representation of data. We apply our approach on ICBHI, a
respiratory sound classification dataset suited for this setting. We show that
learning representations using only metadata, without class labels, obtains
similar performance as using cross entropy with those labels only. In addition,
we obtain state-of-the-art score when combining class labels with metadata
using multiple supervised contrastive learning. This work suggests the
potential of using multiple metadata sources in supervised contrastive
settings, in particular in settings with class imbalance and few data. Our code
is released at https://github.com/ilyassmoummad/scl_icbhi2017
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Clinical <span class="highlight-title">BERT</span>Score: An Improved Measure of Automatic Speech Recognition
  Performance in Clinical Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05737v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05737v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joel Shor, Ruyue Agnes Bi, Subhashini Venugopalan, Steven Ibara, Roman Goldenberg, Ehud Rivlin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Recognition (ASR) in medical contexts has the potential to
save time, cut costs, increase report accuracy, and reduce physician burnout.
However, the healthcare industry has been slower to adopt this technology, in
part due to the importance of avoiding medically-relevant transcription
mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR
metric that penalizes clinically-relevant mistakes more than others. We
demonstrate that this metric more closely aligns with clinician preferences on
medical sentences as compared to other metrics (WER, BLUE, METEOR, etc),
sometimes by wide margins. We collect a benchmark of 13 clinician preferences
on 149 realistic medical sentences called the Clinician Transcript Preference
benchmark (CTP), demonstrate that CBERTScore more closely matches what
clinicians prefer, and release the benchmark for the community to further
develop clinically-aware ASR metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial random forests for density estimation and generative
  modeling <span class="chip">AISTATS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.09435v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.09435v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David S. Watson, Kristin Blesch, Jan Kapar, Marvin N. Wright
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose methods for density estimation and data synthesis using a novel
form of unsupervised random forests. Inspired by generative adversarial
networks, we implement a recursive procedure in which trees gradually learn
structural properties of the data through alternating rounds of generation and
discrimination. The method is provably consistent under minimal assumptions.
Unlike classic tree-based alternatives, our approach provides smooth
(un)conditional densities and allows for fully synthetic data generation. We
achieve comparable or superior performance to state-of-the-art probabilistic
circuits and deep learning models on various tabular data benchmarks while
executing about two orders of magnitude faster on average. An accompanying
$\texttt{R}$ package, $\texttt{arf}$, is available on $\texttt{CRAN}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera ready version (AISTATS 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attribution and Obfuscation of Neural Text Authorship: A Data Mining
  Perspective <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.10488v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.10488v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adaku Uchendu, Thai Le, Dongwon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two interlocking research questions of growing interest and importance in
privacy research are Authorship Attribution (AA) and Authorship Obfuscation
(AO). Given an artifact, especially a text t in question, an AA solution aims
to accurately attribute t to its true author out of many candidate authors
while an AO solution aims to modify t to hide its true authorship.
Traditionally, the notion of authorship and its accompanying privacy concern is
only toward human authors. However, in recent years, due to the explosive
advancements in Neural Text Generation (NTG) techniques in NLP, capable of
synthesizing human-quality open-ended texts (so-called "neural texts"), one has
to now consider authorships by humans, machines, or their combination. Due to
the implications and potential threats of neural texts when used maliciously,
it has become critical to understand the limitations of traditional AA/AO
solutions and develop novel AA/AO solutions in dealing with neural texts. In
this survey, therefore, we make a comprehensive review of recent literature on
the attribution and obfuscation of neural text authorship from a Data Mining
perspective, and share our view on their limitations and promising research
directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACM SIGKDD Explorations, Vol. 25, June 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Neural Networks on SPD Manifolds for Motor Imagery Classification:
  A Perspective from the Time-Frequency Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.02641v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.02641v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ce Ju, Cuntai Guan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The classification of motor imagery (MI) is a highly sought-after research
topic in the field of Electroencephalography (EEG)-based brain-computer
interfaces (BCIs), with immense commercial value. Over the past two decades,
there has been a fundamental shift in the trend of MI-EEG classifiers,
resulting in a gradual increase in their performance. The emergence of
Tensor-CSPNet, the first geometric deep learning (GDL) framework in BCI
research, is attributed to the imperative of characterizing the non-Euclidean
nature of signals. Fundamentally, Tensor-CSPNet is a deep learning-based
classifier that capitalizes on the second-order statistics of EEGs. In contrast
to the conventional approach of utilizing first-order statistics for EEG
signals, the utilization of these second-order statistics represents the
classical treatment. These statistics provide adequate discriminative
information, rendering them suitable for MI-EEG classification. In this study,
we introduce another GDL classifier, called Graph-CSPNet, for MI-EEG
classification. Graph-CSPNet utilizes graph-based techniques to characterize
EEG signals in both the time and frequency domains, realizing the fundamental
perspective of time-frequency analysis. The architecture of Graph-CSPNet is
further simplified, offering greater flexibility to cope with variable
time-frequency resolution for signal segmentation and capturing localized
fluctuations. In contrast to Tensor-CSPNet, this approach enables Graph-CSPNet
to achieve better results in MI-EEG classification. To evaluate the efficacy of
Graph-CSPNet, we utilize five commonly-used publicly available MI-EEG datasets,
and it produces near-optimal classification accuracies, winning nine out of
eleven subject-specific scenarios. The Python implementation of Graph-CSPNet is
available on a GitHub repository
https://github.com/GeometricBCI/Tensor-CSPNet-and-Graph-CSPNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures, 11 Tables; This work has been submitted to the
  IEEE for possible publication. Copyright may be transferred without notice,
  after which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Optimization-based Combinatorial Assignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.14698v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.14698v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakob Weissteiner, Jakob Heiss, Julien Siems, Sven Seuken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the combinatorial assignment domain, which includes combinatorial
auctions and course allocation. The main challenge in this domain is that the
bundle space grows exponentially in the number of items. To address this,
several papers have recently proposed machine learning-based preference
elicitation algorithms that aim to elicit only the most important information
from agents. However, the main shortcoming of this prior work is that it does
not model a mechanism's uncertainty over values for not yet elicited bundles.
In this paper, we address this shortcoming by presenting a Bayesian
optimization-based combinatorial assignment (BOCA) mechanism. Our key technical
contribution is to integrate a method for capturing model uncertainty into an
iterative combinatorial auction mechanism. Concretely, we design a new method
for estimating an upper uncertainty bound that can be used to define an
acquisition function to determine the next query to the agents. This enables
the mechanism to properly explore (and not just exploit) the bundle space
during its preference elicitation phase. We run computational experiments in
several spectrum auction domains to evaluate BOCA's performance. Our results
show that BOCA achieves higher allocative efficiency than state-of-the-art
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Short-Term Density Forecasting of Low-Voltage Load using
  Bernstein-Polynomial Normalizing Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.13939v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.13939v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcel Arpogaus, Marcus Voss, Beate Sick, Mark Nigge-Uricher, Oliver Dürr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transition to a fully renewable energy grid requires better forecasting
of demand at the low-voltage level to increase efficiency and ensure reliable
control. However, high fluctuations and increasing electrification cause huge
forecast variability, not reflected in traditional point estimates.
Probabilistic load forecasts take future uncertainties into account and thus
allow more informed decision-making for the planning and operation of
low-carbon energy systems. We propose an approach for flexible conditional
density forecasting of short-term load based on Bernstein polynomial
normalizing flows, where a neural network controls the parameters of the flow.
In an empirical study with 363 smart meter customers, our density predictions
compare favorably against Gaussian and Gaussian mixture densities. Also, they
outperform a non-parametric approach based on the pinball loss for 24h-ahead
load forecasting for two different neural network architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistic Point Cloud Modeling via Self-Organizing Gaussian Mixture
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.00047v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.00047v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kshitij Goel, Nathan Michael, Wennie Tabib
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This letter presents a continuous probabilistic modeling methodology for
spatial point cloud data using finite Gaussian Mixture Models (GMMs) where the
number of components are adapted based on the scene complexity. Few
hierarchical and adaptive methods have been proposed to address the challenge
of balancing model fidelity with size. Instead, state-of-the-art mapping
approaches require tuning parameters for specific use cases, but do not
generalize across diverse environments. To address this gap, we utilize a
self-organizing principle from information-theoretic learning to automatically
adapt the complexity of the GMM model based on the relevant information in the
sensor data. The approach is evaluated against existing point cloud modeling
techniques on real-world data with varying degrees of scene complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, to appear in IEEE Robotics and Automation Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Efficient Tester-Learner for Halfspaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14853v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14853v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aravind Gollakota, Adam R. Klivans, Konstantinos Stavropoulos, Arsen Vasilyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We give the first efficient algorithm for learning halfspaces in the testable
learning model recently defined by Rubinfeld and Vasilyan (2023). In this
model, a learner certifies that the accuracy of its output hypothesis is near
optimal whenever the training set passes an associated test, and training sets
drawn from some target distribution -- e.g., the Gaussian -- must pass the
test. This model is more challenging than distribution-specific agnostic or
Massart noise models where the learner is allowed to fail arbitrarily if the
distributional assumption does not hold.
  We consider the setting where the target distribution is Gaussian (or more
generally any strongly log-concave distribution) in $d$ dimensions and the
noise model is either Massart or adversarial (agnostic). For Massart noise, our
tester-learner runs in polynomial time and outputs a hypothesis with
(information-theoretically optimal) error $\mathsf{opt} + \epsilon$ for any
strongly log-concave target distribution. For adversarial noise, our
tester-learner obtains error $O(\mathsf{opt}) + \epsilon$ in polynomial time
when the target distribution is Gaussian; for strongly log-concave
distributions, we obtain $\tilde{O}(\mathsf{opt}) + \epsilon$ in
quasipolynomial time.
  Prior work on testable learning ignores the labels in the training set and
checks that the empirical moments of the covariates are close to the moments of
the base distribution. Here we develop new tests of independent interest that
make critical use of the labels and combine them with the moment-matching
approach of Gollakota et al. (2023). This enables us to simulate a variant of
the algorithm of Diakonikolas et al. (2020) for learning noisy halfspaces using
nonconvex SGD but in the testable learning setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 3 figures, Version v2: strengthened the agnostic guarantee</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Adaptive Gradient Algorithms for Nonconvex-PL Minimax
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03984v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03984v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feihu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the paper, we study a class of nonconvex nonconcave minimax optimization
problems (i.e., $\min_x\max_y f(x,y)$), where $f(x,y)$ is possible nonconvex in
$x$, and it is nonconcave and satisfies the Polyak-Lojasiewicz (PL) condition
in $y$. Moreover, we propose a class of enhanced momentum-based gradient
descent ascent methods (i.e., MSGDA and AdaMSGDA) to solve these stochastic
Nonconvex-PL minimax problems. In particular, our AdaMSGDA algorithm can use
various adaptive learning rates in updating the variables $x$ and $y$ without
relying on any global and coordinate-wise adaptive learning rates.
Theoretically, we present an effective convergence analysis framework for our
methods. Specifically, we prove that our MSGDA and AdaMSGDA methods have the
best known sample (gradient) complexity of $O(\epsilon^{-3})$ only requiring
one sample at each loop in finding an $\epsilon$-stationary solution (i.e.,
$\mathbb{E}\|\nabla F(x)\|\leq \epsilon$, where $F(x)=\max_y f(x,y)$). This
manuscript commemorates the mathematician Boris Polyak (1935-2023).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DynLight: Realize dynamic phase duration with multi-level traffic signal
  control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.03471v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.03471v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Zhang, Shubin Xie, Jianming Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We would like to withdraw this article for the following reasons: 1 this
article is not satisfactory for limited language and theoretical description; 2
we have enriched and revised this article with the help of other authors; 3 we
must update the author contribution information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We would like to withdraw this article for the following reasons: 1
  this article is not satisfactory for limited language and theoretical
  description; 2 we have enriched and revised this article with the help of
  other authors; 3 we must update the author contribution information. PLease
  see: arXiv:2211.01025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Symmetry Defense Against CNN Adversarial Perturbation Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.04087v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.04087v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Blerta Lindqvist
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural network classifiers (CNNs) are susceptible to
adversarial attacks that perturb original samples to fool classifiers such as
an autonomous vehicle's road sign image classifier. CNNs also lack invariance
in the classification of symmetric samples because CNNs can classify symmetric
samples differently. Considered together, the CNN lack of adversarial
robustness and the CNN lack of invariance mean that the classification of
symmetric adversarial samples can differ from their incorrect classification.
Could symmetric adversarial samples revert to their correct classification?
This paper answers this question by designing a symmetry defense that inverts
or horizontally flips adversarial samples before classification against
adversaries unaware of the defense. Against adversaries aware of the defense,
the defense devises a Klein four symmetry subgroup that includes the horizontal
flip and pixel inversion symmetries. The symmetry defense uses the subgroup
symmetries in accuracy evaluation and the subgroup closure property to confine
the transformations that an adaptive adversary can apply before or after
generating the adversarial sample. Without changing the preprocessing,
parameters, or model, the proposed symmetry defense counters the Projected
Gradient Descent (PGD) and AutoAttack attacks with near-default accuracies for
ImageNet. Without using attack knowledge or adversarial samples, the proposed
defense exceeds the current best defense, which trains on adversarial samples.
The defense maintains and even improves the classification accuracy of
non-adversarial samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PreFallKD: Pre-Impact Fall Detection via CNN-ViT Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tin-Han Chi, Kai-Chun Liu, Chia-Yeh Hsieh, Yu Tsao, Chia-Tai Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fall accidents are critical issues in an aging and aged society. Recently,
many researchers developed pre-impact fall detection systems using deep
learning to support wearable-based fall protection systems for preventing
severe injuries. However, most works only employed simple neural network models
instead of complex models considering the usability in resource-constrained
mobile devices and strict latency requirements. In this work, we propose a
novel pre-impact fall detection via CNN-ViT knowledge distillation, namely
PreFallKD, to strike a balance between detection performance and computational
complexity. The proposed PreFallKD transfers the detection knowledge from the
pre-trained teacher model (vision transformer) to the student model
(lightweight convolutional neural networks). Additionally, we apply data
augmentation techniques to tackle issues of data imbalance. We conduct the
experiment on the KFall public dataset and compare PreFallKD with other
state-of-the-art models. The experiment results show that PreFallKD could boost
the student model during the testing phase and achieves reliable F1-score
(92.66%) and lead time (551.3 ms).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PDEBENCH: An Extensive Benchmark for Scientific Machine Learning <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07182v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07182v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Makoto Takamoto, Timothy Praditia, Raphael Leiteritz, Dan MacKinlay, Francesco Alesiani, Dirk Pflüger, Mathias Niepert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning-based modeling of physical systems has experienced increased
interest in recent years. Despite some impressive progress, there is still a
lack of benchmarks for Scientific ML that are easy to use but still challenging
and representative of a wide range of problems. We introduce PDEBench, a
benchmark suite of time-dependent simulation tasks based on Partial
Differential Equations (PDEs). PDEBench comprises both code and data to
benchmark the performance of novel machine learning models against both
classical numerical simulations and machine learning baselines. Our proposed
set of benchmark problems contribute the following unique features: (1) A much
wider range of PDEs compared to existing benchmarks, ranging from relatively
common examples to more realistic and difficult problems; (2) much larger
ready-to-use datasets compared to prior work, comprising multiple simulation
runs across a larger number of initial and boundary conditions and PDE
parameters; (3) more extensible source codes with user-friendly APIs for data
generation and baseline results with popular machine learning models (FNO,
U-Net, PINN, Gradient-Based Inverse Method). PDEBench allows researchers to
extend the benchmark freely for their own purposes using a standardized API and
to compare the performance of new models to existing baseline methods. We also
propose new evaluation metrics with the aim to provide a more holistic
understanding of learning methods in the context of Scientific ML. With those
metrics we identify tasks which are challenging for recent ML methods and
propose these tasks as future challenges for the community. The code is
available at https://github.com/pdebench/PDEBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages (main body) + 34 pages (supplemental material), accepted for
  publication in NeurIPS 2022 Track Datasets and Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ U-Sleep's resilience to AASM guidelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.11173v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.11173v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luigi Fiorillo, Giuliana Monachino, Julia van der Meer, Marco Pesce, Jan D. Warncke, Markus H. Schmidt, Claudio L. A. Bassetti, Athina Tzovara, Paolo Favaro, Francesca D. Faraci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AASM guidelines are the result of decades of efforts aiming at standardizing
sleep scoring procedure, with the final goal of sharing a worldwide common
methodology. The guidelines cover several aspects from the technical/digital
specifications,e.g., recommended EEG derivations, to detailed sleep scoring
rules accordingly to age. Automated sleep scoring systems have always largely
exploited the standards as fundamental guidelines. In this context, deep
learning has demonstrated better performance compared to classical machine
learning. Our present work shows that a deep learning based sleep scoring
algorithm may not need to fully exploit the clinical knowledge or to strictly
adhere to the AASM guidelines. Specifically, we demonstrate that U-Sleep, a
state-of-the-art sleep scoring algorithm, can be strong enough to solve the
scoring task even using clinically non-recommended or non-conventional
derivations, and with no need to exploit information about the chronological
age of the subjects. We finally strengthen a well-known finding that using data
from multiple data centers always results in a better performing model compared
with training on a single cohort. Indeed, we show that this latter statement is
still valid even by increasing the size and the heterogeneity of the single
data cohort. In all our experiments we used 28528 polysomnography studies from
13 different clinical studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A attention way in Explainable methods for infant brain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00815v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00815v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying reliable deep learning techniques in interdisciplinary applications
needs learned models to output accurate and ({even more importantly})
explainable predictions. Existing approaches typically explicate network
outputs in a post-hoc fashion, under an implicit assumption that faithful
explanations come from accurate predictions/classifications. We have an
opposite claim that explanations boost (or even determine) classification. That
is, end-to-end learning of explanation factors to augment discriminative
representation extraction could be a more intuitive strategy to inversely
assure fine-grained explainability, e.g., in those neuroimaging and
neuroscience studies with high-dimensional data containing noisy, redundant,
and task-irrelevant information. In this paper, we propose such an explainable
geometric deep network dubbed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Some parts of the thesis are still being revised</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anomaly Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2007.14462v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2007.14462v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charanjit K. Khosa, Veronica Sanz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new algorithm for anomaly detection called Anomaly Awareness.
The algorithm learns about normal events while being made aware of the
anomalies through a modification of the cost function. We show how this method
works in different Particle Physics situations and in standard Computer Vision
tasks. For example, we apply the method to images from a Fat Jet topology
generated by Standard Model Top and QCD events, and test it against an array of
new physics scenarios, including Higgs production with EFT effects and
resonances decaying into two, three or four subjets. We find that the algorithm
is effective identifying anomalies not seen before, and becomes robust as we
make it aware of a varied-enough set of anomalies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Federated Learning via Contrastive Representation Ensemble <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08888v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08888v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiying Yu, Yang Liu, Yimu Wang, Ke Xu, Jingjing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing amount of multimedia data on modern mobile systems and
IoT infrastructures, harnessing these rich multimodal data without breaching
user privacy becomes a critical issue. Federated learning (FL) serves as a
privacy-conscious alternative to centralized machine learning. However,
existing FL methods extended to multimodal data all rely on model aggregation
on single modality level, which restrains the server and clients to have
identical model architecture for each modality. This limits the global model in
terms of both model complexity and data capacity, not to mention task
diversity. In this work, we propose Contrastive Representation Ensemble and
Aggregation for Multimodal FL (CreamFL), a multimodal federated learning
framework that enables training larger server models from clients with
heterogeneous model architectures and data modalities, while only communicating
knowledge on public dataset. To achieve better multimodal representation
fusion, we design a global-local cross-modal ensemble strategy to aggregate
client representations. To mitigate local model drift caused by two
unprecedented heterogeneous factors stemming from multimodal discrepancy
(modality gap and task gap), we further propose two inter-modal and intra-modal
contrasts to regularize local training, which complements information of the
absent modality for uni-modal clients and regularizes local clients to head
towards global consensus. Thorough evaluations and ablation studies on
image-text retrieval and visual question answering tasks showcase the
superiority of CreamFL over state-of-the-art FL methods and its practical
value.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023. Code is available at https://github.com/FLAIR-THU/CreamFL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gradient Boosting Performs Gaussian Process Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.05608v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.05608v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksei Ustimenko, Artem Beliakov, Liudmila Prokhorenkova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper shows that gradient boosting based on symmetric decision trees can
be equivalently reformulated as a kernel method that converges to the solution
of a certain Kernel Ridge Regression problem. Thus, we obtain the convergence
to a Gaussian Process' posterior mean, which, in turn, allows us to easily
transform gradient boosting into a sampler from the posterior to provide better
knowledge uncertainty estimates through Monte-Carlo estimation of the posterior
variance. We show that the proposed sampler allows for better knowledge
uncertainty estimates leading to improved out-of-domain detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient ECG-based Atrial Fibrillation Detection via Parameterised
  Hypercomplex Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.02678v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.02678v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonie Basso, Zhao Ren, Wolfgang Nejdl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Atrial fibrillation (AF) is the most common cardiac arrhythmia and associated
with a high risk for serious conditions like stroke. The use of wearable
devices embedded with automatic and timely AF assessment from
electrocardiograms (ECGs) has shown to be promising in preventing
life-threatening situations. Although deep neural networks have demonstrated
superiority in model performance, their use on wearable devices is limited by
the trade-off between model performance and complexity. In this work, we
propose to use lightweight convolutional neural networks (CNNs) with
parameterised hypercomplex (PH) layers for AF detection based on ECGs. The
proposed approach trains small-scale CNNs, thus overcoming the limited
computing resources on wearable devices. We show comparable performance to
corresponding real-valued CNNs on two publicly available ECG datasets using
significantly fewer model parameters. PH models are more flexible than other
hypercomplex neural networks and can operate on any number of input ECG leads.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revised paper organisation. Further experiments to emphasise flexible
  model compression and comparison with other baselines</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cutting Through the Noise: An Empirical Comparison of Psychoacoustic and
  Envelope-based Features for Machinery Fault Detection <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01704v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01704v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Wißbrock, Yvonne Richter, David Pelkmann, Zhao Ren, Gregory Palmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acoustic-based fault detection has a high potential to monitor the health
condition of mechanical parts. However, the background noise of an industrial
environment may negatively influence the performance of fault detection.
Limited attention has been paid to improving the robustness of fault detection
against industrial environmental noise. Therefore, we present the Lenze
production background-noise (LPBN) real-world dataset and an automated and
noise-robust auditory inspection (ARAI) system for the end-of-line inspection
of geared motors. An acoustic array is used to acquire data from motors with a
minor fault, major fault, or which are healthy. A benchmark is provided to
compare the psychoacoustic features with different types of envelope features
based on expert knowledge of the gearbox. To the best of our knowledge, we are
the first to apply time-varying psychoacoustic features for fault detection. We
train a state-of-the-art one-class-classifier, on samples from healthy motors
and separate the faulty ones for fault detection using a threshold. The
best-performing approaches achieve an area under curve of 0.87 (logarithm
envelope), 0.86 (time-varying psychoacoustics), and 0.91 (combination of both).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>the final published version at ICASSP 2023 include small additional
  content as well as some minor revisions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature-Based Interpolation and Geodesics in the Latent Spaces of
  Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1904.03445v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1904.03445v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Łukasz Struski, Michał Sadowski, Tomasz Danel, Jacek Tabor, Igor T. Podolak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpolating between points is a problem connected simultaneously with
finding geodesics and study of generative models. In the case of geodesics, we
search for the curves with the shortest length, while in the case of generative
models we typically apply linear interpolation in the latent space. However,
this interpolation uses implicitly the fact that Gaussian is unimodal. Thus the
problem of interpolating in the case when the latent density is non-Gaussian is
an open problem.
  In this paper, we present a general and unified approach to interpolation,
which simultaneously allows us to search for geodesics and interpolating curves
in latent space in the case of arbitrary density. Our results have a strong
theoretical background based on the introduced quality measure of an
interpolating curve. In particular, we show that maximising the quality measure
of the curve can be equivalently understood as a search of geodesic for a
certain redefinition of the Riemannian metric on the space.
  We provide examples in three important cases. First, we show that our
approach can be easily applied to finding geodesics on manifolds. Next, we
focus our attention in finding interpolations in pre-trained generative models.
We show that our model effectively works in the case of arbitrary density.
Moreover, we can interpolate in the subset of the space consisting of data
possessing a given feature. The last case is focused on finding interpolation
in the space of chemical compounds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Sparse Graphon Mean Field Games <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.03880v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.03880v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Fabian, Kai Cui, Heinz Koeppl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although the field of multi-agent reinforcement learning (MARL) has made
considerable progress in the last years, solving systems with a large number of
agents remains a hard challenge. Graphon mean field games (GMFGs) enable the
scalable analysis of MARL problems that are otherwise intractable. By the
mathematical structure of graphons, this approach is limited to dense graphs
which are insufficient to describe many real-world networks such as power law
graphs. Our paper introduces a novel formulation of GMFGs, called LPGMFGs,
which leverages the graph theoretical concept of $L^p$ graphons and provides a
machine learning tool to efficiently and accurately approximate solutions for
sparse network problems. This especially includes power law networks which are
empirically observed in various application areas and cannot be captured by
standard graphons. We derive theoretical existence and convergence guarantees
and give empirical examples that demonstrate the accuracy of our learning
approach for systems with many agents. Furthermore, we extend the Online Mirror
Descent (OMD) learning algorithm to our setup to accelerate learning speed,
empirically show its capabilities, and conduct a theoretical analysis using the
novel concept of smoothed step graphons. In general, we provide a scalable,
mathematically well-founded machine learning approach to a large class of
otherwise intractable problems of great relevance in numerous research fields.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted for publication at the International Conference on
  Artificial Intelligence and Statistics (AISTATS) 2023; code available at:
  https://github.com/ChrFabian/Learning_sparse_GMFGs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Learning by Detecting Incorrect Location Embeddings <span class="chip">AAAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.04788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.04788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sepehr Sameni, Simon Jenni, Paolo Favaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel self-supervised learning (SSL) loss for
image representation learning. There is a growing belief that generalization in
deep neural networks is linked to their ability to discriminate object shapes.
Since object shape is related to the location of its parts, we propose to
detect those that have been artificially misplaced. We represent object parts
with image tokens and train a ViT to detect which token has been combined with
an incorrect positional embedding. We then introduce sparsity in the inputs to
make the model more robust to occlusions and to speed up the training. We call
our method DILEMMA, which stands for Detection of Incorrect Location EMbeddings
with MAsked inputs. We apply DILEMMA to MoCoV3, DINO and SimCLR and show an
improvement in their performance of respectively 4.41%, 3.97%, and 0.5% under
the same training time and with a linear probing transfer on ImageNet-1K. We
also show full fine-tuning improvements of MAE combined with our method on
ImageNet-100. We evaluate our method via fine-tuning on common SSL benchmarks.
Moreover, we show that when downstream tasks are strongly reliant on shape
(such as in the YOGA-82 pose dataset), our pre-trained features yield a
significant gain over prior work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at AAAI2023, https://github.com/Separius/DILEMMA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enabling Non-Linear Quantum Operations through Variational Quantum
  Splines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Antonio Inajetovic, Filippo Orazi, Antonio Macaluso, Stefano Lodi, Claudio Sartori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The postulates of quantum mechanics impose only unitary transformations on
quantum states, which is a severe limitation for quantum machine learning
algorithms. Quantum Splines (QSplines) have recently been proposed to
approximate quantum activation functions to introduce non-linearity in quantum
algorithms. However, QSplines make use of the HHL as a subroutine and require a
fault-tolerant quantum computer to be correctly implemented. This work proposes
the Generalised QSplines (GQSplines), a novel method for approximating
non-linear quantum activation functions using hybrid quantum-classical
computation. The GQSplines overcome the highly demanding requirements of the
original QSplines in terms of quantum hardware and can be implemented using
near-term quantum computers. Furthermore, the proposed method relies on a
flexible problem representation for non-linear approximation and it is suitable
to be embedded in existing quantum neural network architectures. In addition,
we provide a practical implementation of GQSplines using Pennylane and show
that our model outperforms the original QSplines in terms of quality of
fitting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarially Regularized Graph Attention Networks for Inductive
  Learning on Partially Labeled Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.03393v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.03393v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaren Xiao, Quanyu Dai, Xiaochen Xie, James Lam, Ka-Wai Kwok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high cost of data labeling often results in node label shortage in real
applications. To improve node classification accuracy, graph-based
semi-supervised learning leverages the ample unlabeled nodes to train together
with the scarce available labeled nodes. However, most existing methods require
the information of all nodes, including those to be predicted, during model
training, which is not practical for dynamic graphs with newly added nodes. To
address this issue, an adversarially regularized graph attention model is
proposed to classify newly added nodes in a partially labeled graph. An
attention-based aggregator is designed to generate the representation of a node
by aggregating information from its neighboring nodes, thus naturally
generalizing to previously unseen nodes. In addition, adversarial training is
employed to improve the model's robustness and generalization ability by
enforcing node representations to match a prior distribution. Experiments on
real-world datasets demonstrate the effectiveness of the proposed method in
comparison with the state-of-the-art methods. The code is available at
https://github.com/JiarenX/AGAIN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PSVRF: Learning to restore Pitch-Shifted Voice without reference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.02731v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.02731v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangfu Li, Xiaodan Lin, Jiaxin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pitch scaling algorithms have a significant impact on the security of
Automatic Speaker Verification (ASV) systems. Although numerous anti-spoofing
algorithms have been proposed to identify the pitch-shifted voice and even
restore it to the original version, they either have poor performance or
require the original voice as a reference, limiting the prospects of
applications. In this paper, we propose a no-reference approach termed
PSVRF$^1$ for high-quality restoration of pitch-shifted voice. Experiments on
AISHELL-1 and AISHELL-3 demonstrate that PSVRF can restore the voice disguised
by various pitch-scaling techniques, which obviously enhances the robustness of
ASV systems to pitch-scaling attacks. Furthermore, the performance of PSVRF
even surpasses that of the state-of-the-art reference-based approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Have some errors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning a model is paramount for sample efficiency in reinforcement
  learning control of PDEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.07160v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.07160v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Werner, Sebastian Peitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of this paper is to make a strong point for the usage of dynamical
models when using reinforcement learning (RL) for feedback control of dynamical
systems governed by partial differential equations (PDEs). To breach the gap
between the immense promises we see in RL and the applicability in complex
engineering systems, the main challenges are the massive requirements in terms
of the training data, as well as the lack of performance guarantees. We present
a solution for the first issue using a data-driven surrogate model in the form
of a convolutional LSTM with actuation. We demonstrate that learning an
actuated model in parallel to training the RL agent significantly reduces the
total amount of required data sampled from the real system. Furthermore, we
show that iteratively updating the model is of major importance to avoid biases
in the RL training. Detailed ablation studies reveal the most important
ingredients of the modeling process. We use the chaotic Kuramoto-Sivashinsky
equation do demonstarte our findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UnbiasedNets: A <span class="highlight-title">Dataset</span> Diversification Framework for Robustness Bias
  Alleviation in Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.12538v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.12538v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahum Naseer, Bharath Srinivas Prabakaran, Osman Hasan, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performance of trained neural network (NN) models, in terms of testing
accuracy, has improved remarkably over the past several years, especially with
the advent of deep learning. However, even the most accurate NNs can be biased
toward a specific output classification due to the inherent bias in the
available training datasets, which may propagate to the real-world
implementations. This paper deals with the robustness bias, i.e., the bias
exhibited by the trained NN by having a significantly large robustness to noise
for a certain output class, as compared to the remaining output classes. The
bias is shown to result from imbalanced datasets, i.e., the datasets where all
output classes are not equally represented. Towards this, we propose the
UnbiasedNets framework, which leverages K-means clustering and the NN's noise
tolerance to diversify the given training dataset, even from relatively smaller
datasets. This generates balanced datasets and reduces the bias within the
datasets themselves. To the best of our knowledge, this is the first framework
catering to the robustness bias problem in NNs. We use real-world datasets to
demonstrate the efficacy of the UnbiasedNets for data diversification, in case
of both binary and multi-label classifiers. The results are compared to
well-known tools aimed at generating balanced datasets, and illustrate how
existing works have limited success while addressing the robustness bias. In
contrast, UnbiasedNets provides a notable improvement over existing works,
while even reducing the robustness bias significantly in some cases, as
observed by comparing the NNs trained on the diversified and original datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Springer Machine Learning 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Dynamical System View of Langevin-Based Non-Convex Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.13867v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.13867v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Reza Karimi, Ya-Ping Hsieh, Andreas Krause
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-convex sampling is a key challenge in machine learning, central to
non-convex optimization in deep learning as well as to approximate
probabilistic inference. Despite its significance, theoretically there remain
many important challenges: Existing guarantees (1) typically only hold for the
averaged iterates rather than the more desirable last iterates, (2) lack
convergence metrics that capture the scales of the variables such as
Wasserstein distances, and (3) mainly apply to elementary schemes such as
stochastic gradient Langevin dynamics. In this paper, we develop a new
framework that lifts the above issues by harnessing several tools from the
theory of dynamical systems. Our key result is that, for a large class of
state-of-the-art sampling schemes, their last-iterate convergence in
Wasserstein distances can be reduced to the study of their continuous-time
counterparts, which is much better understood. Coupled with standard
assumptions of MCMC sampling, our theory immediately yields the last-iterate
Wasserstein convergence of many advanced sampling schemes such as proximal,
randomized mid-point, and Runge-Kutta integrators. Beyond existing methods, our
framework also motivates more efficient schemes that enjoy the same rigorous
guarantees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>typos corrected, references added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Maximum Mean Discrepancy on Exponential Windows for Online Change
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.12706v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.12706v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Kalinke, Marco Heyden, Edouard Fouché, Klemens Böhm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting changes is of fundamental importance when analyzing data streams
and has many applications, e.g., predictive maintenance, fraud detection, or
medicine. A principled approach to detect changes is to compare the
distributions of observations within the stream to each other via hypothesis
testing. Maximum mean discrepancy (MMD; also called energy distance) is a
well-known (semi-)metric on the space of probability distributions. MMD gives
rise to powerful non-parametric two-sample tests on kernel-enriched domains
under mild conditions, which makes its deployment for change detection
desirable. However, the classic MMD estimators suffer quadratic complexity,
which prohibits their application in the online change detection setting. We
propose a general-purpose change detection algorithm, Maximum Mean Discrepancy
on Exponential Windows (MMDEW), which leverages the MMD two-sample test,
facilitates its efficient online computation on any kernel-enriched domain, and
is able to detect any disparity between distributions. Our experiments and
analysis show that (1) MMDEW achieves better detection quality than
state-of-the-art competitors and that (2) the algorithm has polylogarithmic
runtime and logarithmic memory requirements, which allow its deployment to the
streaming setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SoK: Training Machine Learning Models over Multiple Sources with Privacy
  Preservation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.03386v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.03386v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lushan Song, Guopeng Lin, Jiaxuan Wang, Haoqi Wu, Wenqiang Ruan, Weili Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, gathering high-quality training data from multiple data sources
with privacy preservation is a crucial challenge to training high-performance
machine learning models. The potential solutions could break the barriers among
isolated data corpus, and consequently enlarge the range of data available for
processing. To this end, both academic researchers and industrial vendors are
recently strongly motivated to propose two main-stream folders of solutions
mainly based on software constructions: 1) Secure Multi-party Learning (MPL for
short); and 2) Federated Learning (FL for short). The above two technical
folders have their advantages and limitations when we evaluate them according
to the following five criteria: security, efficiency, data distribution, the
accuracy of trained models, and application scenarios.
  Motivated to demonstrate the research progress and discuss the insights on
the future directions, we thoroughly investigate these protocols and frameworks
of both MPL and FL. At first, we define the problem of Training machine
learning Models over Multiple data sources with Privacy Preservation (TMMPP for
short). Then, we compare the recent studies of TMMPP from the aspects of the
technical routes, the number of parties supported, data partitioning, threat
model, and machine learning models supported, to show their advantages and
limitations. Next, we investigate and evaluate five popular FL platforms.
Finally, we discuss the potential directions to resolve the problem of TMMPP in
the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting data-driven robust statistical arbitrage strategies with deep
  neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.03179v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.03179v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ariel Neufeld, Julian Sester, Daiying Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an approach, based on deep neural networks, that allows
identifying robust statistical arbitrage strategies in financial markets.
Robust statistical arbitrage strategies refer to trading strategies that enable
profitable trading under model ambiguity. The presented novel methodology
allows to consider a large amount of underlying securities simultaneously and
does not depend on the identification of cointegrated pairs of assets, hence it
is applicable on high-dimensional financial markets or in markets where
classical pairs trading approaches fail. Moreover, we provide a method to build
an ambiguity set of admissible probability measures that can be derived from
observed market data. Thus, the approach can be considered as being model-free
and entirely data-driven. We showcase the applicability of our method by
providing empirical investigations with highly profitable trading performances
even in 50 dimensions, during financial crises, and when the cointegration
relationship between asset pairs stops to persist.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Max-Margin Works while Large Margin Fails: Generalization without
  Uniform Convergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.07892v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.07892v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Margalit Glasgow, Colin Wei, Mary Wootters, Tengyu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major challenge in modern machine learning is theoretically understanding
the generalization properties of overparameterized models. Many existing tools
rely on uniform convergence (UC), a property that, when it holds, guarantees
that the test loss will be close to the training loss, uniformly over a class
of candidate models. Nagarajan and Kolter (2019) show that in certain simple
linear and neural-network settings, any uniform convergence bound will be
vacuous, leaving open the question of how to prove generalization in settings
where UC fails. Our main contribution is proving novel generalization bounds in
two such settings, one linear, and one non-linear. We study the linear
classification setting of Nagarajan and Kolter, and a quadratic ground truth
function learned via a two-layer neural network in the non-linear regime. We
prove a new type of margin bound showing that above a certain signal-to-noise
threshold, any near-max-margin classifier will achieve almost no test loss in
these two settings. Our results show that near-max-margin is important: while
any model that achieves at least a $(1 - \epsilon)$-fraction of the max-margin
generalizes well, a classifier achieving half of the max-margin may fail
terribly. Building on the impossibility results of Nagarajan and Kolter, under
slightly stronger assumptions, we show that one-sided UC bounds and classical
margin bounds will fail on near-max-margin classifiers. Our analysis provides
insight on why memorization can coexist with generalization: we show that in
this challenging regime where generalization occurs but UC fails,
near-max-margin classifiers simultaneously contain some generalizable
components and some overfitting components that memorize the data. The presence
of the overfitting components is enough to preclude UC, but the near-extremal
margin guarantees that sufficient generalizable components are present.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-time scheduling of renewable power systems through planning-based
  reinforcement learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05205v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05205v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaohuai Liu, Jinbo Liu, Weirui Ye, Nan Yang, Guanglun Zhang, Haiwang Zhong, Chongqing Kang, Qirong Jiang, Xuri Song, Fangchun Di, Yang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing renewable energy sources have posed significant challenges to
traditional power scheduling. It is difficult for operators to obtain accurate
day-ahead forecasts of renewable generation, thereby requiring the future
scheduling system to make real-time scheduling decisions aligning with
ultra-short-term forecasts. Restricted by the computation speed, traditional
optimization-based methods can not solve this problem. Recent developments in
reinforcement learning (RL) have demonstrated the potential to solve this
challenge. However, the existing RL methods are inadequate in terms of
constraint complexity, algorithm performance, and environment fidelity. We are
the first to propose a systematic solution based on the state-of-the-art
reinforcement learning algorithm and the real power grid environment. The
proposed approach enables planning and finer time resolution adjustments of
power generators, including unit commitment and economic dispatch, thus
increasing the grid's ability to admit more renewable energy. The well-trained
scheduling agent significantly reduces renewable curtailment and load shedding,
which are issues arising from traditional scheduling's reliance on inaccurate
day-ahead forecasts. High-frequency control decisions exploit the existing
units' flexibility, reducing the power grid's dependence on hardware
transformations and saving investment and operating costs, as demonstrated in
experimental results. This research exhibits the potential of reinforcement
learning in promoting low-carbon and intelligent power systems and represents a
solid step toward sustainable electricity generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Separate and conquer heuristic allows robust mining of contrast sets in
  classification, regression, and survival data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.00497v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.00497v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Gudyś, Marek Sikora, Łukasz Wróbel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying differences between groups is one of the most important knowledge
discovery problems. The procedure, also known as contrast sets mining, is
applied in a wide range of areas like medicine, industry, or economics.
  In the paper we present RuleKit-CS, an algorithm for contrast set mining
based on separate and conquer - a well established heuristic for decision rule
induction. Multiple passes accompanied with an attribute penalization scheme
provide contrast sets describing same examples with different attributes,
distinguishing presented approach from the standard separate and conquer. The
algorithm was also generalized for regression and survival data allowing
identification of contrast sets whose label attribute/survival prognosis is
consistent with the label/prognosis for the predefined contrast groups. This
feature, not provided by the existing approaches, further extends the usability
of RuleKit-CS.
  Experiments on over 130 data sets from various areas and detailed analysis of
selected cases confirmed RuleKit-CS to be a useful tool for discovering
differences between defined groups. The algorithm was implemented as a part of
the RuleKit suite available at GitHub under GNU AGPL 3 licence
(https://github.com/adaa-polsl/RuleKit).
  Keywords: contrast sets, separate and conquer, regression, survival
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 6 figures, 3 tables, 3 algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty Estimation by Fisher Information-based Evidential Deep
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02045v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02045v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danruo Deng, Guangyong Chen, Yang Yu, Furui Liu, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty estimation is a key factor that makes deep learning reliable in
practical applications. Recently proposed evidential neural networks explicitly
account for different uncertainties by treating the network's outputs as
evidence to parameterize the Dirichlet distribution, and achieve impressive
performance in uncertainty estimation. However, for high data uncertainty
samples but annotated with the one-hot label, the evidence-learning process for
those mislabeled classes is over-penalized and remains hindered. To address
this problem, we propose a novel method, Fisher Information-based Evidential
Deep Learning ($\mathcal{I}$-EDL). In particular, we introduce Fisher
Information Matrix (FIM) to measure the informativeness of evidence carried by
each sample, according to which we can dynamically reweight the objective loss
terms to make the network more focused on the representation learning of
uncertain classes. The generalization ability of our network is further
improved by optimizing the PAC-Bayesian bound. As demonstrated empirically, our
proposed method consistently outperforms traditional EDL-related algorithms in
multiple uncertainty estimation tasks, especially in the more challenging
few-shot classification settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Client Selection for Federated Bayesian Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarong Yang, Yuan Liu, Rahif Kassab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed Stein Variational Gradient Descent (DSVGD) is a non-parametric
distributed learning framework for federated Bayesian learning, where multiple
clients jointly train a machine learning model by communicating a number of
non-random and interacting particles with the server. Since communication
resources are limited, selecting the clients with most informative local
learning updates can improve the model convergence and communication
efficiency. In this paper, we propose two selection schemes for DSVGD based on
Kernelized Stein Discrepancy (KSD) and Hilbert Inner Product (HIP). We derive
the upper bound on the decrease of the global free energy per iteration for
both schemes, which is then minimized to speed up the model convergence. We
evaluate and compare our schemes with conventional schemes in terms of model
accuracy, convergence speed, and stability using various learning tasks and
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in IEEE Journal on Selected Areas in Communications Special
  Issue on Communication-Efficient Distributed Learning over Networks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EGFR Mutation Prediction of Lung Biopsy Images using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.12506v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.12506v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ravi Kant Gupta, Shivani Nandgaonkar, Nikhil Cherian Kurian, Swapnil Rane, Amit Sethi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The standard diagnostic procedures for targeted therapies in lung cancer
treatment involve histological subtyping and subsequent detection of key driver
mutations, such as EGFR. Even though molecular profiling can uncover the driver
mutation, the process is often expensive and time-consuming. Deep
learning-oriented image analysis offers a more economical alternative for
discovering driver mutations directly from whole slide images (WSIs). In this
work, we used customized deep learning pipelines with weak supervision to
identify the morphological correlates of EGFR mutation from hematoxylin and
eosin-stained WSIs, in addition to detecting tumor and histologically subtyping
it. We demonstrate the effectiveness of our pipeline by conducting rigorous
experiments and ablation studies on two lung cancer datasets - TCGA and a
private dataset from India. With our pipeline, we achieved an average area
under the curve (AUC) of 0.964 for tumor detection, and 0.942 for histological
subtyping between adenocarcinoma and squamous cell carcinoma on the TCGA
dataset. For EGFR detection, we achieved an average AUC of 0.864 on the TCGA
dataset and 0.783 on the dataset from India. Our key learning points include
the following. Firstly, there is no particular advantage of using a feature
extractor layers trained on histology, if one is going to fine-tune the feature
extractor on the target dataset. Secondly, selecting patches with high
cellularity, presumably capturing tumor regions, is not always helpful, as the
sign of a disease class may be present in the tumor-adjacent stroma.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We need to improve</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is the Performance of My Deep Network Too Good to Be True? A Direct
  Approach to Estimating the Bayes Error in Binary Classification <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.00395v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.00395v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takashi Ishida, Ikko Yamane, Nontawat Charoenphakdee, Gang Niu, Masashi Sugiyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a fundamental limitation in the prediction performance that a
machine learning model can achieve due to the inevitable uncertainty of the
prediction target. In classification problems, this can be characterized by the
Bayes error, which is the best achievable error with any classifier. The Bayes
error can be used as a criterion to evaluate classifiers with state-of-the-art
performance and can be used to detect test set overfitting. We propose a simple
and direct Bayes error estimator, where we just take the mean of the labels
that show \emph{uncertainty} of the class assignments. Our flexible approach
enables us to perform Bayes error estimation even for weakly supervised data.
In contrast to others, our method is model-free and even instance-free.
Moreover, it has no hyperparameters and gives a more accurate estimate of the
Bayes error than several baselines empirically. Experiments using our method
suggest that recently proposed deep networks such as the Vision Transformer may
have reached, or is about to reach, the Bayes error for benchmark datasets.
Finally, we discuss how we can study the inherent difficulty of the
acceptance/rejection decision for scientific articles, by estimating the Bayes
error of the ICLR papers from 2017 to 2023.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023 (notable-top-5%)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentiable Parsing and Visual Grounding of Natural Language
  Instructions for Object Placement <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00215v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00215v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zirui Zhao, Wee Sun Lee, David Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new method, PARsing And visual GrOuNding (ParaGon), for
grounding natural language in object placement tasks. Natural language
generally describes objects and spatial relations with compositionality and
ambiguity, two major obstacles to effective language grounding. For
compositionality, ParaGon parses a language instruction into an object-centric
graph representation to ground objects individually. For ambiguity, ParaGon
uses a novel particle-based graph neural network to reason about object
placements with uncertainty. Essentially, ParaGon integrates a parsing
algorithm into a probabilistic, data-driven learning framework. It is fully
differentiable and trained end-to-end from data for robustness against complex,
ambiguous language input.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProxyBO: Accelerating Neural Architecture Search via Bayesian
  Optimization with Zero-cost Proxies <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.10423v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.10423v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Shen, Yang Li, Jian Zheng, Wentao Zhang, Peng Yao, Jixiang Li, Sen Yang, Ji Liu, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing neural architectures requires immense manual efforts. This has
promoted the development of neural architecture search (NAS) to automate the
design. While previous NAS methods achieve promising results but run slowly,
zero-cost proxies run extremely fast but are less promising. Therefore, it is
of great potential to accelerate NAS via those zero-cost proxies. The existing
method has two limitations, which are unforeseeable reliability and one-shot
usage. To address the limitations, we present ProxyBO, an efficient Bayesian
optimization (BO) framework that utilizes the zero-cost proxies to accelerate
neural architecture search. We apply the generalization ability measurement to
estimate the fitness of proxies on the task during each iteration and design a
novel acquisition function to combine BO with zero-cost proxies based on their
dynamic influence. Extensive empirical studies show that ProxyBO consistently
outperforms competitive baselines on five tasks from three public benchmarks.
Concretely, ProxyBO achieves up to 5.41x and 3.86x speedups over the
state-of-the-art approaches REA and BRP-NAS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Location Leakage in Federated Signal Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.03452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.03452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evita Bakopoulou, Jiang Zhang, Mengwei Yang, Konstantinos Psounis, Athina Markopoulou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of predicting cellular network performance (signal
maps) from measurements collected by several mobile devices. We formulate the
problem within the online federated learning framework: (i) federated learning
(FL) enables users to collaboratively train a model, while keeping their
training data on their devices; (ii) measurements are collected as users move
around over time and are used for local training in an online fashion. We
consider an honest-but-curious server, who observes the updates from target
users participating in FL and infers their location using a deep leakage from
gradients (DLG) type of attack, originally developed to reconstruct training
data of DNN image classifiers. We make the key observation that a DLG attack,
applied to our setting, infers the average location of a batch of local data,
and can thus be used to reconstruct the target users' trajectory at a coarse
granularity. We build on this observation to protect location privacy, in our
setting, by revisiting and designing mechanisms within the federated learning
framework including: tuning the FL parameters for averaging, curating local
batches so as to mislead the DLG attacker, and aggregating across multiple
users with different trajectories. We evaluate the performance of our
algorithms through both analysis and simulation based on real-world mobile
datasets, and we show that they achieve a good privacy-utility tradeoff.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-Stationary Bandit Learning via Predictive Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.01970v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.01970v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueyang Liu, Benjamin Van Roy, Kuang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thompson sampling has proven effective across a wide range of stationary
bandit environments. However, as we demonstrate in this paper, it can perform
poorly when applied to non-stationary environments. We show that such failures
are attributed to the fact that, when exploring, the algorithm does not
differentiate actions based on how quickly the information acquired loses its
usefulness due to non-stationarity. Building upon this insight, we propose
predictive sampling, an algorithm that deprioritizes acquiring information that
quickly loses usefulness. Theoretical guarantee on the performance of
predictive sampling is established through a Bayesian regret bound. We provide
versions of predictive sampling for which computations tractably scale to
complex bandit environments of practical interest. Through numerical
simulations, we demonstrate that predictive sampling outperforms Thompson
sampling in all non-stationary environments examined.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FRMDN: Flow-based Recurrent Mixture Density Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2008.02144v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2008.02144v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyedeh Fatemeh Razavi, Reshad Hosseini, Tina Behzad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The class of recurrent mixture density networks is an important class of
probabilistic models used extensively in sequence modeling and
sequence-to-sequence mapping applications. In this class of models, the density
of a target sequence in each time-step is modeled by a Gaussian mixture model
with the parameters given by a recurrent neural network. In this paper, we
generalize recurrent mixture density networks by defining a Gaussian mixture
model on a non-linearly transformed target sequence in each time-step. The
non-linearly transformed space is created by normalizing flow. We observed that
this model significantly improves the fit to image sequences measured by the
log-likelihood. We also applied the proposed model on some speech and image
data, and observed that the model has significant modeling power outperforming
other state-of-the-art methods in terms of the log-likelihood.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimation of Correlation Matrices from Limited time series Data using
  Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.01198v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.01198v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Easaw, Woo Seok Lee, Prashant Singh Lohiya, Sarika Jalan, Priodyuti Pradhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Correlation matrices contain a wide variety of spatio-temporal information
about a dynamical system. Predicting correlation matrices from partial time
series information of a few nodes characterizes the spatio-temporal dynamics of
the entire underlying system. This information can help to predict the
underlying network structure, e.g., inferring neuronal connections from spiking
data, deducing causal dependencies between genes from expression data, and
discovering long spatial range influences in climate variations. Traditional
methods of predicting correlation matrices utilize time series data of all the
nodes of the underlying networks. Here, we use a supervised machine learning
technique to predict the correlation matrix of entire systems from finite time
series information of a few randomly selected nodes. The accuracy of the
prediction validates that only a limited time series of a subset of the entire
system is enough to make good correlation matrix predictions. Furthermore,
using an unsupervised learning algorithm, we furnish insights into the success
of the predictions from our model. Finally, we employ the machine learning
model developed here to real-world data sets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.01217v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.01217v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Zhang, Xingjun Ma, Qi Yi, Jitao Sang, Yugang Jiang, Yaowei Wang, Changsheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a growing interest in developing unlearnable examples (UEs) against
visual privacy leaks on the Internet. UEs are training samples added with
invisible but unlearnable noise, which have been found can prevent unauthorized
training of machine learning models. UEs typically are generated via a bilevel
optimization framework with a surrogate model to remove (minimize) errors from
the original samples, and then applied to protect the data against unknown
target models. However, existing UE generation methods all rely on an ideal
assumption called label-consistency, where the hackers and protectors are
assumed to hold the same label for a given sample. In this work, we propose and
promote a more practical label-agnostic setting, where the hackers may exploit
the protected data quite differently from the protectors. E.g., a m-class
unlearnable dataset held by the protector may be exploited by the hacker as a
n-class dataset. Existing UE generation methods are rendered ineffective in
this challenging setting. To tackle this challenge, we present a novel
technique called Unlearnable Clusters (UCs) to generate label-agnostic
unlearnable examples with cluster-wise perturbations. Furthermore, we propose
to leverage VisionandLanguage Pre-trained Models (VLPMs) like CLIP as the
surrogate model to improve the transferability of the crafted UCs to diverse
domains. We empirically verify the effectiveness of our proposed approach under
a variety of settings with different datasets, target models, and even
commercial platforms Microsoft Azure and Baidu PaddlePaddle. Code is available
at \url{https://github.com/jiamingzhang94/Unlearnable-Clusters}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Landslide Susceptibility Modeling by Interpretable Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.06837v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.06837v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khaled Youssef, Kevin Shao, Seulgi Moon, Louis-Serge Bouchard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Landslides are notoriously difficult to predict because numerous spatially
and temporally varying factors contribute to slope stability. Artificial neural
networks (ANN) have been shown to improve prediction accuracy but are largely
uninterpretable. Here we introduce an additive ANN optimization framework to
assess landslide susceptibility, as well as dataset division and outcome
interpretation techniques. We refer to our approach, which features full
interpretability, high accuracy, high generalizability and low model
complexity, as superposable neural network (SNN) optimization. We validate our
approach by training models on landslide inventory from three different
easternmost Himalaya regions. Our SNN outperformed physically-based and
statistical models and achieved similar performance to state-of-the-art deep
neural networks. The SNN models found the product of slope and precipitation
and hillslope aspect to be important primary contributors to high landslide
susceptibility, which highlights the importance of strong slope-climate
couplings, along with microclimates, on landslide occurrences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>79 pages (including SI section); 8 main figures; 12 supplementary
  figures; 9 supplementary tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DM$^2$: Decentralized Multi-Agent Reinforcement Learning for
  Distribution Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.00233v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.00233v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caroline Wang, Ishan Durugkar, Elad Liebman, Peter Stone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current approaches to multi-agent cooperation rely heavily on centralized
mechanisms or explicit communication protocols to ensure convergence. This
paper studies the problem of distributed multi-agent learning without resorting
to centralized components or explicit communication. It examines the use of
distribution matching to facilitate the coordination of independent agents. In
the proposed scheme, each agent independently minimizes the distribution
mismatch to the corresponding component of a target visitation distribution.
The theoretical analysis shows that under certain conditions, each agent
minimizing its individual distribution mismatch allows the convergence to the
joint policy that generated the target distribution. Further, if the target
distribution is from a joint policy that optimizes a cooperative task, the
optimal policy for a combination of this task reward and the distribution
matching reward is the same joint policy. This insight is used to formulate a
practical algorithm (DM$^2$), in which each individual agent matches a target
distribution derived from concurrently sampled trajectories from a joint expert
policy. Experimental validation on the StarCraft domain shows that combining
(1) a task reward, and (2) a distribution matching reward for expert
demonstrations for the same task, allows agents to outperform a naive
distributed baseline. Additional experiments probe the conditions under which
expert demonstrations need to be sampled to obtain the learning benefits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recipro-CAM: Fast gradient-free visual explanations for convolutional
  neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.14074v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.14074v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seok-Yong Byun, Wonju Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Convolutional Neural Network (CNN) is a widely used deep learning
architecture for computer vision. However, its black box nature makes it
difficult to interpret the behavior of the model. To mitigate this issue, AI
practitioners have explored explainable AI methods like Class Activation Map
(CAM) and Grad-CAM. Although these methods have shown promise, they are limited
by architectural constraints or the burden of gradient computing. To overcome
this issue, Score-CAM and Ablation-CAM have been proposed as gradient-free
methods, but they have longer execution times compared to CAM or Grad-CAM based
methods, making them unsuitable for real-world solution though they resolved
gradient related issues and enabled inference mode XAI. To address this
challenge, we propose a fast gradient-free Reciprocal CAM (Recipro-CAM) method.
Our approach involves spatially masking the extracted feature maps to exploit
the correlation between activation maps and network predictions for target
classes. Our proposed method has yielded promising results, outperforming
current state-of-the-art method in the Average Drop-Coherence-Complexity (ADCC)
metric by $1.78 \%$ to $3.72 \%$, excluding VGG-16 backbone. Moreover,
Recipro-CAM generates saliency maps at a similar rate to Grad-CAM and is
approximately $148$ times faster than Score-CAM. The source code for
Recipro-CAM is available in our data analysis framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Self-Training with Regularized Pseudo-Labeling for Tabular
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14013v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14013v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minwook Kim, Juseong Kim, Giltae Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in semi- and self-supervised learning has caused a rift in
the long-held belief about the need for an enormous amount of labeled data for
machine learning and the irrelevancy of unlabeled data. Although it has been
successful in various data, there is no dominant semi- and self-supervised
learning method that can be generalized for tabular data (i.e. most of the
existing methods require appropriate tabular datasets and architectures). In
this paper, we revisit self-training which can be applied to any kind of
algorithm including the most widely used architecture, gradient boosting
decision tree, and introduce curriculum pseudo-labeling (a state-of-the-art
pseudo-labeling technique in image) for a tabular domain. Furthermore, existing
pseudo-labeling techniques do not assure the cluster assumption when computing
confidence scores of pseudo-labels generated from unlabeled data. To overcome
this issue, we propose a novel pseudo-labeling approach that regularizes the
confidence scores based on the likelihoods of the pseudo-labels so that more
reliable pseudo-labels which lie in high density regions can be obtained. We
exhaustively validate the superiority of our approaches using various models
and tabular datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages for the main part and 8 extra pages for the appendix. 2
  figures and 3 tables for the main part</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Stability Analysis of Open Federated Learning Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.12307v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.12307v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youbang Sun, Heshan Fernando, Tianyi Chen, Shahin Shahrampour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the open federated learning (FL) systems, where clients may join
and/or leave the system during the FL process. Given the variability of the
number of present clients, convergence to a fixed model cannot be guaranteed in
open systems. Instead, we resort to a new performance metric that we term the
stability of open FL systems, which quantifies the magnitude of the learned
model in open systems. Under the assumption that local clients' functions are
strongly convex and smooth, we theoretically quantify the radius of stability
for two FL algorithms, namely local SGD and local Adam. We observe that this
radius relies on several key parameters, including the function condition
number as well as the variance of the stochastic gradient. Our theoretical
results are further verified by numerical simulations on both synthetic and
real-world benchmark data-sets.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model-tuning Via <span class="highlight-title">Prompt</span>s Makes NLP Models Adversarially Robust 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mrigank Raman, Pratyush Maini, J. Zico Kolter, Zachary C. Lipton, Danish Pruthi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, NLP practitioners have converged on the following practice:
(i) import an off-the-shelf pretrained (masked) language model; (ii) append a
multilayer perceptron atop the CLS token's hidden representation (with randomly
initialized weights); and (iii) fine-tune the entire model on a downstream task
(MLP). This procedure has produced massive gains on standard NLP benchmarks,
but these models remain brittle, even to mild adversarial perturbations, such
as word-level synonym substitutions. In this work, we demonstrate surprising
gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an
alternative method of adapting to downstream tasks. Rather than modifying the
model (by appending an MLP head), MVP instead modifies the input (by appending
a prompt template). Across three classification datasets, MVP improves
performance against adversarial word-level synonym substitutions by an average
of 8% over standard methods and even outperforms adversarial training-based
state-of-art defenses by 3.5%. By combining MVP with adversarial training, we
achieve further improvements in robust accuracy while maintaining clean
accuracy. Finally, we conduct ablations to investigate the mechanism underlying
these gains. Notably, we find that the main causes of vulnerability of MLP can
be attributed to the misalignment between pre-training and fine-tuning tasks,
and the randomly initialized MLP parameters. Code is available at
https://github.com/acmi-lab/mvp
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meet in the Middle: A New <span class="highlight-title">Pre-train</span>ing Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anh Nguyen, Nikos Karampatziakis, Weizhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most language models (LMs) are trained and applied in an autoregressive
left-to-right fashion, assuming that the next token only depends on the
preceding ones. However, this assumption ignores the potential benefits of
using the full sequence information during training, and the possibility of
having context from both sides during inference. In this paper, we propose a
new pre-training paradigm with techniques that jointly improve the training
data efficiency and the capabilities of the LMs in the infilling task. The
first is a training objective that aligns the predictions of a left-to-right LM
with those of a right-to-left LM, trained on the same data but in reverse
order. The second is a bidirectional inference procedure that enables both LMs
to meet in the middle. We show the effectiveness of our pre-training paradigm
with extensive experiments on both programming and natural language models,
outperforming strong baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>-based approaches to Sentiment Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olumide Ebenezer Ojo, Hoang Thang Ta, Alexander Gelbukh, Hiram Calvo, Olaronke Oluwayemisi Adebanji, Grigori Sidorov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of transfer learning methods is largely responsible for the present
breakthrough in Natural Learning Processing (NLP) tasks across multiple
domains. In order to solve the problem of sentiment detection, we examined the
performance of four different types of well-known state-of-the-art transformer
models for text classification. Models such as Bidirectional Encoder
Representations from Transformers (BERT), Robustly Optimized BERT Pre-training
Approach (RoBERTa), a distilled version of BERT (DistilBERT), and a large
bidirectional neural network architecture (XLNet) were proposed. The
performance of the four models that were used to detect disaster in the text
was compared. All the models performed well enough, indicating that
transformer-based models are suitable for the detection of disaster in text.
The RoBERTa transformer model performs best on the test dataset with a score of
82.6% and is highly recommended for quality predictions. Furthermore, we
discovered that the learning algorithms' performance was influenced by the
pre-processing techniques, the nature of words in the vocabulary, unbalanced
labeling, and the model parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Publisher: Springer Nature Switzerland AG, Gewerbestrasse 11, 6330
  Cham, Switzerland Published in Book Titled: Recent Developments and the New
  Directions of Research, Foundations, and Applications: Selected Papers of the
  8th World Conference on Soft Computing, February 03-05, 2022, Baku,
  Azerbaijan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of
  Synthetic and Compositional Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky, Roy Schwartz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weird, unusual, and uncanny images pique the curiosity of observers because
they challenge commonsense. For example, an image released during the 2022
world cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo
playing chess, which playfully violates our expectation that their competition
should occur on the football field. Humans can easily recognize and interpret
these unconventional images, but can AI models do the same? We introduce
WHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is
comprised of purposefully commonsense-defying images created by designers using
publicly-available image generation tools like Midjourney. We consider several
tasks posed over the dataset. In addition to image captioning, cross-modal
matching, and visual question answering, we introduce a difficult explanation
generation task, where models must identify and explain why a given image is
unusual. Our results show that state-of-the-art models such as GPT3 and BLIP2
still lag behind human performance on WHOOPS!. We hope our dataset will inspire
the development of AI models with stronger visual commonsense reasoning
abilities. Data, models and code are available at the project website:
whoops-benchmark.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Models Trained on Indian Legal Data Fair? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahil Girhepuje, Anmol Goel, Gokul Krishnan, Shreya Goyal, Satyendra Pandey, Ponnurangam Kumaraguru, Balaram Ravindran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances and applications of language technology and artificial
intelligence have enabled much success across multiple domains like law,
medical and mental health. AI-based Language Models, like Judgement Prediction,
have recently been proposed for the legal sector. However, these models are
strife with encoded social biases picked up from the training data. While bias
and fairness have been studied across NLP, most studies primarily locate
themselves within a Western context. In this work, we present an initial
investigation of fairness from the Indian perspective in the legal domain. We
highlight the propagation of learnt algorithmic biases in the bail prediction
task for models trained on Hindi legal documents. We evaluate the fairness gap
using demographic parity and show that a decision tree model trained for the
bail prediction task has an overall fairness disparity of 0.237 between input
features associated with Hindus and Muslims. Additionally, we highlight the
need for further research and studies in the avenues of fairness/bias in
applying AI in the legal sector with a specific focus on the Indian context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the Symposium on AI and Law (SAIL) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PMC-CLIP: Contrastive Language-Image <span class="highlight-title">Pre-train</span>ing using Biomedical
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models trained on large-scale dataset gain a recent surge in CV
and NLP. In contrast, development in biomedical domain lags far behind due to
data scarcity. To address this issue, we build and release PMC-OA, a biomedical
dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess
subset, which is 8 times larger than before. PMC-OA covers diverse modalities
or diseases, with majority of the image-caption samples aligned at
finer-grained level, i.e., subfigure and subcaption. While pretraining a
CLIP-style model on PMC-OA, our model named PMC-CLIP achieves state-of-the-art
results on various downstream tasks, including image-text retrieval on ROCO,
MedMNIST image classification, Medical VQA, i.e. +8.1% R@10 on image-text
retrieval, +3.9% accuracy on image classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Vision-Language Models with Sparse Mixture of Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, Yuxiong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of natural language processing (NLP) has made significant strides
in recent years, particularly in the development of large-scale vision-language
models (VLMs). These models aim to bridge the gap between text and visual
information, enabling a more comprehensive understanding of multimedia data.
However, as these models become larger and more complex, they also become more
challenging to train and deploy. One approach to addressing this challenge is
the use of sparsely-gated mixture-of-experts (MoE) techniques, which divide the
model into smaller, specialized sub-models that can jointly solve a task. In
this paper, we explore the effectiveness of MoE in scaling vision-language
models, demonstrating its potential to achieve state-of-the-art performance on
a range of benchmarks over dense models of equivalent computational cost. Our
research offers valuable insights into stabilizing the training of MoE models,
understanding the impact of MoE on model interpretability, and balancing the
trade-offs between compute performance when scaling VLMs. We hope our work will
inspire further research into the use of MoE for scaling large-scale
vision-language models and other multimodal machine learning applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive Empirical Evaluation of Existing Word Embedding
  Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Obaidullah Zaland, Muhammad Abulaish, Mohd. Fazil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector-based word representations help countless Natural Language Processing
(NLP) tasks capture both semantic and syntactic regularities of the language.
In this paper, we present the characteristics of existing word embedding
approaches and analyze them with regards to many classification tasks. We
categorize the methods into two main groups - Traditional approaches mostly use
matrix factorization to produce word representations, and they are not able to
capture the semantic and syntactic regularities of the language very well.
Neural-Network based approaches, on the other hand, can capture sophisticated
regularities of the language and preserve the word relationships in the
generated word representations. We report experimental results on multiple
classification tasks and highlight the scenarios where one approach performs
better than the rest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuroQL: A Neuro-Symbolic Language and <span class="highlight-title">Dataset</span> for Inter-Subjective
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Papoulias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new AI task and baseline solution for Inter-Subjective
Reasoning. We define inter-subjective information, to be a mixture of objective
and subjective information possibly shared by different parties. Examples may
include commodities and their objective properties as reported by IR
(Information Retrieval) systems, that need to be cross-referenced with
subjective user reviews from an online forum. For an AI system to successfully
reason about both, it needs to be able to combine symbolic reasoning of
objective facts with the shared consensus found on subjective user reviews. To
this end we introduce the NeuroQL dataset and DSL (Domain-specific Language) as
a baseline solution for this problem. NeuroQL is a neuro-symbolic language that
extends logical unification with neural primitives for extraction and
retrieval. It can function as a target for automatic translation of
inter-subjective questions (posed in natural language) into the neuro-symbolic
code that can answer them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models in the Workplace: A Case Study on <span class="highlight-title">Prompt</span>
  Engineering for Job Type Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Clavié, Alexandru Ciceu, Frederick Naylor, Guillaume Soulié, Thomas Brightwell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This case study investigates the task of job classification in a real-world
setting, where the goal is to determine whether an English-language job posting
is appropriate for a graduate or entry-level position. We explore multiple
approaches to text classification, including supervised approaches such as
traditional models like Support Vector Machines (SVMs) and state-of-the-art
deep learning methods such as DeBERTa. We compare them with Large Language
Models (LLMs) used in both few-shot and zero-shot classification settings. To
accomplish this task, we employ prompt engineering, a technique that involves
designing prompts to guide the LLMs towards the desired output. Specifically,
we evaluate the performance of two commercially available state-of-the-art
GPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also
conduct a detailed analysis of the impact of different aspects of prompt
engineering on the model's performance. Our results show that, with a
well-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms all
other models, achieving a 6% increase in Precision@95% Recall compared to the
best supervised approach. Furthermore, we observe that the wording of the
prompt is a critical factor in eliciting the appropriate "reasoning" in the
model, and that seemingly minor aspects of the prompt significantly affect the
model's performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating multiple-choice questions for medical question answering with
  distractors and cue-masking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Damien Sileo, Kanimozhi Uma, Marie-Francine Moens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical multiple-choice question answering (MCQA) is particularly difficult.
Questions may describe patient symptoms and ask for the correct diagnosis,
which requires domain knowledge and complex reasoning. Standard language
modeling pretraining alone is not sufficient to achieve the best results.
\citet{jin2020disease} showed that focusing masked language modeling on disease
name prediction when using medical encyclopedic paragraphs as input leads to
considerable MCQA accuracy improvement. In this work, we show that (1)
fine-tuning on generated MCQA dataset outperforms the masked language modeling
based objective and (2) correctly masking the cues to the answers is critical
for good performance. We release new pretraining datasets and achieve
state-of-the-art results on 4 MCQA datasets, notably +5.7\% with base-size
model on MedQA-USMLE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Biases in the Texts using an End-to-End Pipeline Approach <span class="chip">ECIR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaina Raza, Syed Raza Bashir,  Sneha, Urooj Qamar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The concept of fairness is gaining popularity in academia and industry.
Social media is especially vulnerable to media biases and toxic language and
comments. We propose a fair ML pipeline that takes a text as input and
determines whether it contains biases and toxic content. Then, based on
pre-trained word embeddings, it suggests a set of new words by substituting the
bi-ased words, the idea is to lessen the effects of those biases by replacing
them with alternative words. We compare our approach to existing fairness
models to determine its effectiveness. The results show that our proposed
pipeline can de-tect, identify, and mitigate biases in social media data
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Bias @ ECIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Human Subject Study of Named Entity Recognition (NER) in
  Conversational Music Recommendation Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elena V. Epure, Romain Hennequin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We conducted a human subject study of named entity recognition on a noisy
corpus of conversational music recommendation queries, with many irregular and
novel named entities. We evaluated the human NER linguistic behaviour in these
challenging conditions and compared it with the most common NER systems
nowadays, fine-tuned transformers. Our goal was to learn about the task to
guide the design of better evaluation methods and NER algorithms. The results
showed that NER in our context was quite hard for both human and algorithms
under a strict evaluation schema; humans had higher precision, while the model
higher recall because of entity exposure especially during pre-training; and
entity types had different error patterns (e.g. frequent typing errors for
artists). The released corpus goes beyond predefined frames of interaction and
can support future work in conversational music recommendation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextually-rich human affect perception using multimodal scene
  information <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Digbalay Bose, Rajat Hebbar, Krishna Somandepalli, Shrikanth Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The process of human affect understanding involves the ability to infer
person specific emotional states from various sources including images, speech,
and language. Affect perception from images has predominantly focused on
expressions extracted from salient face crops. However, emotions perceived by
humans rely on multiple contextual cues including social settings, foreground
interactions, and ambient visual scenes. In this work, we leverage pretrained
vision-language (VLN) models to extract descriptions of foreground context from
images. Further, we propose a multimodal context fusion (MCF) module to combine
foreground cues with the visual scene and person-based contextual information
for emotion prediction. We show the effectiveness of our proposed modular
design on two datasets associated with natural scenes and TV shows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP), 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The System Description of dun_oscar team for The ICPR MSR Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binbin Du, Rui Deng, Yingxin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the system submitted by dun_oscar team for the ICPR MSR
Challenge. Three subsystems for task1-task3 are descripted respectively. In
task1, we develop a visual system which includes a OCR model, a text tracker,
and a NLP classifier for distinguishing subtitles and non-subtitles. In task2,
we employ an ASR system which includes an AM with 18 layers and a 4-gram LM.
Semi-supervised learning on unlabeled data is also vital. In task3, we employ
the ASR system to improve the visual system, some false subtitles can be
corrected by a fusion module.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Contrastive Language-Image <span class="highlight-title">Pretrain</span>ing against Adversarial
  Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhan Yang, Baharan Mirzasoleiman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive vision-language representation learning has achieved
state-of-the-art performance for zero-shot classification, by learning from
millions of image-caption pairs crawled from the internet. However, the massive
data that powers large multimodal models such as CLIP, makes them extremely
vulnerable to various types of adversarial attacks, including targeted and
backdoor data poisoning attacks. Despite this vulnerability, robust contrastive
vision-language pretraining against adversarial attacks has remained
unaddressed. In this work, we propose RoCLIP, the first effective method for
robust pretraining {and fine-tuning} multimodal vision-language models. RoCLIP
effectively breaks the association between poisoned image-caption pairs by
considering a pool of random examples, and (1) matching every image with the
text that is most similar to its caption in the pool, and (2) matching every
caption with the image that is most similar to its image in the pool. Our
extensive experiments show that our method renders state-of-the-art targeted
data poisoning and backdoor attacks ineffective during pre-training or
fine-tuning of CLIP. In particular, RoCLIP decreases the poison and backdoor
attack success rates down to 0\% during pre-training and 1\%-4\% during
fine-tuning, and effectively improves the model's performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Transductions and Alignments with RNN Seq2seq Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengxiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper studies the capabilities of Recurrent-Neural-Network sequence to
sequence (RNN seq2seq) models in learning four string-to-string transduction
tasks: identity, reversal, total reduplication, and input-specified
reduplication. These transductions are traditionally well studied under finite
state transducers and attributed with varying complexity. We find that RNN
seq2seq models are only able to approximate a mapping that fits the training or
in-distribution data. Attention helps significantly, but does not solve the
out-of-distribution generalization limitation. Task complexity and RNN variants
also play a role in the results. Our results are best understood in terms of
the complexity hierarchy of formal languages as opposed to that of string
transductions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages; 9 figures; 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Diarization with Non-autoregressive Intermediate Attractors <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Fujita, Tatsuya Komatsu, Robin Scheibler, Yusuke Kida, Tetsuji Ogawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end neural diarization (EEND) with encoder-decoder-based attractors
(EDA) is a promising method to handle the whole speaker diarization problem
simultaneously with a single neural network. While the EEND model can produce
all frame-level speaker labels simultaneously, it disregards output label
dependency. In this work, we propose a novel EEND model that introduces the
label dependency between frames. The proposed method generates
non-autoregressive intermediate attractors to produce speaker labels at the
lower layers and conditions the subsequent layers with these labels. While the
proposed model works in a non-autoregressive manner, the speaker labels are
refined by referring to the whole sequence of intermediate labels. The
experiments with the two-speaker CALLHOME dataset show that the intermediate
labels with the proposed non-autoregressive intermediate attractors boost the
diarization performance. The proposed method with the deeper network benefits
more from the intermediate labels, resulting in better performance and training
throughput than EEND-EDA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Single Items: Exploring User Preferences in Item Sets with the
  Conversational Playlist Curation <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arun Tejasvi Chaganty, Megan Leszczynski, Shu Zhang, Ravi Ganti, Krisztian Balog, Filip Radlinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Users in consumption domains, like music, are often able to more efficiently
provide preferences over a set of items (e.g. a playlist or radio) than over
single items (e.g. songs). Unfortunately, this is an underexplored area of
research, with most existing recommendation systems limited to understanding
preferences over single items. Curating an item set exponentiates the search
space that recommender systems must consider (all subsets of items!): this
motivates conversational approaches-where users explicitly state or refine
their preferences and systems elicit preferences in natural language-as an
efficient way to understand user needs. We call this task conversational item
set curation and present a novel data collection methodology that efficiently
collects realistic preferences about item sets in a conversational setting by
observing both item-level and set-level feedback. We apply this methodology to
music recommendation to build the Conversational Playlist Curation Dataset
(CPCD), where we show that it leads raters to express preferences that would
not be otherwise expressed. Finally, we propose a wide range of conversational
retrieval models as baselines for this task and evaluate them on the dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unifying Vision, Text, and Layout for Universal Document Processing <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02623v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02623v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Universal Document Processing (UDOP), a foundation Document AI
model which unifies text, image, and layout modalities together with varied
task formats, including document understanding and generation. UDOP leverages
the spatial correlation between textual content and document image to model
image, text, and layout modalities with one uniform representation. With a
novel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain
downstream tasks into a prompt-based sequence generation scheme. UDOP is
pretrained on both large-scale unlabeled document corpora using innovative
self-supervised objectives and diverse labeled data. UDOP also learns to
generate document images from text and layout modalities via masked image
reconstruction. To the best of our knowledge, this is the first time in the
field of document AI that one model simultaneously achieves high-quality neural
document editing and content customization. Our method sets the
state-of-the-art on 8 Document AI tasks, e.g., document understanding and QA,
across diverse data domains like finance reports, academic papers, and
websites. UDOP ranks first on the leaderboard of the Document Understanding
Benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Clinical <span class="highlight-title">BERT</span>Score: An Improved Measure of Automatic Speech Recognition
  Performance in Clinical Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05737v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05737v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joel Shor, Ruyue Agnes Bi, Subhashini Venugopalan, Steven Ibara, Roman Goldenberg, Ehud Rivlin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Recognition (ASR) in medical contexts has the potential to
save time, cut costs, increase report accuracy, and reduce physician burnout.
However, the healthcare industry has been slower to adopt this technology, in
part due to the importance of avoiding medically-relevant transcription
mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR
metric that penalizes clinically-relevant mistakes more than others. We
demonstrate that this metric more closely aligns with clinician preferences on
medical sentences as compared to other metrics (WER, BLUE, METEOR, etc),
sometimes by wide margins. We collect a benchmark of 13 clinician preferences
on 149 realistic medical sentences called the Clinician Transcript Preference
benchmark (CTP), demonstrate that CBERTScore more closely matches what
clinicians prefer, and release the benchmark for the community to further
develop clinically-aware ASR metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdapterSoup: Weight Averaging to Improve Generalization of <span class="highlight-title">Pretrain</span>ed
  Language Models <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.07027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.07027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandra Chronopoulou, Matthew E. Peters, Alexander Fraser, Jesse Dodge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models (PLMs) are trained on massive corpora, but often
need to specialize to specific domains. A parameter-efficient adaptation method
suggests training an adapter for each domain on the task of language modeling.
This leads to good in-domain scores but can be impractical for domain- or
resource-restricted settings. A solution is to use a related-domain adapter for
the novel domain at test time. In this paper, we introduce AdapterSoup, an
approach that performs weight-space averaging of adapters trained on different
domains. Our approach is embarrassingly parallel: first, we train a set of
domain-specific adapters; then, for each novel domain, we determine which
adapters should be averaged at test time. We present extensive experiments
showing that AdapterSoup consistently improves performance to new domains
without extra training. We also explore weight averaging of adapters trained on
the same domain with different hyper-parameters, and show that it preserves the
performance of a PLM on new domains while obtaining strong in-domain results.
We explore various approaches for choosing which adapters to combine, such as
text clustering and semantic similarity. We find that using clustering leads to
the most competitive results on novel domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EACL 2023; camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attribution and Obfuscation of Neural Text Authorship: A Data Mining
  Perspective <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.10488v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.10488v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adaku Uchendu, Thai Le, Dongwon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two interlocking research questions of growing interest and importance in
privacy research are Authorship Attribution (AA) and Authorship Obfuscation
(AO). Given an artifact, especially a text t in question, an AA solution aims
to accurately attribute t to its true author out of many candidate authors
while an AO solution aims to modify t to hide its true authorship.
Traditionally, the notion of authorship and its accompanying privacy concern is
only toward human authors. However, in recent years, due to the explosive
advancements in Neural Text Generation (NTG) techniques in NLP, capable of
synthesizing human-quality open-ended texts (so-called "neural texts"), one has
to now consider authorships by humans, machines, or their combination. Due to
the implications and potential threats of neural texts when used maliciously,
it has become critical to understand the limitations of traditional AA/AO
solutions and develop novel AA/AO solutions in dealing with neural texts. In
this survey, therefore, we make a comprehensive review of recent literature on
the attribution and obfuscation of neural text authorship from a Data Mining
perspective, and share our view on their limitations and promising research
directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACM SIGKDD Explorations, Vol. 25, June 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05100v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05100v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        BigScience Workshop,  :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo González Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, Gérard Dupont, Germán Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis López, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Taşar, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre François Lavallée, Rémi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aurélie Névéol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdeněk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Muñoz Ferrandis, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clémentine Fourrier, Daniel León Periñán, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc Pàmies, Maria A Castillo, Marianna Nezhurina, Mario Sänger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, Thomas Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been shown to be able to perform new tasks
based on a few demonstrations or natural language instructions. While these
capabilities have led to widespread adoption, most LLMs are developed by
resource-rich organizations and are frequently kept from the public. As a step
towards democratizing this powerful technology, we present BLOOM, a
176B-parameter open-access language model designed and built thanks to a
collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer
language model that was trained on the ROOTS corpus, a dataset comprising
hundreds of sources in 46 natural and 13 programming languages (59 in total).
We find that BLOOM achieves competitive performance on a wide variety of
benchmarks, with stronger results after undergoing multitask prompted
finetuning. To facilitate future research and applications using LLMs, we
publicly release our models and code under the Responsible AI License.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Transducer Training: Reduced Memory Consumption with Sample-wise
  Computation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Braun, Erik McDermott, Roger Hsiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The neural transducer is an end-to-end model for automatic speech recognition
(ASR). While the model is well-suited for streaming ASR, the training process
remains challenging. During training, the memory requirements may quickly
exceed the capacity of state-of-the-art GPUs, limiting batch size and sequence
lengths. In this work, we analyze the time and space complexity of a typical
transducer training setup. We propose a memory-efficient training method that
computes the transducer loss and gradients sample by sample. We present
optimizations to increase the efficiency and parallelism of the sample-wise
method. In a set of thorough benchmarks, we show that our sample-wise method
significantly reduces memory usage, and performs at competitive speed when
compared to the default batched computation. As a highlight, we manage to
compute the transducer loss and gradients for a batch size of 1024, and audio
length of 40 seconds, using only 6 GB of memory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, 1 table, 1 algorithm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Sentence Grounding in Videos: A <span class="highlight-title">Survey</span> and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.08071v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.08071v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhang, Aixin Sun, Wei Jing, Joey Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal sentence grounding in videos (TSGV), \aka natural language video
localization (NLVL) or video moment retrieval (VMR), aims to retrieve a
temporal moment that semantically corresponds to a language query from an
untrimmed video. Connecting computer vision and natural language, TSGV has
drawn significant attention from researchers in both communities. This survey
attempts to provide a summary of fundamental concepts in TSGV and current
research status, as well as future research directions. As the background, we
present a common structure of functional components in TSGV, in a tutorial
style: from feature extraction from raw video and language query, to answer
prediction of the target moment. Then we review the techniques for multimodal
understanding and interaction, which is the key focus of TSGV for effective
alignment between the two modalities. We construct a taxonomy of TSGV
techniques and elaborate the methods in different categories with their
strengths and weaknesses. Lastly, we discuss issues with the current TSGV
research and share our insights about promising research directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accidental Learners: Spoken Language Identification in Multilingual
  <span class="highlight-title">Self-Supervised</span> Models <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05103v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05103v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Travis M. Bartley, Fei Jia, Krishna C. Puvvada, Samuel Kriman, Boris Ginsburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we extend previous self-supervised approaches for language
identification by experimenting with Conformer based architecture in a
multilingual pre-training paradigm. We find that pre-trained speech models
optimally encode language discriminatory information in lower layers. Further,
we demonstrate that the embeddings obtained from these layers are significantly
robust to classify unseen languages and different acoustic environments without
additional training. After fine-tuning a pre-trained Conformer model on the
VoxLingua107 dataset, we achieve results similar to current state-of-the-art
systems for language identification. More, our model accomplishes this with 5x
less parameters. We open-source the model through the NVIDIA NeMo toolkit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EDU-level Extractive Summarization with Varying Summary Lengths <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.04029v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.04029v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuping Wu, Ching-Hsun Tseng, Jiayu Shang, Shengzhong Mao, Goran Nenadic, Xiao-Jun Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extractive models usually formulate text summarization as extracting fixed
top-$k$ salient sentences from the document as a summary. Few works exploited
extracting finer-grained Elementary Discourse Unit (EDU) with little analysis
and justification for the extractive unit selection. Further, the selection
strategy of the fixed top-$k$ salient sentences fits the summarization need
poorly, as the number of salient sentences in different documents varies and
therefore a common or best $k$ does not exist in reality. To fill these gaps,
this paper first conducts the comparison analysis of oracle summaries based on
EDUs and sentences, which provides evidence from both theoretical and
experimental perspectives to justify and quantify that EDUs make summaries with
higher automatic evaluation scores than sentences. Then, considering this merit
of EDUs, this paper further proposes an EDU-level extractive model with Varying
summary Lengths and develops the corresponding learning algorithm. EDU-VL
learns to encode and predict probabilities of EDUs in the document, generate
multiple candidate summaries with varying lengths based on various $k$ values,
and encode and score candidate summaries, in an end-to-end training manner.
Finally, EDU-VL is experimented on single and multi-document benchmark datasets
and shows improved performances on ROUGE scores in comparison with
state-of-the-art extractive models, and further human evaluation suggests that
EDU-constituent summaries maintain good grammaticality and readability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EACL 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EasyNLP: A Comprehensive and Easy-to-use Toolkit for Natural Language
  Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.00258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.00258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyu Wang, Minghui Qiu, Chen Shi, Taolin Zhang, Tingting Liu, Lei Li, Jianing Wang, Ming Wang, Jun Huang, Wei Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of Pre-Trained Models (PTMs) has reshaped the development of
Natural Language Processing (NLP). Yet, it is not easy to obtain
high-performing models and deploy them online for industrial practitioners. To
bridge this gap, EasyNLP is designed to make it easy to build NLP applications,
which supports a comprehensive suite of NLP algorithms. It further features
knowledge-enhanced pre-training, knowledge distillation and few-shot learning
functionalities for large-scale PTMs, and provides a unified framework of model
training, inference and deployment for real-world applications. Currently,
EasyNLP has powered over ten business units within Alibaba Group and is
seamlessly integrated to the Platform of AI (PAI) products on Alibaba Cloud.
The source code of our EasyNLP toolkit is released at GitHub
(https://github.com/alibaba/EasyNLP).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking the Reasonability of the Test Set for Simultaneous Machine
  Translation <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengge Liu, Wen Zhang, Xiang Li, Jian Luan, Bin Wang, Yuhang Guo, Shuoying Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous machine translation (SimulMT) models start translation before
the end of the source sentence, making the translation monotonically aligned
with the source sentence. However, the general full-sentence translation test
set is acquired by offline translation of the entire source sentence, which is
not designed for SimulMT evaluation, making us rethink whether this will
underestimate the performance of SimulMT models. In this paper, we manually
annotate a monotonic test set based on the MuST-C English-Chinese test set,
denoted as SiMuST-C. Our human evaluation confirms the acceptability of our
annotated test set. Evaluations on three different SimulMT models verify that
the underestimation problem can be alleviated on our test set. Further
experiments show that finetuning on an automatically extracted monotonic
training set improves SimulMT models by up to 3 BLEU points.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 48th IEEE International Conference on Acoustics, Speech,
  and Signal Processing (ICASSP 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NASTyLinker: NIL-Aware Scalable <span class="highlight-title">Transformer</span>-based Entity Linker <span class="chip">ESWC'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Heist, Heiko Paulheim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity Linking (EL) is the task of detecting mentions of entities in text and
disambiguating them to a reference knowledge base. Most prevalent EL approaches
assume that the reference knowledge base is complete. In practice, however, it
is necessary to deal with the case of linking to an entity that is not
contained in the knowledge base (NIL entity). Recent works have shown that,
instead of focusing only on affinities between mentions and entities,
considering inter-mention affinities can be used to represent NIL entities by
producing clusters of mentions. At the same time, inter-mention affinities can
help to substantially improve linking performance for known entities. With
NASTyLinker, we introduce an EL approach that is aware of NIL entities and
produces corresponding mention clusters while maintaining high linking
performance for known entities. The approach clusters mentions and entities
based on dense representations from Transformers and resolves conflicts (if
more than one entity is assigned to a cluster) by computing transitive
mention-entity affinities. We show the effectiveness and scalability of
NASTyLinker on NILK, a dataset that is explicitly constructed to evaluate EL
with respect to NIL entities. Further, we apply the presented approach to an
actual EL task, namely to knowledge graph population by linking entities in
Wikipedia listings, and provide an analysis of the outcome.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of a paper in the research track of the 20th Extended
  Semantic Web Conference (ESWC'23)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Machine Translation with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13294v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13294v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasmin Moslem, Rejwanul Haque, John D. Kelleher, Andy Way
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consistency is a key requirement of high-quality translation. It is
especially important to adhere to pre-approved terminology and adapt to
corrected translations in domain-specific projects. Machine translation (MT)
has achieved significant progress in the area of domain adaptation. However,
real-time adaptation remains challenging. Large-scale language models (LLMs)
have recently shown interesting capabilities of in-context learning, where they
learn to replicate certain input-output text generation patterns, without
further fine-tuning. By feeding an LLM at inference time with a prompt that
consists of a list of translation pairs, it can then simulate the domain and
style characteristics. This work aims to investigate how we can utilize
in-context learning to improve real-time adaptive MT. Our extensive experiments
show promising results at translation time. For example, GPT-3.5 can adapt to a
set of in-domain sentence pairs and/or terminology while translating a new
sentence. We observe that the translation quality with few-shot in-context
learning can surpass that of strong encoder-decoder MT systems, especially for
high-resource languages. Moreover, we investigate whether we can combine MT
from strong encoder-decoder models with fuzzy matches, which can further
improve translation quality, especially for less supported languages. We
conduct our experiments across five diverse language pairs, namely
English-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French
(EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I-Tuning: Tuning Frozen Language Models with Image for Lightweight Image
  Captioning <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06574v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06574v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Luo, Zhipeng Hu, Yadong Xi, Rongsheng Zhang, Jing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Captioning is a traditional vision-and-language task that aims to
generate the language description of an image. Recent studies focus on scaling
up the model size and the number of training data, which significantly increase
the cost of model training. Different to these heavy-cost models, we introduce
a lightweight image captioning framework (I-Tuning), which contains a small
number of trainable parameters. We design a novel I-Tuning cross-attention
module to connect the non-trainable pre-trained language decoder GPT2 and
vision encoder CLIP-ViT. Since most parameters are not required to be updated
during training, our framework is lightweight and fast. Experimental results
conducted on three image captioning benchmarks reveal that our framework
achieves comparable or better performance than the large-scale baseline
systems. But our models contain up to 10 times fewer trainable parameters and
require much fewer data for training compared with state-of-the-art baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DailyTalk: Spoken Dialogue <span class="highlight-title">Dataset</span> for Conversational Text-to-Speech <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01063v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01063v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keon Lee, Kyumin Park, Daeyoung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The majority of current Text-to-Speech (TTS) datasets, which are collections
of individual utterances, contain few conversational aspects. In this paper, we
introduce DailyTalk, a high-quality conversational speech dataset designed for
conversational TTS. We sampled, modified, and recorded 2,541 dialogues from the
open-domain dialogue dataset DailyDialog inheriting its annotated attributes.
On top of our dataset, we extend prior work as our baseline, where a
non-autoregressive TTS is conditioned on historical information in a dialogue.
From the baseline experiment with both general and our novel metrics, we show
that DailyTalk can be used as a general TTS dataset, and more than that, our
baseline can represent contextual information from DailyTalk. The DailyTalk
dataset and baseline code are freely available for academic use with CC-BY-SA
4.0 license.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figures, 4 tables. Accepted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Alternate Intermediate Conditioning with Syllable-level and
  Character-level Targets for Japanese ASR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.00175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.00175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Fujita, Tatsuya Komatsu, Yusuke Kida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end automatic speech recognition directly maps input speech to
characters. However, the mapping can be problematic when several different
pronunciations should be mapped into one character or when one pronunciation is
shared among many different characters. Japanese ASR suffers the most from such
many-to-one and one-to-many mapping problems due to Japanese kanji characters.
To alleviate the problems, we introduce explicit interaction between characters
and syllables using Self-conditioned connectionist temporal classification
(CTC), in which the upper layers are ``self-conditioned'' on the intermediate
predictions from the lower layers. The proposed method utilizes character-level
and syllable-level intermediate predictions as conditioning features to deal
with mutual dependency between characters and syllables. Experimental results
on Corpus of Spontaneous Japanese show that the proposed method outperformed
the conventional multi-task and Self-conditioned CTC methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SLT 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Attention Networks Can Process Bounded Hierarchical Languages <span class="chip">ACL 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.11115v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.11115v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunyu Yao, Binghui Peng, Christos Papadimitriou, Karthik Narasimhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their impressive performance in NLP, self-attention networks were
recently proved to be limited for processing formal languages with hierarchical
structure, such as $\mathsf{Dyck}_k$, the language consisting of well-nested
parentheses of $k$ types. This suggested that natural language can be
approximated well with models that are too weak for formal languages, or that
the role of hierarchy and recursion in natural language might be limited. We
qualify this implication by proving that self-attention networks can process
$\mathsf{Dyck}_{k, D}$, the subset of $\mathsf{Dyck}_{k}$ with depth bounded by
$D$, which arguably better captures the bounded hierarchical structure of
natural language. Specifically, we construct a hard-attention network with
$D+1$ layers and $O(\log k)$ memory size (per token per layer) that recognizes
$\mathsf{Dyck}_{k, D}$, and a soft-attention network with two layers and
$O(\log k)$ memory size that generates $\mathsf{Dyck}_{k, D}$. Experiments show
that self-attention networks trained on $\mathsf{Dyck}_{k, D}$ generalize to
longer inputs with near-perfect accuracy, and also verify the theoretical
memory advantage of self-attention networks over recurrent networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2021. 19 pages with extended appendix. Fixed a small typo in the
  formula at the end of page 5 (thank to Gabriel Faria). Code:
  https://github.com/princeton-nlp/dyck-transformer</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TriDet: Temporal Action Detection with Relative Boundary Modeling <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingfeng Shi, Yujie Zhong, Qiong Cao, Lin Ma, Jia Li, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a one-stage framework TriDet for temporal action
detection. Existing methods often suffer from imprecise boundary predictions
due to the ambiguous action boundaries in videos. To alleviate this problem, we
propose a novel Trident-head to model the action boundary via an estimated
relative probability distribution around the boundary. In the feature pyramid
of TriDet, we propose an efficient Scalable-Granularity Perception (SGP) layer
to mitigate the rank loss problem of self-attention that takes place in the
video features and aggregate information across different temporal
granularities. Benefiting from the Trident-head and the SGP-based feature
pyramid, TriDet achieves state-of-the-art performance on three challenging
benchmarks: THUMOS14, HACS and EPIC-KITCHEN 100, with lower computational
costs, compared to previous methods. For example, TriDet hits an average mAP of
$69.3\%$ on THUMOS14, outperforming the previous best by $2.5\%$, but with only
$74.6\%$ of its latency. The code is released to
https://github.com/sssste/TriDet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2023; Temporal Action Detection; Temporal Action Localization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Erasing Concepts from Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, David Bau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by recent advancements in text-to-image diffusion, we study erasure
of specific concepts from the model's weights. While Stable Diffusion has shown
promise in producing explicit or realistic artwork, it has raised concerns
regarding its potential for misuse. We propose a fine-tuning method that can
erase a visual concept from a pre-trained diffusion model, given only the name
of the style and using negative guidance as a teacher. We benchmark our method
against previous approaches that remove sexually explicit content and
demonstrate its effectiveness, performing on par with Safe Latent Diffusion and
censored training. To evaluate artistic style removal, we conduct experiments
erasing five modern artists from the network and conduct a user study to assess
the human perception of the removed styles. Unlike previous methods, our
approach can remove concepts from a diffusion model permanently rather than
modifying the output at the inference time, so it cannot be circumvented even
if a user has access to model weights. Our code, data, and results are
available at https://erasing.baulab.info/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Class-Incremental Learning with <span class="highlight-title">Pre-Train</span>ed Models:
  Generalizability and Adaptivity are All You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Da-Wei Zhou, Han-Jia Ye, De-Chuan Zhan, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning (CIL) aims to adapt to emerging new classes
without forgetting old ones. Traditional CIL models are trained from scratch to
continually acquire knowledge as data evolves. Recently, pre-training has
achieved substantial progress, making vast pre-trained models (PTMs) accessible
for CIL. Contrary to traditional methods, PTMs possess generalizable
embeddings, which can be easily transferred. In this work, we revisit CIL with
PTMs and argue that the core factors in CIL are adaptivity for model updating
and generalizability for knowledge transferring. 1) We first reveal that frozen
PTM can already provide generalizable embeddings for CIL. Surprisingly, a
simple baseline (SimpleCIL) which continually sets the classifiers of PTM to
prototype features can beat state-of-the-art even without training on the
downstream task. 2) Due to the distribution gap between pre-trained and
downstream datasets, PTM can be further cultivated with adaptivity via model
adapting. We propose ADapt And Merge (ADAM), which aggregates the embeddings of
PTM and adapted models for classifier construction. ADAM is a general framework
that can be orthogonally combined with any parameter-efficient tuning method,
which holds the advantages of PTM's generalizability and adapted model's
adaptivity. 3) Additionally, we find previous benchmarks are unsuitable in the
era of PTM due to data overlapping and propose four new benchmarks for
assessment, namely ImageNet-A, ObjectNet, OmniBenchmark, and VTAB. Extensive
experiments validate the effectiveness of ADAM with a unified and concise
framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at: https://github.com/zhoudw-zdw/RevisitingCIL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PoseExaminer: Automated Testing of Out-of-Distribution Robustness in
  Human Pose and Shape Estimation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihao Liu, Adam Kortylewski, Alan Yuille
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human pose and shape (HPS) estimation methods achieve remarkable results.
However, current HPS benchmarks are mostly designed to test models in scenarios
that are similar to the training data. This can lead to critical situations in
real-world applications when the observed data differs significantly from the
training data and hence is out-of-distribution (OOD). It is therefore important
to test and improve the OOD robustness of HPS methods. To address this
fundamental problem, we develop a simulator that can be controlled in a
fine-grained manner using interpretable parameters to explore the manifold of
images of human pose, e.g. by varying poses, shapes, and clothes. We introduce
a learning-based testing method, termed PoseExaminer, that automatically
diagnoses HPS algorithms by searching over the parameter space of human pose
images to find the failure modes. Our strategy for exploring this
high-dimensional parameter space is a multi-agent reinforcement learning
system, in which the agents collaborate to explore different parts of the
parameter space. We show that our PoseExaminer discovers a variety of
limitations in current state-of-the-art models that are relevant in real-world
scenarios but are missed by current benchmarks. For example, it finds large
regions of realistic human poses that are not predicted correctly, as well as
reduced performance for humans with skinny and corpulent body shapes. In
addition, we show that fine-tuning HPS methods by exploiting the failure modes
found by PoseExaminer improve their robustness and even their performance on
standard benchmarks by a significant margin. The code are available for
research purposes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lite DETR : An Interleaved Multi-Scale Encoder for Efficient DETR <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Li, Ailing Zeng, Shilong Liu, Hao Zhang, Hongyang Li, Lei Zhang, Lionel M. Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent DEtection TRansformer-based (DETR) models have obtained remarkable
performance. Its success cannot be achieved without the re-introduction of
multi-scale feature fusion in the encoder. However, the excessively increased
tokens in multi-scale features, especially for about 75\% of low-level
features, are quite computationally inefficient, which hinders real
applications of DETR models. In this paper, we present Lite DETR, a simple yet
efficient end-to-end object detection framework that can effectively reduce the
GFLOPs of the detection head by 60\% while keeping 99\% of the original
performance. Specifically, we design an efficient encoder block to update
high-level features (corresponding to small-resolution feature maps) and
low-level features (corresponding to large-resolution feature maps) in an
interleaved way. In addition, to better fuse cross-scale features, we develop a
key-aware deformable attention to predict more reliable attention weights.
Comprehensive experiments validate the effectiveness and efficiency of the
proposed Lite DETR, and the efficient encoder strategy can generalize well
across existing DETR-based models. The code will be available in
\url{https://github.com/IDEA-Research/Lite-DETR}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MP-Former: Mask-Piloted <span class="highlight-title">Transformer</span> for Image Segmentation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhang, Feng Li, Huaizhe Xu, Shijia Huang, Shilong Liu, Lionel M. Ni, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a mask-piloted Transformer which improves masked-attention in
Mask2Former for image segmentation. The improvement is based on our observation
that Mask2Former suffers from inconsistent mask predictions between consecutive
decoder layers, which leads to inconsistent optimization goals and low
utilization of decoder queries. To address this problem, we propose a
mask-piloted training approach, which additionally feeds noised ground-truth
masks in masked-attention and trains the model to reconstruct the original
ones. Compared with the predicted masks used in mask-attention, the
ground-truth masks serve as a pilot and effectively alleviate the negative
impact of inaccurate mask predictions in Mask2Former. Based on this technique,
our \M achieves a remarkable performance improvement on all three image
segmentation tasks (instance, panoptic, and semantic), yielding $+2.3$AP and
$+1.6$mIoU on the Cityscapes instance and semantic segmentation tasks with a
ResNet-50 backbone. Our method also significantly speeds up the training,
outperforming Mask2Former with half of the number of training epochs on ADE20K
with both a ResNet-50 and a Swin-L backbones. Moreover, our method only
introduces little computation during training and no extra computation during
inference. Our code will be released at
\url{https://github.com/IDEA-Research/MP-Former}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised HDR Image and Video Tone Mapping via Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Cao, Huanjing Yue, Xin Liu, Jingyu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Capturing high dynamic range (HDR) images (videos) is attractive because it
can reveal the details in both dark and bright regions. Since the mainstream
screens only support low dynamic range (LDR) content, tone mapping algorithm is
required to compress the dynamic range of HDR images (videos). Although image
tone mapping has been widely explored, video tone mapping is lagging behind,
especially for the deep-learning-based methods, due to the lack of HDR-LDR
video pairs. In this work, we propose a unified framework (IVTMNet) for
unsupervised image and video tone mapping. To improve unsupervised training, we
propose domain and instance based contrastive learning loss. Instead of using a
universal feature extractor, such as VGG to extract the features for similarity
measurement, we propose a novel latent code, which is an aggregation of the
brightness and contrast of extracted features, to measure the similarity of
different pairs. We totally construct two negative pairs and three positive
pairs to constrain the latent codes of tone mapped results. For video tone
mapping, we propose a temporal-feature-replaced (TFR) module to efficiently
utilize the temporal correlation and improve the temporal consistency of video
tone-mapped results. We construct a large-scale unpaired HDR-LDR video dataset
to facilitate the unsupervised training process for video tone mapping.
Experimental results demonstrate that our method outperforms state-of-the-art
image and video tone mapping methods. Our code and dataset will be released
after the acceptance of this work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages,5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collision Cross-entropy and EM Algorithm for Self-labeled Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongwen Zhang, Yuri Boykov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose "collision cross-entropy" as a robust alternative to the Shannon's
cross-entropy in the context of self-labeled classification with posterior
models. Assuming unlabeled data, self-labeling works by estimating latent
pseudo-labels, categorical distributions y, that optimize some discriminative
clustering criteria, e.g. "decisiveness" and "fairness". All existing
self-labeled losses incorporate Shannon's cross-entropy term targeting the
model prediction, softmax, at the estimated distribution y. In fact, softmax is
trained to mimic the uncertainty in y exactly. Instead, we propose the negative
log-likelihood of "collision" to maximize the probability of equality between
two random variables represented by distributions softmax and y. We show that
our loss satisfies some properties of a generalized cross-entropy.
Interestingly, it agrees with the Shannon's cross-entropy for one-hot
pseudo-labels y, but the training from softer labels weakens. For example, if y
is a uniform distribution at some data point, it has zero contribution to the
training. Our self-labeling loss combining collision cross entropy with basic
clustering criteria is convex w.r.t. pseudo-labels, but non-trivial to optimize
over the probability simplex. We derive a practical EM algorithm optimizing
pseudo-labels y significantly faster than generic methods, e.g. the projectile
gradient descent. The collision cross-entropy consistently improves the results
on multiple self-labeled clustering examples using different DNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nearest-Neighbor Inter-Intra Contrastive Learning from Unlabeled Videos <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Fan, Deyu Yang, Xinyu Li, Vimal Bhat, Rohith MV
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has recently narrowed the gap between self-supervised
and supervised methods in image and video domain. State-of-the-art video
contrastive learning methods such as CVRL and $\rho$-MoCo spatiotemporally
augment two clips from the same video as positives. By only sampling positive
clips locally from a single video, these methods neglect other semantically
related videos that can also be useful. To address this limitation, we leverage
nearest-neighbor videos from the global space as additional positive pairs,
thus improving positive key diversity and introducing a more relaxed notion of
similarity that extends beyond video and even class boundaries. Our method,
Inter-Intra Video Contrastive Learning (IIVCL), improves performance on a range
of video tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the ICLR 2023 Workshop on Mathematical and Empirical
  Understanding of Foundation Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial
  Understanding with Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahui Fu, Yilun Du, Kurran Singh, Joshua B. Tenenbaum, John J. Leonard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present NeuSE, a novel Neural SE(3)-Equivariant Embedding for objects, and
illustrate how it supports object SLAM for consistent spatial understanding
with long-term scene changes. NeuSE is a set of latent object embeddings
created from partial object observations. It serves as a compact point cloud
surrogate for complete object models, encoding full shape information while
transforming SE(3)-equivariantly in tandem with the object in the physical
world. With NeuSE, relative frame transforms can be directly derived from
inferred latent codes. Our proposed SLAM paradigm, using NeuSE for object shape
and pose characterization, can operate independently or in conjunction with
typical SLAM systems. It directly infers SE(3) camera pose constraints that are
compatible with general SLAM pose graph optimization, while also maintaining a
lightweight object-centric map that adapts to real-world changes. Our approach
is evaluated on synthetic and real-world sequences featuring changed objects
and shows improved localization accuracy and change-aware mapping capability,
when working either standalone or jointly with a common SLAM pipeline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://neuse-slam.github.io/neuse/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Align and Attend: Multimodal Summarization with Dual Contrastive Losses <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo He, Jun Wang, Jielin Qiu, Trung Bui, Abhinav Shrivastava, Zhaowen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of multimodal summarization is to extract the most important
information from different modalities to form summaries. Unlike unimodal
summarization, the multimodal summarization task explicitly leverages
cross-modal information to help generate more reliable and high-quality
summaries. However, existing methods fail to leverage the temporal
correspondence between different modalities and ignore the intrinsic
correlation between different samples. To address this issue, we introduce
Align and Attend Multimodal Summarization (A2Summ), a unified multimodal
transformer-based model which can effectively align and attend the multimodal
input. In addition, we propose two novel contrastive losses to model both
inter-sample and intra-sample correlations. Extensive experiments on two
standard video summarization datasets (TVSum and SumMe) and two multimodal
summarization datasets (Daily Mail and CNN) demonstrate the superiority of
A2Summ, achieving state-of-the-art performances on all datasets. Moreover, we
collected a large-scale multimodal summarization dataset BLiSS, which contains
livestream videos and transcribed texts with annotated summaries. Our code and
dataset are publicly available at ~\url{https://boheumd.github.io/A2Summ/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision-Language Models as Success Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqing Du, Ksenia Konyushkova, Misha Denil, Akhil Raju, Jessica Landon, Felix Hill, Nando de Freitas, Serkan Cabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting successful behaviour is crucial for training intelligent agents. As
such, generalisable reward models are a prerequisite for agents that can learn
to generalise their behaviour. In this work we focus on developing robust
success detectors that leverage large, pretrained vision-language models
(Flamingo, Alayrac et al. (2022)) and human reward annotations. Concretely, we
treat success detection as a visual question answering (VQA) problem, denoted
SuccessVQA. We study success detection across three vastly different domains:
(i) interactive language-conditioned agents in a simulated household, (ii) real
world robotic manipulation, and (iii) "in-the-wild" human egocentric videos. We
investigate the generalisation properties of a Flamingo-based success detection
model across unseen language and visual changes in the first two domains, and
find that the proposed method is able to outperform bespoke reward models in
out-of-distribution test scenarios with either variation. In the last domain of
"in-the-wild" human videos, we show that success detection on unseen real
videos presents an even more challenging generalisation task warranting future
work. We hope our initial results encourage further work in real world success
detection and reward modelling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of
  Synthetic and Compositional Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky, Roy Schwartz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weird, unusual, and uncanny images pique the curiosity of observers because
they challenge commonsense. For example, an image released during the 2022
world cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo
playing chess, which playfully violates our expectation that their competition
should occur on the football field. Humans can easily recognize and interpret
these unconventional images, but can AI models do the same? We introduce
WHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is
comprised of purposefully commonsense-defying images created by designers using
publicly-available image generation tools like Midjourney. We consider several
tasks posed over the dataset. In addition to image captioning, cross-modal
matching, and visual question answering, we introduce a difficult explanation
generation task, where models must identify and explain why a given image is
unusual. Our results show that state-of-the-art models such as GPT3 and BLIP2
still lag behind human performance on WHOOPS!. We hope our dataset will inspire
the development of AI models with stronger visual commonsense reasoning
abilities. Data, models and code are available at the project website:
whoops-benchmark.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InPL: Pseudo-labeling the Inliers First for Imbalanced Semi-supervised
  Learning <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoran Yu, Yin Li, Yong Jae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent state-of-the-art methods in imbalanced semi-supervised learning (SSL)
rely on confidence-based pseudo-labeling with consistency regularization. To
obtain high-quality pseudo-labels, a high confidence threshold is typically
adopted. However, it has been shown that softmax-based confidence scores in
deep networks can be arbitrarily high for samples far from the training data,
and thus, the pseudo-labels for even high-confidence unlabeled samples may
still be unreliable. In this work, we present a new perspective of
pseudo-labeling for imbalanced SSL. Without relying on model confidence, we
propose to measure whether an unlabeled sample is likely to be
``in-distribution''; i.e., close to the current training data. To decide
whether an unlabeled sample is ``in-distribution'' or ``out-of-distribution'',
we adopt the energy score from out-of-distribution detection literature. As
training progresses and more unlabeled samples become in-distribution and
contribute to training, the combined labeled and pseudo-labeled data can better
approximate the true class distribution to improve the model. Experiments
demonstrate that our energy-based pseudo-labeling method, \textbf{InPL}, albeit
conceptually simple, significantly outperforms confidence-based methods on
imbalanced SSL benchmarks. For example, it produces around 3\% absolute
accuracy improvement on CIFAR10-LT. When combined with state-of-the-art
long-tailed SSL methods, further improvements are attained. In particular, in
one of the most challenging scenarios, InPL achieves a 6.9\% accuracy
improvement over the best competitor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Surface-normal Based Neural Framework for Colonoscopy Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuxian Wang, Yubo Zhang, Sarah K. McGill, Julian G. Rosenman, Jan-Michael Frahm, Soumyadip Sengupta, Stephen M. Pizer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing a 3D surface from colonoscopy video is challenging due to
illumination and reflectivity variation in the video frame that can cause
defective shape predictions. Aiming to overcome this challenge, we utilize the
characteristics of surface normal vectors and develop a two-step neural
framework that significantly improves the colonoscopy reconstruction quality.
The normal-based depth initialization network trained with self-supervised
normal consistency loss provides depth map initialization to the normal-depth
refinement module, which utilizes the relationship between illumination and
surface normals to refine the frame-wise normal and depth predictions
recursively. Our framework's depth accuracy performance on phantom colonoscopy
data demonstrates the value of exploiting the surface normals in colonoscopy
reconstruction, especially on en face views. Due to its low depth error, the
prediction result from our framework will require limited post-processing to be
clinically applicable for real-time colonoscopy reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IPMI 2023; first two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PMC-CLIP: Contrastive Language-Image <span class="highlight-title">Pre-train</span>ing using Biomedical
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models trained on large-scale dataset gain a recent surge in CV
and NLP. In contrast, development in biomedical domain lags far behind due to
data scarcity. To address this issue, we build and release PMC-OA, a biomedical
dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess
subset, which is 8 times larger than before. PMC-OA covers diverse modalities
or diseases, with majority of the image-caption samples aligned at
finer-grained level, i.e., subfigure and subcaption. While pretraining a
CLIP-style model on PMC-OA, our model named PMC-CLIP achieves state-of-the-art
results on various downstream tasks, including image-text retrieval on ROCO,
MedMNIST image classification, Medical VQA, i.e. +8.1% R@10 on image-text
retrieval, +3.9% accuracy on image classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Vision-Language Models with Sparse Mixture of Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, Yuxiong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of natural language processing (NLP) has made significant strides
in recent years, particularly in the development of large-scale vision-language
models (VLMs). These models aim to bridge the gap between text and visual
information, enabling a more comprehensive understanding of multimedia data.
However, as these models become larger and more complex, they also become more
challenging to train and deploy. One approach to addressing this challenge is
the use of sparsely-gated mixture-of-experts (MoE) techniques, which divide the
model into smaller, specialized sub-models that can jointly solve a task. In
this paper, we explore the effectiveness of MoE in scaling vision-language
models, demonstrating its potential to achieve state-of-the-art performance on
a range of benchmarks over dense models of equivalent computational cost. Our
research offers valuable insights into stabilizing the training of MoE models,
understanding the impact of MoE on model interpretability, and balancing the
trade-offs between compute performance when scaling VLMs. We hope our work will
inspire further research into the use of MoE for scaling large-scale
vision-language models and other multimodal machine learning applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Semantic Segmentation by Altering Resolutions for Compressed
  Videos <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubin Hu, Yuze He, Yanghao Li, Jisheng Li, Yuxing Han, Jiangtao Wen, Yong-Jin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video semantic segmentation (VSS) is a computationally expensive task due to
the per-frame prediction for videos of high frame rates. In recent work,
compact models or adaptive network strategies have been proposed for efficient
VSS. However, they did not consider a crucial factor that affects the
computational cost from the input side: the input resolution. In this paper, we
propose an altering resolution framework called AR-Seg for compressed videos to
achieve efficient VSS. AR-Seg aims to reduce the computational cost by using
low resolution for non-keyframes. To prevent the performance degradation caused
by downsampling, we design a Cross Resolution Feature Fusion (CReFF) module,
and supervise it with a novel Feature Similarity Training (FST) strategy.
Specifically, CReFF first makes use of motion vectors stored in a compressed
video to warp features from high-resolution keyframes to low-resolution
non-keyframes for better spatial alignment, and then selectively aggregates the
warped features with local attention mechanism. Furthermore, the proposed FST
supervises the aggregated features with high-resolution features through an
explicit similarity loss and an implicit constraint from the shared decoding
layer. Extensive experiments on CamVid and Cityscapes show that AR-Seg achieves
state-of-the-art performance and is compatible with different segmentation
backbones. On CamVid, AR-Seg saves 67% computational cost (measured in GFLOPs)
with the PSPNet18 backbone while maintaining high segmentation accuracy. Code:
https://github.com/THU-LYJ-Lab/AR-Seg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>Fusion: Decoupling Stability and Plasticity for Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Chen, Zuxuan Wu, Xintong Han, Menglin Jia, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning refers to the capability of continuously learning from a
stream of data. Current research mainly focuses on relieving catastrophic
forgetting, and most of their success is at the cost of limiting the
performance of newly incoming tasks. Such a trade-off is referred to as the
stabilityplasticity dilemma and is a more general and challenging problem for
continual learning. However, the inherent conflict between these two concepts
makes it seemingly impossible to devise a satisfactory solution to both of them
simultaneously. Therefore, we ask, "is it possible to divide them into two
problems to conquer independently?" To this end, we propose a
prompt-tuning-based method termed PromptFusion to enable the decoupling of
stability and plasticity. Specifically, PromptFusion consists of a carefully
designed Stabilizer module that deals with catastrophic forgetting and a
Booster module to learn new knowledge concurrently. During training,
PromptFusion first passes an input image to the two modules separately. Then
the resulting logits are further fused with a learnable weight parameter.
Finally, a weight mask is applied to the derived logits to balance between old
and new classes. Extensive experiments show that our method achieves promising
results on popular continual learning datasets for both class-incremental and
domain incremental settings. Especially on Split-Imagenet-R, one of the most
challenging datasets for class-incremental learning, our method exceeds
state-of-the-art prompt-based methods L2P and DualPrompt by more than 10%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallel Vertex Diffusion for Unified Visual Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zesen Cheng, Kehan Li, Peng Jin, Xiangyang Ji, Li Yuan, Chang Liu, Jie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unified visual grounding pursues a simple and generic technical route to
leverage multi-task data with less task-specific design. The most advanced
methods typically present boxes and masks as vertex sequences to model
referring detection and segmentation as an autoregressive sequential vertex
generation paradigm. However, generating high-dimensional vertex sequences
sequentially is error-prone because the upstream of the sequence remains static
and cannot be refined based on downstream vertex information, even if there is
a significant location gap. Besides, with limited vertexes, the inferior
fitting of objects with complex contours restricts the performance upper bound.
To deal with this dilemma, we propose a parallel vertex generation paradigm for
superior high-dimension scalability with a diffusion model by simply modifying
the noise dimension. An intuitive materialization of our paradigm is Parallel
Vertex Diffusion (PVD) to directly set vertex coordinates as the generation
target and use a diffusion model to train and infer. We claim that it has two
flaws: (1) unnormalized coordinate caused a high variance of loss value; (2)
the original training objective of PVD only considers point consistency but
ignores geometry consistency. To solve the first flaw, Center Anchor Mechanism
(CAM) is designed to convert coordinates as normalized offset values to
stabilize the training loss value. For the second flaw, Angle summation loss
(ASL) is designed to constrain the geometry difference of prediction and ground
truth vertexes for geometry-level consistency. Empirical results show that our
PVD achieves state-of-the-art in both referring detection and segmentation, and
our paradigm is more scalable and efficient than sequential vertex generation
with high-dimension data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Convolutional Neural Networks for Chronic Obstructive
  Pulmonary Disease Detection in Clinical Computed Tomography Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tina Dorosti, Manuel Schultheiss, Felix Hofmann, Luisa Kirchner, Theresa Urban, Franz Pfeiffer, Johannes Thalhammer, Florian Schaff, Tobias Lasser, Daniela Pfeiffer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chronic Obstructive Pulmonary Disease (COPD) is a leading cause of death
worldwide, yet early detection and treatment can prevent the progression of the
disease. In contrast to the conventional method of detecting COPD with
spirometry tests, X-ray Computed Tomography (CT) scans of the chest provide a
measure of morphological changes in the lung. It has been shown that automated
detection of COPD can be performed with deep learning models. However, the
potential of incorporating optimal window setting selection, typically carried
out by clinicians during examination of CT scans for COPD, is generally
overlooked in deep learning approaches. We aim to optimize the binary
classification of COPD with densely connected convolutional neural networks
(DenseNets) through implementation of manual and automated Window-Setting
Optimization (WSO) steps. Our dataset consisted of 78 CT scans from the
Klinikum rechts der Isar research hospital. Repeated inference on the test set
showed that without WSO, the plain DenseNet resulted in a mean slice-level AUC
of 0.80$\pm$0.05. With input images manually adjusted to the emphysema window
setting, the plain DenseNet model predicted COPD with a mean AUC of
0.86$\pm$0.04. By automating the WSO through addition of a customized layer to
the DenseNet, an optimal window setting in the proximity of the emphysema
window setting was learned and a mean AUC of 0.82$\pm$0.04 was achieved.
Detection of COPD with DenseNet models was optimized by WSO of CT data to the
emphysema window setting range, demonstrating the importance of implementing
optimal window setting selection in the deep learning pipeline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mobile Mapping Mesh Change Detection and Update 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teng Wu, Bruno Vallet, Cédric Demonceaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile mapping, in particular, Mobile Lidar Scanning (MLS) is increasingly
widespread to monitor and map urban scenes at city scale with unprecedented
resolution and accuracy. The resulting point cloud sampling of the scene
geometry can be meshed in order to create a continuous representation for
different applications: visualization, simulation, navigation, etc. Because of
the highly dynamic nature of these urban scenes, long term mapping should rely
on frequent map updates. A trivial solution is to simply replace old data with
newer data each time a new acquisition is made. However it has two drawbacks:
1) the old data may be of higher quality (resolution, precision) than the new
and 2) the coverage of the scene might be different in various acquisitions,
including varying occlusions. In this paper, we propose a fully automatic
pipeline to address these two issues by formulating the problem of merging
meshes with different quality, coverage and acquisition time. Our method is
based on a combined distance and visibility based change detection, a time
series analysis to assess the sustainability of changes, a mesh mosaicking
based on a global boolean optimization and finally a stitching of the resulting
mesh pieces boundaries with triangle strips. Finally, our method is
demonstrated on Robotcar and Stereopolis datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages without reference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incomplete Multi-View Multi-Label Learning via Label-Guided Masked View-
  and Category-Aware <span class="highlight-title">Transformer</span>s <span class="chip">AAAI-23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengliang Liu, Jie Wen, Xiaoling Luo, Yong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As we all know, multi-view data is more expressive than single-view data and
multi-label annotation enjoys richer supervision information than single-label,
which makes multi-view multi-label learning widely applicable for various
pattern recognition tasks. In this complex representation learning problem,
three main challenges can be characterized as follows: i) How to learn
consistent representations of samples across all views? ii) How to exploit and
utilize category correlations of multi-label to guide inference? iii) How to
avoid the negative impact resulting from the incompleteness of views or labels?
To cope with these problems, we propose a general multi-view multi-label
learning framework named label-guided masked view- and category-aware
transformers in this paper. First, we design two transformer-style based
modules for cross-view features aggregation and multi-label classification,
respectively. The former aggregates information from different views in the
process of extracting view-specific features, and the latter learns subcategory
embedding to improve classification performance. Second, considering the
imbalance of expressive power among views, an adaptively weighted view fusion
module is proposed to obtain view-consistent embedding features. Third, we
impose a label manifold constraint in sample-level representation learning to
maximize the utilization of supervised information. Last but not least, all the
modules are designed under the premise of incomplete views and labels, which
makes our method adaptable to arbitrary multi-view and multi-label data.
Extensive experiments on five datasets confirm that our method has clear
advantages over other state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI-23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Visual Number Discrimination in Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivana Kajić, Aida Nematzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to discriminate between large and small quantities is a core
aspect of basic numerical competence in both humans and animals. In this work,
we examine the extent to which the state-of-the-art neural networks designed
for vision exhibit this basic ability. Motivated by studies in animal and
infant numerical cognition, we use the numerical bisection procedure to test
number discrimination in different families of neural architectures. Our
results suggest that vision-specific inductive biases are helpful in numerosity
discrimination, as models with such biases have lowest test errors on the task,
and often have psychometric curves that qualitatively resemble those of humans
and animals performing the task. However, even the strongest models, as
measured on standard metrics of performance, fail to discriminate quantities in
transfer experiments with differing training and testing conditions, indicating
that such inductive biases might not be sufficient.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Event-based Optical Flow Identification and Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel von Arnim, Jules Lecomte, Stanislaw Wozniak, Naima Elosegui Borras, Angeliki Pantazi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical identification is often done with spatial or temporal visual pattern
recognition and localization. Temporal pattern recognition, depending on the
technology, involves a trade-off between communication frequency, range and
accurate tracking. We propose a solution with light-emitting beacons that
improves this trade-off by exploiting fast event-based cameras and, for
tracking, sparse neuromorphic optical flow computed with spiking neurons. In an
asset monitoring use case, we demonstrate that the system, embedded in a
simulated drone, is robust to relative movements and enables simultaneous
communication with, and tracking of, multiple moving beacons. Finally, in a
hardware lab prototype, we achieve state-of-the-art optical camera
communication frequencies in the kHz magnitude.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 6 figures and 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Amélioration de la qualité d'images avec un algorithme
  d'optimisation inspirée par la nature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivier Parisot, Thomas Tamisier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reproducible images preprocessing is important in the field of computer
vision, for efficient algorithms comparison or for new images corpus
preparation. In this paper, we propose a method to obtain an explicit and
ordered sequence of transformations that improves a given image: the
computation is performed via a nature-inspired optimization algorithm based on
quality assessment techniques. Preliminary tests show the impact of the
approach on different state-of-the-art data sets.
  --
  L'application de pr\'etraitements explicites et reproductibles est
fondamentale dans le domaine de la vision par ordinateur, pour pouvoir comparer
efficacement des algorithmes ou pour pr\'eparer un nouveau corpus d'images.
Dans cet article, nous proposons une m\'ethode pour obtenir une s\'equence
reproductible de transformations qui am\'eliore une image donn\'ee: le calcul
est r\'ealis\'e via un algorithme d'optimisation inspir\'ee par la nature et
bas\'e sur des techniques d'\'evaluation de la qualit\'e. Des tests montrent
l'impact de l'approche sur diff\'erents ensembles d'images de l'\'etat de
l'art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, in French language</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi PILOT: Learned Feasible Multiple Acquisition Trajectories for
  Dynamic MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tamir Shor, Tomer Weiss, Dor Noti, Alex Bronstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic Magnetic Resonance Imaging (MRI) is known to be a powerful and
reliable technique for the dynamic imaging of internal organs and tissues,
making it a leading diagnostic tool. A major difficulty in using MRI in this
setting is the relatively long acquisition time (and, hence, increased cost)
required for imaging in high spatio-temporal resolution, leading to the
appearance of related motion artifacts and decrease in resolution. Compressed
Sensing (CS) techniques have become a common tool to reduce MRI acquisition
time by subsampling images in the k-space according to some acquisition
trajectory. Several studies have particularly focused on applying deep learning
techniques to learn these acquisition trajectories in order to attain better
image reconstruction, rather than using some predefined set of trajectories. To
the best of our knowledge, learning acquisition trajectories has been only
explored in the context of static MRI. In this study, we consider acquisition
trajectory learning in the dynamic imaging setting. We design an end-to-end
pipeline for the joint optimization of multiple per-frame acquisition
trajectories along with a reconstruction neural network, and demonstrate
improved image reconstruction quality in shorter acquisition times. The code
for reproducing all experiments is accessible at
https://github.com/tamirshor7/MultiPILOT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Improved Baseline Framework for Pose Estimation Challenge at ECCV
  2022 Visual Perception for Navigation in Human Environments Workshop 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Fu, Yonghao Dang, Ruoqi Yin, Shaojie Zhang, Feng Zhou, Wending Zhao, Jianqin Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This technical report describes our first-place solution to the pose
estimation challenge at ECCV 2022 Visual Perception for Navigation in Human
Environments Workshop. In this challenge, we aim to estimate human poses from
in-the-wild stitched panoramic images. Our method is built based on Faster
R-CNN for human detection, and HRNet for human pose estimation. We describe
technical details for the JRDB-Pose dataset, together with some experimental
results. In the competition, we achieved 0.303 $\text{OSPA}_{\text{IOU}}$ and
64.047\% $\text{AP}_{\text{0.5}}$ on the test set of JRDB-Pose.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing COVID-19 Severity Analysis through Ensemble Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anand Thyagachandran, Hema A Murthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computed Tomography (CT) scans provide a detailed image of the lungs,
allowing clinicians to observe the extent of damage caused by COVID-19. The CT
severity score (CTSS) of COVID-19 can be categorized based on the extent of
lung involvement observed on a CT scan. This paper proposes a domain
knowledge-based pipeline to extract the infection regions using diverse
image-processing algorithms and a pre-trained UNET model. An ensemble of three
machine-learning models, Random Forest (RF), Extremely Randomized Trees (ERT),
and Support Vector Machine (SVM), is employed to classify the CT scans into
different severity classes. The proposed system achieved a macro F1 score of
57.47% on the validation dataset in the AI-Enabled Medical Image Analysis
Workshop and COVID-19 Diagnosis Competition (AI-MIA-COV19D).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mirror U-Net: Marrying Multimodal Fission with Multi-task Learning for
  Semantic Segmentation in Medical Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zdravko Marinov, Simon Reiß, David Kersting, Jens Kleesiek, Rainer Stiefelhagen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Positron Emission Tomography (PET) and Computer Tomography (CT) are routinely
used together to detect tumors. PET/CT segmentation models can automate tumor
delineation, however, current multimodal models do not fully exploit the
complementary information in each modality, as they either concatenate PET and
CT data or fuse them at the decision level. To combat this, we propose Mirror
U-Net, which replaces traditional fusion methods with multimodal fission by
factorizing the multimodal representation into modality-specific branches and
an auxiliary multimodal decoder. At these branches, Mirror U-Net assigns a task
tailored to each modality to reinforce unimodal features while preserving
multimodal features in the shared representation. In contrast to previous
methods that use either fission or multi-task learning, Mirror U-Net combines
both paradigms in a unified framework. We explore various task combinations and
examine which parameters to share in the model. We evaluate Mirror U-Net on the
AutoPET PET/CT and on the multimodal MSD BrainTumor datasets, demonstrating its
effectiveness in multimodal segmentation and achieving state-of-the-art
performance on both datasets. Our code will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages; 8 figures; 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't PANIC: Prototypical Additive Neural Network for Interpretable
  Classification of Alzheimer's Disease 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Nuno Wolf, Sebastian Pölster, Christian Wachinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's disease (AD) has a complex and multifactorial etiology, which
requires integrating information about neuroanatomy, genetics, and
cerebrospinal fluid biomarkers for accurate diagnosis. Hence, recent deep
learning approaches combined image and tabular information to improve
diagnostic performance. However, the black-box nature of such neural networks
is still a barrier for clinical applications, in which understanding the
decision of a heterogeneous model is integral. We propose PANIC, a prototypical
additive neural network for interpretable AD classification that integrates 3D
image and tabular data. It is interpretable by design and, thus, avoids the
need for post-hoc explanations that try to approximate the decision of a
network. Our results demonstrate that PANIC achieves state-of-the-art
performance in AD classification, while directly providing local and global
explanations. Finally, we show that PANIC extracts biologically meaningful
signatures of AD, and satisfies a set of desirable desiderata for trustworthy
machine learning. Our implementation is available at
\url{https://github.com/ai-med/PANIC}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in proceedings of Information Processing In Medical
  Imaging 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modality-Agnostic Debiasing for Single Domain Generalization <span class="chip">CVPR-2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanqing Qu, Yingwei Pan, Guang Chen, Ting Yao, Changjun Jiang, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) usually fail to generalize well to outside of
distribution (OOD) data, especially in the extreme case of single domain
generalization (single-DG) that transfers DNNs from single domain to multiple
unseen domains. Existing single-DG techniques commonly devise various
data-augmentation algorithms, and remould the multi-source domain
generalization methodology to learn domain-generalized (semantic) features.
Nevertheless, these methods are typically modality-specific, thereby being only
applicable to one single modality (e.g., image). In contrast, we target a
versatile Modality-Agnostic Debiasing (MAD) framework for single-DG, that
enables generalization for different modalities. Technically, MAD introduces a
novel two-branch classifier: a biased-branch encourages the classifier to
identify the domain-specific (superficial) features, and a general-branch
captures domain-generalized features based on the knowledge from biased-branch.
Our MAD is appealing in view that it is pluggable to most single-DG models. We
validate the superiority of our MAD in a variety of single-DG scenarios with
different modalities, including recognition on 1D texts, 2D images, 3D point
clouds, and semantic segmentation on 2D images. More remarkably, for
recognition on 3D point clouds and semantic segmentation on 2D images, MAD
improves DSU by 2.82\% and 1.5\% in accuracy and mIOU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in CVPR-2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeurEPDiff: Neural Operators to Predict Geodesics in Deformation Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nian Wu, Miaomiao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents NeurEPDiff, a novel network to fast predict the geodesics
in deformation spaces generated by a well known Euler-Poincar\'e differential
equation (EPDiff). To achieve this, we develop a neural operator that for the
first time learns the evolving trajectory of geodesic deformations
parameterized in the tangent space of diffeomorphisms(a.k.a velocity fields).
In contrast to previous methods that purely fit the training images, our
proposed NeurEPDiff learns a nonlinear mapping function between the
time-dependent velocity fields. A composition of integral operators and smooth
activation functions is formulated in each layer of NeurEPDiff to effectively
approximate such mappings. The fact that NeurEPDiff is able to rapidly provide
the numerical solution of EPDiff (given any initial condition) results in a
significantly reduced computational cost of geodesic shooting of
diffeomorphisms in a high-dimensional image space. Additionally, the properties
of discretiztion/resolution-invariant of NeurEPDiff make its performance
generalizable to multiple image resolutions after being trained offline. We
demonstrate the effectiveness of NeurEPDiff in registering two image datasets:
2D synthetic data and 3D brain resonance imaging (MRI). The registration
accuracy and computational efficiency are compared with the state-of-the-art
diffeomophic registration algorithms with geodesic shooting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Upcycling Models under Domain and Category Shift <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanqing Qu, Tianpei Zou, Florian Roehrbein, Cewu Lu, Guang Chen, Dacheng Tao, Changjun Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) often perform poorly in the presence of domain
shift and category shift. How to upcycle DNNs and adapt them to the target task
remains an important open problem. Unsupervised Domain Adaptation (UDA),
especially recently proposed Source-free Domain Adaptation (SFDA), has become a
promising technology to address this issue. Nevertheless, existing SFDA methods
require that the source domain and target domain share the same label space,
consequently being only applicable to the vanilla closed-set setting. In this
paper, we take one step further and explore the Source-free Universal Domain
Adaptation (SF-UniDA). The goal is to identify "known" data samples under both
domain and category shift, and reject those "unknown" data samples (not present
in source classes), with only the knowledge from standard pre-trained source
model. To this end, we introduce an innovative global and local clustering
learning technique (GLC). Specifically, we design a novel, adaptive one-vs-all
global clustering algorithm to achieve the distinction across different target
classes and introduce a local k-NN clustering strategy to alleviate negative
transfer. We examine the superiority of our GLC on multiple benchmarks with
different category shift scenarios, including partial-set, open-set, and
open-partial-set DA. Remarkably, in the most challenging open-partial-set DA
scenario, GLC outperforms UMAD by 14.8\% on the VisDA benchmark. The code is
available at https://github.com/ispc-lab/GLC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in CVPR 2023. The code has been made public</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Feature-based Approach for the Recognition of Image Quality
  Degradation in Automotive Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Bauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cameras play a crucial role in modern driver assistance systems and are an
essential part of the sensor technology for automated driving. The quality of
images captured by in-vehicle cameras highly influences the performance of
visual perception systems. This paper presents a feature-based algorithm to
detect certain effects that can degrade image quality in automotive
applications. The algorithm is based on an intelligent selection of significant
features. Due to the small number of features, the algorithm performs well even
with small data sets. Experiments with different data sets show that the
algorithm can detect soiling adhering to camera lenses and classify different
types of image degradation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prototype-based Embedding Network for Scene Graph Generation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaofan Zheng, Xinyu Lyu, Lianli Gao, Bo Dai, Jingkuan Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Scene Graph Generation (SGG) methods explore contextual information
to predict relationships among entity pairs. However, due to the diverse visual
appearance of numerous possible subject-object combinations, there is a large
intra-class variation within each predicate category, e.g., "man-eating-pizza,
giraffe-eating-leaf", and the severe inter-class similarity between different
classes, e.g., "man-holding-plate, man-eating-pizza", in model's latent space.
The above challenges prevent current SGG methods from acquiring robust features
for reliable relation prediction. In this paper, we claim that the predicate's
category-inherent semantics can serve as class-wise prototypes in the semantic
space for relieving the challenges. To the end, we propose the Prototype-based
Embedding Network (PE-Net), which models entities/predicates with
prototype-aligned compact and distinctive representations and thereby
establishes matching between entity pairs and predicates in a common embedding
space for relation recognition. Moreover, Prototype-guided Learning (PL) is
introduced to help PE-Net efficiently learn such entitypredicate matching, and
Prototype Regularization (PR) is devised to relieve the ambiguous
entity-predicate matching caused by the predicate's semantic overlap. Extensive
experiments demonstrate that our method gains superior relation recognition
capability on SGG, achieving new state-of-the-art performances on both Visual
Genome and Open Images datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weakly Unsupervised Domain Adaptation for Vestibular Schwannoma
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahad Hardan, Hussain Alasmawi, Xiangjian Hou, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vestibular schwannoma (VS) is a non-cancerous tumor located next to the ear
that can cause hearing loss. Most brain MRI images acquired from patients are
contrast-enhanced T1 (ceT1), with a growing interest in high-resolution T2
images (hrT2) to replace ceT1, which involves the use of a contrast agent. As
hrT2 images are currently scarce, it is less likely to train robust machine
learning models to segment VS or other brain structures. In this work, we
propose a weakly supervised machine learning approach that learns from only
ceT1 scans and adapts to segment two structures from hrT2 scans: the VS and the
cochlea from the crossMoDA dataset. Our model 1) generates fake hrT2 scans from
ceT1 images and segmentation masks, 2) is trained using the fake hrT2 scans, 3)
predicts the augmented real hrT2 scans, and 4) is retrained again using both
the fake and real hrT2. The final result of this model has been computed on an
unseen testing dataset provided by the 2022 crossMoDA challenge organizers. The
mean dice score and average symmetric surface distance (ASSD) are 0.78 and
0.46, respectively. The predicted segmentation masks achieved a dice score of
0.83 and an ASSD of 0.56 on the VS, and a dice score of 0.74 and an ASSD of
0.35 on the cochleas.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The challenge of representation learning: Improved accuracy in deep
  vision models does not come with better predictions of perceptual similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fritz Günther, Marco Marelli, Marco Alessandro Petilli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the last years, advancements in deep learning models for computer vision
have led to a dramatic improvement in their image classification accuracy.
However, models with a higher accuracy in the task they were trained on do not
necessarily develop better image representations that allow them to also
perform better in other tasks they were not trained on. In order to investigate
the representation learning capabilities of prominent high-performing computer
vision models, we investigated how well they capture various indices of
perceptual similarity from large-scale behavioral datasets. We find that higher
image classification accuracy rates are not associated with a better
performance on these datasets, and in fact we observe no improvement in
performance since GoogLeNet (released 2015) and VGG-M (released 2014). We
speculate that more accurate classification may result from hyper-engineering
towards very fine-grained distinctions between highly similar classes, which
does not incentivize the models to capture overall perceptual similarities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bag of Tricks with Quantized Convolutional Neural Networks for image
  classification <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Hu, Mengze Zeng, Enhua Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have been proven effective in a wide range of tasks.
However, their high computational and memory costs make them impractical to
deploy on resource-constrained devices. To address this issue, quantization
schemes have been proposed to reduce the memory footprint and improve inference
speed. While numerous quantization methods have been proposed, they lack
systematic analysis for their effectiveness. To bridge this gap, we collect and
improve existing quantization methods and propose a gold guideline for
post-training quantization. We evaluate the effectiveness of our proposed
method with two popular models, ResNet50 and MobileNetV2, on the ImageNet
dataset. By following our guidelines, no accuracy degradation occurs even after
directly quantizing the model to 8-bits without additional training. A
quantization-aware training based on the guidelines can further improve the
accuracy in lower-bits quantization. Moreover, we have integrated a multi-stage
fine-tuning strategy that works harmoniously with existing pruning techniques
to reduce costs even further. Remarkably, our results reveal that a quantized
MobileNetV2 with 30\% sparsity actually surpasses the performance of the
equivalent full-precision model, underscoring the effectiveness and resilience
of our proposed scheme.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatial Attention and Syntax Rule Enhanced Tree Decoder for Offine
  Handwritten Mathematical Expression Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Lin, Jinrong Li, Fan Yang, Shuangping Huang, Xu Yang, Jianmin Lin, Ming Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline Handwritten Mathematical Expression Recognition (HMER) has been
dramatically advanced recently by employing tree decoders as part of the
encoder-decoder method. Despite the tree decoder-based methods regard the
expressions as a tree and parse 2D spatial structure to the tree nodes
sequence, the performance of existing works is still poor due to the inevitable
tree nodes prediction errors. Besides, they lack syntax rules to regulate the
output of expressions. In this paper, we propose a novel model called Spatial
Attention and Syntax Rule Enhanced Tree Decoder (SS-TD), which is equipped with
spatial attention mechanism to alleviate the prediction error of tree structure
and use syntax masks (obtained from the transformation of syntax rules) to
constrain the occurrence of ungrammatical mathematical expression. In this way,
our model can effectively describe tree structure and increase the accuracy of
output expression. Experiments show that SS-TD achieves better recognition
performance than prior models on CROHME 14/16/19 datasets, demonstrating the
effectiveness of our model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSINet: Twins Contrastive Search of Multi-Scale Interaction for Object
  ReID <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyang Gu, Kai Wang, Hao Luo, Chen Chen, Wei Jiang, Yuqiang Fang, Shanghang Zhang, Yang You, Jian Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Architecture Search (NAS) has been increasingly appealing to the
society of object Re-Identification (ReID), for that task-specific
architectures significantly improve the retrieval performance. Previous works
explore new optimizing targets and search spaces for NAS ReID, yet they neglect
the difference of training schemes between image classification and ReID. In
this work, we propose a novel Twins Contrastive Mechanism (TCM) to provide more
appropriate supervision for ReID architecture search. TCM reduces the category
overlaps between the training and validation data, and assists NAS in
simulating real-world ReID training schemes. We then design a Multi-Scale
Interaction (MSI) search space to search for rational interaction operations
between multi-scale features. In addition, we introduce a Spatial Alignment
Module (SAM) to further enhance the attention consistency confronted with
images from different sources. Under the proposed NAS scheme, a specific
architecture is automatically searched, named as MSINet. Extensive experiments
demonstrate that our method surpasses state-of-the-art ReID methods on both
in-domain and cross-domain scenarios. Source code available in
https://github.com/vimar-gu/MSINet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Generalized Multi-Modal Fusion Detection Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leichao Cui, Xiuxian Li, Min Meng, Xiaoyu Mo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR point clouds have become the most common data source in autonomous
driving. However, due to the sparsity of point clouds, accurate and reliable
detection cannot be achieved in specific scenarios. Because of their
complementarity with point clouds, images are getting increasing attention.
Although with some success, existing fusion methods either perform hard fusion
or do not fuse in a direct manner. In this paper, we propose a generic 3D
detection framework called MMFusion, using multi-modal features. The framework
aims to achieve accurate fusion between LiDAR and images to improve 3D
detection in complex scenes. Our framework consists of two separate streams:
the LiDAR stream and the camera stream, which can be compatible with any
single-modal feature extraction network. The Voxel Local Perception Module in
the LiDAR stream enhances local feature representation, and then the
Multi-modal Feature Fusion Module selectively combines feature output from
different streams to achieve better fusion. Extensive experiments have shown
that our framework not only outperforms existing benchmarks but also improves
their detection, especially for detecting cyclists and pedestrians on KITTI
benchmarks, with strong robustness and generalization capabilities. Hopefully,
our work will stimulate more research into multi-modal fusion for autonomous
driving tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FireRisk: A Remote Sensing <span class="highlight-title">Dataset</span> for Fire Risk Assessment with
  Benchmarks Using Supervised and <span class="highlight-title">Self-supervised</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuchang Shen, Sachith Seneviratne, Xinye Wanyan, Michael Kirley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent decades, wildfires, as widespread and extremely destructive natural
disasters, have caused tremendous property losses and fatalities, as well as
extensive damage to forest ecosystems. Many fire risk assessment projects have
been proposed to prevent wildfires, but GIS-based methods are inherently
challenging to scale to different geographic areas due to variations in data
collection and local conditions. Inspired by the abundance of publicly
available remote sensing projects and the burgeoning development of deep
learning in computer vision, our research focuses on assessing fire risk using
remote sensing imagery.
  In this work, we propose a novel remote sensing dataset, FireRisk, consisting
of 7 fire risk classes with a total of 91872 labelled images for fire risk
assessment. This remote sensing dataset is labelled with the fire risk classes
supplied by the Wildfire Hazard Potential (WHP) raster dataset, and remote
sensing images are collected using the National Agriculture Imagery Program
(NAIP), a high-resolution remote sensing imagery program. On FireRisk, we
present benchmark performance for supervised and self-supervised
representations, with Masked Autoencoders (MAE) pre-trained on ImageNet1k
achieving the highest classification accuracy, 65.29%.
  This remote sensing dataset, FireRisk, provides a new direction for fire risk
assessment, and we make it publicly available on
https://github.com/CharmonyShen/FireRisk.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, 1 table, 1 equation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pretrain</span>ed ViTs Yield Versatile Representations For Medical Images <span class="chip">ICCV
  2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christos Matsoukas, Johan Fredin Haslum, Magnus Söderberg, Kevin Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional Neural Networks (CNNs) have reigned for a decade as the de
facto approach to automated medical image diagnosis, pushing the
state-of-the-art in classification, detection and segmentation tasks. Over the
last years, vision transformers (ViTs) have appeared as a competitive
alternative to CNNs, yielding impressive levels of performance in the natural
image domain, while possessing several interesting properties that could prove
beneficial for medical imaging tasks. In this work, we explore the benefits and
drawbacks of transformer-based models for medical image classification. We
conduct a series of experiments on several standard 2D medical image benchmark
datasets and tasks. Our findings show that, while CNNs perform better if
trained from scratch, off-the-shelf vision transformers can perform on par with
CNNs when pretrained on ImageNet, both in a supervised and self-supervised
setting, rendering them as a viable alternative to CNNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of "Is it Time to Replace CNNs with Transformers for
  Medical Images?" (Matsoukas et al. 2022) originally published at the ICCV
  2021 Workshop on Computer Vision for Automated Medical Diagnosis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SelfPromer: Self-<span class="highlight-title">Prompt</span> Dehazing <span class="highlight-title">Transformer</span>s with Depth-Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Wang, Jinshan Pan, Wanyu Lin, Jiangxin Dong, Xiao-Ming Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents an effective depth-consistency self-prompt Transformer for
image dehazing. It is motivated by an observation that the estimated depths of
an image with haze residuals and its clear counterpart vary. Enforcing the
depth consistency of dehazed images with clear ones, therefore, is essential
for dehazing. For this purpose, we develop a prompt based on the features of
depth differences between the hazy input images and corresponding clear
counterparts that can guide dehazing models for better restoration.
Specifically, we first apply deep features extracted from the input images to
the depth difference features for generating the prompt that contains the haze
residual information in the input. Then we propose a prompt embedding module
that is designed to perceive the haze residuals, by linearly adding the prompt
to the deep features. Further, we develop an effective prompt attention module
to pay more attention to haze residuals for better removal. By incorporating
the prompt, prompt embedding, and prompt attention into an encoder-decoder
network based on VQGAN, we can achieve better perception quality. As the depths
of clear images are not available at inference, and the dehazed images with
one-time feed-forward execution may still contain a portion of haze residuals,
we propose a new continuous self-prompt inference that can iteratively correct
the dehazing model towards better haze-free image generation. Extensive
experiments show that our method performs favorably against the
state-of-the-art approaches on both synthetic and real-world datasets in terms
of perception metrics including NIQE, PI, and PIQE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HOOV: Hand Out-Of-View Tracking for Proprioceptive Interaction using
  Inertial Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Streli, Rayan Armani, Yi Fei Cheng, Christian Holz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Virtual Reality systems are designed for interaction under visual
control. Using built-in cameras, headsets track the user's hands or hand-held
controllers while they are inside the field of view. Current systems thus
ignore the user's interaction with off-screen content -- virtual objects that
the user could quickly access through proprioception without requiring
laborious head motions to bring them into focus. In this paper, we present
HOOV, a wrist-worn sensing method that allows VR users to interact with objects
outside their field of view. Based on the signals of a single wrist-worn
inertial sensor, HOOV continuously estimates the user's hand position in
3-space to complement the headset's tracking as the hands leave the tracking
range. Our novel data-driven method predicts hand positions and trajectories
from just the continuous estimation of hand orientation, which by itself is
stable based solely on inertial observations. Our inertial sensing
simultaneously detects finger pinching to register off-screen selection events,
confirms them using a haptic actuator inside our wrist device, and thus allows
users to select, grab, and drop virtual content. We compared HOOV's performance
with a camera-based optical motion capture system in two folds. In the first
evaluation, participants interacted based on tracking information from the
motion capture system to assess the accuracy of their proprioceptive input,
whereas in the second, they interacted based on HOOV's real-time estimations.
We found that HOOV's target-agnostic estimations had a mean tracking error of
7.7 cm, which allowed participants to reliably access virtual objects around
their body without first bringing them into focus. We demonstrate several
applications that leverage the larger input space HOOV opens up for quick
proprioceptive interaction, and conclude by discussing the potential of our
technique.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2023 CHI Conference on Human Factors in Computing Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reference-Guided Large-Scale Face Inpainting with Identity and Texture
  Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wuyang Luo, Su Yang, Weishan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face inpainting aims at plausibly predicting missing pixels of face images
within a corrupted region. Most existing methods rely on generative models
learning a face image distribution from a big dataset, which produces
uncontrollable results, especially with large-scale missing regions. To
introduce strong control for face inpainting, we propose a novel
reference-guided face inpainting method that fills the large-scale missing
region with identity and texture control guided by a reference face image.
However, generating high-quality results under imposing two control signals is
challenging. To tackle such difficulty, we propose a dual control one-stage
framework that decouples the reference image into two levels for flexible
control: High-level identity information and low-level texture information,
where the identity information figures out the shape of the face and the
texture information depicts the component-aware texture. To synthesize
high-quality results, we design two novel modules referred to as Half-AdaIN and
Component-Wise Style Injector (CWSI) to inject the two kinds of control
information into the inpainting processing. Our method produces realistic
results with identity and texture control faithful to reference images. To the
best of our knowledge, it is the first work to concurrently apply identity and
component-level controls in face inpainting to promise more precise and
controllable results. Code is available at
https://github.com/WuyangLuo/RefFaceInpainting
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by IEEE Transactions on Circuits and Systems for Video
  Technology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AGTGAN: Unpaired Image Translation for Photographic Ancient Character
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongxiang Huang, Daihui Yang, Gang Dai, Zhen Han, Yuyi Wang, Kin-Man Lam, Fan Yang, Shuangping Huang, Yongge Liu, Mengchao He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of ancient writings has great value for archaeology and philology.
Essential forms of material are photographic characters, but manual
photographic character recognition is extremely time-consuming and
expertise-dependent. Automatic classification is therefore greatly desired.
However, the current performance is limited due to the lack of annotated data.
Data generation is an inexpensive but useful solution for data scarcity.
Nevertheless, the diverse glyph shapes and complex background textures of
photographic ancient characters make the generation task difficult, leading to
the unsatisfactory results of existing methods. In this paper, we propose an
unsupervised generative adversarial network called AGTGAN. By the explicit
global and local glyph shape style modeling followed by the stroke-aware
texture transfer, as well as an associate adversarial learning mechanism, our
method can generate characters with diverse glyphs and realistic textures. We
evaluate our approach on the photographic ancient character datasets, e.g.,
OBC306 and CSDD. Our method outperforms the state-of-the-art approaches in
various metrics and performs much better in terms of the diversity and
authenticity of generated samples. With our generated images, experiments on
the largest photographic oracle bone character dataset show that our method can
achieve a significant increase in classification accuracy, up to 16.34%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OSIS: Efficient One-stage Network for 3D Instance Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuan Tang, Xi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current 3D instance segmentation models generally use multi-stage methods to
extract instance objects, including clustering, feature extraction, and
post-processing processes. However, these multi-stage approaches rely on
hyperparameter settings and hand-crafted processes, which restrict the
inference speed of the model. In this paper, we propose a new 3D point cloud
instance segmentation network, named OSIS. OSIS is a one-stage network, which
directly segments instances from 3D point cloud data using neural network. To
segment instances directly from the network, we propose an instance decoder,
which decodes instance features from the network into instance segments. Our
proposed OSIS realizes the end-to-end training by bipartite matching,
therefore, our network does not require computationally expensive
post-processing steps such as non maximum suppression (NMS) and clustering
during inference. The results show that our network finally achieves excellent
performance in the commonly used indoor scene instance segmentation dataset,
and the inference speed of our network is only an average of 138ms per scene,
which substantially exceeds the previous fastest method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying Label Errors in Object Detection <span class="highlight-title">Dataset</span>s by Loss Inspection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marius Schubert, Tobias Riedlinger, Karsten Kahl, Daniel Kröll, Sebastian Schoenen, Siniša Šegvić, Matthias Rottmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Labeling datasets for supervised object detection is a dull and
time-consuming task. Errors can be easily introduced during annotation and
overlooked during review, yielding inaccurate benchmarks and performance
degradation of deep neural networks trained on noisy labels. In this work, we
for the first time introduce a benchmark for label error detection methods on
object detection datasets as well as a label error detection method and a
number of baselines. We simulate four different types of randomly introduced
label errors on train and test sets of well-labeled object detection datasets.
For our label error detection method we assume a two-stage object detector to
be given and consider the sum of both stages' classification and regression
losses. The losses are computed with respect to the predictions and the noisy
labels including simulated label errors, aiming at detecting the latter. We
compare our method to three baselines: a naive one without deep learning, the
object detector's score and the entropy of the classification softmax
distribution. We outperform all baselines and demonstrate that among the
considered methods, ours is the only one that detects label errors of all four
types efficiently. Furthermore, we detect real label errors a) on commonly used
test datasets in object detection and b) on a proprietary dataset. In both
cases we achieve low false positives rates, i.e., when considering 200
proposals from our method, we detect label errors with a precision for a) of up
to 71.5% and for b) with 97%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesizing Realistic Image Restoration Training Pairs: A Diffusion
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06994v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06994v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Yang, Peiran Ren, Xuansong xie, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In supervised image restoration tasks, one key issue is how to obtain the
aligned high-quality (HQ) and low-quality (LQ) training image pairs.
Unfortunately, such HQ-LQ training pairs are hard to capture in practice, and
hard to synthesize due to the complex unknown degradation in the wild. While
several sophisticated degradation models have been manually designed to
synthesize LQ images from their HQ counterparts, the distribution gap between
the synthesized and real-world LQ images remains large. We propose a new
approach to synthesizing realistic image restoration training pairs using the
emerging denoising diffusion probabilistic model (DDPM).
  First, we train a DDPM, which could convert a noisy input into the desired LQ
image, with a large amount of collected LQ images, which define the target data
distribution. Then, for a given HQ image, we synthesize an initial LQ image by
using an off-the-shelf degradation model, and iteratively add proper Gaussian
noises to it. Finally, we denoise the noisy LQ image using the pre-trained DDPM
to obtain the final LQ image, which falls into the target distribution of
real-world LQ images. Thanks to the strong capability of DDPM in distribution
approximation, the synthesized HQ-LQ image pairs can be used to train robust
models for real-world image restoration tasks, such as blind face image
restoration and blind image super-resolution. Experiments demonstrated the
superiority of our proposed approach to existing degradation models. Code and
data will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Table Structure Recognition with Visual-Alignment Sequential
  Coordinate Modeling <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongshuai Huang, Ning Lu, Dapeng Chen, Yibo Li, Zecheng Xie, Shenggao Zhu, Liangcai Gao, Wei Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Table structure recognition aims to extract the logical and physical
structure of unstructured table images into a machine-readable format. The
latest end-to-end image-to-text approaches simultaneously predict the two
structures by two decoders, where the prediction of the physical structure (the
bounding boxes of the cells) is based on the representation of the logical
structure. However, the previous methods struggle with imprecise bounding boxes
as the logical representation lacks local visual information. To address this
issue, we propose an end-to-end sequential modeling framework for table
structure recognition called VAST. It contains a novel coordinate sequence
decoder triggered by the representation of the non-empty cell from the logical
structure decoder. In the coordinate sequence decoder, we model the bounding
box coordinates as a language sequence, where the left, top, right and bottom
coordinates are decoded sequentially to leverage the inter-coordinate
dependency. Furthermore, we propose an auxiliary visual-alignment loss to
enforce the logical representation of the non-empty cells to contain more local
visual details, which helps produce better cell bounding boxes. Extensive
experiments demonstrate that our proposed method can achieve state-of-the-art
results in both logical and physical structure recognition. The ablation study
also validates that the proposed coordinate sequence decoder and the
visual-alignment loss are the keys to the success of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guiding the Guidance: A Comparative Analysis of User Guidance Signals
  for Interactive Segmentation of Volumetric Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zdravko Marinov, Rainer Stiefelhagen, Jens Kleesiek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive segmentation reduces the annotation time of medical images and
allows annotators to iteratively refine labels with corrective interactions,
such as clicks. While existing interactive models transform clicks into user
guidance signals, which are combined with images to form (image, guidance)
pairs, the question of how to best represent the guidance has not been fully
explored. To address this, we conduct a comparative study of existing guidance
signals by training interactive models with different signals and parameter
settings to identify crucial parameters for the model's design. Based on our
findings, we design a guidance signal that retains the benefits of other
signals while addressing their limitations. We propose an adaptive Gaussian
heatmaps guidance signal that utilizes the geodesic distance transform to
dynamically adapt the radius of each heatmap when encoding clicks. We conduct
our study on the MSD Spleen and the AutoPET datasets to explore the
segmentation of both anatomy (spleen) and pathology (tumor lesions). Our
results show that choosing the guidance signal is crucial for interactive
segmentation as we improve the performance by 14% Dice with our adaptive
heatmaps on the challenging AutoPET dataset when compared to non-interactive
models. This brings interactive models one step closer to deployment on
clinical workflows. We will make our code publically available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages; 2 figures; 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Twin Contrastive Learning with Noisy Labels <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhizhong Huang, Junping Zhang, Hongming Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning from noisy data is a challenging task that significantly degenerates
the model performance. In this paper, we present TCL, a novel twin contrastive
learning model to learn robust representations and handle noisy labels for
classification. Specifically, we construct a Gaussian mixture model (GMM) over
the representations by injecting the supervised model predictions into GMM to
link label-free latent variables in GMM with label-noisy annotations. Then, TCL
detects the examples with wrong labels as the out-of-distribution examples by
another two-component GMM, taking into account the data distribution. We
further propose a cross-supervision with an entropy regularization loss that
bootstraps the true targets from model predictions to handle the noisy labels.
As a result, TCL can learn discriminative representations aligned with
estimated labels through mixup and contrastive learning. Extensive experimental
results on several standard benchmarks and real-world datasets demonstrate the
superior performance of TCL. In particular, TCL achieves 7.5\% improvements on
CIFAR-10 with 90\% noisy label -- an extremely noisy scenario. The source code
is available at \url{https://github.com/Hzzone/TCL}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Super-Resolution Information Enhancement For Crowd Counting <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Xie, Wei Xu, Dingkang Liang, Zhanyu Ma, Kongming Liang, Weidong Liu, Rui Wang, Ling Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crowd counting is a challenging task due to the heavy occlusions, scales, and
density variations. Existing methods handle these challenges effectively while
ignoring low-resolution (LR) circumstances. The LR circumstances weaken the
counting performance deeply for two crucial reasons: 1) limited detail
information; 2) overlapping head regions accumulate in density maps and result
in extreme ground-truth values. An intuitive solution is to employ
super-resolution (SR) pre-processes for the input LR images. However, it
complicates the inference steps and thus limits application potentials when
requiring real-time. We propose a more elegant method termed Multi-Scale
Super-Resolution Module (MSSRM). It guides the network to estimate the lost de
tails and enhances the detailed information in the feature space. Noteworthy
that the MSSRM is plug-in plug-out and deals with the LR problems with no
inference cost. As the proposed method requires SR labels, we further propose a
Super-Resolution Crowd Counting dataset (SR-Crowd). Extensive experiments on
three datasets demonstrate the superiority of our method. The code will be
available at https://github.com/PRIS-CV/MSSRM.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023. The code will be available at
  https://github.com/PRIS-CV/MSSRM.git</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pixel-wise Gradient Uncertainty for Convolutional Neural Networks
  applied to Out-of-Distribution Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kira Maag, Tobias Riedlinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, deep neural networks have defined the state-of-the-art in
semantic segmentation where their predictions are constrained to a predefined
set of semantic classes. They are to be deployed in applications such as
automated driving, although their categorically confined expressive power runs
contrary to such open world scenarios. Thus, the detection and segmentation of
objects from outside their predefined semantic space, i.e., out-of-distribution
(OoD) objects, is of highest interest. Since uncertainty estimation methods
like softmax entropy or Bayesian models are sensitive to erroneous predictions,
these methods are a natural baseline for OoD detection. Here, we present a
method for obtaining uncertainty scores from pixel-wise loss gradients which
can be computed efficiently during inference. Our approach is simple to
implement for a large class of models, does not require any additional training
or auxiliary data and can be readily used on pre-trained segmentation models.
Our experiments show the ability of our method to identify wrong pixel
classifications and to estimate prediction quality. In particular, we observe
superior performance in terms of OoD segmentation to comparable baselines on
the SegmentMeIfYouCan benchmark, clearly outperforming methods which are
similarly flexible to implement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeRFLiX: High-Quality Neural View Synthesis by Learning a
  Degradation-Driven Inter-viewpoint MiXer <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Zhou, Wenbo Li, Yi Wang, Tao Hu, Nianjuan Jiang, Xiaoguang Han, Jiangbo Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural radiance fields (NeRF) show great success in novel view synthesis.
However, in real-world scenes, recovering high-quality details from the source
images is still challenging for the existing NeRF-based approaches, due to the
potential imperfect calibration information and scene representation
inaccuracy. Even with high-quality training frames, the synthetic novel views
produced by NeRF models still suffer from notable rendering artifacts, such as
noise, blur, etc. Towards to improve the synthesis quality of NeRF-based
approaches, we propose NeRFLiX, a general NeRF-agnostic restorer paradigm by
learning a degradation-driven inter-viewpoint mixer. Specially, we design a
NeRF-style degradation modeling approach and construct large-scale training
data, enabling the possibility of effectively removing NeRF-native rendering
artifacts for existing deep neural networks. Moreover, beyond the degradation
removal, we propose an inter-viewpoint aggregation framework that is able to
fuse highly related high-quality training images, pushing the performance of
cutting-edge NeRF models to entirely new levels and producing highly
photo-realistic synthetic views.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023; Project Page: see
  https://redrock303.github.io/nerflix/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViM: Vision Middleware for Unified Downstream Transferring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06911v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06911v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Feng, Biao Gong, Jianwen Jiang, Yiliang Lv, Yujun Shen, Deli Zhao, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models are pre-trained on massive data and transferred to
downstream tasks via fine-tuning. This work presents Vision Middleware (ViM), a
new learning paradigm that targets unified transferring from a single
foundation model to a variety of downstream tasks. ViM consists of a zoo of
lightweight plug-in modules, each of which is independently learned on a
midstream dataset with a shared frozen backbone. Downstream tasks can then
benefit from an adequate aggregation of the module zoo thanks to the rich
knowledge inherited from midstream tasks. There are three major advantages of
such a design. From the efficiency aspect, the upstream backbone can be trained
only once and reused for all downstream tasks without tuning. From the
scalability aspect, we can easily append additional modules to ViM with no
influence on existing modules. From the performance aspect, ViM can include as
many midstream tasks as possible, narrowing the task gap between upstream and
downstream. Considering these benefits, we believe that ViM, which the
community could maintain and develop together, would serve as a powerful tool
to assist foundation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CrossFormer++: A Versatile Vision <span class="highlight-title">Transformer</span> Hinging on Cross-scale
  Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxiao Wang, Wei Chen, Qibo Qiu, Long Chen, Boxi Wu, Binbin Lin, Xiaofei He, Wei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While features of different scales are perceptually important to visual
inputs, existing vision transformers do not yet take advantage of them
explicitly. To this end, we first propose a cross-scale vision transformer,
CrossFormer. It introduces a cross-scale embedding layer (CEL) and a long-short
distance attention (LSDA). On the one hand, CEL blends each token with multiple
patches of different scales, providing the self-attention module itself with
cross-scale features. On the other hand, LSDA splits the self-attention module
into a short-distance one and a long-distance counterpart, which not only
reduces the computational burden but also keeps both small-scale and
large-scale features in the tokens. Moreover, through experiments on
CrossFormer, we observe another two issues that affect vision transformers'
performance, i.e. the enlarging self-attention maps and amplitude explosion.
Thus, we further propose a progressive group size (PGS) paradigm and an
amplitude cooling layer (ACL) to alleviate the two issues, respectively. The
CrossFormer incorporating with PGS and ACL is called CrossFormer++. Extensive
experiments show that CrossFormer++ outperforms the other vision transformers
on image classification, object detection, instance segmentation, and semantic
segmentation tasks. The code will be available at:
https://github.com/cheerss/CrossFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ST360IQ: No-Reference Omnidirectional Image Quality Assessment with
  Spherical Vision <span class="highlight-title">Transformer</span>s <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nafiseh Jabbari Tofighi, Mohamed Hedi Elfkir, Nevrez Imamoglu, Cagri Ozcinar, Erkut Erdem, Aykut Erdem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Omnidirectional images, aka 360 images, can deliver immersive and interactive
visual experiences. As their popularity has increased dramatically in recent
years, evaluating the quality of 360 images has become a problem of interest
since it provides insights for capturing, transmitting, and consuming this new
media. However, directly adapting quality assessment methods proposed for
standard natural images for omnidirectional data poses certain challenges.
These models need to deal with very high-resolution data and implicit
distortions due to the spherical form of the images. In this study, we present
a method for no-reference 360 image quality assessment. Our proposed ST360IQ
model extracts tangent viewports from the salient parts of the input
omnidirectional image and employs a vision-transformers based module processing
saliency selective patches/tokens that estimates a quality score from each
viewport. Then, it aggregates these scores to give a final quality score. Our
experiments on two benchmark datasets, namely OIQA and CVIQ datasets,
demonstrate that as compared to the state-of-the-art, our approach predicts the
quality of an omnidirectional image correlated with the human-perceived image
quality. The code has been available on
https://github.com/Nafiseh-Tofighi/ST360IQ
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DEHRFormer: Real-time <span class="highlight-title">Transformer</span> for Depth Estimation and Haze Removal
  from Varicolored Haze Scenes <span class="chip">ICASSP'2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sixiang Chen, Tian Ye, Jun Shi, Yun Liu, JingXia Jiang, Erkang Chen, Peng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Varicolored haze caused by chromatic casts poses haze removal and depth
estimation challenges. Recent learning-based depth estimation methods are
mainly targeted at dehazing first and estimating depth subsequently from
haze-free scenes. This way, the inner connections between colored haze and
scene depth are lost. In this paper, we propose a real-time transformer for
simultaneous single image Depth Estimation and Haze Removal (DEHRFormer).
DEHRFormer consists of a single encoder and two task-specific decoders. The
transformer decoders with learnable queries are designed to decode coupling
features from the task-agnostic encoder and project them into clean image and
depth map, respectively. In addition, we introduce a novel learning paradigm
that utilizes contrastive learning and domain consistency learning to tackle
weak-generalization problem for real-world dehazing, while predicting the same
depth map from the same scene with varicolored haze. Experiments demonstrate
that DEHRFormer achieves significant performance improvement across diverse
varicolored haze scenes over previous depth estimation networks and dehazing
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP'2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextually-rich human affect perception using multimodal scene
  information <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Digbalay Bose, Rajat Hebbar, Krishna Somandepalli, Shrikanth Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The process of human affect understanding involves the ability to infer
person specific emotional states from various sources including images, speech,
and language. Affect perception from images has predominantly focused on
expressions extracted from salient face crops. However, emotions perceived by
humans rely on multiple contextual cues including social settings, foreground
interactions, and ambient visual scenes. In this work, we leverage pretrained
vision-language (VLN) models to extract descriptions of foreground context from
images. Further, we propose a multimodal context fusion (MCF) module to combine
foreground cues with the visual scene and person-based contextual information
for emotion prediction. We show the effectiveness of our proposed modular
design on two datasets associated with natural scenes and TV shows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP), 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DR2: Diffusion-based Robust Degradation Remover for Blind Face
  Restoration <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixin Wang, Xiaoyun Zhang, Ziying Zhang, Huangjie Zheng, Mingyuan Zhou, Ya Zhang, Yanfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blind face restoration usually synthesizes degraded low-quality data with a
pre-defined degradation model for training, while more complex cases could
happen in the real world. This gap between the assumed and actual degradation
hurts the restoration performance where artifacts are often observed in the
output. However, it is expensive and infeasible to include every type of
degradation to cover real-world cases in the training data. To tackle this
robustness issue, we propose Diffusion-based Robust Degradation Remover (DR2)
to first transform the degraded image to a coarse but degradation-invariant
prediction, then employ an enhancement module to restore the coarse prediction
to a high-quality image. By leveraging a well-performing denoising diffusion
probabilistic model, our DR2 diffuses input images to a noisy status where
various types of degradation give way to Gaussian noise, and then captures
semantic information through iterative denoising steps. As a result, DR2 is
robust against common degradation (e.g. blur, resize, noise and compression)
and compatible with different designs of enhancement modules. Experiments in
various settings show that our framework outperforms state-of-the-art methods
on heavily degraded synthetic and real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCPNet: Semantic Scene Completion on Point Cloud <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Xia, Youquan Liu, Xin Li, Xinge Zhu, Yuexin Ma, Yikang Li, Yuenan Hou, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training deep models for semantic scene completion (SSC) is challenging due
to the sparse and incomplete input, a large quantity of objects of diverse
scales as well as the inherent label noise for moving objects. To address the
above-mentioned problems, we propose the following three solutions: 1)
Redesigning the completion sub-network. We design a novel completion
sub-network, which consists of several Multi-Path Blocks (MPBs) to aggregate
multi-scale features and is free from the lossy downsampling operations. 2)
Distilling rich knowledge from the multi-frame model. We design a novel
knowledge distillation objective, dubbed Dense-to-Sparse Knowledge Distillation
(DSKD). It transfers the dense, relation-based semantic knowledge from the
multi-frame teacher to the single-frame student, significantly improving the
representation learning of the single-frame model. 3) Completion label
rectification. We propose a simple yet effective label rectification strategy,
which uses off-the-shelf panoptic segmentation labels to remove the traces of
dynamic objects in completion labels, greatly improving the performance of deep
models especially for those moving objects. Extensive experiments are conducted
in two public SSC benchmarks, i.e., SemanticKITTI and SemanticPOSS. Our SCPNet
ranks 1st on SemanticKITTI semantic scene completion challenge and surpasses
the competitive S3CNet by 7.2 mIoU. SCPNet also outperforms previous completion
algorithms on the SemanticPOSS dataset. Besides, our method also achieves
competitive results on SemanticKITTI semantic segmentation tasks, showing that
knowledge learned in the scene completion is beneficial to the segmentation
task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OverlapNetVLAD: A Coarse-to-Fine Framework for LiDAR-based Place
  Recognition <span class="chip">IROS2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chencan Fu, Lin Li, Linpeng Peng, Yukai Ma, Xiangrui Zhao, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Place recognition is a challenging yet crucial task in robotics. Existing 3D
LiDAR place recognition methods suffer from limited feature representation
capability and long search times. To address these challenges, we propose a
novel coarse-to-fine framework for 3D LiDAR place recognition that combines
Birds' Eye View (BEV) feature extraction, coarse-grained matching, and
fine-grained verification. In the coarse stage, our framework leverages the
rich contextual information contained in BEV features to produce global
descriptors. Then the top-\textit{K} most similar candidates are identified via
descriptor matching, which is fast but coarse-grained. In the fine stage, our
overlap estimation network reuses the corresponding BEV features to predict the
overlap region, enabling meticulous and precise matching. Experimental results
on the KITTI odometry benchmark demonstrate that our framework achieves leading
performance compared to state-of-the-art methods. Our code is available at:
\url{https://github.com/fcchit/OverlapNetVLAD}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uni3D: A Unified Baseline for Multi-<span class="highlight-title">dataset</span> 3D Object Detection <span class="chip">CVPR-2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Zhang, Jiakang Yuan, Botian Shi, Tao Chen, Yikang Li, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current 3D object detection models follow a single dataset-specific training
and testing paradigm, which often faces a serious detection accuracy drop when
they are directly deployed in another dataset. In this paper, we study the task
of training a unified 3D detector from multiple datasets. We observe that this
appears to be a challenging task, which is mainly due to that these datasets
present substantial data-level differences and taxonomy-level variations caused
by different LiDAR types and data acquisition standards. Inspired by such
observation, we present a Uni3D which leverages a simple data-level correction
operation and a designed semantic-level coupling-and-recoupling module to
alleviate the unavoidable data-level and taxonomy-level differences,
respectively. Our method is simple and easily combined with many 3D object
detection baselines such as PV-RCNN and Voxel-RCNN, enabling them to
effectively learn from multiple off-the-shelf 3D datasets to obtain more
discriminative and generalizable representations. Experiments are conducted on
many dataset consolidation settings including Waymo-nuScenes, nuScenes-KITTI,
Waymo-KITTI, and Waymo-nuScenes-KITTI consolidations. Their results demonstrate
that Uni3D exceeds a series of individual detectors trained on a single
dataset, with a 1.04x parameter increase over a selected baseline detector. We
expect this work will inspire the research of 3D generalization since it will
push the limits of perceptual performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR-2023, and our code is available at
  https://github.com/PJLab-ADG/3DTrans</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spacecraft Anomaly Detection with Attention Temporal Convolution Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Liu, Ling Tian, Zhao Kang, Tianqi Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spacecraft faces various situations when carrying out exploration missions in
complex space, thus monitoring the anomaly status of spacecraft is crucial to
the development of \textcolor{blue}{the} aerospace industry. The time series
telemetry data generated by on-orbit spacecraft \textcolor{blue}{contains}
important information about the status of spacecraft. However, traditional
domain knowledge-based spacecraft anomaly detection methods are not effective
due to high dimensionality and complex correlation among variables. In this
work, we propose an anomaly detection framework for spacecraft multivariate
time-series data based on temporal convolution networks (TCNs). First, we
employ dynamic graph attention to model the complex correlation among variables
and time series. Second, temporal convolution networks with parallel processing
ability are used to extract multidimensional \textcolor{blue}{features} for
\textcolor{blue}{the} downstream prediction task. Finally, many potential
anomalies are detected by the best threshold. Experiments on real NASA SMAP/MSL
spacecraft datasets show the superiority of our proposed model with respect to
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Open Space Expansion for Open-Set Model Attribution <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyun Yang, Danding Wang, Fan Tang, Xinying Zhao, Juan Cao, Sheng Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable progress in generative technology, the Janus-faced
issues of intellectual property protection and malicious content supervision
have arisen. Efforts have been paid to manage synthetic images by attributing
them to a set of potential source models. However, the closed-set
classification setting limits the application in real-world scenarios for
handling contents generated by arbitrary models. In this study, we focus on a
challenging task, namely Open-Set Model Attribution (OSMA), to simultaneously
attribute images to known models and identify those from unknown ones. Compared
to existing open-set recognition (OSR) tasks focusing on semantic novelty, OSMA
is more challenging as the distinction between images from known and unknown
models may only lie in visually imperceptible traces. To this end, we propose a
Progressive Open Space Expansion (POSE) solution, which simulates open-set
samples that maintain the same semantics as closed-set samples but embedded
with different imperceptible traces. Guided by a diversity constraint, the open
space is simulated progressively by a set of lightweight augmentation models.
We consider three real-world scenarios and construct an OSMA benchmark dataset,
including unknown models trained with different random seeds, architectures,
and datasets from known ones. Extensive experiments on the dataset demonstrate
POSE is superior to both existing model attribution methods and off-the-shelf
OSR methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting model self-interpretability in a decision-theoretic way for
  binary medical image classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06876v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06876v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourya Sengupta, Mark A. Anastasio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpretability is highly desired for deep neural network-based classifiers,
especially when addressing high-stake decisions in medical imaging. Commonly
used post-hoc interpretability methods may not be always useful because
different such methods can produce several plausible but different
interpretations of a given model, leading to confusion about which one to
choose. {In this work, an {inherently} interpretable encoder-decoder model
coupled with a single-layer fully connected network with unity weights is
proposed for binary medical image classification problems. The feature
extraction component of a trained black-box network for the same task is
employed as the pre-trained encoder of the interpretable model. The model is
trained to estimate the decision statistic of the given trained black-box deep
binary classifier to maintain a similar accuracy.} The decoder output
represents a transformed version of the to-be-classified image that, when
processed by the fixed fully connected layer, produces the same decision
statistic value as the original classifier. This is accomplished by minimizing
the mean squared error between the decision statistic values of the black-box
model and encoder-decoder based model during training. The decoder output image
is referred to as an equivalency map. Because the single-layer network is fully
interpretable, the equivalency map provides a visualization of the transformed
image features that contribute to the decision statistic value and, moreover,
permits quantification of their relative contributions. Unlike the traditional
post-hoc interpretability methods, the proposed method is inherently
interpretable, quantitative, and fundamentally based on decision theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interventional Bag Multi-Instance Learning On Whole-Slide Pathological
  Images <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiancheng Lin, Zhimiao Yu, Hongyu Hu, Yi Xu, Chang Wen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-instance learning (MIL) is an effective paradigm for whole-slide
pathological images (WSIs) classification to handle the gigapixel resolution
and slide-level label. Prevailing MIL methods primarily focus on improving the
feature extractor and aggregator. However, one deficiency of these methods is
that the bag contextual prior may trick the model into capturing spurious
correlations between bags and labels. This deficiency is a confounder that
limits the performance of existing MIL methods. In this paper, we propose a
novel scheme, Interventional Bag Multi-Instance Learning (IBMIL), to achieve
deconfounded bag-level prediction. Unlike traditional likelihood-based
strategies, the proposed scheme is based on the backdoor adjustment to achieve
the interventional training, thus is capable of suppressing the bias caused by
the bag contextual prior. Note that the principle of IBMIL is orthogonal to
existing bag MIL methods. Therefore, IBMIL is able to bring consistent
performance boosting to existing schemes, achieving new state-of-the-art
performance. Code is available at https://github.com/HHHedo/IBMIL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023; Code at https://github.com/HHHedo/IBMIL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FusionLoc: Camera-2D LiDAR Fusion Using Multi-Head Self-Attention for
  End-to-End Serving Robot Relocalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieun Lee, Hakjun Lee, Jiyong Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the recent development of autonomous driving technology, as the pursuit
of efficiency for repetitive tasks and the value of non-face-to-face services
increase, mobile service robots such as delivery robots and serving robots
attract attention, and their demands are increasing day by day. However, when
something goes wrong, most commercial serving robots need to return to their
starting position and orientation to operate normally again. In this paper, we
focus on end-to-end relocalization of serving robots to address the problem. It
is to predict robot pose directly from only the onboard sensor data using
neural networks. In particular, we propose a deep neural network architecture
for the relocalization based on camera-2D LiDAR sensor fusion. We call the
proposed method FusionLoc. In the proposed method, the multi-head
self-attention complements different types of information captured by the two
sensors. Our experiments on a dataset collected by a commercial serving robot
demonstrate that FusionLoc can provide better performances than previous
relocalization methods taking only a single image or a 2D LiDAR point cloud as
well as a straightforward fusion method concatenating their features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Three Guidelines You Should Know for Universally Slimmable
  <span class="highlight-title">Self-Supervised</span> Learning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun-Hao Cao, Peiqin Sun, Shuchang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose universally slimmable self-supervised learning (dubbed as US3L) to
achieve better accuracy-efficiency trade-offs for deploying self-supervised
models across different devices. We observe that direct adaptation of
self-supervised learning (SSL) to universally slimmable networks misbehaves as
the training process frequently collapses. We then discover that temporal
consistent guidance is the key to the success of SSL for universally slimmable
networks, and we propose three guidelines for the loss design to ensure this
temporal consistency from a unified gradient perspective. Moreover, we propose
dynamic sampling and group regularization strategies to simultaneously improve
training efficiency and accuracy. Our US3L method has been empirically
validated on both convolutional neural networks and vision transformers. With
only once training and one copy of weights, our method outperforms various
state-of-the-art methods (individually trained or not) on benchmarks including
recognition, object detection and instance segmentation. Our code is available
at https://github.com/megvii-research/US3L-CVPR2023.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Data-Free Quantization <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biao Qian, Yang Wang, Richang Hong, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-free quantization (DFQ) recovers the performance of quantized network
(Q) without accessing the real data, but generates the fake sample via a
generator (G) by learning from full-precision network (P) instead. However,
such sample generation process is totally independent of Q, overlooking the
adaptability of the knowledge from generated samples, i.e., informative or not
to the learning process of Q, resulting into the overflow of generalization
error. Building on this, several critical questions -- how to measure the
sample adaptability to Q under varied bit-width scenarios? how to generate the
samples with large adaptability to improve Q's generalization? whether the
largest adaptability is the best? To answer the above questions, in this paper,
we propose an Adaptive Data-Free Quantization (AdaDFQ) method, which
reformulates DFQ as a zero-sum game upon the sample adaptability between two
players -- a generator and a quantized network. Following this viewpoint, we
further define the disagreement and agreement samples to form two boundaries,
where the margin is optimized to address the over-and-under fitting issues, so
as to generate the samples with the desirable adaptability to Q. Our AdaDFQ
reveals: 1) the largest adaptability is NOT the best for sample generation to
benefit Q's generalization; 2) the knowledge of the generated sample should not
be informative to Q only, but also related to the category and distribution
information of the training data for P. The theoretical and empirical analysis
validate the advantages of AdaDFQ over the state-of-the-arts. Our code is
available at https: github.com/hfutqian/AdaDFQ.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, accepted by CVPR 2023. arXiv admin note:
  substantial text overlap with arXiv:2302.09572</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning-based Eye-Tracking Analysis for Diagnosis of Alzheimer's
  Disease Using 3D Comprehensive Visual Stimuli 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangyu Zuo, Peiguang Jing, Jinglin Sun,  Jizhong,  Duan, Yong Ji, Yu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's Disease (AD) causes a continuous decline in memory, thinking, and
judgment. Traditional diagnoses are usually based on clinical experience, which
is limited by some realistic factors. In this paper, we focus on exploiting
deep learning techniques to diagnose AD based on eye-tracking behaviors. Visual
attention, as typical eye-tracking behavior, is of great clinical value to
detect cognitive abnormalities in AD patients. To better analyze the
differences in visual attention between AD patients and normals, we first
conduct a 3D comprehensive visual task on a non-invasive eye-tracking system to
collect visual attention heatmaps. We then propose a multi-layered comparison
convolution neural network (MC-CNN) to distinguish the visual attention
differences between AD patients and normals. In MC-CNN, the multi-layered
representations of heatmaps are obtained by hierarchical convolution to better
encode eye-movement behaviors, which are further integrated into a distance
vector to benefit the comprehensive visual task. Extensive experimental results
on the collected dataset demonstrate that MC-CNN achieves consistent validity
in classifying AD patients and normals with eye-tracking data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OTOV2: Automatic, Generic, User-Friendly <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Chen, Luming Liang, Tianyu Ding, Zhihui Zhu, Ilya Zharkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The existing model compression methods via structured pruning typically
require complicated multi-stage procedures. Each individual stage necessitates
numerous engineering efforts and domain-knowledge from the end-users which
prevent their wider applications onto broader scenarios. We propose the second
generation of Only-Train-Once (OTOv2), which first automatically trains and
compresses a general DNN only once from scratch to produce a more compact model
with competitive performance without fine-tuning. OTOv2 is automatic and
pluggable into various deep learning applications, and requires almost minimal
engineering efforts from the users. Methodologically, OTOv2 proposes two major
improvements: (i) Autonomy: automatically exploits the dependency of general
DNNs, partitions the trainable variables into Zero-Invariant Groups (ZIGs), and
constructs the compressed model; and (ii) Dual Half-Space Projected Gradient
(DHSPG): a novel optimizer to more reliably solve structured-sparsity problems.
Numerically, we demonstrate the generality and autonomy of OTOv2 on a variety
of model architectures such as VGG, ResNet, CARN, ConvNeXt, DenseNet and
StackedUnets, the majority of which cannot be handled by other methods without
extensive handcrafting efforts. Together with benchmark datasets including
CIFAR10/100, DIV2K, Fashion-MNIST, SVNH and ImageNet, its effectiveness is
validated by performing competitively or even better than the
state-of-the-arts. The source code is available at
https://github.com/tianyic/only_train_once.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published on ICLR 2023. Remark here that a few images of dependency
  graphs can not be included in arXiv due to exceeding size limit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ View Adaptive Light Field Deblurring Networks with Depth Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeqi Shen, Shuo Zhang, Zhuhao Zhang, Qihua Chen, Xueyao Dong, Youfang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Light Field (LF) deblurring task is a challenging problem as the blur
images are caused by different reasons like the camera shake and the object
motion. The single image deblurring method is a possible way to solve this
problem. However, since it deals with each view independently and cannot
effectively utilize and maintain the LF structure, the restoration effect is
usually not ideal. Besides, the LF blur is more complex because the degree is
affected by the views and depth. Therefore, we carefully designed a novel LF
deblurring network based on the LF blur characteristics. On one hand, since the
blur degree varies a lot in different views, we design a novel view adaptive
spatial convolution to deblur blurred LFs, which calculates the exclusive
convolution kernel for each view. On the other hand, because the blur degree
also varies with the depth of the object, a depth perception view attention is
designed to deblur different depth areas by selectively integrating information
from different views. Besides, we introduce an angular position embedding to
maintain the LF structure better, which ensures the model correctly restores
the view information. Quantitative and qualitative experimental results on
synthetic and real images show that the deblurring effect of our method is
better than other state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Distortion Invariant Representation for Image Restoration from
  A Causality Perspective <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Li, Bingchen Li, Xin Jin, Cuiling Lan, Zhibo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, we have witnessed the great advancement of Deep neural
networks (DNNs) in image restoration. However, a critical limitation is that
they cannot generalize well to real-world degradations with different degrees
or types. In this paper, we are the first to propose a novel training strategy
for image restoration from the causality perspective, to improve the
generalization ability of DNNs for unknown degradations. Our method, termed
Distortion Invariant representation Learning (DIL), treats each distortion type
and degree as one specific confounder, and learns the distortion-invariant
representation by eliminating the harmful confounding effect of each
degradation. We derive our DIL with the back-door criterion in causality by
modeling the interventions of different distortions from the optimization
perspective. Particularly, we introduce counterfactual distortion augmentation
to simulate the virtual distortion types and degrees as the confounders. Then,
we instantiate the intervention of each distortion with a virtual model
updating based on corresponding distorted images, and eliminate them from the
meta-learning perspective. Extensive experiments demonstrate the effectiveness
of our DIL on the generalization capability for unseen distortion types and
degrees. Our code will be available at
https://github.com/lixinustc/Casual-IRDIL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An automated pipeline to create an atlas of in situ hybridization gene
  expression data in the adult marmoset brain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06857v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06857v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charissa Poon, Muhammad Febrian Rachmadi, Michal Byra, Matthias Schlachter, Binbin Xu, Tomomi Shimogori, Henrik Skibbe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the first automated pipeline to create an atlas of in situ
hybridization gene expression in the adult marmoset brain in the same
stereotaxic space. The pipeline consists of segmentation of gene expression
from microscopy images and registration of images to a standard space.
Automation of this pipeline is necessary to analyze the large volume of data in
the genome-wide whole-brain dataset, and to process images that have varying
intensity profiles and expression patterns with minimal human bias. To reduce
the number of labelled images required for training, we develop a
semi-supervised segmentation model. We further develop an iterative algorithm
to register images to a standard space, enabling comparative analysis between
genes and concurrent visualization with other datasets, thereby facilitating a
more holistic understanding of primate brain structure and function.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Neural Network for Multi-Task Learning Searching across Diverse
  Network Topologies <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonhyeok Choi, Sunghoon Im
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a new MTL framework that searches for structures
optimized for multiple tasks with diverse graph topologies and shares features
among tasks. We design a restricted DAG-based central network with
read-in/read-out layers to build topologically diverse task-adaptive structures
while limiting search space and time. We search for a single optimized network
that serves as multiple task adaptive sub-networks using our three-stage
training process. To make the network compact and discretized, we propose a
flow-based reduction algorithm and a squeeze loss used in the training process.
We evaluate our optimized network on various public MTL datasets and show ours
achieves state-of-the-art performance. An extensive ablation study
experimentally validates the effectiveness of the sub-module and schemes in our
framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2023, 13 pages, 10 encapsulated postscript figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Contrastive Language-Image <span class="highlight-title">Pretrain</span>ing against Adversarial
  Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhan Yang, Baharan Mirzasoleiman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive vision-language representation learning has achieved
state-of-the-art performance for zero-shot classification, by learning from
millions of image-caption pairs crawled from the internet. However, the massive
data that powers large multimodal models such as CLIP, makes them extremely
vulnerable to various types of adversarial attacks, including targeted and
backdoor data poisoning attacks. Despite this vulnerability, robust contrastive
vision-language pretraining against adversarial attacks has remained
unaddressed. In this work, we propose RoCLIP, the first effective method for
robust pretraining {and fine-tuning} multimodal vision-language models. RoCLIP
effectively breaks the association between poisoned image-caption pairs by
considering a pool of random examples, and (1) matching every image with the
text that is most similar to its caption in the pool, and (2) matching every
caption with the image that is most similar to its image in the pool. Our
extensive experiments show that our method renders state-of-the-art targeted
data poisoning and backdoor attacks ineffective during pre-training or
fine-tuning of CLIP. In particular, RoCLIP decreases the poison and backdoor
attack success rates down to 0\% during pre-training and 1\%-4\% during
fine-tuning, and effectively improves the model's performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-Shot Segmentation of Novel White Matter Tracts via Extensive Data
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wan Liu, Qi Lu, ZhiZheng Zhuo, Yaou Liu, Chuyang Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning based methods have achieved state-of-the-art performance for
automated white matter (WM) tract segmentation. In these methods, the
segmentation model needs to be trained with a large number of manually
annotated scans, which can be accumulated throughout time. When novel WM
tracts, i.e., tracts not included in the existing annotated WM tracts, are to
be segmented, additional annotations of these novel WM tracts need to be
collected. Since tract annotation is time-consuming and costly, it is desirable
to make only a few annotations of novel WM tracts for training the segmentation
model, and previous work has addressed this problem by transferring the
knowledge learned for segmenting existing WM tracts to the segmentation of
novel WM tracts. However, accurate segmentation of novel WM tracts can still be
challenging in the one-shot setting, where only one scan is annotated for the
novel WM tracts. In this work, we explore the problem of one-shot segmentation
of novel WM tracts. Since in the one-shot setting the annotated training data
is extremely scarce, based on the existing knowledge transfer framework, we
propose to further perform extensive data augmentation for the single annotated
scan, where synthetic annotated training data is produced. We have designed
several different strategies that mask out regions in the single annotated scan
for data augmentation. Our method was evaluated on public and in-house
datasets. The experimental results show that our method improves the accuracy
of one-shot segmentation of novel WM tracts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scene Graph Generation from Hierarchical Relationship Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Jiang, Camillo J. Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes a novel approach to deducing relationships between
objects in a visual scene. It explicitly exploits an informative hierarchical
structure that can be imposed to divide the object and relationship categories
into disjoint super-categories. Specifically, our proposed scheme implements a
Bayes prediction head to jointly predict the super-category or type of
relationship between the two objects, along with the detailed relationship
within that super-category. This design reduces the impact of class imbalance
problems. We present experimental results on the Visual Genome and OpenImage V6
datasets showing that this factorized approach allows a relatively simple model
to achieve competitive performance, especially on predicate classification and
zero-shot tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DDFM: Denoising Diffusion Model for Multi-Modality Image Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixiang Zhao, Haowen Bai, Yuanzhi Zhu, Jiangshe Zhang, Shuang Xu, Yulun Zhang, Kai Zhang, Deyu Meng, Radu Timofte, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modality image fusion aims to combine different modalities to produce
fused images that retain the complementary features of each modality, such as
functional highlights and texture details. To leverage strong generative priors
and address challenges such as unstable training and lack of interpretability
for GAN-based generative methods, we propose a novel fusion algorithm based on
the denoising diffusion probabilistic model (DDPM). The fusion task is
formulated as a conditional generation problem under the DDPM sampling
framework, which is further divided into an unconditional generation subproblem
and a maximum likelihood subproblem. The latter is modeled in a hierarchical
Bayesian manner with latent variables and inferred by the
expectation-maximization algorithm. By integrating the inference solution into
the diffusion sampling iteration, our method can generate high-quality fused
images with natural image generative priors and cross-modality information from
source images. Note that all we required is an unconditional pre-trained
generative model, and no fine-tuning is needed. Our extensive experiments
indicate that our approach yields promising fusion results in infrared-visible
image fusion and medical image fusion. The code will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DarkVisionNet: Low-Light Imaging via RGB-NIR Fusion with Deep
  Inconsistency Prior <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuangping Jin, Bingbing Yu, Minhao Jing, Yi Zhou, Jiajun Liang, Renhe Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RGB-NIR fusion is a promising method for low-light imaging. However,
high-intensity noise in low-light images amplifies the effect of structure
inconsistency between RGB-NIR images, which fails existing algorithms. To
handle this, we propose a new RGB-NIR fusion algorithm called Dark Vision Net
(DVN) with two technical novelties: Deep Structure and Deep Inconsistency Prior
(DIP). The Deep Structure extracts clear structure details in deep multiscale
feature space rather than raw input space, which is more robust to noisy
inputs. Based on the deep structures from both RGB and NIR domains, we
introduce the DIP to leverage the structure inconsistency to guide the fusion
of RGB-NIR. Benefiting from this, the proposed DVN obtains high-quality
lowlight images without the visual artifacts. We also propose a new dataset
called Dark Vision Dataset (DVD), consisting of aligned RGB-NIR image pairs, as
the first public RGBNIR fusion benchmark. Quantitative and qualitative results
on the proposed benchmark show that DVN significantly outperforms other
comparison algorithms in PSNR and SSIM, especially in extremely low light
conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instate: Predicting the State of Residence From Last Name 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atul Dhingra, Gaurav Sood
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  India has twenty-two official languages. Serving such a diverse language base
is a challenge for survey statisticians, call center operators, software
developers, and other such service providers. To help provide better services
to different language communities via better localization, we introduce a new
machine learning model that predicts the language(s) that the user can speak
from their name. Using nearly 438M records spanning 33 Indian states and 1.13M
unique last names from the Indian Electoral Rolls Corpus (?), we build a
character-level transformer-based machine-learning model that predicts the
state of residence based on the last name. The model has a top-3 accuracy of
85.3% on unseen names. We map the states to languages using the Indian census
to infer languages understood by the respondent. We provide open-source
software that implements the method discussed in the paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SDF-3DGAN: A 3D Object Generative Method Based on Implicit Signed
  Distance Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lutao Jiang, Ruyi Ji, Libo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we develop a new method, termed SDF-3DGAN, for 3D object
generation and 3D-Aware image synthesis tasks, which introduce implicit Signed
Distance Function (SDF) as the 3D object representation method in the
generative field. We apply SDF for higher quality representation of 3D object
in space and design a new SDF neural renderer, which has higher efficiency and
higher accuracy. To train only on 2D images, we first generate the objects,
which are represented by SDF, from Gaussian distribution. Then we render them
to 2D images and use them to apply GAN training method together with 2D images
in the dataset. In the new rendering method, we relieve all the potential of
SDF mathematical property to alleviate computation pressure in the previous SDF
neural renderer. In specific, our new SDF neural renderer can solve the problem
of sampling ambiguity when the number of sampling point is not enough, \ie use
the less points to finish higher quality sampling task in the rendering
pipeline. And in this rendering pipeline, we can locate the surface easily.
Therefore, we apply normal loss on it to control the smoothness of generated
object surface, which can make our method enjoy the much higher generation
quality. Quantitative and qualitative experiments conducted on public
benchmarks demonstrate favorable performance against the state-of-the-art
methods in 3D object generation task and 3D-Aware image synthesis task. Our
codes will be released at https://github.com/lutao2021/SDF-3DGAN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous sign language recognition based on cross-resolution knowledge
  distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qidan Zhu, Jing Li, Fei Yuan, Quan Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of continuous sign language recognition(CSLR) research is to apply
CSLR models as a communication tool in real life, and the real-time requirement
of the models is important. In this paper, we address the model real-time
problem through cross-resolution knowledge distillation. In our study, we found
that keeping the frame-level feature scales consistent between the output of
the student network and the teacher network is better than recovering the
frame-level feature sizes for feature distillation. Based on this finding, we
propose a new frame-level feature extractor that keeps the output frame-level
features at the same scale as the output of by the teacher network. We further
combined with the TSCM+2D hybrid convolution proposed in our previous study to
form a new lightweight end-to-end CSLR network-Low resolution input
net(LRINet). It is then used to combine cross-resolution knowledge distillation
and traditional knowledge distillation methods to form a CSLR model based on
cross-resolution knowledge distillation (CRKD). The CRKD uses high-resolution
frames as input to the teacher network for training, locks the weights after
training, and then uses low-resolution frames as input to the student network
LRINet to perform knowledge distillation on frame-level features and
classification features respectively. Experiments on two large-scale continuous
sign language datasets have proved the effectiveness of CRKD. Compared with the
model with high-resolution data as input, the calculation amount, parameter
amount and inference time of the model have been significantly reduced under
the same experimental conditions, while ensuring the accuracy of the model, and
has achieved very competitive results in comparison with other advanced
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TranSG: <span class="highlight-title">Transformer</span>-Based Skeleton Graph Prototype Contrastive Learning
  with Structure-Trajectory <span class="highlight-title">Prompt</span>ed Reconstruction for Person
  Re-Identification <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haocong Rao, Chunyan Miao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person re-identification (re-ID) via 3D skeleton data is an emerging topic
with prominent advantages. Existing methods usually design skeleton descriptors
with raw body joints or perform skeleton sequence representation learning.
However, they typically cannot concurrently model different body-component
relations, and rarely explore useful semantics from fine-grained
representations of body joints. In this paper, we propose a generic
Transformer-based Skeleton Graph prototype contrastive learning (TranSG)
approach with structure-trajectory prompted reconstruction to fully capture
skeletal relations and valuable spatial-temporal semantics from skeleton graphs
for person re-ID. Specifically, we first devise the Skeleton Graph Transformer
(SGT) to simultaneously learn body and motion relations within skeleton graphs,
so as to aggregate key correlative node features into graph representations.
Then, we propose the Graph Prototype Contrastive learning (GPC) to mine the
most typical graph features (graph prototypes) of each identity, and contrast
the inherent similarity between graph representations and different prototypes
from both skeleton and sequence levels to learn discriminative graph
representations. Last, a graph Structure-Trajectory Prompted Reconstruction
(STPR) mechanism is proposed to exploit the spatial and temporal contexts of
graph nodes to prompt skeleton graph reconstruction, which facilitates
capturing more valuable patterns and graph semantics for person re-ID.
Empirical evaluations demonstrate that TranSG significantly outperforms
existing state-of-the-art methods. We further show its generality under
different graph modeling, RGB-estimated skeletons, and unsupervised scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023. Codes are available at
  https://github.com/Kali-Hac/TranSG. Supplemental material is included in the
  conference proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transformation-Invariant Network for Few-Shot Object Detection in Remote
  Sensing Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nanqing Liu, Xun Xu, Turgay Celik, Zongxin Gan, Heng-Chao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection in remote sensing images relies on a large amount of labeled
data for training. The growing new categories and class imbalance render
exhaustive annotation non-scalable. Few-shot object detection~(FSOD) tackles
this issue by meta-learning on seen base classes and then fine-tuning on novel
classes with few labeled samples. However, the object's scale and orientation
variations are particularly large in remote sensing images, thus posing
challenges to existing few-shot object detection methods. To tackle these
challenges, we first propose to integrate a feature pyramid network and use
prototype features to highlight query features to improve upon existing FSOD
methods. We refer to the modified FSOD as a Strong Baseline which is
demonstrated to perform significantly better than the original baselines. To
improve the robustness of orientation variation, we further propose a
transformation-invariant network (TINet) to allow the network to be invariant
to geometric transformations. Extensive experiments on three widely used remote
sensing object detection datasets, i.e., NWPU VHR-10.v2, DIOR, and HRRSD
demonstrated the effectiveness of the proposed method. Finally, we reproduced
multiple FSOD methods for remote sensing images to create an extensive
benchmark for follow-up works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Clustering and Cluster Contrastive Learning for Unsupervised
  Person Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi He, Mengjia Xue, Yunhao Du, Zhicheng Zhao, Fei Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised Re-ID methods aim at learning robust and discriminative features
from unlabeled data. However, existing methods often ignore the relationship
between module parameters of Re-ID framework and feature distributions, which
may lead to feature misalignment and hinder the model performance. To address
this problem, we propose a dynamic clustering and cluster contrastive learning
(DCCC) method. Specifically, we first design a dynamic clustering parameters
scheduler (DCPS) which adjust the hyper-parameter of clustering to fit the
variation of intra- and inter-class distances. Then, a dynamic cluster
contrastive learning (DyCL) method is designed to match the cluster
representation vectors' weights with the local feature association. Finally, a
label smoothing soft contrastive loss ($L_{ss}$) is built to keep the balance
between cluster contrastive learning and self-supervised learning with low
computational consumption and high computational efficiency. Experiments on
several widely used public datasets validate the effectiveness of our proposed
DCCC which outperforms previous state-of-the-art methods by achieving the best
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vessel-Promoted OCT to OCTA Image Translation by Heuristic Contextual
  Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhan Li, Dong Zhang, Xiaomeng Li, Chubin Ou, Lin An, Yanwu Xu, Kwang-Ting Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical Coherence Tomography Angiography (OCTA) has become increasingly vital
in the clinical screening of fundus diseases due to its ability to capture
accurate 3D imaging of blood vessels in a non-contact scanning manner. However,
the acquisition of OCTA images remains challenging due to the requirement of
exclusive sensors and expensive devices. In this paper, we propose a novel
framework, TransPro, that translates 3D Optical Coherence Tomography (OCT)
images into exclusive 3D OCTA images using an image translation pattern. Our
main objective is to address two issues in existing image translation
baselines, namely, the aimlessness in the translation process and
incompleteness of the translated object. The former refers to the overall
quality of the translated OCTA images being satisfactory, but the retinal
vascular quality being low. The latter refers to incomplete objects in
translated OCTA images due to the lack of global contexts. TransPro merges a 2D
retinal vascular segmentation model and a 2D OCTA image translation model into
a 3D image translation baseline for the 2D projection map projected by the
translated OCTA images. The 2D retinal vascular segmentation model can improve
attention to the retinal vascular, while the 2D OCTA image translation model
introduces beneficial heuristic contextual information. Extensive experimental
results on two challenging datasets demonstrate that TransPro can consistently
outperform existing approaches with minimal computational overhead during
training and none during testing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at: https://github.com/ustlsh/TransPro</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object-Centric Multi-Task Learning for Human Instances 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyeongseok Son, Sangil Jung, Solae Lee, Seongeun Kim, Seung-In Park, ByungIn Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human is one of the most essential classes in visual recognition tasks such
as detection, segmentation, and pose estimation. Although much effort has been
put into individual tasks, multi-task learning for these three tasks has been
rarely studied. In this paper, we explore a compact multi-task network
architecture that maximally shares the parameters of the multiple tasks via
object-centric learning. To this end, we propose a novel query design to encode
the human instance information effectively, called human-centric query (HCQ).
HCQ enables for the query to learn explicit and structural information of human
as well such as keypoints. Besides, we utilize HCQ in prediction heads of the
target tasks directly and also interweave HCQ with the deformable attention in
Transformer decoders to exploit a well-learned object-centric representation.
Experimental results show that the proposed multi-task network achieves
comparable accuracy to state-of-the-art task-specific models in human
detection, segmentation, and pose estimation task, while it consumes less
computational costs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Orthogonal Transform Domain Approaches for the Convolutional Layer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyi Pan, Xin Zhu, Salih Atici, Ahmet Enis Cetin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a set of transform-based neural network layers as
an alternative to the $3\times3$ Conv2D layers in Convolutional Neural Networks
(CNNs). The proposed layers can be implemented based on orthogonal transforms
such as Discrete Cosine Transform (DCT) and Hadamard transform (HT), and the
biorthogonal Block Wavelet Transform (BWT). Convolutional filtering operations
are performed in the transform domain using element-wise multiplications by
taking advantage of the convolution theorems. Trainable soft-thresholding
layers that remove noise in the transform domain bring nonlinearity to the
transform domain layers. Compared to the Conv2D layer which is spatial-agnostic
and channel-specific, the proposed layers are location-specific and
channel-specific. The proposed layers reduce the number of parameters and
multiplications significantly while improving the accuracy results of regular
ResNets on the ImageNet-1K classification task. Furthermore, the proposed
layers can be inserted with a batch normalization layer before the global
average pooling layer in the conventional ResNets as an additional layer to
improve classification accuracy with a negligible increase in the number of
parameters and computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2211.08577</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ins-ATP: Deep Estimation of ATP for Organoid Based on High Throughput
  Microscopic Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuesheng Bian, Cheng Wang, Shuting Chen, Weiquan Liu, Sen Xu, Jinxin Zhu, Rugang Wang, Zexin Chen, Min Huang, Gang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adenosine triphosphate (ATP) is a high-energy phosphate compound and the most
direct energy source in organisms. ATP is an essential biomarker for evaluating
cell viability in biology. Researchers often use ATP bioluminescence to measure
the ATP of organoid after drug to evaluate the drug efficacy. However, ATP
bioluminescence has some limitations, leading to unreliable drug screening
results. Performing ATP bioluminescence causes cell lysis of organoids, so it
is impossible to observe organoids' long-term viability changes after
medication continually. To overcome the disadvantages of ATP bioluminescence,
we propose Ins-ATP, a non-invasive strategy, the first organoid ATP estimation
model based on the high-throughput microscopic image. Ins-ATP directly
estimates the ATP of organoids from high-throughput microscopic images, so that
it does not influence the drug reactions of organoids. Therefore, the ATP
change of organoids can be observed for a long time to obtain more stable
results. Experimental results show that the ATP estimation by Ins-ATP is in
good agreement with those determined by ATP bioluminescence. Specifically, the
predictions of Ins-ATP are consistent with the results measured by ATP
bioluminescence in the efficacy evaluation experiments of different drugs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PØDA: <span class="highlight-title">Prompt</span>-driven Zero-shot Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.03241v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.03241v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick Pérez, Raoul de Charette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain adaptation has been vastly investigated in computer vision but still
requires access to target images at train time, which might be intractable in
some uncommon conditions. In this paper, we propose the task of `Prompt-driven
Zero-shot Domain Adaptation', where we adapt a model trained on a source domain
using only a single general textual description of the target domain, i.e., a
prompt. First, we leverage a pretrained contrastive vision-language model
(CLIP) to optimize affine transformations of source features, steering them
towards target text embeddings, while preserving their content and semantics.
Second, we show that augmented features can be used to perform zero-shot domain
adaptation for semantic segmentation. Experiments demonstrate that our method
significantly outperforms CLIP-based style transfer baselines on several
datasets for the downstream task at hand. Our prompt-driven approach even
outperforms one-shot unsupervised domain adaptation on some datasets, and gives
comparable results on others. Our code is available at
https://github.com/astra-vision/PODA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://astra-vision.github.io/PODA/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unifying Vision, Text, and Layout for Universal Document Processing <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02623v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02623v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Universal Document Processing (UDOP), a foundation Document AI
model which unifies text, image, and layout modalities together with varied
task formats, including document understanding and generation. UDOP leverages
the spatial correlation between textual content and document image to model
image, text, and layout modalities with one uniform representation. With a
novel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain
downstream tasks into a prompt-based sequence generation scheme. UDOP is
pretrained on both large-scale unlabeled document corpora using innovative
self-supervised objectives and diverse labeled data. UDOP also learns to
generate document images from text and layout modalities via masked image
reconstruction. To the best of our knowledge, this is the first time in the
field of document AI that one model simultaneously achieves high-quality neural
document editing and content customization. Our method sets the
state-of-the-art on 8 Document AI tasks, e.g., document understanding and QA,
across diverse data domains like finance reports, academic papers, and
websites. UDOP ranks first on the leaderboard of the Document Understanding
Benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Next3D: Generative Neural Texture Rasterization for 3D-Aware Head
  Avatars <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11208v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11208v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingxiang Sun, Xuan Wang, Lizhen Wang, Xiaoyu Li, Yong Zhang, Hongwen Zhang, Yebin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D-aware generative adversarial networks (GANs) synthesize high-fidelity and
multi-view-consistent facial images using only collections of single-view 2D
imagery. Towards fine-grained control over facial attributes, recent efforts
incorporate 3D Morphable Face Model (3DMM) to describe deformation in
generative radiance fields either explicitly or implicitly. Explicit methods
provide fine-grained expression control but cannot handle topological changes
caused by hair and accessories, while implicit ones can model varied topologies
but have limited generalization caused by the unconstrained deformation fields.
We propose a novel 3D GAN framework for unsupervised learning of generative,
high-quality and 3D-consistent facial avatars from unstructured 2D images. To
achieve both deformation accuracy and topological flexibility, we propose a 3D
representation called Generative Texture-Rasterized Tri-planes. The proposed
representation learns Generative Neural Textures on top of parametric mesh
templates and then projects them into three orthogonal-viewed feature planes
through rasterization, forming a tri-plane feature representation for volume
rendering. In this way, we combine both fine-grained expression control of
mesh-guided explicit deformation and the flexibility of implicit volumetric
representation. We further propose specific modules for modeling mouth interior
which is not taken into account by 3DMM. Our method demonstrates
state-of-the-art 3D-aware synthesis quality and animation ability through
extensive experiments. Furthermore, serving as 3D prior, our animatable 3D
representation boosts multiple applications including one-shot facial avatars
and 3D-aware stylization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023. Project page:
  https://mrtornado24.github.io/Next3D/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LA-VocE: Low-SNR Audio-visual Speech Enhancement using Neural Vocoders <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10999v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10999v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Mira, Buye Xu, Jacob Donley, Anurag Kumar, Stavros Petridis, Vamsi Krishna Ithapu, Maja Pantic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual speech enhancement aims to extract clean speech from a noisy
environment by leveraging not only the audio itself but also the target
speaker's lip movements. This approach has been shown to yield improvements
over audio-only speech enhancement, particularly for the removal of interfering
speech. Despite recent advances in speech synthesis, most audio-visual
approaches continue to use spectral mapping/masking to reproduce the clean
audio, often resulting in visual backbones added to existing speech enhancement
architectures. In this work, we propose LA-VocE, a new two-stage approach that
predicts mel-spectrograms from noisy audio-visual speech via a
transformer-based architecture, and then converts them into waveform audio
using a neural vocoder (HiFi-GAN). We train and evaluate our framework on
thousands of speakers and 11+ different languages, and study our model's
ability to adapt to different levels of background noise and speech
interference. Our experiments show that LA-VocE outperforms existing methods
according to multiple metrics, particularly under very noisy scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Validation of a photogrammetric approach for the objective study of
  ancient bowed instruments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.08745v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.08745v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philémon Beghin, Anne-Emmanuelle Ceulemans, Paul Fisette, François Glineur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Some early violins have been reduced during their history to fit imposed
morphological standards, while more recent ones have been built directly to
these standards. We propose an objective photogrammetric approach to
differentiate between a reduced and an unreduced instrument, whereby a
three-dimensional mesh is studied geometrically by examining 2D slices. Our
contribution is twofold. First, we validate the quality of the photogrammetric
mesh through a comparison with reference images obtained by medical imaging,
and conclude that a sub-millimetre accuracy is achieved. Then, we show how
quantitative and qualitative features such as contour lines, channel of minima
and a measure of asymmetry between the upper and lower surfaces of a violin can
be automatically extracted from the validated photogrammetric meshes, allowing
to successfully highlight differences between instruments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decompose, Adjust, Compose: Effective Normalization by Playing with
  Frequency for Domain Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangrok Lee, Jongseong Bae, Ha Young Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain generalization (DG) is a principal task to evaluate the robustness of
computer vision models. Many previous studies have used normalization for DG.
In normalization, statistics and normalized features are regarded as style and
content, respectively. However, it has a content variation problem when
removing style because the boundary between content and style is unclear. This
study addresses this problem from the frequency domain perspective, where
amplitude and phase are considered as style and content, respectively. First,
we verify the quantitative phase variation of normalization through the
mathematical derivation of the Fourier transform formula. Then, based on this,
we propose a novel normalization method, PCNorm, which eliminates style only as
the preserving content through spectral decomposition. Furthermore, we propose
advanced PCNorm variants, CCNorm and SCNorm, which adjust the degrees of
variations in content and style, respectively. Thus, they can learn
domain-agnostic representations for DG. With the normalization methods, we
propose ResNet-variant models, DAC-P and DAC-SC, which are robust to the domain
gap. The proposed models outperform other recent DG methods. The DAC-SC
achieves an average state-of-the-art performance of 65.6% on five datasets:
PACS, VLCS, Office-Home, DomainNet, and TerraIncognita.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages,6 figures, Conference on Computer Vision and Pattern
  Recognition 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Magic Mirror: Clothing Reconstruction from a Single Image via a
  Causal Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.13096v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.13096v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhedong Zheng, Jiayin Zhu, Wei Ji, Yi Yang, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research aims to study a self-supervised 3D clothing reconstruction
method, which recovers the geometry shape and texture of human clothing from a
single image. Compared with existing methods, we observe that three primary
challenges remain: (1) 3D ground-truth meshes of clothing are usually
inaccessible due to annotation difficulties and time costs; (2) Conventional
template-based methods are limited to modeling non-rigid objects, e.g.,
handbags and dresses, which are common in fashion images; (3) The inherent
ambiguity compromises the model training, such as the dilemma between a large
shape with a remote camera or a small shape with a close camera.
  In an attempt to address the above limitations, we propose a causality-aware
self-supervised learning method to adaptively reconstruct 3D non-rigid objects
from 2D images without 3D annotations. In particular, to solve the inherent
ambiguity among four implicit variables, i.e., camera position, shape, texture,
and illumination, we introduce an explainable structural causal map (SCM) to
build our model. The proposed model structure follows the spirit of the causal
map, which explicitly considers the prior template in the camera estimation and
shape prediction. When optimization, the causality intervention tool, i.e., two
expectation-maximization loops, is deeply embedded in our algorithm to (1)
disentangle four encoders and (2) facilitate the prior template. Extensive
experiments on two 2D fashion benchmarks (ATR and Market-HQ) show that the
proposed method could yield high-fidelity 3D reconstruction. Furthermore, we
also verify the scalability of the proposed method on a fine-grained bird
dataset, i.e., CUB. The code is available at https://github.com/layumi/
3D-Magic-Mirror .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update results. Report person re-id performance. Add details in
  Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Super-Resolution of BVOC Maps by Adapting Deep Learning Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.07570v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.07570v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Giganti, Sara Mandelli, Paolo Bestagini, Marco Marcon, Stefano Tubaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biogenic Volatile Organic Compounds (BVOCs) play a critical role in
biosphere-atmosphere interactions, being a key factor in the physical and
chemical properties of the atmosphere and climate. Acquiring large and
fine-grained BVOC emission maps is expensive and time-consuming, so most
available BVOC data are obtained on a loose and sparse sampling grid or on
small regions. However, high-resolution BVOC data are desirable in many
applications, such as air quality, atmospheric chemistry, and climate
monitoring. In this work, we investigate the possibility of enhancing BVOC
acquisitions, further explaining the relationships between the environment and
these compounds. We do so by comparing the performances of several
state-of-the-art neural networks proposed for image Super-Resolution (SR),
adapting them to overcome the challenges posed by the large dynamic range of
the emission and reduce the impact of outliers in the prediction. Moreover, we
also consider realistic scenarios, considering both temporal and geographical
constraints. Finally, we present possible future developments regarding SR
generalization, considering the scale-invariance property and super-resolving
emissions from unseen compounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weighted Bayesian Gaussian Mixture Model for Roadside LiDAR Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.09804v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.09804v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianya Zhang, Yi Ge, Peter J. Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background modeling is widely used for intelligent surveillance systems to
detect moving targets by subtracting the static background components. Most
roadside LiDAR object detection methods filter out foreground points by
comparing new data points to pre-trained background references based on
descriptive statistics over many frames (e.g., voxel density, number of
neighbors, maximum distance). However, these solutions are inefficient under
heavy traffic, and parameter values are hard to transfer from one scenario to
another. In early studies, the probabilistic background modeling methods widely
used for the video-based system were considered unsuitable for roadside LiDAR
surveillance systems due to the sparse and unstructured point cloud data. In
this paper, the raw LiDAR data were transformed into a structured
representation based on the elevation and azimuth value of each LiDAR point.
With this high-order tensor representation, we break the barrier to allow
efficient high-dimensional multivariate analysis for roadside LiDAR background
modeling. The Bayesian Nonparametric (BNP) approach integrates the intensity
value and 3D measurements to exploit the measurement data using 3D and
intensity info entirely. The proposed method was compared against two
state-of-the-art roadside LiDAR background models, computer vision benchmark,
and deep learning baselines, evaluated at point, object, and path levels under
heavy traffic and challenging weather. This multimodal Weighted Bayesian
Gaussian Mixture Model (GMM) can handle dynamic backgrounds with noisy
measurements and substantially enhances the infrastructure-based LiDAR object
detection, whereby various 3D modeling for smart city applications could be
created.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StuArt: Individualized Classroom Observation of Students with Automatic
  Behavior Recognition and Tracking <span class="chip">ICASSP2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.03127v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.03127v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huayi Zhou, Fei Jiang, Jiaxin Si, Lili Xiong, Hongtao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Each student matters, but it is hardly for instructors to observe all the
students during the courses and provide helps to the needed ones immediately.
In this paper, we present StuArt, a novel automatic system designed for the
individualized classroom observation, which empowers instructors to concern the
learning status of each student. StuArt can recognize five representative
student behaviors (hand-raising, standing, sleeping, yawning, and smiling) that
are highly related to the engagement and track their variation trends during
the course. To protect the privacy of students, all the variation trends are
indexed by the seat numbers without any personal identification information.
Furthermore, StuArt adopts various user-friendly visualization designs to help
instructors quickly understand the individual and whole learning status.
Experimental results on real classroom videos have demonstrated the superiority
and robustness of the embedded algorithms. We expect our system promoting the
development of large-scale individualized guidance of students. More
information is in https://github.com/hnuzhy/StuArt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ICASSP2023. Novel pedagogical approaches in signal
  processing for K-12 education</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Body-Part Joint Detection and Association via Extended Object
  Representation <span class="chip">ICME2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07652v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07652v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huayi Zhou, Fei Jiang, Hongtao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection of human body and its related parts (e.g., face, head or hands)
have been intensively studied and greatly improved since the breakthrough of
deep CNNs. However, most of these detectors are trained independently, making
it a challenging task to associate detected body parts with people. This paper
focuses on the problem of joint detection of human body and its corresponding
parts. Specifically, we propose a novel extended object representation that
integrates the center location offsets of body or its parts, and construct a
dense single-stage anchor-based Body-Part Joint Detector (BPJDet). Body-part
associations in BPJDet are embedded into the unified representation which
contains both the semantic and geometric information. Therefore, BPJDet does
not suffer from error-prone association post-matching, and has a better
accuracy-speed trade-off. Furthermore, BPJDet can be seamlessly generalized to
jointly detect any body part. To verify the effectiveness and superiority of
our method, we conduct extensive experiments on the CityPersons, CrowdHuman and
BodyHands datasets. The proposed BPJDet detector achieves state-of-the-art
association performance on these three benchmarks while maintains high accuracy
of detection. Code is in https://github.com/hnuzhy/BPJDet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ICME2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Symmetry Defense Against CNN Adversarial Perturbation Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.04087v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.04087v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Blerta Lindqvist
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural network classifiers (CNNs) are susceptible to
adversarial attacks that perturb original samples to fool classifiers such as
an autonomous vehicle's road sign image classifier. CNNs also lack invariance
in the classification of symmetric samples because CNNs can classify symmetric
samples differently. Considered together, the CNN lack of adversarial
robustness and the CNN lack of invariance mean that the classification of
symmetric adversarial samples can differ from their incorrect classification.
Could symmetric adversarial samples revert to their correct classification?
This paper answers this question by designing a symmetry defense that inverts
or horizontally flips adversarial samples before classification against
adversaries unaware of the defense. Against adversaries aware of the defense,
the defense devises a Klein four symmetry subgroup that includes the horizontal
flip and pixel inversion symmetries. The symmetry defense uses the subgroup
symmetries in accuracy evaluation and the subgroup closure property to confine
the transformations that an adaptive adversary can apply before or after
generating the adversarial sample. Without changing the preprocessing,
parameters, or model, the proposed symmetry defense counters the Projected
Gradient Descent (PGD) and AutoAttack attacks with near-default accuracies for
ImageNet. Without using attack knowledge or adversarial samples, the proposed
defense exceeds the current best defense, which trains on adversarial samples.
The defense maintains and even improves the classification accuracy of
non-adversarial samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-aligned supervision for Real Image Dehazing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04940v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04940v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junkai Fan, Fei Guo, Jianjun Qian, Xiang Li, Jun Li, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Removing haze from real-world images is challenging due to unpredictable
weather conditions, resulting in misaligned hazy and clear image pairs. In this
paper, we propose a non-aligned supervision framework that consists of three
networks - dehazing, airlight, and transmission. In particular, we explore a
non-alignment setting by utilizing a clear reference image that is not aligned
with the hazy input image to supervise the dehazing network through a
multi-scale reference loss that compares the features of the two images. Our
setting makes it easier to collect hazy/clear image pairs in real-world
environments, even under conditions of misalignment and shift views. To
demonstrate this, we have created a new hazy dataset called "Phone-Hazy", which
was captured using mobile phones in both rural and urban areas. Additionally,
we present a mean and variance self-attention network to model the infinite
airlight using dark channel prior as position guidance, and employ a channel
attention network to estimate the three-channel transmission. Experimental
results show that our framework outperforms current state-of-the-art methods in
the real-world image dehazing. Phone-Hazy and code will be available at
https://github.com/hello2377/NSDNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Sentence Grounding in Videos: A <span class="highlight-title">Survey</span> and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.08071v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.08071v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhang, Aixin Sun, Wei Jing, Joey Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal sentence grounding in videos (TSGV), \aka natural language video
localization (NLVL) or video moment retrieval (VMR), aims to retrieve a
temporal moment that semantically corresponds to a language query from an
untrimmed video. Connecting computer vision and natural language, TSGV has
drawn significant attention from researchers in both communities. This survey
attempts to provide a summary of fundamental concepts in TSGV and current
research status, as well as future research directions. As the background, we
present a common structure of functional components in TSGV, in a tutorial
style: from feature extraction from raw video and language query, to answer
prediction of the target moment. Then we review the techniques for multimodal
understanding and interaction, which is the key focus of TSGV for effective
alignment between the two modalities. We construct a taxonomy of TSGV
techniques and elaborate the methods in different categories with their
strengths and weaknesses. Lastly, we discuss issues with the current TSGV
research and share our insights about promising research directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PDEBENCH: An Extensive Benchmark for Scientific Machine Learning <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07182v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07182v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Makoto Takamoto, Timothy Praditia, Raphael Leiteritz, Dan MacKinlay, Francesco Alesiani, Dirk Pflüger, Mathias Niepert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning-based modeling of physical systems has experienced increased
interest in recent years. Despite some impressive progress, there is still a
lack of benchmarks for Scientific ML that are easy to use but still challenging
and representative of a wide range of problems. We introduce PDEBench, a
benchmark suite of time-dependent simulation tasks based on Partial
Differential Equations (PDEs). PDEBench comprises both code and data to
benchmark the performance of novel machine learning models against both
classical numerical simulations and machine learning baselines. Our proposed
set of benchmark problems contribute the following unique features: (1) A much
wider range of PDEs compared to existing benchmarks, ranging from relatively
common examples to more realistic and difficult problems; (2) much larger
ready-to-use datasets compared to prior work, comprising multiple simulation
runs across a larger number of initial and boundary conditions and PDE
parameters; (3) more extensible source codes with user-friendly APIs for data
generation and baseline results with popular machine learning models (FNO,
U-Net, PINN, Gradient-Based Inverse Method). PDEBench allows researchers to
extend the benchmark freely for their own purposes using a standardized API and
to compare the performance of new models to existing baseline methods. We also
propose new evaluation metrics with the aim to provide a more holistic
understanding of learning methods in the context of Scientific ML. With those
metrics we identify tasks which are challenging for recent ML methods and
propose these tasks as future challenges for the community. The code is
available at https://github.com/pdebench/PDEBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages (main body) + 34 pages (supplemental material), accepted for
  publication in NeurIPS 2022 Track Datasets and Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Localized Sparse Incomplete Multi-view Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.02998v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.02998v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengliang Liu, Zhihao Wu, Jie Wen, Chao Huang, Yong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incomplete multi-view clustering, which aims to solve the clustering problem
on the incomplete multi-view data with partial view missing, has received more
and more attention in recent years. Although numerous methods have been
developed, most of the methods either cannot flexibly handle the incomplete
multi-view data with arbitrary missing views or do not consider the negative
factor of information imbalance among views. Moreover, some methods do not
fully explore the local structure of all incomplete views. To tackle these
problems, this paper proposes a simple but effective method, named localized
sparse incomplete multi-view clustering (LSIMVC). Different from the existing
methods, LSIMVC intends to learn a sparse and structured consensus latent
representation from the incomplete multi-view data by optimizing a sparse
regularized and novel graph embedded multi-view matrix factorization model.
Specifically, in such a novel model based on the matrix factorization, a l1
norm based sparse constraint is introduced to obtain the sparse low-dimensional
individual representations and the sparse consensus representation. Moreover, a
novel local graph embedding term is introduced to learn the structured
consensus representation. Different from the existing works, our local graph
embedding term aggregates the graph embedding task and consensus representation
learning task into a concise term. Furthermore, to reduce the imbalance factor
of incomplete multi-view learning, an adaptive weighted learning scheme is
introduced to LSIMVC. Finally, an efficient optimization strategy is given to
solve the optimization problem of our proposed model. Comprehensive
experimental results performed on six incomplete multi-view databases verify
that the performance of our LSIMVC is superior to the state-of-the-art IMC
approaches. The code is available in https://github.com/justsmart/LSIMVC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in IEEE Transactions on Multimedia (TMM). The code is
  available at Github https://github.com/justsmart/LSIMVC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ECM-OPCC: Efficient Context Model for Octree-based Point Cloud
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10916v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10916v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqi Jin, Ziyu Zhu, Tongda Xu, Yuhuan Lin, Yan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, deep learning methods have shown promising results in point cloud
compression. For octree-based point cloud compression, previous works show that
the information of ancestor nodes and sibling nodes are equally important for
predicting current node. However, those works either adopt insufficient context
or bring intolerable decoding complexity (e.g. >600s). To address this problem,
we propose a sufficient yet efficient context model and design an efficient
deep learning codec for point clouds. Specifically, we first propose a
window-constrained multi-group coding strategy to exploit the autoregressive
context while maintaining decoding efficiency. Then, we propose a dual
transformer architecture to utilize the dependency of current node on its
ancestors and siblings. We also propose a random-masking pre-train method to
enhance our model. Experimental results show that our approach achieves
state-of-the-art performance for both lossy and lossless point cloud
compression. Moreover, our multi-group coding strategy saves 98% decoding time
compared with previous octree-based compression method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Get3DHuman: Lifting StyleGAN-Human into a 3D Generative Model using
  Pixel-aligned Reconstruction Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01162v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01162v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangyang Xiong, Di Kang, Derong Jin, Weikai Chen, Linchao Bao, Shuguang Cui, Xiaoguang Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fast generation of high-quality 3D digital humans is important to a vast
number of applications ranging from entertainment to professional concerns.
Recent advances in differentiable rendering have enabled the training of 3D
generative models without requiring 3D ground truths. However, the quality of
the generated 3D humans still has much room to improve in terms of both
fidelity and diversity. In this paper, we present Get3DHuman, a novel 3D human
framework that can significantly boost the realism and diversity of the
generated outcomes by only using a limited budget of 3D ground-truth data. Our
key observation is that the 3D generator can profit from human-related priors
learned through 2D human generators and 3D reconstructors. Specifically, we
bridge the latent space of Get3DHuman with that of StyleGAN-Human via a
specially-designed prior network, where the input latent code is mapped to the
shape and texture feature volumes spanned by the pixel-aligned 3D
reconstructor. The outcomes of the prior network are then leveraged as the
supervisory signals for the main generator network. To ensure effective
training, we further propose three tailored losses applied to the generated
feature volumes and the intermediate feature maps. Extensive experiments
demonstrate that Get3DHuman greatly outperforms the other state-of-the-art
approaches and can support a wide range of applications including shape
interpolation, shape re-texturing, and single-view reconstruction through
latent inversion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CR-FIQA: Face Image Quality Assessment by Learning Sample Relative
  Classifiability <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.06592v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.06592v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fadi Boutros, Meiling Fang, Marcel Klemt, Biying Fu, Naser Damer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quality of face images significantly influences the performance of
underlying face recognition algorithms. Face image quality assessment (FIQA)
estimates the utility of the captured image in achieving reliable and accurate
recognition performance. In this work, we propose a novel learning paradigm
that learns internal network observations during the training process. Based on
that, our proposed CR-FIQA uses this paradigm to estimate the face image
quality of a sample by predicting its relative classifiability. This
classifiability is measured based on the allocation of the training sample
feature representation in angular space with respect to its class center and
the nearest negative class center. We experimentally illustrate the correlation
between the face image quality and the sample relative classifiability. As such
property is only observable for the training dataset, we propose to learn this
property from the training dataset and utilize it to predict the quality
measure on unseen samples. This training is performed simultaneously while
optimizing the class centers by an angular margin penalty-based softmax loss
used for face recognition model training. Through extensive evaluation
experiments on eight benchmarks and four face recognition models, we
demonstrate the superiority of our proposed CR-FIQA over state-of-the-art
(SOTA) FIQA algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition 2023 (CVPR2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dunhuang murals contour generation network based on convolution and
  self-attention fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00935v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00935v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baokai Liu, Fengjie He, Shiqiang Du, Kaiwu Zhang, Jianhua Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dunhuang murals are a collection of Chinese style and national style, forming
a self-contained Chinese-style Buddhist art. It has very high historical and
cultural value and research significance. Among them, the lines of Dunhuang
murals are highly general and expressive. It reflects the character's
distinctive character and complex inner emotions. Therefore, the outline
drawing of murals is of great significance to the research of Dunhuang Culture.
The contour generation of Dunhuang murals belongs to image edge detection,
which is an important branch of computer vision, aims to extract salient
contour information in images. Although convolution-based deep learning
networks have achieved good results in image edge extraction by exploring the
contextual and semantic features of images. However, with the enlargement of
the receptive field, some local detail information is lost. This makes it
impossible for them to generate reasonable outline drawings of murals. In this
paper, we propose a novel edge detector based on self-attention combined with
convolution to generate line drawings of Dunhuang murals. Compared with
existing edge detection methods, firstly, a new residual self-attention and
convolution mixed module (Ramix) is proposed to fuse local and global features
in feature maps. Secondly, a novel densely connected backbone extraction
network is designed to efficiently propagate rich edge feature information from
shallow layers into deep layers. Compared with existing methods, it is shown on
different public datasets that our method is able to generate sharper and
richer edge maps. In addition, testing on the Dunhuang mural dataset shows that
our method can achieve very competitive performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HCGMNET: A Hierarchical Change Guiding Map Network For Change Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10420v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10420v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengxi Han, Chen Wu, Bo Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Very-high-resolution (VHR) remote sensing (RS) image change detection (CD)
has been a challenging task for its very rich spatial information and sample
imbalance problem. In this paper, we have proposed a hierarchical change
guiding map network (HCGMNet) for change detection. The model uses hierarchical
convolution operations to extract multiscale features, continuously merges
multi-scale features layer by layer to improve the expression of global and
local information, and guides the model to gradually refine edge features and
comprehensive performance by a change guide module (CGM), which is a
self-attention with changing guide map. Extensive experiments on two CD
datasets show that the proposed HCGMNet architecture achieves better CD
performance than existing state-of-the-art (SOTA) CD methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatio-Temporal Crop Aggregation for Video Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.17042v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.17042v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sepehr Sameni, Simon Jenni, Paolo Favaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Spatio-temporal Crop Aggregation for video representation LEarning
(SCALE), a novel method that enjoys high scalability at both training and
inference time. Our model builds long-range video features by learning from
sets of video clip-level features extracted with a pre-trained backbone. To
train the model, we propose a self-supervised objective consisting of masked
clip feature prediction. We apply sparsity to both the input, by extracting a
random set of video clips, and to the loss function, by only reconstructing the
sparse inputs. Moreover, we use dimensionality reduction by working in the
latent space of a pre-trained backbone applied to single video clips. These
techniques make our method not only extremely efficient to train but also
highly effective in transfer learning. We demonstrate that our video
representation yields state-of-the-art performance with linear, non-linear, and
KNN probing on common action classification and video understanding datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient 3D Reconstruction, Streaming and Visualization of Static and
  Dynamic Scene Parts for Multi-client Live-telepresence in Large-scale
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14310v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14310v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leif Van Holland, Patrick Stotko, Stefan Krumpen, Reinhard Klein, Michael Weinmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the impressive progress of telepresence systems for room-scale scenes
with static and dynamic scene entities, expanding their capabilities to
scenarios with larger dynamic environments beyond a fixed size of a few
square-meters remains challenging.
  In this paper, we aim at sharing 3D live-telepresence experiences in
large-scale environments beyond room scale with both static and dynamic scene
entities at practical bandwidth requirements only based on light-weight scene
capture with a single moving consumer-grade RGB-D camera. To this end, we
present a system which is built upon a novel hybrid volumetric scene
representation in terms of the combination of a voxel-based scene
representation for the static contents, that not only stores the reconstructed
surface geometry but also contains information about the object semantics as
well as their accumulated dynamic movement over time, and a point-cloud-based
representation for dynamic scene parts, where the respective separation from
static parts is achieved based on semantic and instance information extracted
for the input frames. With an independent yet simultaneous streaming of both
static and dynamic content, where we seamlessly integrate potentially moving
but currently static scene entities in the static model until they are becoming
dynamic again, as well as the fusion of static and dynamic data at the remote
client, our system is able to achieve VR-based live-telepresence at close to
real-time rates. Our evaluation demonstrates the potential of our novel
approach in terms of visual quality, performance, and ablation studies
regarding involved design choices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Learning by Detecting Incorrect Location Embeddings <span class="chip">AAAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.04788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.04788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sepehr Sameni, Simon Jenni, Paolo Favaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel self-supervised learning (SSL) loss for
image representation learning. There is a growing belief that generalization in
deep neural networks is linked to their ability to discriminate object shapes.
Since object shape is related to the location of its parts, we propose to
detect those that have been artificially misplaced. We represent object parts
with image tokens and train a ViT to detect which token has been combined with
an incorrect positional embedding. We then introduce sparsity in the inputs to
make the model more robust to occlusions and to speed up the training. We call
our method DILEMMA, which stands for Detection of Incorrect Location EMbeddings
with MAsked inputs. We apply DILEMMA to MoCoV3, DINO and SimCLR and show an
improvement in their performance of respectively 4.41%, 3.97%, and 0.5% under
the same training time and with a linear probing transfer on ImageNet-1K. We
also show full fine-tuning improvements of MAE combined with our method on
ImageNet-100. We evaluate our method via fine-tuning on common SSL benchmarks.
Moreover, we show that when downstream tasks are strongly reliant on shape
(such as in the YOGA-82 pose dataset), our pre-trained features yield a
significant gain over prior work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at AAAI2023, https://github.com/Separius/DILEMMA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Domain-Invariance in <span class="highlight-title">Self-Supervised</span> Learning via Batch Styles
  Standardization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06088v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06088v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marin Scalbert, Maria Vakalopoulou, Florent Couzinié-Devy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent rise of Self-Supervised Learning (SSL) as one of the preferred
strategies for learning with limited labeled data, and abundant unlabeled data
has led to the widespread use of these models. They are usually pretrained,
finetuned, and evaluated on the same data distribution, i.e., within an
in-distribution setting. However, they tend to perform poorly in
out-of-distribution evaluation scenarios, a challenge that Unsupervised Domain
Generalization (UDG) seeks to address.
  This paper introduces a novel method to standardize the styles of images in a
batch. Batch styles standardization, relying on Fourier-based augmentations,
promotes domain invariance in SSL by preventing spurious correlations from
leaking into the features. The combination of batch styles standardization with
the well-known contrastive-based method SimCLR leads to a novel UDG method
named CLaSSy ($\textbf{C}$ontrastive $\textbf{L}$e$\textbf{a}$rning with
$\textbf{S}$tandardized $\textbf{S}$t$\textbf{y}$les). CLaSSy offers serious
advantages over prior methods, as it does not rely on domain labels and is
scalable to handle a large number of domains. Experimental results on various
UDG datasets demonstrate the superior performance of CLaSSy compared to
existing UDG methods. Finally, the versatility of the proposed batch styles
standardization is demonstrated by extending respectively the contrastive-based
and non-contrastive-based SSL methods, SWaV and MSN, while considering
different backbone architectures (convolutional-based, transformers-based).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAQE: Enhancing the Quality of Compressed Images by Exploiting the
  Inherent Characteristic of Defocus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10984v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10984v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qunliang Xing, Mai Xu, Xin Deng, Yichen Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image defocus is inherent in the physics of image formation caused by the
optical aberration of lenses, providing plentiful information on image quality.
Unfortunately, existing quality enhancement approaches for compressed images
neglect the inherent characteristic of defocus, resulting in inferior
performance. This paper finds that in compressed images, significantly
defocused regions have better compression quality, and two regions with
different defocus values possess diverse texture patterns. These observations
motivate our defocus-aware quality enhancement (DAQE) approach. Specifically,
we propose a novel dynamic region-based deep learning architecture of the DAQE
approach, which considers the regionwise defocus difference of compressed
images in two aspects. (1) The DAQE approach employs fewer computational
resources to enhance the quality of significantly defocused regions and more
resources to enhance the quality of other regions; (2) The DAQE approach learns
to separately enhance diverse texture patterns for regions with different
defocus values, such that texture-specific enhancement can be achieved.
Extensive experiments validate the superiority of our DAQE approach over
state-of-the-art approaches in terms of quality enhancement and resource
savings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $L_2$BN: Enhancing Batch Normalization by Equalizing the $L_2$ Norms of
  Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.02625v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.02625v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhennan Wang, Kehan Li, Runyi Yu, Yian Zhao, Pengchong Qiao, Fan Xu, Guoli Song, Jie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we show that the difference in $l_2$ norms of sample features
can hinder batch normalization from obtaining more distinguished inter-class
features and more compact intra-class features. To address this issue, we
propose an intuitive but effective method to equalize the $l_2$ norms of sample
features. Concretely, we $l_2$-normalize each sample feature before feeding
them into batch normalization, and therefore the features are of the same
magnitude. Since the proposed method combines the $l_2$ normalization and batch
normalization, we name our method $L_2$BN. The $L_2$BN can strengthen the
compactness of intra-class features and enlarge the discrepancy of inter-class
features. The $L_2$BN is easy to implement and can exert its effect without any
additional parameters or hyper-parameters. Therefore, it can be used as a basic
normalization method for neural networks. We evaluate the effectiveness of
$L_2$BN through extensive experiments with various models on image
classification and acoustic scene classification tasks. The results demonstrate
that the $L_2$BN can boost the generalization ability of various neural network
models and achieve considerable performance improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Equation (4) and Figure 5 are confusing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MGTAB: A Multi-Relational Graph-Based Twitter Account Detection
  Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.01123v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.01123v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhao Shi, Kai Qiao, Jian Chen, Shuai Yang, Jie Yang, Baojie Song, Linyuan Wang, Bin Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of social media user stance detection and bot detection
methods rely heavily on large-scale and high-quality benchmarks. However, in
addition to low annotation quality, existing benchmarks generally have
incomplete user relationships, suppressing graph-based account detection
research. To address these issues, we propose a Multi-Relational Graph-Based
Twitter Account Detection Benchmark (MGTAB), the first standardized graph-based
benchmark for account detection. To our knowledge, MGTAB was built based on the
largest original data in the field, with over 1.55 million users and 130
million tweets. MGTAB contains 10,199 expert-annotated users and 7 types of
relationships, ensuring high-quality annotation and diversified relations. In
MGTAB, we extracted the 20 user property features with the greatest information
gain and user tweet features as the user features. In addition, we performed a
thorough evaluation of MGTAB and other public datasets. Our experiments found
that graph-based approaches are generally more effective than feature-based
approaches and perform better when introducing multiple relations. By analyzing
experiment results, we identify effective approaches for account detection and
provide potential future research directions in this field. Our benchmark and
standardized evaluation procedures are freely available at:
https://github.com/GraphDetec/MGTAB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ParCNetV2: Oversized Kernel with Enhanced Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.07157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.07157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihan Xu, Haokui Zhang, Wenze Hu, Shiliang Zhang, Xiaoyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have achieved tremendous success in various computer vision
tasks. By borrowing design concepts from transformers, many studies
revolutionized CNNs and showed remarkable results. This paper falls in this
line of studies. More specifically, we introduce a convolutional neural network
architecture named ParCNetV2, which extends position-aware circular convolution
(ParCNet) with oversized convolutions and strengthens attention through
bifurcate gate units. The oversized convolution utilizes a kernel with
$2\times$ the input size to model long-range dependencies through a global
receptive field. Simultaneously, it achieves implicit positional encoding by
removing the shift-invariant property from convolutional kernels, i.e., the
effective kernels at different spatial locations are different when the kernel
size is twice as large as the input size. The bifurcate gate unit implements an
attention mechanism similar to self-attention in transformers. It splits the
input into two branches, one serves as feature transformation while the other
serves as attention weights. The attention is applied through element-wise
multiplication of the two branches. Besides, we introduce a unified
local-global convolution block to unify the design of the early and late stage
convolutional blocks. Extensive experiments demonstrate that our method
outperforms other pure convolutional neural networks as well as neural networks
hybridizing CNNs and transformers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SadTalker: Learning Realistic 3D Motion Coefficients for Stylized
  Audio-Driven Single Image Talking Face Animation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12194v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12194v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, Fei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating talking head videos through a face image and a piece of speech
audio still contains many challenges. ie, unnatural head movement, distorted
expression, and identity modification. We argue that these issues are mainly
because of learning from the coupled 2D motion fields. On the other hand,
explicitly using 3D information also suffers problems of stiff expression and
incoherent video. We present SadTalker, which generates 3D motion coefficients
(head pose, expression) of the 3DMM from audio and implicitly modulates a novel
3D-aware face render for talking head generation. To learn the realistic motion
coefficients, we explicitly model the connections between audio and different
types of motion coefficients individually. Precisely, we present ExpNet to
learn the accurate facial expression from audio by distilling both coefficients
and 3D-rendered faces. As for the head pose, we design PoseVAE via a
conditional VAE to synthesize head motion in different styles. Finally, the
generated 3D motion coefficients are mapped to the unsupervised 3D keypoints
space of the proposed face render, and synthesize the final video. We conducted
extensive experiments to demonstrate the superiority of our method in terms of
motion and video quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023, Project page: https://sadtalker.github.io,
  Code: https://github.com/Winfredy/SadTalker</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Trajectory-Aware Body Interaction <span class="highlight-title">Transformer</span> for Multi-Person Pose
  Forecasting <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05095v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05095v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaogang Peng, Siyuan Mao, Zizhao Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-person pose forecasting remains a challenging problem, especially in
modeling fine-grained human body interaction in complex crowd scenarios.
Existing methods typically represent the whole pose sequence as a temporal
series, yet overlook interactive influences among people based on skeletal body
parts. In this paper, we propose a novel Trajectory-Aware Body Interaction
Transformer (TBIFormer) for multi-person pose forecasting via effectively
modeling body part interactions. Specifically, we construct a Temporal Body
Partition Module that transforms all the pose sequences into a Multi-Person
Body-Part sequence to retain spatial and temporal information based on body
semantics. Then, we devise a Social Body Interaction Self-Attention (SBI-MSA)
module, utilizing the transformed sequence to learn body part dynamics for
inter- and intra-individual interactions. Furthermore, different from prior
Euclidean distance-based spatial encodings, we present a novel and efficient
Trajectory-Aware Relative Position Encoding for SBI-MSA to offer discriminative
spatial information and additional interactive clues. On both short- and
long-term horizons, we empirically evaluate our framework on CMU-Mocap,
MuPoTS-3D as well as synthesized datasets (6 ~ 10 persons), and demonstrate
that our method greatly outperforms the state-of-the-art methods. Code will be
made publicly available upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023, 8 pages, 6 figures. arXiv admin note: text
  overlap with arXiv:2208.09224</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EGFR Mutation Prediction of Lung Biopsy Images using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.12506v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.12506v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ravi Kant Gupta, Shivani Nandgaonkar, Nikhil Cherian Kurian, Swapnil Rane, Amit Sethi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The standard diagnostic procedures for targeted therapies in lung cancer
treatment involve histological subtyping and subsequent detection of key driver
mutations, such as EGFR. Even though molecular profiling can uncover the driver
mutation, the process is often expensive and time-consuming. Deep
learning-oriented image analysis offers a more economical alternative for
discovering driver mutations directly from whole slide images (WSIs). In this
work, we used customized deep learning pipelines with weak supervision to
identify the morphological correlates of EGFR mutation from hematoxylin and
eosin-stained WSIs, in addition to detecting tumor and histologically subtyping
it. We demonstrate the effectiveness of our pipeline by conducting rigorous
experiments and ablation studies on two lung cancer datasets - TCGA and a
private dataset from India. With our pipeline, we achieved an average area
under the curve (AUC) of 0.964 for tumor detection, and 0.942 for histological
subtyping between adenocarcinoma and squamous cell carcinoma on the TCGA
dataset. For EGFR detection, we achieved an average AUC of 0.864 on the TCGA
dataset and 0.783 on the dataset from India. Our key learning points include
the following. Firstly, there is no particular advantage of using a feature
extractor layers trained on histology, if one is going to fine-tune the feature
extractor on the target dataset. Secondly, selecting patches with high
cellularity, presumably capturing tumor regions, is not always helpful, as the
sign of a disease class may be present in the tumor-adjacent stroma.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We need to improve</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentiable Parsing and Visual Grounding of Natural Language
  Instructions for Object Placement <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00215v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00215v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zirui Zhao, Wee Sun Lee, David Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new method, PARsing And visual GrOuNding (ParaGon), for
grounding natural language in object placement tasks. Natural language
generally describes objects and spatial relations with compositionality and
ambiguity, two major obstacles to effective language grounding. For
compositionality, ParaGon parses a language instruction into an object-centric
graph representation to ground objects individually. For ambiguity, ParaGon
uses a novel particle-based graph neural network to reason about object
placements with uncertainty. Essentially, ParaGon integrates a parsing
algorithm into a probabilistic, data-driven learning framework. It is fully
differentiable and trained end-to-end from data for robustness against complex,
ambiguous language input.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context-Based Trit-Plane Coding for Progressive Image Compression <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05715v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05715v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungmin Jeon, Kwang Pyo Choi, Youngo Park, Chang-Su Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trit-plane coding enables deep progressive image compression, but it cannot
use autoregressive context models. In this paper, we propose the context-based
trit-plane coding (CTC) algorithm to achieve progressive compression more
compactly. First, we develop the context-based rate reduction module to
estimate trit probabilities of latent elements accurately and thus encode the
trit-planes compactly. Second, we develop the context-based distortion
reduction module to refine partial latent tensors from the trit-planes and
improve the reconstructed image quality. Third, we propose a retraining scheme
for the decoder to attain better rate-distortion tradeoffs. Extensive
experiments show that CTC outperforms the baseline trit-plane codec
significantly in BD-rate on the Kodak lossless dataset, while increasing the
time complexity only marginally. Our codes are available at
https://github.com/seungminjeon-github/CTC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measurement-Consistent Networks via a Deep Implicit Layer for Solving
  Inverse Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.03177v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.03177v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Mourya, João F. C. Mota
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end deep neural networks (DNNs) have become the state-of-the-art
(SOTA) for solving inverse problems. Despite their outstanding performance,
during deployment, such networks are sensitive to minor variations in the
testing pipeline and often fail to reconstruct small but important details, a
feature critical in medical imaging, astronomy, or defence. Such instabilities
in DNNs can be explained by the fact that they ignore the forward measurement
model during deployment, and thus fail to enforce consistency between their
output and the input measurements. To overcome this, we propose a framework
that transforms any DNN for inverse problems into a measurement-consistent one.
This is done by appending to it an implicit layer (or deep equilibrium network)
designed to solve a model-based optimization problem. The implicit layer
consists of a shallow learnable network that can be integrated into the
end-to-end training while keeping the SOTA DNN fixed. Experiments on
single-image super-resolution show that the proposed framework leads to
significant improvements in reconstruction quality and robustness over the SOTA
DNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Galaxy Image Deconvolution for Weak Gravitational Lensing with Unrolled
  Plug-and-Play ADMM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01567v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01567v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianao Li, Emma Alexander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Removing optical and atmospheric blur from galaxy images significantly
improves galaxy shape measurements for weak gravitational lensing and galaxy
evolution studies. This ill-posed linear inverse problem is usually solved with
deconvolution algorithms enhanced by regularisation priors or deep learning. We
introduce a so-called "physics-informed deep learning" approach to the Point
Spread Function (PSF) deconvolution problem in galaxy surveys. We apply
algorithm unrolling and the Plug-and-Play technique to the Alternating
Direction Method of Multipliers (ADMM), in which a neural network learns
appropriate hyperparameters and denoising priors from simulated galaxy images.
We characterise the time-performance trade-off of several methods for galaxies
of differing brightness levels as well as our method's robustness to systematic
PSF errors and network ablations. We show an improvement in reduced shear
ellipticity error of 38.6% (SNR=20)/45.0% (SNR=200) compared to classic methods
and 7.4% (SNR=20)/33.2% (SNR=200) compared to modern methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Object-Aware Attention Guided Frame Association for RGB-D SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.12047v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.12047v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Caglayan, Nevrez Imamoglu, Oguzhan Guclu, Ali Osman Serhatoglu, Weimin Wang, Ahmet Burak Can, Ryosuke Nakamura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models as an emerging topic have shown great progress in
various fields. Especially, visualization tools such as class activation
mapping methods provided visual explanation on the reasoning of convolutional
neural networks (CNNs). By using the gradients of the network layers, it is
possible to demonstrate where the networks pay attention during a specific
image recognition task. Moreover, these gradients can be integrated with CNN
features for localizing more generalized task dependent attentive (salient)
objects in scenes. Despite this progress, there is not much explicit usage of
this gradient (network attention) information to integrate with CNN
representations for object semantics. This can be very useful for visual tasks
such as simultaneous localization and mapping (SLAM) where CNN representations
of spatially attentive object locations may lead to improved performance.
Therefore, in this work, we propose the use of task specific network attention
for RGB-D indoor SLAM. To do so, we integrate layer-wise object attention
information (layer gradients) with CNN layer representations to improve frame
association performance in an RGB-D indoor SLAM method. Experiments show
promising results with improved performance over the baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Support and Trivial Prototypes for Interpretable Image
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.04011v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.04011v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chong Wang, Yuyuan Liu, Yuanhong Chen, Fengbei Liu, Yu Tian, Davis J. McCarthy, Helen Frazer, Gustavo Carneiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prototypical part network (ProtoPNet) methods have been designed to achieve
interpretable classification by associating predictions with a set of training
prototypes, which we refer to as trivial prototypes because they are trained to
lie far from the classification boundary in the feature space. Note that it is
possible to make an analogy between ProtoPNet and support vector machine (SVM)
given that the classification from both methods relies on computing similarity
with a set of training points (i.e., trivial prototypes in ProtoPNet, and
support vectors in SVM). However, while trivial prototypes are located far from
the classification boundary, support vectors are located close to this
boundary, and we argue that this discrepancy with the well-established SVM
theory can result in ProtoPNet models with inferior classification accuracy. In
this paper, we aim to improve the classification of ProtoPNet with a new method
to learn support prototypes that lie near the classification boundary in the
feature space, as suggested by the SVM theory. In addition, we target the
improvement of classification results with a new model, named ST-ProtoPNet,
which exploits our support prototypes and the trivial prototypes to provide
more effective classification. Experimental results on CUB-200-2011, Stanford
Cars, and Stanford Dogs datasets demonstrate that ST-ProtoPNet achieves
state-of-the-art classification accuracy and interpretability results. We also
show that the proposed support prototypes tend to be better localised in the
object of interest rather than in the background region.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffusionDepth: Diffusion Denoising Approach for Monocular Depth
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05021v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05021v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqun Duan, Zheng Zhu, Xianda Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular depth estimation is a challenging task that predicts the pixel-wise
depth from a single 2D image. Current methods typically model this problem as a
regression or classification task. We propose DiffusionDepth, a new approach
that reformulates monocular depth estimation as a denoising diffusion process.
It learns an iterative denoising process to `denoise' random depth distribution
into a depth map with the guidance of monocular visual conditions. The process
is performed in the latent space encoded by a dedicated depth encoder and
decoder. Instead of diffusing ground truth (GT) depth, the model learns to
reverse the process of diffusing the refined depth of itself into random depth
distribution. This self-diffusion formulation overcomes the difficulty of
applying generative models to sparse GT depth scenarios. The proposed approach
benefits this task by refining depth estimation step by step, which is superior
for generating accurate and highly detailed depth maps. Experimental results on
KITTI and NYU-Depth-V2 datasets suggest that a simple yet efficient diffusion
approach could reach state-of-the-art performance in both indoor and outdoor
scenarios with acceptable inference time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EcoTTA: Memory-Efficient Continual Test-time Adaptation via
  Self-distilled Regularization <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01904v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01904v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junha Song, Jungsoo Lee, In So Kweon, Sungha Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a simple yet effective approach that improves continual
test-time adaptation (TTA) in a memory-efficient manner. TTA may primarily be
conducted on edge devices with limited memory, so reducing memory is crucial
but has been overlooked in previous TTA studies. In addition, long-term
adaptation often leads to catastrophic forgetting and error accumulation, which
hinders applying TTA in real-world deployments. Our approach consists of two
components to address these issues. First, we present lightweight meta networks
that can adapt the frozen original networks to the target domain. This novel
architecture minimizes memory consumption by decreasing the size of
intermediate activations required for backpropagation. Second, our novel
self-distilled regularization controls the output of the meta networks not to
deviate significantly from the output of the frozen original networks, thereby
preserving well-trained knowledge from the source domain. Without additional
memory, this regularization prevents error accumulation and catastrophic
forgetting, resulting in stable performance even in long-term test-time
adaptation. We demonstrate that our simple yet effective strategy outperforms
other state-of-the-art methods on various benchmarks for image classification
and semantic segmentation tasks. Notably, our proposed method with ResNet-50
and WideResNet-40 takes 86% and 80% less memory than the recent
state-of-the-art method, CoTTA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I-Tuning: Tuning Frozen Language Models with Image for Lightweight Image
  Captioning <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06574v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06574v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Luo, Zhipeng Hu, Yadong Xi, Rongsheng Zhang, Jing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Captioning is a traditional vision-and-language task that aims to
generate the language description of an image. Recent studies focus on scaling
up the model size and the number of training data, which significantly increase
the cost of model training. Different to these heavy-cost models, we introduce
a lightweight image captioning framework (I-Tuning), which contains a small
number of trainable parameters. We design a novel I-Tuning cross-attention
module to connect the non-trainable pre-trained language decoder GPT2 and
vision encoder CLIP-ViT. Since most parameters are not required to be updated
during training, our framework is lightweight and fast. Experimental results
conducted on three image captioning benchmarks reveal that our framework
achieves comparable or better performance than the large-scale baseline
systems. But our models contain up to 10 times fewer trainable parameters and
require much fewer data for training compared with state-of-the-art baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hypergraph <span class="highlight-title">Transformer</span> for Skeleton-based Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09590v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09590v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhou, Zhi-Qi Cheng, Chao Li, Yanwen Fang, Yifeng Geng, Xuansong Xie, Margret Keuper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skeleton-based action recognition aims to recognize human actions given human
joint coordinates with skeletal interconnections. By defining a graph with
joints as vertices and their natural connections as edges, previous works
successfully adopted Graph Convolutional networks (GCNs) to model joint
co-occurrences and achieved superior performance. More recently, a limitation
of GCNs is identified, i.e., the topology is fixed after training. To relax
such a restriction, Self-Attention (SA) mechanism has been adopted to make the
topology of GCNs adaptive to the input, resulting in the state-of-the-art
hybrid models. Concurrently, attempts with plain Transformers have also been
made, but they still lag behind state-of-the-art GCN-based methods due to the
lack of structural prior. Unlike hybrid models, we propose a more elegant
solution to incorporate the bone connectivity into Transformer via a graph
distance embedding. Our embedding retains the information of skeletal structure
during training, whereas GCNs merely use it for initialization. More
importantly, we reveal an underlying issue of graph models in general, i.e.,
pairwise aggregation essentially ignores the high-order kinematic dependencies
between body joints. To fill this gap, we propose a new self-attention (SA)
mechanism on hypergraph, termed Hypergraph Self-Attention (HyperSA), to
incorporate intrinsic higher-order relations into the model. We name the
resulting model Hyperformer, and it beats state-of-the-art graph models w.r.t.
accuracy and efficiency on NTU RGB+D, NTU RGB+D 120, and Northwestern-UCLA
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Recurrent Long-term Temporal Fusion for Multi-view 3D
  Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05970v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05970v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunrui Han, Jianjian Sun, Zheng Ge, Jinrong Yang, Runpei Dong, Hongyu Zhou, Weixin Mao, Yuang Peng, Xiangyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-term temporal fusion is a crucial but often overlooked technique in
camera-based Bird's-Eye-View (BEV) 3D perception. Existing methods are mostly
in a parallel manner. While parallel fusion can benefit from long-term
information, it suffers from increasing computational and memory overheads as
the fusion window size grows. Alternatively, BEVFormer adopts a recurrent
fusion pipeline so that history information can be efficiently integrated, yet
it fails to benefit from longer temporal frames. In this paper, we explore an
embarrassingly simple long-term recurrent fusion strategy built upon the
LSS-based methods and find it already able to enjoy the merits from both sides,
i.e., rich long-term information and efficient fusion pipeline. A temporal
embedding module is further proposed to improve the model's robustness against
occasionally missed frames in practical scenarios. We name this simple but
effective fusing pipeline VideoBEV. Experimental results on the nuScenes
benchmark show that VideoBEV obtains leading performance on various
camera-based 3D perception tasks, including object detection (55.4% mAP and
62.9% NDS), segmentation (48.6% vehicle mIoU), tracking (54.8% AMOTA), and
motion prediction (0.80m minADE and 0.463 EPA). Code will be available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One-shot recognition of any material anywhere using contrastive learning
  with physics-based rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00648v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00648v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel S. Drehwald, Sagi Eppel, Jolina Li, Han Hao, Alan Aspuru-Guzik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual recognition of materials and their states is essential for
understanding most aspects of the world, from determining whether food is
cooked, metal is rusted, or a chemical reaction has occurred. However, current
image recognition methods are limited to specific classes and properties and
can't handle the vast number of material states and textures in the world. To
address this, we present MatSim: the first dataset and benchmark for computer
vision-based recognition of similarities and transitions between materials and
textures, focusing on identifying any material under any conditions using one
or a few examples. The dataset contains synthetic and real images. The
synthetic images were rendered using giant collections of textures, objects,
and environments generated by computer graphics artists. We use mixtures and
gradual transitions between materials to allow the system to learn cases with
smooth transitions between states (like gradually cooked food). We also render
images with materials inside transparent containers to support beverage and
chemistry lab use cases. We use this dataset to train a siamese net that
identifies the same material in different objects, mixtures, and environments.
The descriptor generated by this net can be used to identify the states of
materials and their subclasses using a single image. We also present the first
few-shot material recognition benchmark with images from a wide range of
fields, including the state of metals and chemical reactions types of ground
and many other use cases. We show that a net trained on the MatSim synthetic
dataset outperforms state-of-the-art models like Clip on the benchmark and also
achieves good results on other unsupervised material classification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>for associated code and dataset, see
  https://zenodo.org/record/7390166#.Y5ku6mHMJH4 or
  https://e1.pcloud.link/publink/show?code=kZIiSQZCYU5M4HOvnQykql9jxF4h0KiC5MX
  and https://icedrive.net/s/A13FWzZ8V2aP9T4ufGQ1N3fBZxDF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiP: Learning Discriminative Implicit Parts for Person Re-Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.13906v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.13906v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dengjie Li, Siyu Chen, Yujie Zhong, Lin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In person re-identification (ReID) tasks, many works explore the learning of
part features to improve the performance over global image features. Existing
methods explicitly extract part features by either using a hand-designed image
division or keypoints obtained with external visual systems. In this work, we
propose to learn Discriminative implicit Parts (DiPs) which are decoupled from
explicit body parts. Therefore, DiPs can learn to extract any discriminative
features that can benefit in distinguishing identities, which is beyond
predefined body parts (such as accessories). Moreover, we propose a novel
implicit position to give a geometric interpretation for each DiP. The implicit
position can also serve as a learning signal to encourage DiPs to be more
position-equivariant with the identity in the image. Lastly, an additional DiP
weighting is introduced to handle the invisible or occluded situation and
further improve the feature representation of DiPs. Extensive experiments show
that the proposed method achieves state-of-the-art performance on multiple
person ReID benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Open-Set Domain Adaptation with Threshold Self-Tuning and
  Cross-Domain Mixup 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05933v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05933v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghong Liu, Yi Zhou, Tao Zhou, Jie Qin, Shengcai Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-set domain adaptation (OSDA) aims to not only recognize target samples
belonging to common classes shared by source and target domains but also
perceive unknown class samples. Existing OSDA methods suffer from two
obstacles. Firstly, a tedious process of manually tuning a hyperparameter
$threshold$ is required for most OSDA approaches to separate common and unknown
classes. It is difficult to determine a proper threshold when the target domain
data is unlabeled. Secondly, most OSDA methods rely only on confidence values
to distinguish between common and unknown classes, using limited source and
target samples to train models, leading to unsatisfactory performance when the
target domain has mostly unknown classes. Our studies demonstrate that
exploiting multiple criteria within a more continuous latent space is
beneficial for the model's performance. In this paper, we design a novel
threshold self-tuning and cross-domain mixup (TSCM) method to overcome the two
drawbacks. TSCM can automatically tune a proper threshold utilizing unlabeled
target samples rather than manually setting an empirical hyperparameter. Our
method considers multiple criteria instead of only the confidence and uses the
threshold generated by itself to separate common and unknown classes in the
target domain. Moreover, we introduce a cross-domain mixup method designed for
OSDA scenarios to learn domain-invariant features in a more continuous latent
space. Comprehensive experiments illustrate that our method consistently
achieves superior performance on different benchmarks compared with various
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recipro-CAM: Fast gradient-free visual explanations for convolutional
  neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.14074v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.14074v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seok-Yong Byun, Wonju Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Convolutional Neural Network (CNN) is a widely used deep learning
architecture for computer vision. However, its black box nature makes it
difficult to interpret the behavior of the model. To mitigate this issue, AI
practitioners have explored explainable AI methods like Class Activation Map
(CAM) and Grad-CAM. Although these methods have shown promise, they are limited
by architectural constraints or the burden of gradient computing. To overcome
this issue, Score-CAM and Ablation-CAM have been proposed as gradient-free
methods, but they have longer execution times compared to CAM or Grad-CAM based
methods, making them unsuitable for real-world solution though they resolved
gradient related issues and enabled inference mode XAI. To address this
challenge, we propose a fast gradient-free Reciprocal CAM (Recipro-CAM) method.
Our approach involves spatially masking the extracted feature maps to exploit
the correlation between activation maps and network predictions for target
classes. Our proposed method has yielded promising results, outperforming
current state-of-the-art method in the Average Drop-Coherence-Complexity (ADCC)
metric by $1.78 \%$ to $3.72 \%$, excluding VGG-16 backbone. Moreover,
Recipro-CAM generates saliency maps at a similar rate to Grad-CAM and is
approximately $148$ times faster than Score-CAM. The source code for
Recipro-CAM is available in our data analysis framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Y-KD: A Hybrid Approach to Continual Instance Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06015v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06015v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathieu Pagé-Fortin, Brahim Chaib-draa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the success of deep learning methods on instance segmentation, these
models still suffer from catastrophic forgetting in continual learning
scenarios. In this paper, our contributions for continual instance segmentation
are threefold. First, we propose the Y-knowledge distillation (Y-KD), a
knowledge distillation strategy that shares a common feature extractor between
the teacher and student networks. As the teacher is also updated with new data
in Y-KD, the increased plasticity results in new modules that are specialized
on new classes. Second, our Y-KD approach is supported by a dynamic
architecture method that grows new modules for each task and uses all of them
for inference with a unique instance segmentation head, which significantly
reduces forgetting. Third, we complete our approach by leveraging checkpoint
averaging as a simple method to manually balance the trade-off between the
performance on the various sets of classes, thus increasing the control over
the model's behavior without any additional cost. These contributions are
united in our model that we name the Dynamic Y-KD network.
  We perform extensive experiments on several single-step and multi-steps
scenarios on Pascal-VOC, and we show that our approach outperforms previous
methods both on past and new classes. For instance, compared to recent work,
our method obtains +2.1% mAP on old classes in 15-1, +7.6% mAP on new classes
in 19-1 and reaches 91.5% of the mAP obtained by joint-training on all classes
in 15-5.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TriDet: Temporal Action Detection with Relative Boundary Modeling <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingfeng Shi, Yujie Zhong, Qiong Cao, Lin Ma, Jia Li, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a one-stage framework TriDet for temporal action
detection. Existing methods often suffer from imprecise boundary predictions
due to the ambiguous action boundaries in videos. To alleviate this problem, we
propose a novel Trident-head to model the action boundary via an estimated
relative probability distribution around the boundary. In the feature pyramid
of TriDet, we propose an efficient Scalable-Granularity Perception (SGP) layer
to mitigate the rank loss problem of self-attention that takes place in the
video features and aggregate information across different temporal
granularities. Benefiting from the Trident-head and the SGP-based feature
pyramid, TriDet achieves state-of-the-art performance on three challenging
benchmarks: THUMOS14, HACS and EPIC-KITCHEN 100, with lower computational
costs, compared to previous methods. For example, TriDet hits an average mAP of
$69.3\%$ on THUMOS14, outperforming the previous best by $2.5\%$, but with only
$74.6\%$ of its latency. The code is released to
https://github.com/sssste/TriDet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2023; Temporal Action Detection; Temporal Action Localization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PMC-CLIP: Contrastive Language-Image <span class="highlight-title">Pre-train</span>ing using Biomedical
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models trained on large-scale dataset gain a recent surge in CV
and NLP. In contrast, development in biomedical domain lags far behind due to
data scarcity. To address this issue, we build and release PMC-OA, a biomedical
dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess
subset, which is 8 times larger than before. PMC-OA covers diverse modalities
or diseases, with majority of the image-caption samples aligned at
finer-grained level, i.e., subfigure and subcaption. While pretraining a
CLIP-style model on PMC-OA, our model named PMC-CLIP achieves state-of-the-art
results on various downstream tasks, including image-text retrieval on ROCO,
MedMNIST image classification, Medical VQA, i.e. +8.1% R@10 on image-text
retrieval, +3.9% accuracy on image classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Distortion Invariant Representation for Image Restoration from
  A Causality Perspective <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Li, Bingchen Li, Xin Jin, Cuiling Lan, Zhibo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, we have witnessed the great advancement of Deep neural
networks (DNNs) in image restoration. However, a critical limitation is that
they cannot generalize well to real-world degradations with different degrees
or types. In this paper, we are the first to propose a novel training strategy
for image restoration from the causality perspective, to improve the
generalization ability of DNNs for unknown degradations. Our method, termed
Distortion Invariant representation Learning (DIL), treats each distortion type
and degree as one specific confounder, and learns the distortion-invariant
representation by eliminating the harmful confounding effect of each
degradation. We derive our DIL with the back-door criterion in causality by
modeling the interventions of different distortions from the optimization
perspective. Particularly, we introduce counterfactual distortion augmentation
to simulate the virtual distortion types and degrees as the confounders. Then,
we instantiate the intervention of each distortion with a virtual model
updating based on corresponding distorted images, and eliminate them from the
meta-learning perspective. Extensive experiments demonstrate the effectiveness
of our DIL on the generalization capability for unseen distortion types and
degrees. Our code will be available at
https://github.com/lixinustc/Casual-IRDIL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Sentence Grounding in Videos: A <span class="highlight-title">Survey</span> and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.08071v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.08071v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhang, Aixin Sun, Wei Jing, Joey Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal sentence grounding in videos (TSGV), \aka natural language video
localization (NLVL) or video moment retrieval (VMR), aims to retrieve a
temporal moment that semantically corresponds to a language query from an
untrimmed video. Connecting computer vision and natural language, TSGV has
drawn significant attention from researchers in both communities. This survey
attempts to provide a summary of fundamental concepts in TSGV and current
research status, as well as future research directions. As the background, we
present a common structure of functional components in TSGV, in a tutorial
style: from feature extraction from raw video and language query, to answer
prediction of the target moment. Then we review the techniques for multimodal
understanding and interaction, which is the key focus of TSGV for effective
alignment between the two modalities. We construct a taxonomy of TSGV
techniques and elaborate the methods in different categories with their
strengths and weaknesses. Lastly, we discuss issues with the current TSGV
research and share our insights about promising research directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Making a Trojan-horse Attack on Text-to-Image Retrieval <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.03861v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.03861v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Hu, Aozhu Chen, Xirong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning based image retrieval is reported to be vulnerable to
adversarial attacks, existing works are mainly on image-to-image retrieval with
their attacks performed at the front end via query modification. By contrast,
we present in this paper the first study about a threat that occurs at the back
end of a text-to-image retrieval (T2IR) system. Our study is motivated by the
fact that the image collection indexed by the system will be regularly updated
due to the arrival of new images from various sources such as web crawlers and
advertisers. With malicious images indexed, it is possible for an attacker to
indirectly interfere with the retrieval process, letting users see certain
images that are completely irrelevant w.r.t. their queries. We put this thought
into practice by proposing a novel Trojan-horse attack (THA). In particular, we
construct a set of Trojan-horse images by first embedding word-specific
adversarial information into a QR code and then putting the code on benign
advertising images. A proof-of-concept evaluation, conducted on two popular
T2IR datasets (Flickr30k and MS-COCO), shows the effectiveness of the proposed
THA in a white-box mode.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ETMA: Efficient <span class="highlight-title">Transformer</span> Based Multilevel Attention framework for
  Multimodal Fake News Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.07331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.07331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashima Yadav, Shivani Gaba, Haneef Khan, Ishan Budhiraja, Akansha Singh, Krishan Kant Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this new digital era, social media has created a severe impact on the
lives of people. In recent times, fake news content on social media has become
one of the major challenging problems for society. The dissemination of
fabricated and false news articles includes multimodal data in the form of text
and images. The previous methods have mainly focused on unimodal analysis.
Moreover, for multimodal analysis, researchers fail to keep the unique
characteristics corresponding to each modality. This paper aims to overcome
these limitations by proposing an Efficient Transformer based Multilevel
Attention (ETMA) framework for multimodal fake news detection, which comprises
the following components: visual attention-based encoder, textual
attention-based encoder, and joint attention-based learning. Each component
utilizes the different forms of attention mechanism and uniquely deals with
multimodal data to detect fraudulent content. The efficacy of the proposed
network is validated by conducting several experiments on four real-world fake
news datasets: Twitter, Jruvika Fake News Dataset, Pontes Fake News Dataset,
and Risdal Fake News Dataset using multiple evaluation metrics. The results
show that the proposed method outperforms the baseline methods on all four
datasets. Further, the computation time of the model is also lower than the
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Transactions on Computational Social
  Systems</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-12T00:00:00Z">2023-03-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ P-MMF: Provider Max-min Fairness Re-ranking in Recommender System <span class="chip">WWW23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Xu, Sirui Chen, Jun Xu, Weiran Shen, Xiao Zhang, Gang Wang, Zhenghua Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the issue of recommending fairly from the aspect of
providers, which has become increasingly essential in multistakeholder
recommender systems. Existing studies on provider fairness usually focused on
designing proportion fairness (PF) metrics that first consider systematic
fairness. However, sociological researches show that to make the market more
stable, max-min fairness (MMF) is a better metric. The main reason is that MMF
aims to improve the utility of the worst ones preferentially, guiding the
system to support the providers in weak market positions. When applying MMF to
recommender systems, how to balance user preferences and provider fairness in
an online recommendation scenario is still a challenging problem. In this
paper, we proposed an online re-ranking model named Provider Max-min Fairness
Re-ranking (P-MMF) to tackle the problem. Specifically, P-MMF formulates
provider fair recommendation as a resource allocation problem, where the
exposure slots are considered the resources to be allocated to providers and
the max-min fairness is used as the regularizer during the process. We show
that the problem can be further represented as a regularized online optimizing
problem and solved efficiently in its dual space. During the online re-ranking
phase, a momentum gradient descent method is designed to conduct the dynamic
re-ranking. Theoretical analysis showed that the regret of P-MMF can be
bounded. Experimental results on four public recommender datasets demonstrated
that P-MMF can outperformed the state-of-the-art baselines. Experimental
results also show that P-MMF can retain small computationally costs on a corpus
with the large number of items.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in WWW23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoDenoise: Automatic Data Instance Denoising for Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weilin Lin, Xiangyu Zhao, Yejing Wang, Yuanshao Zhu, Wanyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Historical user-item interaction datasets are essential in training modern
recommender systems for predicting user preferences. However, the arbitrary
user behaviors in most recommendation scenarios lead to a large volume of noisy
data instances being recorded, which cannot fully represent their true
interests. While a large number of denoising studies are emerging in the
recommender system community, all of them suffer from highly dynamic data
distributions. In this paper, we propose a Deep Reinforcement Learning (DRL)
based framework, AutoDenoise, with an Instance Denoising Policy Network, for
denoising data instances with an instance selection manner in deep recommender
systems. To be specific, AutoDenoise serves as an agent in DRL to adaptively
select noise-free and predictive data instances, which can then be utilized
directly in training representative recommendation models. In addition, we
design an alternate two-phase optimization strategy to train and validate the
AutoDenoise properly. In the searching phase, we aim to train the policy
network with the capacity of instance denoising; in the validation phase, we
find out and evaluate the denoised subset of data instances selected by the
trained policy network, so as to validate its denoising ability. We conduct
extensive experiments to validate the effectiveness of AutoDenoise combined
with multiple representative recommender system models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, 5 tables, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MobileRec: A Large-Scale <span class="highlight-title">Dataset</span> for Mobile Apps Recommendation <span class="chip">SIGIR'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. H. Maqbool, Umar Farooq, Adib Mosharrof, A. B. Siddique, Hassan Foroosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have become ubiquitous in our digital lives, from
recommending products on e-commerce websites to suggesting movies and music on
streaming platforms. Existing recommendation datasets, such as Amazon Product
Reviews and MovieLens, greatly facilitated the research and development of
recommender systems in their respective domains. While the number of mobile
users and applications (aka apps) has increased exponentially over the past
decade, research in mobile app recommender systems has been significantly
constrained, primarily due to the lack of high-quality benchmark datasets, as
opposed to recommendations for products, movies, and news. To facilitate
research for app recommendation systems, we introduce a large-scale dataset,
called MobileRec. We constructed MobileRec from users' activity on the Google
play store. MobileRec contains 19.3 million user interactions (i.e., user
reviews on apps) with over 10K unique apps across 48 categories. MobileRec
records the sequential activity of a total of 0.7 million distinct users. Each
of these users has interacted with no fewer than five distinct apps, which
stands in contrast to previous datasets on mobile apps that recorded only a
single interaction per user. Furthermore, MobileRec presents users' ratings as
well as sentiments on installed apps, and each app contains rich metadata such
as app name, category, description, and overall rating, among others. We
demonstrate that MobileRec can serve as an excellent testbed for app
recommendation through a comparative study of several state-of-the-art
recommendation approaches. The quantitative results can act as a baseline for
other researchers to compare their results against. The MobileRec dataset is
available at https://huggingface.co/datasets/recmeapp/mobilerec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 tables, 4 figures, Under submission at SIGIR'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proactive Prioritization of App Issues via Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moghis Fereidouni, Adib Mosharrof, Umar Farooq, AB Siddique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile app stores produce a tremendous amount of data in the form of user
reviews, which is a huge source of user requirements and sentiments; such
reviews allow app developers to proactively address issues in their apps.
However, only a small number of reviews capture common issues and sentiments
which creates a need for automatically identifying prominent reviews.
Unfortunately, most existing work in text ranking and popularity prediction
focuses on social contexts where other signals are available, which renders
such works ineffective in the context of app reviews. In this work, we propose
a new framework, PPrior, that enables proactive prioritization of app issues
through identifying prominent reviews (ones predicted to receive a large number
of votes in a given time window). Predicting highly-voted reviews is
challenging given that, unlike social posts, social network features of users
are not available. Moreover, there is an issue of class imbalance, since a
large number of user reviews receive little to no votes. PPrior employs a
pre-trained T5 model and works in three phases. Phase one adapts the
pre-trained T5 model to the user reviews data in a self-supervised fashion. In
phase two, we leverage contrastive training to learn a generic and
task-independent representation of user reviews. Phase three uses radius
neighbors classifier t o m ake t he final predictions. This phase also uses
FAISS index for scalability and efficient search. To conduct extensive
experiments, we acquired a large dataset of over 2.1 million user reviews from
Google Play. Our experimental results demonstrate the effectiveness of the
proposed framework when compared against several state-of-the-art approaches.
Moreover, the accuracy of PPrior in predicting prominent reviews is comparable
to that of experienced app developers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2022 IEEE International Conference on Big Data (Big Data)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Know Your Contextual Search Intent: A <span class="highlight-title">Prompt</span>ing
  Framework for Conversational Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kelong Mao, Zhicheng Dou, Haonan Chen, Fengran Mo, Hongjin Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a prompting framework called LLMCS that leverages
large language models, such as code-davinci-002 of GPT-3, to perform few-shot
conversational query rewriting for conversational search. We explore three
prompting methods to generate multiple query rewrites and hypothetical
responses, and propose aggregating them into an integrated representation that
can robustly represent the user's real contextual search intent. Experimental
results on two conversational search datasets, including CAst-19 and CAsT-20,
show that our approach achieves significant improvements in search
effectiveness over existing baselines and manual rewrites. Notably, LLMCS can
significantly outperform the state-of-the-art baselines by up to +5.9\% and
+32.9\% w.r.t. NDCG@3 on CAsT-19 and CAsT-20, highlighting the vast potential
of large language models for conversational search. Our code will be released
at https://github.com/kyriemao/LLMCS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attention over Self-attention:Intention-aware Re-ranking with Dynamic
  <span class="highlight-title">Transformer</span> Encoders for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.05333v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.05333v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyi Lin, Sheng Zang, Rundong Wang, Zhu Sun, J. Senthilnath, Chi Xu, Chee-Keong Kwoh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Re-ranking models refine item recommendation lists generated by the prior
global ranking model, which have demonstrated their effectiveness in improving
the recommendation quality. However, most existing re-ranking solutions only
learn from implicit feedback with a shared prediction model, which regrettably
ignore inter-item relationships under diverse user intentions. In this paper,
we propose a novel Intention-aware Re-ranking Model with Dynamic Transformer
Encoder (RAISE), aiming to perform user-specific prediction for each individual
user based on her intentions. Specifically, we first propose to mine latent
user intentions from text reviews with an intention discovering module (IDM).
By differentiating the importance of review information with a co-attention
network, the latent user intention can be explicitly modeled for each user-item
pair. We then introduce a dynamic transformer encoder (DTE) to capture
user-specific inter-item relationships among item candidates by seamlessly
accommodating the learned latent user intentions via IDM. As such, one can not
only achieve more personalized recommendations but also obtain corresponding
explanations by constructing RAISE upon existing recommendation engines.
Empirical study on four public datasets shows the superiority of our proposed
RAISE, with up to 13.95%, 9.60%, and 13.03% relative improvements evaluated by
Precision@5, MAP@5, and NDCG@5 respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TKDE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COMET: Convolutional Dimension Interaction for Collaborative Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2007.14129v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2007.14129v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyi Lin, Lei Feng, Xingzhi Guo, Yu Zhang, Rui Yin, Chee Keong Kwoh, Chi Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation learning-based recommendation models play a dominant role
among recommendation techniques. However, most of the existing methods assume
both historical interactions and embedding dimensions are independent of each
other, and thus regrettably ignore the high-order interaction information among
historical interactions and embedding dimensions. In this paper, we propose a
novel representation learning-based model called COMET (COnvolutional diMEnsion
inTeraction), which simultaneously models the high-order interaction patterns
among historical interactions and embedding dimensions. To be specific, COMET
stacks the embeddings of historical interactions horizontally at first, which
results in two "embedding maps". In this way, internal interactions and
dimensional interactions can be exploited by convolutional neural networks
(CNN) with kernels of different sizes simultaneously. A fully-connected
multi-layer perceptron (MLP) is then applied to obtain two interaction vectors.
Lastly, the representations of users and items are enriched by the learnt
interaction vectors, which can further be used to produce the final prediction.
Extensive experiments and ablation studies on various public implicit feedback
datasets clearly demonstrate the effectiveness and rationality of our proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM TIST</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Dataset</span> for Learning Graph Representations to Predict Customer Returns
  in Fashion Retail <span class="chip">RecSys 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14096v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14096v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamie McGowan, Elizabeth Guest, Ziyang Yan, Cong Zheng, Neha Patel, Mason Cusack, Charlie Donaldson, Sofie de Cnudde, Gabriel Facini, Fabon Dzogang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel dataset collected by ASOS (a major online fashion
retailer) to address the challenge of predicting customer returns in a fashion
retail ecosystem. With the release of this substantial dataset we hope to
motivate further collaboration between research communities and the fashion
industry. We first explore the structure of this dataset with a focus on the
application of Graph Representation Learning in order to exploit the natural
data structure and provide statistical insights into particular features within
the data. In addition to this, we show examples of a return prediction
classification task with a selection of baseline models (i.e. with no
intermediate representation learning step) and a graph representation based
model. We show that in a downstream return prediction classification task, an
F1-score of 0.792 can be found using a Graph Neural Network (GNN), improving
upon other models discussed in this work. Alongside this increased F1-score, we
also present a lower cross-entropy loss by recasting the data into a graph
structure, indicating more robust predictions from a GNN based solution. These
results provide evidence that GNNs could provide more impactful and usable
classifications than other baseline models on the presented dataset and with
this motivation, we hope to encourage further research into graph-based
approaches using the ASOS GraphReturns dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The ASOS GraphReturns dataset can be found at https://osf.io/c793h/.
  Accepted at FashionXRecSys 2022 workshop. Published Version</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Asynchronous Decentralized Federated Lifelong Learning for Landmark
  Localization in Medical Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyao Zheng, Michael A. Jacobs, Vladimir Braverman, Vishwa S. Parekh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a recent development in the machine learning area that
allows a system of devices to train on one or more tasks without sharing their
data to a single location or device. However, this framework still requires a
centralized global model to consolidate individual models into one, and the
devices train synchronously, which both can be potential bottlenecks for using
federated learning. In this paper, we propose a novel method of asynchronous
decentralized federated lifelong learning (ADFLL) method that inherits the
merits of federated learning and can train on multiple tasks simultaneously
without the need for a central node or synchronous training. Thus, overcoming
the potential drawbacks of conventional federated learning. We demonstrate
excellent performance on the brain tumor segmentation (BRATS) dataset for
localizing the left ventricle on multiple image sequences and image
orientation. Our framework allows agents to achieve the best performance with a
mean distance error of 7.81, better than the conventional all-knowing agent's
mean distance error of 11.78, and significantly (p=0.01) better than a
conventional lifelong learning agent with a distance error of 15.17 after eight
rounds of training. In addition, all ADFLL agents have comparable or better
performance than a conventional LL agent. In conclusion, we developed an ADFLL
framework with excellent performance and speed-up compared to conventional RL
agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AidUI: Toward Automated Recognition of Dark Patterns in User Interfaces <span class="chip">ICSE 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SM Hasan Mansur, Sabiha Salma, Damilola Awofisayo, Kevin Moran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Past studies have illustrated the prevalence of UI dark patterns, or user
interfaces that can lead end-users toward (unknowingly) taking actions that
they may not have intended. Such deceptive UI designs can result in adverse
effects on end users, such as oversharing personal information or financial
loss. While significant research progress has been made toward the development
of dark pattern taxonomies, developers and users currently lack guidance to
help recognize, avoid, and navigate these often subtle design motifs. However,
automated recognition of dark patterns is a challenging task, as the
instantiation of a single type of pattern can take many forms, leading to
significant variability.
  In this paper, we take the first step toward understanding the extent to
which common UI dark patterns can be automatically recognized in modern
software applications. To do this, we introduce AidUI, a novel automated
approach that uses computer vision and natural language processing techniques
to recognize a set of visual and textual cues in application screenshots that
signify the presence of ten unique UI dark patterns, allowing for their
detection, classification, and localization. To evaluate our approach, we have
constructed ContextDP, the current largest dataset of fully-localized UI dark
patterns that spans 175 mobile and 83 web UI screenshots containing 301 dark
pattern instances. The results of our evaluation illustrate that \AidUI
achieves an overall precision of 0.66, recall of 0.67, F1-score of 0.65 in
detecting dark pattern instances, reports few false positives, and is able to
localize detected patterns with an IoU score of ~0.84. Furthermore, a
significant subset of our studied dark patterns can be detected quite reliably
(F1 score of over 0.82), and future research directions may allow for improved
detection of additional patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, Accepted at The 45th IEEE/ACM International Conference on
  Software Engineering (ICSE 2023), Melbourne, Australia, May 14th-20th, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Module-Wise Network Quantization for 6D Object Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saqib Javed, Andrew Price, Yinlin Hu, Mathieu Salzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many edge applications, such as collaborative robotics and spacecraft
rendezvous, can benefit from 6D object pose estimation, but must do so on
embedded platforms. Unfortunately, existing 6D pose estimation networks are
typically too large for deployment in such situations and must therefore be
compressed, while maintaining reliable performance. In this work, we present an
approach to doing so by quantizing such networks. More precisely, we introduce
a module-wise quantization strategy that, in contrast to uniform and
mixed-precision quantization, accounts for the modular structure of typical 6D
pose estimation frameworks. We demonstrate that uniquely compressing these
modules outperforms uniform and mixed-precision quantization techniques.
Moreover, our experiments evidence that module-wise quantization can lead to a
significant accuracy boost. We showcase the generality of our approach using
different datasets, quantization methodologies, and network architectures,
including the recent ZebraPose.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-tuning Strategies for Faster Inference using Speech <span class="highlight-title">Self-Supervised</span>
  Models: A Comparative Study <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salah Zaiem, Robin Algayres, Titouan Parcollet, Slim Essid, Mirco Ravanelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) has allowed substantial progress in Automatic
Speech Recognition (ASR) performance in low-resource settings. In this context,
it has been demonstrated that larger self-supervised feature extractors are
crucial for achieving lower downstream ASR error rates. Thus, better
performance might be sanctioned with longer inferences. This article explores
different approaches that may be deployed during the fine-tuning to reduce the
computations needed in the SSL encoder, leading to faster inferences. We adapt
a number of existing techniques to common ASR settings and benchmark them,
displaying performance drops and gains in inference times. Interestingly, we
found that given enough downstream data, a simple downsampling of the input
sequences outperforms the other methods with both low performance drops and
high computational savings, reducing computations by 61.3% with an WER increase
of only 0.81. Finally, we analyze the robustness of the comparison to changes
in dataset conditions, revealing sensitivity to dataset size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP "Self-supervision in Audio, Speech and Beyond"
  workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global Optimality of Elman-type RNN in the Mean-Field Regime 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Agazzi, Jianfeng Lu, Sayan Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze Elman-type Recurrent Reural Networks (RNNs) and their training in
the mean-field regime. Specifically, we show convergence of gradient descent
training dynamics of the RNN to the corresponding mean-field formulation in the
large width limit. We also show that the fixed points of the limiting
infinite-width dynamics are globally optimal, under some assumptions on the
initialization of the weights. Our results establish optimality for
feature-learning with wide RNNs in the mean-field regime
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge-integrated AutoEncoder Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teddy Lazebnik, Liron Simon-Keren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data encoding is a common and central operation in most data analysis tasks.
The performance of other models, downstream in the computational process,
highly depends on the quality of data encoding. One of the most powerful ways
to encode data is using the neural network AutoEncoder (AE) architecture.
However, the developers of AE are not able to easily influence the produced
embedding space, as it is usually treated as a \textit{black box} technique,
which makes it uncontrollable and not necessarily has desired properties for
downstream tasks. In this paper, we introduce a novel approach for developing
AE models that can integrate external knowledge sources into the learning
process, possibly leading to more accurate results. The proposed
\methodNamefull{} (\methodName{}) model is able to leverage domain-specific
information to make sure the desired distance and neighborhood properties
between samples are preservative in the embedding space. The proposed model is
evaluated on three large-scale datasets from three different scientific fields
and is compared to nine existing encoding models. The results demonstrate that
the \methodName{} model effectively captures the underlying structures and
relationships between the input data and external knowledge, meaning it
generates a more useful representation. This leads to outperforming the rest of
the models in terms of reconstruction accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decision Making for Human-in-the-loop Robotic Agents via
  Uncertainty-Aware Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Singi, Zhanpeng He, Alvin Pan, Sandip Patel, Gunnar A. Sigurdsson, Robinson Piramuthu, Shuran Song, Matei Ciocarlie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a Human-in-the-Loop paradigm, a robotic agent is able to act mostly
autonomously in solving a task, but can request help from an external expert
when needed. However, knowing when to request such assistance is critical: too
few requests can lead to the robot making mistakes, but too many requests can
overload the expert. In this paper, we present a Reinforcement Learning based
approach to this problem, where a semi-autonomous agent asks for external
assistance when it has low confidence in the eventual success of the task. The
confidence level is computed by estimating the variance of the return from the
current state. We show that this estimate can be iteratively improved during
training using a Bellman-like recursion. On discrete navigation problems with
both fully- and partially-observable state information, we show that our method
makes effective use of a limited budget of expert calls at run-time, despite
having no access to the expert at training time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Branch & Learn with Post-hoc Correction for Predict+Optimize with
  Unknown Parameters in Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Hu, Jasper C. H. Lee, Jimmy H. M. Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining machine learning and constrained optimization, Predict+Optimize
tackles optimization problems containing parameters that are unknown at the
time of solving. Prior works focus on cases with unknowns only in the
objectives. A new framework was recently proposed to cater for unknowns also in
constraints by introducing a loss function, called Post-hoc Regret, that takes
into account the cost of correcting an unsatisfiable prediction. Since Post-hoc
Regret is non-differentiable, the previous work computes only its
approximation. While the notion of Post-hoc Regret is general, its specific
implementation is applicable to only packing and covering linear programming
problems. In this paper, we first show how to compute Post-hoc Regret exactly
for any optimization problem solvable by a recursive algorithm satisfying
simple conditions. Experimentation demonstrates substantial improvement in the
quality of solutions as compared to the earlier approximation approach.
Furthermore, we show experimentally the empirical behavior of different
combinations of correction and penalty functions used in the Post-hoc Regret of
the same benchmarks. Results provide insights for defining the appropriate
Post-hoc Regret in different application scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Holistic Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.15829v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.15829v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitris Bertsimas, Kimberly Villalobos Carballo, Léonard Boussioux, Michael Lingzhi Li, Alex Paskov, Ivan Paskov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel holistic deep learning framework that
simultaneously addresses the challenges of vulnerability to input
perturbations, overparametrization, and performance instability from different
train-validation splits. The proposed framework holistically improves accuracy,
robustness, sparsity, and stability over standard deep learning models, as
demonstrated by extensive experiments on both tabular and image data sets. The
results are further validated by ablation experiments and SHAP value analysis,
which reveal the interactions and trade-offs between the different evaluation
metrics. To support practitioners applying our framework, we provide a
prescriptive approach that offers recommendations for selecting an appropriate
training loss function based on their specific objectives. All the code to
reproduce the results can be found at https://github.com/kimvc7/HDL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An FPGA-Based On-Device Reinforcement Learning Approach using Online
  Sequential Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2005.04646v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2005.04646v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hirohisa Watanabe, Mineto Tsukada, Hiroki Matsutani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DQN (Deep Q-Network) is a method to perform Q-learning for reinforcement
learning using deep neural networks. DQNs require a large buffer and batch
processing for an experience replay and rely on a backpropagation based
iterative optimization, making them difficult to be implemented on
resource-limited edge devices. In this paper, we propose a lightweight
on-device reinforcement learning approach for low-cost FPGA devices. It
exploits a recently proposed neural-network based on-device learning approach
that does not rely on the backpropagation method but uses OS-ELM (Online
Sequential Extreme Learning Machine) based training algorithm. In addition, we
propose a combination of L2 regularization and spectral normalization for the
on-device reinforcement learning so that output values of the neural network
can be fit into a certain range and the reinforcement learning becomes stable.
The proposed reinforcement learning approach is designed for PYNQ-Z1 board as a
low-cost FPGA platform. The evaluation results using OpenAI Gym demonstrate
that the proposed algorithm and its FPGA implementation complete a CartPole-v0
task 29.77x and 89.40x faster than a conventional DQN-based approach when the
number of hidden-layer nodes is 64.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RAW'21</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentiable Uncalibrated Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sidharth Gupta, Konik Kothari, Valentin Debarnot, Ivan Dokmanić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a differentiable imaging framework to address uncertainty in
measurement coordinates such as sensor locations and projection angles. We
formulate the problem as measurement interpolation at unknown nodes supervised
through the forward operator. To solve it we apply implicit neural networks,
also known as neural fields, which are naturally differentiable with respect to
the input coordinates. We also develop differentiable spline interpolators
which perform as well as neural networks, require less time to optimize and
have well-understood properties. Differentiability is key as it allows us to
jointly fit a measurement representation, optimize over the uncertain
measurement coordinates, and perform image reconstruction which in turn ensures
consistent calibration. We apply our approach to 2D and 3D computed tomography
and show that it produces improved reconstructions compared to baselines that
do not account for the lack of calibration. The flexibility of the proposed
framework makes it easy to apply to almost arbitrary imaging problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LOCUS: A Novel Decomposition Method for Brain Network Connectivity
  Matrices using Low-rank Structure with Uniform Sparsity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2008.08915v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2008.08915v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikai Wang, Ying Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Network-oriented research has been increasingly popular in many scientific
areas. In neuroscience research, imaging-based network connectivity measures
have become the key for understanding brain organizations, potentially serving
as individual neural fingerprints. There are major challenges in analyzing
connectivity matrices including the high dimensionality of brain networks,
unknown latent sources underlying the observed connectivity, and the large
number of brain connections leading to spurious findings. In this paper, we
propose a novel blind source separation method with low-rank structure and
uniform sparsity (LOCUS) as a fully data-driven decomposition method for
network measures. Compared with the existing method that vectorizes
connectivity matrices ignoring brain network topology, LOCUS achieves more
efficient and accurate source separation for connectivity matrices using
low-rank structure. We propose a novel angle-based uniform sparsity
regularization that demonstrates better performance than the existing sparsity
controls for low-rank tensor methods. We propose a highly efficient iterative
Node-Rotation algorithm that exploits the block multi-convexity of the
objective function to solve the non-convex optimization problem for learning
LOCUS. We illustrate the advantage of LOCUS through extensive simulation
studies. Application of LOCUS to Philadelphia Neurodevelopmental Cohort
neuroimaging study reveals biologically insightful connectivity traits which
are not found using the existing method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Disparate Impact of Uncertainty: Affirmative Action vs. Affirmative
  Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.10019v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.10019v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claire Lazar Reich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Critical decisions like loan approvals, medical interventions, and college
admissions are guided by predictions made in the presence of uncertainty. In
this paper, we prove that uncertainty has a disparate impact. While it imparts
errors across all demographic groups, the types of errors vary systematically:
Groups with higher average outcomes are typically assigned higher false
positive rates, while those with lower average outcomes are assigned higher
false negative rates. We show that additional data acquisition can eliminate
the disparity and broaden access to opportunity. The strategy, which we call
Affirmative Information, could stand as an alternative to Affirmative Action.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08253v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08253v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingang Qu, Thibault Faney, Ze Wang, Patrick Gallinari, Soleiman Yousef, Jean-Charles de Hemptinne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to domain shift, machine learning systems typically fail to generalize
well to domains different from those of training data, which is what domain
generalization (DG) aims to address. Although various DG methods have been
developed, most of them lack interpretability and require domain labels that
are not available in many real-world scenarios. This paper presents a novel DG
method, called HMOE: Hypernetwork-based Mixture of Experts (MoE), which does
not rely on domain labels and is more interpretable. MoE proves effective in
identifying heterogeneous patterns in data. For the DG problem, heterogeneity
arises exactly from domain shift. HMOE uses hypernetworks taking vectors as
input to generate experts' weights, which allows experts to share useful
meta-knowledge and enables exploring experts' similarities in a low-dimensional
vector space. We compare HMOE with other DG algorithms under a fair and unified
benchmark-DomainBed. Our extensive experiments show that HMOE can divide
mixed-domain data into distinct clusters that are surprisingly more consistent
with human intuition than original domain labels. Compared to other DG methods,
HMOE shows competitive performance and achieves SOTA results in some cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Continual Test-Time Adaptation for Contextual and Semantic
  Domain Shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.08767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.08767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tommie Kerssies, Mert Kılıçkaya, Joaquin Vanschoren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, our goal is to adapt a pre-trained convolutional neural
network to domain shifts at test time. We do so continually with the incoming
stream of test batches, without labels. The existing literature mostly operates
on artificial shifts obtained via adversarial perturbations of a test image.
Motivated by this, we evaluate the state of the art on two realistic and
challenging sources of domain shifts, namely contextual and semantic shifts.
Contextual shifts correspond to the environment types, for example, a model
pre-trained on indoor context has to adapt to the outdoor context on CORe-50.
Semantic shifts correspond to the capture types, for example a model
pre-trained on natural images has to adapt to cliparts, sketches, and paintings
on DomainNet. We include in our analysis recent techniques such as
Prediction-Time Batch Normalization (BN), Test Entropy Minimization (TENT) and
Continual Test-Time Adaptation (CoTTA). Our findings are three-fold: i)
Test-time adaptation methods perform better and forget less on contextual
shifts compared to semantic shifts, ii) TENT outperforms other methods on
short-term adaptation, whereas CoTTA outpeforms other methods on long-term
adaptation, iii) BN is most reliable and robust. Our code is available at
https://github.com/tommiekerssies/Evaluating-Continual-Test-Time-Adaptation-for-Contextual-and-Semantic-Domain-Shifts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How I Learned to Stop Worrying and Love Retraining <span class="chip">ICLR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.00843v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.00843v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Zimmer, Christoph Spiegel, Sebastian Pokutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many Neural Network Pruning approaches consist of several iterative training
and pruning steps, seemingly losing a significant amount of their performance
after pruning and then recovering it in the subsequent retraining phase. Recent
works of Renda et al. (2020) and Le & Hua (2021) demonstrate the significance
of the learning rate schedule during the retraining phase and propose specific
heuristics for choosing such a schedule for IMP (Han et al., 2015). We place
these findings in the context of the results of Li et al. (2020) regarding the
training of models within a fixed training budget and demonstrate that,
consequently, the retraining phase can be massively shortened using a simple
linear learning rate schedule. Improving on existing retraining approaches, we
additionally propose a method to adaptively select the initial value of the
linear schedule. Going a step further, we propose similarly imposing a budget
on the initial dense training phase and show that the resulting simple and
efficient method is capable of outperforming significantly more complex or
heavily parameterized state-of-the-art approaches that attempt to sparsify the
network during training. These findings not only advance our understanding of
the retraining phase, but more broadly question the belief that one should aim
to avoid the need for retraining and reduce the negative effects of 'hard'
pruning by incorporating the sparsification process into the standard training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR2023 camera-ready version, 9 pages main text, 34 pages appendix,
  2 tables, 3 figures in main text</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A large-scale and PCR-referenced vocal audio <span class="highlight-title">dataset</span> for COVID-19 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07738v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07738v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jobie Budd, Kieran Baker, Emma Karoune, Harry Coppock, Selina Patel, Ana Tendero Cañadas, Alexander Titcomb, Richard Payne, David Hurley, Sabrina Egglestone, Lorraine Butler, Jonathon Mellor, George Nicholson, Ivan Kiskin, Vasiliki Koutra, Radka Jersakova, Rachel A. McKendry, Peter Diggle, Sylvia Richardson, Björn W. Schuller, Steven Gilmour, Davide Pigoli, Stephen Roberts, Josef Packham, Tracey Thornley, Chris Holmes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The UK COVID-19 Vocal Audio Dataset is designed for the training and
evaluation of machine learning models that classify SARS-CoV-2 infection status
or associated respiratory symptoms using vocal audio. The UK Health Security
Agency recruited voluntary participants through the national Test and Trace
programme and the REACT-1 survey in England from March 2021 to March 2022,
during dominant transmission of the Alpha and Delta SARS-CoV-2 variants and
some Omicron variant sublineages. Audio recordings of volitional coughs,
exhalations, and speech were collected in the 'Speak up to help beat
coronavirus' digital survey alongside demographic, self-reported symptom and
respiratory condition data, and linked to SARS-CoV-2 test results. The UK
COVID-19 Vocal Audio Dataset represents the largest collection of SARS-CoV-2
PCR-referenced audio recordings to date. PCR results were linked to 70,794 of
72,999 participants and 24,155 of 25,776 positive cases. Respiratory symptoms
were reported by 45.62% of participants. This dataset has additional potential
uses for bioacoustics research, with 11.30% participants reporting asthma, and
27.20% with linked influenza PCR test results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COMET: Convolutional Dimension Interaction for Collaborative Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2007.14129v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2007.14129v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyi Lin, Lei Feng, Xingzhi Guo, Yu Zhang, Rui Yin, Chee Keong Kwoh, Chi Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation learning-based recommendation models play a dominant role
among recommendation techniques. However, most of the existing methods assume
both historical interactions and embedding dimensions are independent of each
other, and thus regrettably ignore the high-order interaction information among
historical interactions and embedding dimensions. In this paper, we propose a
novel representation learning-based model called COMET (COnvolutional diMEnsion
inTeraction), which simultaneously models the high-order interaction patterns
among historical interactions and embedding dimensions. To be specific, COMET
stacks the embeddings of historical interactions horizontally at first, which
results in two "embedding maps". In this way, internal interactions and
dimensional interactions can be exploited by convolutional neural networks
(CNN) with kernels of different sizes simultaneously. A fully-connected
multi-layer perceptron (MLP) is then applied to obtain two interaction vectors.
Lastly, the representations of users and items are enriched by the learnt
interaction vectors, which can further be used to produce the final prediction.
Extensive experiments and ablation studies on various public implicit feedback
datasets clearly demonstrate the effectiveness and rationality of our proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM TIST</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LUKE-Graph: A <span class="highlight-title">Transformer</span>-based Approach with Gated Relational Graph
  Attention for Cloze-style Reading Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shima Foolad, Kourosh Kiani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating prior knowledge can improve existing pre-training models in
cloze-style machine reading and has become a new trend in recent studies.
Notably, most of the existing models have integrated external knowledge graphs
(KG) and transformer-based models, such as BERT into a unified data structure.
However, selecting the most relevant ambiguous entities in KG and extracting
the best subgraph remains a challenge. In this paper, we propose the
LUKE-Graph, a model that builds a heterogeneous graph based on the intuitive
relationships between entities in a document without using any external KG. We
then use a Relational Graph Attention (RGAT) network to fuse the graph's
reasoning information and the contextual representation encoded by the
pre-trained LUKE model. In this way, we can take advantage of LUKE, to derive
an entity-aware representation; and a graph model - to exploit relation-aware
representation. Moreover, we propose Gated-RGAT by augmenting RGAT with a
gating mechanism that regulates the question information for the graph
convolution operation. This is very similar to human reasoning processing
because they always choose the best entity candidate based on the question
information. Experimental results demonstrate that the LUKE-Graph achieves
state-of-the-art performance on the ReCoRD dataset with commonsense reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted for neurocomputing journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fuzzy Alignments in Directed Acyclic Graph for Non-Autoregressive
  Machine Translation <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengrui Ma, Chenze Shao, Shangtong Gui, Min Zhang, Yang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-autoregressive translation (NAT) reduces the decoding latency but suffers
from performance degradation due to the multi-modality problem. Recently, the
structure of directed acyclic graph has achieved great success in NAT, which
tackles the multi-modality problem by introducing dependency between vertices.
However, training it with negative log-likelihood loss implicitly requires a
strict alignment between reference tokens and vertices, weakening its ability
to handle multiple translation modalities. In this paper, we hold the view that
all paths in the graph are fuzzily aligned with the reference sentence. We do
not require the exact alignment but train the model to maximize a fuzzy
alignment score between the graph and reference, which takes captured
translations in all modalities into account. Extensive experiments on major WMT
benchmarks show that our method substantially improves translation performance
and increases prediction confidence, setting a new state of the art for NAT on
the raw training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MWE as WSD: Solving Multiword Expression Identification with Word Sense
  Disambiguation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Tanner, Jacob Hoffman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work in word sense disambiguation (WSD) utilizes encodings of the
sense gloss (definition text), in addition to the input words and context, to
improve performance. In this work we demonstrate that this approach can be
adapted for use in multiword expression (MWE) identification by training a
Bi-encoder model which uses gloss and context information to filter MWE
candidates produced from a simple rule-based extraction pipeline. We achieve
state-of-the-art results in MWE identification on the DiMSUM dataset, and
competitive results on the PARSEME 1.1 English dataset using this method. Our
model also retains most of its ability to perform WSD, demonstrating that a
single model can successfully be applied to both of these tasks. Additionally,
we experiment with applying Poly-encoder models to MWE identification and WSD,
introducing a modified Poly-encoder architecture which outperforms the standard
Poly-encoder on these tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving the Intent Classification accuracy in Noisy Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Nabih Ali, Alessio Brutti, Daniele Falavigna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intent classification is a fundamental task in the spoken language
understanding field that has recently gained the attention of the scientific
community, mainly because of the feasibility of approaching it with end-to-end
neural models. In this way, avoiding using intermediate steps, i.e. automatic
speech recognition, is possible, thus the propagation of errors due to
background noise, spontaneous speech, speaking styles of users, etc. Towards
the development of solutions applicable in real scenarios, it is interesting to
investigate how environmental noise and related noise reduction techniques to
address the intent classification task with end-to-end neural models. In this
paper, we experiment with a noisy version of the fluent speech command data
set, combining the intent classifier with a time-domain speech enhancement
solution based on Wave-U-Net and considering different training strategies.
Experimental results reveal that, for this task, the use of speech enhancement
greatly improves the classification accuracy in noisy conditions, in particular
when the classification model is trained on enhanced signals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards General Purpose Medical AI: Continual Learning Medical
  Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huahui Yi, Ziyuan Qin, Qicheng Lao, Wei Xu, Zekun Jiang, Dequan Wang, Shaoting Zhang, Kang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inevitable domain and task discrepancies in real-world scenarios can impair
the generalization performance of the pre-trained deep models for medical data.
Therefore, we audaciously propose that we should build a general-purpose
medical AI system that can be seamlessly adapted to downstream domains/tasks.
Since the domain/task adaption procedures usually involve additional labeling
work for the target data, designing a data-efficient adaption algorithm is
desired to save the cost of transferring the learned knowledge. Our recent work
found that vision-language models (VLMs) are efficient learners with
extraordinary cross-domain ability. Therefore, in this work, we further explore
the possibility of leveraging pre-trained VLMs as medical foundation models for
building general-purpose medical AI, where we thoroughly investigate three
machine-learning paradigms, i.e., domain/task-specialized learning, joint
learning, and continual learning, for training the VLMs and evaluate their
generalization performance on cross-domain and cross-task test sets. To
alleviate the catastrophic forgetting during sequential training, we employ
rehearsal learning and receive a sharp boost in terms of generalization
capability. In a nutshell, our empirical evidence suggests that continual
learning may be a practical and efficient learning paradigm for the medical
foundation model. And we hope researchers can use our empirical evidence as
basement to further explore the path toward medical foundation model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Models for Non-autoregressive Text Generation: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Li, Kun Zhou, Wayne Xin Zhao, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-autoregressive (NAR) text generation has attracted much attention in the
field of natural language processing, which greatly reduces the inference
latency but has to sacrifice the generation accuracy. Recently, diffusion
models, a class of latent variable generative models, have been introduced into
NAR text generation, showing improved generation quality. In this survey, we
review the recent progress in diffusion models for NAR text generation. As the
background, we first present the general definition of diffusion models and the
text diffusion models, and then discuss their merits for NAR generation. As the
core content, we further introduce two mainstream diffusion models in existing
text diffusion works, and review the key designs of the diffusion process.
Moreover, we discuss the utilization of pre-trained language models (PLMs) for
text diffusion models and introduce optimization techniques for text data.
Finally, we discuss several promising directions and conclude this paper. Our
survey aims to provide researchers with a systematic reference of related
research on text diffusion models for NAR generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compressed Heterogeneous Graph for Abstractive Multi-Document
  Summarization <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miao Li, Jianzhong Qi, Jey Han Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-document summarization (MDS) aims to generate a summary for a number of
related documents. We propose HGSUM, an MDS model that extends an
encoder-decoder architecture, to incorporate a heterogeneous graph to represent
different semantic units (e.g., words and sentences) of the documents. This
contrasts with existing MDS models which do not consider different edge types
of graphs and as such do not capture the diversity of relationships in the
documents. To preserve only key information and relationships of the documents
in the heterogeneous graph, HGSUM uses graph pooling to compress the input
graph. And to guide HGSUM to learn compression, we introduce an additional
objective that maximizes the similarity between the compressed graph and the
graph constructed from the ground-truth summary during training. HGSUM is
trained end-to-end with graph similarity and standard cross-entropy objectives.
Experimental results over MULTI-NEWS, WCEP-100, and ARXIV show that HGSUM
outperforms state-of-the-art MDS models. The code for our model and experiments
is available at: https://github.com/oaimli/HGSum.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Articulation GAN: Unsupervised modeling of articulatory learning <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.15173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.15173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gašper Beguš, Alan Zhou, Peter Wu, Gopala K Anumanchipalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative deep neural networks are widely used for speech synthesis, but
most existing models directly generate waveforms or spectral outputs. Humans,
however, produce speech by controlling articulators, which results in the
production of speech sounds through physical properties of sound propagation.
We introduce the Articulatory Generator to the Generative Adversarial Network
paradigm, a new unsupervised generative model of speech production/synthesis.
The Articulatory Generator more closely mimics human speech production by
learning to generate articulatory representations (electromagnetic
articulography or EMA) in a fully unsupervised manner. A separate pre-trained
physical model (ema2wav) then transforms the generated EMA representations to
speech waveforms, which get sent to the Discriminator for evaluation.
Articulatory analysis suggests that the network learns to control articulators
in a similar manner to humans during speech production. Acoustic analysis of
the outputs suggests that the network learns to generate words that are both
present and absent in the training distribution. We additionally discuss
implications of articulatory representations for cognitive models of human
language and speech technology in general.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pre-train</span>ed Language Models in Biomedical Domain: A Systematic <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.05006v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.05006v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benyou Wang, Qianqian Xie, Jiahuan Pei, Zhihong Chen, Prayag Tiwari, Zhao Li, Jie fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models (PLMs) have been the de facto paradigm for most
natural language processing (NLP) tasks. This also benefits biomedical domain:
researchers from informatics, medicine, and computer science (CS) communities
propose various PLMs trained on biomedical datasets, e.g., biomedical text,
electronic health records, protein, and DNA sequences for various biomedical
tasks. However, the cross-discipline characteristics of biomedical PLMs hinder
their spreading among communities; some existing works are isolated from each
other without comprehensive comparison and discussions. It expects a survey
that not only systematically reviews recent advances of biomedical PLMs and
their applications but also standardizes terminology and benchmarks. In this
paper, we summarize the recent progress of pre-trained language models in the
biomedical domain and their applications in biomedical downstream tasks.
Particularly, we discuss the motivations and propose a taxonomy of existing
biomedical PLMs. Their applications in biomedical downstream tasks are
exhaustively discussed. At last, we illustrate various limitations and future
trends, which we hope can provide inspiration for the future research of the
research community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An improved version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "Correct answers" from the psychology of artificial intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.07267v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.07267v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter S. Park, Philipp Schoenegger, Chongyang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have vastly grown in capabilities. One proposed
application of such AI systems is to support data collection in the social and
cognitive sciences, where perfect experimental control is currently unfeasible
and the collection of large, representative datasets is generally expensive. In
this paper, we re-replicate 14 studies from the Many Labs 2 replication project
with OpenAI's text-davinci-003 model, colloquially known as GPT3.5. We
collected responses from the default setting of GPT3.5 by inputting each
study's survey as text. Among the eight studies we could analyse, our GPT
sample replicated 37.5% of the original results as well as 37.5% of the Many
Labs 2 results. Unexpectedly, we could not analyse the remaining six studies as
we had planned in our pre-registration. This was because for each of these six
studies, GPT3.5 answered at least one of the survey questions (either a
dependent variable or a condition variable) in an extremely predetermined way:
an unexpected phenomenon we call the "correct answer" effect. Different runs of
GPT3.5 answered nuanced questions probing political orientation, economic
preference, judgement, and moral philosophy with zero or near-zero variation in
responses: with the supposedly "correct answer." For example, our survey
questions found the default setting of GPT3.5 to almost always self-identify as
a maximally strong conservative (99.6%, N=1,030), and to always be morally
deontological in opposing the hypothetical pushing of a large man in front of
an incoming trolley to save the lives of five people (100%, N=1,030). Since AI
models of the future may be trained on much of the same data as GPT3.5,
training data from which GPT3.5 may have learned its supposedly "correct
answers," our results raise concerns that a hypothetical AI-led future may in
certain ways be subject to a diminished diversity of thought.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages (31-page main text, 21-page SI); nine visualizations (three
  tables and two figures in the main text, four figures in the SI); added
  corrections regarding the previously erroneous survey for Study 4's
  replication of Graham et al. (2009); preregistered OSF database is available
  at https://osf.io/dzp8t/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reducing Spurious Correlations for Aspect-Based Sentiment Analysis with
  Variational Information Bottleneck and Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02846v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02846v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingshan Chang, Min Yang, Qingshan Jiang, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning techniques have dominated the literature on aspect-based
sentiment analysis (ABSA), yielding state-of-the-art results. However, these
deep models generally suffer from spurious correlation problems between input
features and output labels, which creates significant barriers to robustness
and generalization capability. In this paper, we propose a novel Contrastive
Variational Information Bottleneck framework (called CVIB) to reduce spurious
correlations for ABSA. The proposed CVIB framework is composed of an original
network and a self-pruned network, and these two networks are optimized
simultaneously via contrastive learning. Concretely, we employ the Variational
Information Bottleneck (VIB) principle to learn an informative and compressed
network (self-pruned network) from the original network, which discards the
superfluous patterns or spurious correlations between input features and
prediction labels. Then, self-pruning contrastive learning is devised to pull
together semantically similar positive pairs and push away dissimilar pairs,
where the representations of the anchor learned by the original and self-pruned
networks respectively are regarded as a positive pair while the representations
of two different sentences within a mini-batch are treated as a negative pair.
To verify the effectiveness of our CVIB method, we conduct extensive
experiments on five benchmark ABSA datasets and the experimental results show
that our approach achieves better performance than the strong competitors in
terms of overall prediction performance, robustness, and generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning ASR pathways: A sparse multilingual ASR model <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.05735v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.05735v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mu Yang, Andros Tjandra, Chunxi Liu, David Zhang, Duc Le, Ozlem Kalinli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network pruning compresses automatic speech recognition (ASR) models
effectively. However, in multilingual ASR, language-agnostic pruning may lead
to severe performance drops on some languages because language-agnostic pruning
masks may not fit all languages and discard important language-specific
parameters. In this work, we present ASR pathways, a sparse multilingual ASR
model that activates language-specific sub-networks ("pathways"), such that the
parameters for each language are learned explicitly. With the overlapping
sub-networks, the shared parameters can also enable knowledge transfer for
lower-resource languages via joint multilingual training. We propose a novel
algorithm to learn ASR pathways, and evaluate the proposed method on 4
languages with a streaming RNN-T model. Our proposed ASR pathways outperform
both dense models and a language-agnostically pruned model, and provide better
performance on low-resource languages compared to the monolingual sparse
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Using Emotion Embeddings to Transfer Knowledge Between Emotions,
  Languages, and Annotation Formats <span class="chip">ICASSP'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.00171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.00171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Chochlakis, Gireesh Mahajan, Sabyasachee Baruah, Keith Burghardt, Kristina Lerman, Shrikanth Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The need for emotional inference from text continues to diversify as more and
more disciplines integrate emotions into their theories and applications. These
needs include inferring different emotion types, handling multiple languages,
and different annotation formats. A shared model between different
configurations would enable the sharing of knowledge and a decrease in training
costs, and would simplify the process of deploying emotion recognition models
in novel environments. In this work, we study how we can build a single model
that can transition between these different configurations by leveraging
multilingual models and Demux, a transformer-based model whose input includes
the emotions of interest, enabling us to dynamically change the emotions
predicted by the model. Demux also produces emotion embeddings, and performing
operations on them allows us to transition to clusters of emotions by pooling
the embeddings of each cluster. We show that Demux can simultaneously transfer
knowledge in a zero-shot manner to a new language, to a novel annotation format
and to unseen emotions. Code is available at
https://github.com/gchochla/Demux-MEmo .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP'23, 5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Label Correlations in a Multi-label Setting: A Case Study in
  Emotion <span class="chip">ICASSP'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.15842v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.15842v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Chochlakis, Gireesh Mahajan, Sabyasachee Baruah, Keith Burghardt, Kristina Lerman, Shrikanth Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting emotions expressed in text has become critical to a range of
fields. In this work, we investigate ways to exploit label correlations in
multi-label emotion recognition models to improve emotion detection. First, we
develop two modeling approaches to the problem in order to capture word
associations of the emotion words themselves, by either including the emotions
in the input, or by leveraging Masked Language Modeling (MLM). Second, we
integrate pairwise constraints of emotion representations as regularization
terms alongside the classification loss of the models. We split these terms
into two categories, local and global. The former dynamically change based on
the gold labels, while the latter remain static during training. We demonstrate
state-of-the-art performance across Spanish, English, and Arabic in SemEval
2018 Task 1 E-c using monolingual BERT-based models. On top of better
performance, we also demonstrate improved robustness. Code is available at
https://github.com/gchochla/Demux-MEmo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP'23, 5 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Asynchronous Decentralized Federated Lifelong Learning for Landmark
  Localization in Medical Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyao Zheng, Michael A. Jacobs, Vladimir Braverman, Vishwa S. Parekh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a recent development in the machine learning area that
allows a system of devices to train on one or more tasks without sharing their
data to a single location or device. However, this framework still requires a
centralized global model to consolidate individual models into one, and the
devices train synchronously, which both can be potential bottlenecks for using
federated learning. In this paper, we propose a novel method of asynchronous
decentralized federated lifelong learning (ADFLL) method that inherits the
merits of federated learning and can train on multiple tasks simultaneously
without the need for a central node or synchronous training. Thus, overcoming
the potential drawbacks of conventional federated learning. We demonstrate
excellent performance on the brain tumor segmentation (BRATS) dataset for
localizing the left ventricle on multiple image sequences and image
orientation. Our framework allows agents to achieve the best performance with a
mean distance error of 7.81, better than the conventional all-knowing agent's
mean distance error of 11.78, and significantly (p=0.01) better than a
conventional lifelong learning agent with a distance error of 15.17 after eight
rounds of training. In addition, all ADFLL agents have comparable or better
performance than a conventional LL agent. In conclusion, we developed an ADFLL
framework with excellent performance and speed-up compared to conventional RL
agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AidUI: Toward Automated Recognition of Dark Patterns in User Interfaces <span class="chip">ICSE 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SM Hasan Mansur, Sabiha Salma, Damilola Awofisayo, Kevin Moran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Past studies have illustrated the prevalence of UI dark patterns, or user
interfaces that can lead end-users toward (unknowingly) taking actions that
they may not have intended. Such deceptive UI designs can result in adverse
effects on end users, such as oversharing personal information or financial
loss. While significant research progress has been made toward the development
of dark pattern taxonomies, developers and users currently lack guidance to
help recognize, avoid, and navigate these often subtle design motifs. However,
automated recognition of dark patterns is a challenging task, as the
instantiation of a single type of pattern can take many forms, leading to
significant variability.
  In this paper, we take the first step toward understanding the extent to
which common UI dark patterns can be automatically recognized in modern
software applications. To do this, we introduce AidUI, a novel automated
approach that uses computer vision and natural language processing techniques
to recognize a set of visual and textual cues in application screenshots that
signify the presence of ten unique UI dark patterns, allowing for their
detection, classification, and localization. To evaluate our approach, we have
constructed ContextDP, the current largest dataset of fully-localized UI dark
patterns that spans 175 mobile and 83 web UI screenshots containing 301 dark
pattern instances. The results of our evaluation illustrate that \AidUI
achieves an overall precision of 0.66, recall of 0.67, F1-score of 0.65 in
detecting dark pattern instances, reports few false positives, and is able to
localize detected patterns with an IoU score of ~0.84. Furthermore, a
significant subset of our studied dark patterns can be detected quite reliably
(F1 score of over 0.82), and future research directions may allow for improved
detection of additional patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, Accepted at The 45th IEEE/ACM International Conference on
  Software Engineering (ICSE 2023), Melbourne, Australia, May 14th-20th, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Module-Wise Network Quantization for 6D Object Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saqib Javed, Andrew Price, Yinlin Hu, Mathieu Salzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many edge applications, such as collaborative robotics and spacecraft
rendezvous, can benefit from 6D object pose estimation, but must do so on
embedded platforms. Unfortunately, existing 6D pose estimation networks are
typically too large for deployment in such situations and must therefore be
compressed, while maintaining reliable performance. In this work, we present an
approach to doing so by quantizing such networks. More precisely, we introduce
a module-wise quantization strategy that, in contrast to uniform and
mixed-precision quantization, accounts for the modular structure of typical 6D
pose estimation frameworks. We demonstrate that uniquely compressing these
modules outperforms uniform and mixed-precision quantization techniques.
Moreover, our experiments evidence that module-wise quantization can lead to a
significant accuracy boost. We showcase the generality of our approach using
different datasets, quantization methodologies, and network architectures,
including the recent ZebraPose.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Raising The Limit Of Image Rescaling Using Auxiliary Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenzhong Yin, Zhihong Pan, Xin Zhou, Le Kang, Paul Bogdan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Normalizing flow models using invertible neural networks (INN) have been
widely investigated for successful generative image super-resolution (SR) by
learning the transformation between the normal distribution of latent variable
$z$ and the conditional distribution of high-resolution (HR) images gave a
low-resolution (LR) input. Recently, image rescaling models like IRN utilize
the bidirectional nature of INN to push the performance limit of image
upscaling by optimizing the downscaling and upscaling steps jointly. While the
random sampling of latent variable $z$ is useful in generating diverse
photo-realistic images, it is not desirable for image rescaling when accurate
restoration of the HR image is more important. Hence, in places of random
sampling of $z$, we propose auxiliary encoding modules to further push the
limit of image rescaling performance. Two options to store the encoded latent
variables in downscaled LR images, both readily supported in existing image
file format, are proposed. One is saved as the alpha-channel, the other is
saved as meta-data in the image header, and the corresponding modules are
denoted as suffixes -A and -M respectively. Optimal network architectural
changes are investigated for both options to demonstrate their effectiveness in
raising the rescaling performance limit on different baseline models including
IRN and DLV-IRN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ensemble Learning of Myocardial Displacements for Myocardial Infarction
  Detection in Echocardiography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nguyen Tuan, Phi Nguyen, Dai Tran, Hung Pham, Quang Nguyen, Thanh Le, Hanh Van, Bach Do, Phuong Tran, Vinh Le, Thuy Nguyen, Long Tran, Hieu Pham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early detection and localization of myocardial infarction (MI) can reduce the
severity of cardiac damage through timely treatment interventions. In recent
years, deep learning techniques have shown promise for detecting MI in
echocardiographic images. However, there has been no examination of how
segmentation accuracy affects MI classification performance and the potential
benefits of using ensemble learning approaches. Our study investigates this
relationship and introduces a robust method that combines features from
multiple segmentation models to improve MI classification performance by
leveraging ensemble learning. Our method combines myocardial segment
displacement features from multiple segmentation models, which are then input
into a typical classifier to estimate the risk of MI. We validated the proposed
approach on two datasets: the public HMC-QU dataset (109 echocardiograms) for
training and validation, and an E-Hospital dataset (60 echocardiograms) from a
local clinical site in Vietnam for independent testing. Model performance was
evaluated based on accuracy, sensitivity, and specificity. The proposed
approach demonstrated excellent performance in detecting MI. The results showed
that the proposed approach outperformed the state-of-the-art feature-based
method. Further research is necessary to determine its potential use in
clinical settings as a tool to assist cardiologists and technicians with
objective assessments and reduce dependence on operator subjectivity. Our
research codes are available on GitHub at
https://github.com/vinuni-vishc/mi-detection-echo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08253v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08253v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingang Qu, Thibault Faney, Ze Wang, Patrick Gallinari, Soleiman Yousef, Jean-Charles de Hemptinne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to domain shift, machine learning systems typically fail to generalize
well to domains different from those of training data, which is what domain
generalization (DG) aims to address. Although various DG methods have been
developed, most of them lack interpretability and require domain labels that
are not available in many real-world scenarios. This paper presents a novel DG
method, called HMOE: Hypernetwork-based Mixture of Experts (MoE), which does
not rely on domain labels and is more interpretable. MoE proves effective in
identifying heterogeneous patterns in data. For the DG problem, heterogeneity
arises exactly from domain shift. HMOE uses hypernetworks taking vectors as
input to generate experts' weights, which allows experts to share useful
meta-knowledge and enables exploring experts' similarities in a low-dimensional
vector space. We compare HMOE with other DG algorithms under a fair and unified
benchmark-DomainBed. Our extensive experiments show that HMOE can divide
mixed-domain data into distinct clusters that are surprisingly more consistent
with human intuition than original domain labels. Compared to other DG methods,
HMOE shows competitive performance and achieves SOTA results in some cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Continual Test-Time Adaptation for Contextual and Semantic
  Domain Shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.08767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.08767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tommie Kerssies, Mert Kılıçkaya, Joaquin Vanschoren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, our goal is to adapt a pre-trained convolutional neural
network to domain shifts at test time. We do so continually with the incoming
stream of test batches, without labels. The existing literature mostly operates
on artificial shifts obtained via adversarial perturbations of a test image.
Motivated by this, we evaluate the state of the art on two realistic and
challenging sources of domain shifts, namely contextual and semantic shifts.
Contextual shifts correspond to the environment types, for example, a model
pre-trained on indoor context has to adapt to the outdoor context on CORe-50.
Semantic shifts correspond to the capture types, for example a model
pre-trained on natural images has to adapt to cliparts, sketches, and paintings
on DomainNet. We include in our analysis recent techniques such as
Prediction-Time Batch Normalization (BN), Test Entropy Minimization (TENT) and
Continual Test-Time Adaptation (CoTTA). Our findings are three-fold: i)
Test-time adaptation methods perform better and forget less on contextual
shifts compared to semantic shifts, ii) TENT outperforms other methods on
short-term adaptation, whereas CoTTA outpeforms other methods on long-term
adaptation, iii) BN is most reliable and robust. Our code is available at
https://github.com/tommiekerssies/Evaluating-Continual-Test-Time-Adaptation-for-Contextual-and-Semantic-Domain-Shifts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ U-Flow: A U-shaped Normalizing Flow for Anomaly Detection with
  Unsupervised Threshold 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12353v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12353v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matías Tailanian, Álvaro Pardo, Pablo Musé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we propose a non-contrastive method for anomaly detection and
segmentation in images, that benefits both from a modern machine learning
approach and a more classic statistical detection theory. The method consists
of three phases. First, features are extracted using a multi-scale image
Transformer architecture. Then, these features are fed into a U-shaped
Normalizing Flow that lays the theoretical foundations for the last phase,
which computes a pixel-level anomaly map, and performs a segmentation based on
the a contrario framework. This multiple-hypothesis testing strategy permits to
derive robust automatic detection thresholds, which are crucial in many
real-world applications, where an operational point is needed. The segmentation
results are evaluated using the Intersection over Union (IoU) metric; and for
assessing the generated anomaly maps we report the area under the Receiver
Operating Characteristic curve (AUROC), and the area under the
per-region-overlap curve (AUPRO). Extensive experimentation in various datasets
shows that the proposed approach produces state-of-the-art results for all
metrics and all datasets, ranking first in most MvTec-AD categories, with a
mean pixel-level AUROC of 98.74%. Code and trained models are available at
https:// github.com/mtailanian/uflow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Heterogeneous Graph Learning for Acoustic Event Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Shirian, Mona Ahmadian, Krishna Somandepalli, Tanaya Guha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Heterogeneous graphs provide a compact, efficient, and scalable way to model
data involving multiple disparate modalities. This makes modeling audiovisual
data using heterogeneous graphs an attractive option. However, graph structure
does not appear naturally in audiovisual data. Graphs for audiovisual data are
constructed manually which is both difficult and sub-optimal. In this work, we
address this problem by (i) proposing a parametric graph construction strategy
for the intra-modal edges, and (ii) learning the crossmodal edges. To this end,
we develop a new model, heterogeneous graph crossmodal network (HGCN) that
learns the crossmodal edges. Our proposed model can adapt to various spatial
and temporal scales owing to its parametric construction, while the learnable
crossmodal edges effectively connect the relevant nodes across modalities.
Experiments on a large benchmark dataset (AudioSet) show that our model is
state-of-the-art (0.53 mean average precision), outperforming transformer-based
models and other graph-based models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2207.07935</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision, Deduction and Alignment: An Empirical Study on Multi-modal
  Knowledge Graph Alignment <span class="chip">ICASSP2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangning Li, Jiaoyan Chen, Yinghui Li, Yuejia Xiang, Xi Chen, Hai-Tao Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity alignment (EA) for knowledge graphs (KGs) plays a critical role in
knowledge engineering. Existing EA methods mostly focus on utilizing the graph
structures and entity attributes (including literals), but ignore images that
are common in modern multi-modal KGs. In this study we first constructed
Multi-OpenEA -- eight large-scale, image-equipped EA benchmarks, and then
evaluated some existing embedding-based methods for utilizing images. In view
of the complementary nature of visual modal information and logical deduction,
we further developed a new multi-modal EA method named LODEME using logical
deduction and multi-modal KG embedding, with state-of-the-art performance
achieved on Multi-OpenEA and other existing multi-modal EA benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-11T00:00:00Z">2023-03-11</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PowerMat: context-aware recommender system without user item rating
  values that solves the cold-start problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems serves as an important technical asset in many modern
companies. With the increasing demand for higher precision of the technology,
more and more research and investment has been allocated to the field. One
important sub-field of recommender systems that has been stagnating is
context-aware recommender systems. Due to the difficulty of collecting input
dataset, the amount of research on context-aware recommender systems is much
less than other sub-fields of recommender systems. In this paper, we propose a
new algorithm named PowerMat to tackle the context-aware recommendation
problem. We build our theory on matrix factorization and Zipf's law, and also
more recent research work such as DotMat. We prove by experiments that our
method achieves superior results to the classic matrix factorization algorithm
and other context-aware recommender systems such as MovieMat+. In addition, by
theoretical analysis, we show that our algorithm solves the cold-start problem
for context-aware recommendation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Betti Number for Point Sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topology is the foundation for many industrial applications ranging from CAD
to simulation analysis. Computational topology mostly focuses on structured
data such as mesh, however unstructured dataset such as point set remains a
virgin land for topology scientists. The significance of point-based topology
can never be overemphasized, especially in the area of reverse engineering,
geometric modeling and algorithmic analysis. In this paper, we propose a novel
approach to compute the Betti number for point set data and illustrate its
usefulness in real world examples. To the best of our knowledge, our work is
pioneering and first of its kind in the fields of computational topology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ User Retention-oriented Recommendation with Decision <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kesen Zhao, Lixin Zou, Xiangyu Zhao, Maolin Wang, Dawei yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving user retention with reinforcement learning~(RL) has attracted
increasing attention due to its significant importance in boosting user
engagement. However, training the RL policy from scratch without hurting users'
experience is unavoidable due to the requirement of trial-and-error searches.
Furthermore, the offline methods, which aim to optimize the policy without
online interactions, suffer from the notorious stability problem in value
estimation or unbounded variance in counterfactual policy evaluation. To this
end, we propose optimizing user retention with Decision Transformer~(DT), which
avoids the offline difficulty by translating the RL as an autoregressive
problem. However, deploying the DT in recommendation is a non-trivial problem
because of the following challenges: (1) deficiency in modeling the numerical
reward value; (2) data discrepancy between the policy learning and
recommendation generation; (3) unreliable offline performance evaluation. In
this work, we, therefore, contribute a series of strategies for tackling the
exposed issues. We first articulate an efficient reward prompt by weighted
aggregation of meta embeddings for informative reward embedding. Then, we endow
a weighted contrastive learning method to solve the discrepancy between
training and inference. Furthermore, we design two robust offline metrics to
measure user retention. Finally, the significant improvement in the benchmark
datasets demonstrates the superiority of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoMLP: Automated MLP for Sequential Recommendations <span class="chip">WWW'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muyang Li, Zijian Zhang, Xiangyu Zhao, Wanyu Wang, Minghao Zhao, Runze Wu, Ruocheng Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommender systems aim to predict users' next interested item
given their historical interactions. However, a long-standing issue is how to
distinguish between users' long/short-term interests, which may be
heterogeneous and contribute differently to the next recommendation. Existing
approaches usually set pre-defined short-term interest length by exhaustive
search or empirical experience, which is either highly inefficient or yields
subpar results. The recent advanced transformer-based models can achieve
state-of-the-art performances despite the aforementioned issue, but they have a
quadratic computational complexity to the length of the input sequence. To this
end, this paper proposes a novel sequential recommender system, AutoMLP, aiming
for better modeling users' long/short-term interests from their historical
interactions. In addition, we design an automated and adaptive search algorithm
for preferable short-term interest length via end-to-end optimization. Through
extensive experiments, we show that AutoMLP has competitive performance against
state-of-the-art methods, while maintaining linear computational complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW'23</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transcription free filler word detection with Neural semi-CRFs <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Zhu, Yujia Yan, Juan-Pablo Caceres, Zhiyao Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-linguistic filler words, such as "uh" or "um", are prevalent in
spontaneous speech and serve as indicators for expressing hesitation or
uncertainty. Previous works for detecting certain non-linguistic filler words
are highly dependent on transcriptions from a well-established commercial
automatic speech recognition (ASR) system. However, certain ASR systems are not
universally accessible from many aspects, e.g., budget, target languages, and
computational power. In this work, we investigate filler word detection system
that does not depend on ASR systems. We show that, by using the structured
state space sequence model (S4) and neural semi-Markov conditional random
fields (semi-CRFs), we achieve an absolute F1 improvement of 6.4% (segment
level) and 3.1% (event level) on the PodcastFillers dataset. We also conduct a
qualitative analysis on the detected results to analyze the limitations of our
proposed system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and
  Multilingual Natural Language Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bang Yang, Fenglin Liu, Yuexian Zou, Xian Wu, Yaowei Wang, David A. Clifton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Generation (NLG) accepts input data in the form of images,
videos, or text and generates corresponding natural language text as output.
Existing NLG methods mainly adopt a supervised approach and rely heavily on
coupled data-to-text pairs. However, for many targeted scenarios and for
non-English languages, sufficient quantities of labeled data are often not
available. To relax the dependency on labeled data of downstream tasks, we
propose an intuitive and effective zero-shot learning framework, ZeroNLG, which
can deal with multiple NLG tasks, including image-to-text (image captioning),
video-to-text (video captioning), and text-to-text (neural machine
translation), across English, Chinese, German, and French within a unified
framework. ZeroNLG does not require any labeled downstream pairs for training.
During training, ZeroNLG (i) projects different domains (across modalities and
languages) to corresponding coordinates in a shared common latent space; (ii)
bridges different domains by aligning their corresponding coordinates in this
space; and (iii) builds an unsupervised multilingual auto-encoder to learn to
generate text by reconstructing the input text given its coordinate in shared
latent space. Consequently, during inference, based on the data-to-text
pipeline, ZeroNLG can generate target sentences across different languages
given the coordinate of input data in the common space. Within this unified
framework, given visual (imaging or video) data as input, ZeroNLG can perform
zero-shot visual captioning; given textual sentences as input, ZeroNLG can
perform zero-shot machine translation. We present the results of extensive
experiments on twelve NLG tasks, showing that, without using any labeled
downstream pairs for training, ZeroNLG generates high-quality and believable
outputs and significantly outperforms existing zero-shot methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We will release the codes and models at
  https://github.com/yangbang18/ZeroNLG soon. Without any labeled downstream
  pairs for training, the ZeroNLG can deal with multiple NLG tasks, including
  image-to-text, video-to-text, and text-to-text, across English, Chinese,
  German, and French within a unified framework</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parachute: Evaluating Interactive Human-LM Co-writing Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hua Shen, Tongshuang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A surge of advances in language models (LMs) has led to significant interest
in using LMs to build co-writing systems, in which humans and LMs interactively
contribute to a shared writing artifact. However, there is a lack of studies
assessing co-writing systems in interactive settings. We propose a
human-centered evaluation framework, Parachute, for interactive co-writing
systems. Parachute showcases an integrative view of interaction evaluation,
where each evaluation aspect consists of categorized practical metrics.
Furthermore, we present Parachute with a use case to demonstrate how to
evaluate and compare co-writing systems using Parachute.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CHI'23 In2Writing Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stabilizing <span class="highlight-title">Transformer</span> Training by Preventing Attention Entropy
  Collapse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, Yizhe Zhang, Jiatao Gu, Josh Susskind
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training stability is of great importance to Transformers. In this work, we
investigate the training dynamics of Transformers by examining the evolution of
the attention layers. In particular, we track the attention entropy for each
attention head during the course of training, which is a proxy for model
sharpness. We identify a common pattern across different architectures and
tasks, where low attention entropy is accompanied by high training instability,
which can take the form of oscillating loss or divergence. We denote the
pathologically low attention entropy, corresponding to highly concentrated
attention scores, as $\textit{entropy collapse}$. As a remedy, we propose
$\sigma$Reparam, a simple and efficient solution where we reparametrize all
linear layers with spectral normalization and an additional learned scalar. We
demonstrate that the proposed reparameterization successfully prevents entropy
collapse in the attention layers, promoting more stable training. Additionally,
we prove a tight lower bound of the attention entropy, which decreases
exponentially fast with the spectral norm of the attention logits, providing
additional motivation for our approach. We conduct experiments with
$\sigma$Reparam on image classification, image self-supervised learning,
machine translation, automatic speech recognition, and language modeling tasks,
across Transformer architectures. We show that $\sigma$Reparam provides
stability and robustness with respect to the choice of hyperparameters, going
so far as enabling training (a) a Vision Transformer to competitive performance
without warmup, weight decay, layer normalization or adaptive optimizers; (b)
deep architectures in machine translation and (c) speech recognition to
competitive performance without warmup and adaptive optimizers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Consistency Analysis of Chat<span class="highlight-title">GPT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Myeongjun Jang, Thomas Lukasiewicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT, a question-and-answer dialogue system based on a large language
model, has gained huge popularity since its introduction. Its positive aspects
have been reported through many media platforms, and some analyses even showed
that ChatGPT achieved a decent grade in professional exams, including the law,
medical, and finance domains, adding extra support to the claim that AI now can
assist and, even, replace humans in industrial fields. Others, however, doubt
its reliability and trustworthiness. In this paper, we investigate ChatGPT's
trustworthiness regarding logically consistent behaviours. Our findings suggest
that, although ChatGPT seems to achieve an improved language understanding
ability, it still fails to generate logically correct predictions frequently.
Hence, while it is true that ChatGPT is an impressive and promising new
technique, we conclude that its usage in real-world applications without
thorough human inspection requires further consideration, especially for
risk-sensitive areas.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Interactive UI to Support Sensemaking over Collections of Parallel
  Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joyce Zhou, Elena Glassman, Daniel S. Weld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientists and science journalists, among others, often need to make sense of
a large number of papers and how they compare with each other in scope, focus,
findings, or any other important factors. However, with a large corpus of
papers, it's cognitively demanding to pairwise compare and contrast them all
with each other. Fully automating this review process would be infeasible,
because it often requires domain-specific knowledge, as well as understanding
what the context and motivations for the review are. While there are existing
tools to help with the process of organizing and annotating papers for
literature reviews, at the core they still rely on people to serially read
through papers and manually make sense of relevant information.
  We present AVTALER, which combines peoples' unique skills, contextual
awareness, and knowledge, together with the strength of automation. Given a set
of comparable text excerpts from a paper corpus, it supports users in
sensemaking and contrasting paper attributes by interactively aligning text
excerpts in a table so that comparable details are presented in a shared
column. AVTALER is based on a core alignment algorithm that makes use of modern
NLP tools. Furthermore, AVTALER is a mixed-initiative system: users can
interactively give the system constraints which are integrated into the
alignment construction process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatically Extracting Information in Medical Dialogue: Expert System
  And Attention for Labelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15544v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15544v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinshi Wang, Daniel Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical dialogue information extraction is becoming an increasingly
significant problem in modern medical care. It is difficult to extract key
information from electronic medical records (EMRs) due to their large numbers.
Previously, researchers proposed attention-based models for retrieving features
from EMRs, but their limitations were reflected in their inability to recognize
different categories in medical dialogues. In this paper, we propose a novel
model, Expert System and Attention for Labelling (ESAL). We use mixture of
experts and pre-trained BERT to retrieve the semantics of different categories,
enabling the model to fuse the differences between them. In our experiment,
ESAL was applied to a public dataset and the experimental results indicated
that ESAL significantly improved the performance of Medical Information
Classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing the impact of contextual information in hate speech detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00465v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00465v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Manuel Pérez, Franco Luque, Demian Zayat, Martín Kondratzky, Agustín Moro, Pablo Serrati, Joaquín Zajac, Paula Miguel, Natalia Debandi, Agustín Gravano, Viviana Cotik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, hate speech has gained great relevance in social networks
and other virtual media because of its intensity and its relationship with
violent acts against members of protected groups. Due to the great amount of
content generated by users, great effort has been made in the research and
development of automatic tools to aid the analysis and moderation of this
speech, at least in its most threatening forms. One of the limitations of
current approaches to automatic hate speech detection is the lack of context.
Most studies and resources are performed on data without context; that is,
isolated messages without any type of conversational context or the topic being
discussed. This restricts the available information to define if a post on a
social network is hateful or not. In this work, we provide a novel corpus for
contextualized hate speech detection based on user responses to news posts from
media outlets on Twitter. This corpus was collected in the Rioplatense
dialectal variety of Spanish and focuses on hate speech associated with the
COVID-19 pandemic. Classification experiments using state-of-the-art techniques
show evidence that adding contextual information improves hate speech detection
performance for two proposed tasks (binary and multi-label prediction). We make
our code, models, and corpus available for further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Two-view Graph Neural Networks for Knowledge Graph Completion <span class="chip">ESWC 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09231v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09231v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinh Tong, Dai Quoc Nguyen, Dinh Phung, Dat Quoc Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an effective graph neural network (GNN)-based knowledge graph
embedding model, which we name WGE, to capture entity- and relation-focused
graph structures. Given a knowledge graph, WGE builds a single undirected
entity-focused graph that views entities as nodes. WGE also constructs another
single undirected graph from relation-focused constraints, which views entities
and relations as nodes. WGE then proposes a GNN-based architecture to better
learn vector representations of entities and relations from these two single
entity- and relation-focused graphs. WGE feeds the learned entity and relation
representations into a weighted score function to return the triple scores for
knowledge graph completion. Experimental results show that WGE outperforms
strong baselines on seven benchmark datasets for knowledge graph completion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Proceedings of ESWC 2023; 17 pages; 4 tables; 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Token-level Contrastive Framework for Sign Language Translation <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.04916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.04916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biao Fu, Peigen Ye, Liang Zhang, Pei Yu, Cong Hu, Yidong Chen, Xiaodong Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sign Language Translation (SLT) is a promising technology to bridge the
communication gap between the deaf and the hearing people. Recently,
researchers have adopted Neural Machine Translation (NMT) methods, which
usually require large-scale corpus for training, to achieve SLT. However, the
publicly available SLT corpus is very limited, which causes the collapse of the
token representations and the inaccuracy of the generated tokens. To alleviate
this issue, we propose ConSLT, a novel token-level \textbf{Con}trastive
learning framework for \textbf{S}ign \textbf{L}anguage \textbf{T}ranslation ,
which learns effective token representations by incorporating token-level
contrastive learning into the SLT decoding process. Concretely, ConSLT treats
each token and its counterpart generated by different dropout masks as positive
pairs during decoding, and then randomly samples $K$ tokens in the vocabulary
that are not in the current sentence to construct negative examples. We conduct
comprehensive experiments on two benchmarks (PHOENIX14T and CSL-Daily) for both
end-to-end and cascaded settings. The experimental results demonstrate that
ConSLT can achieve better translation quality than the strong baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FinXABSA: Explainable Finance through Aspect-Based Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keane Ong, Wihan van der Heever, Ranjan Satapathy, Gianmarco Mengaldo, Erik Cambria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach for explainability in financial analysis
by utilizing the Pearson correlation coefficient to establish a relationship
between aspect-based sentiment analysis and stock prices. The proposed
methodology involves constructing an aspect list from financial news articles
and analyzing sentiment intensity scores for each aspect. These scores are then
compared to the stock prices for the relevant companies using the Pearson
coefficient to determine any significant correlations. The results indicate
that the proposed approach provides a more detailed and accurate understanding
of the relationship between sentiment analysis and stock prices, which can be
useful for investors and financial analysts in making informed decisions.
Additionally, this methodology offers a transparent and interpretable way to
explain the sentiment analysis results and their impact on stock prices.
Overall, the findings of this paper demonstrate the importance of
explainability in financial analysis and highlight the potential benefits of
utilizing the Pearson coefficient for analyzing aspect-based sentiment analysis
and stock prices. The proposed approach offers a valuable tool for
understanding the complex relationships between financial news sentiment and
stock prices, providing a new perspective on the financial market and aiding in
making informed investment decisions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Named Entity Detection and Injection for Direct Speech Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.11981v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.11981v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Gaido, Yun Tang, Ilia Kulikov, Rongqing Huang, Hongyu Gong, Hirofumi Inaguma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a sentence, certain words are critical for its semantic. Among them, named
entities (NEs) are notoriously challenging for neural models. Despite their
importance, their accurate handling has been neglected in speech-to-text (S2T)
translation research, and recent work has shown that S2T models perform poorly
for locations and notably person names, whose spelling is challenging unless
known in advance. In this work, we explore how to leverage dictionaries of NEs
known to likely appear in a given context to improve S2T model outputs. Our
experiments show that we can reliably detect NEs likely present in an utterance
starting from S2T encoder outputs. Indeed, we demonstrate that the current
detection quality is sufficient to improve NE accuracy in the translation with
a 31% reduction in person name errors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\c{opyright} 2022 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Augmentation with Projection: Towards an Effective and Efficient Data
  Augmentation Paradigm for Distillation <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.11768v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.11768v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Wang, Yuexin Wu, Frederick Liu, Daogao Liu, Le Hou, Hongkun Yu, Jing Li, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation is one of the primary methods of transferring
knowledge from large to small models. However, it requires massive
task-specific data, which may not be plausible in many real-world applications.
Data augmentation methods such as representation interpolation, token
replacement, or augmentation with models are applied to tackle this problem.
However, these data augmentation methods either potentially cause shifts in
decision boundaries (representation interpolation), are not expressive enough
(token replacement), or introduce too much computational overhead (augmentation
with models). To this end, we propose AugPro (Augmentation with Projection), an
effective and efficient data augmentation method for distillation. Our method
builds on top of representation interpolation augmentation methods to maintain
the diversity of expressions and converts the augmented data to tokens to avoid
shifting decision boundaries. It uses simple operations that come with little
computational overhead. The results on multiple GLUE tasks show that our
methods can improve distillation performance by a large margin at a low time
cost. Codes are available at
https://github.com/google-research/google-research/tree/master/augpro.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 figures. Accepted by ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Surprising Computational Power of Nondeterministic Stack RNNs <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01343v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01343v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian DuSell, David Chiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional recurrent neural networks (RNNs) have a fixed, finite number of
memory cells. In theory (assuming bounded range and precision), this limits
their formal language recognition power to regular languages, and in practice,
RNNs have been shown to be unable to learn many context-free languages (CFLs).
In order to expand the class of languages RNNs recognize, prior work has
augmented RNNs with a nondeterministic stack data structure, putting them on
par with pushdown automata and increasing their language recognition power to
CFLs. Nondeterminism is needed for recognizing all CFLs (not just deterministic
CFLs), but in this paper, we show that nondeterminism and the neural controller
interact to produce two more unexpected abilities. First, the nondeterministic
stack RNN can recognize not only CFLs, but also many non-context-free
languages. Second, it can recognize languages with much larger alphabet sizes
than one might expect given the size of its stack alphabet. Finally, to
increase the information capacity in the stack and allow it to solve more
complicated tasks with large alphabet sizes, we propose a new version of the
nondeterministic stack that simulates stacks of vectors rather than discrete
symbols. We demonstrate perplexity improvements with this new model on the Penn
Treebank language modeling benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 8 figures. Published at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Multimodal Information based Speech Processing (MISP) 2022
  Challenge: Audio-Visual Diarization and Recognition <span class="chip">ICASSP2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Wang, Shilong Wu, Hang Chen, Mao-Kui He, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Siniscalchi, Odette Scharenborg, Diyuan Liu, Baocai Yin, Jia Pan, Jianqing Gao, Cong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Multi-modal Information based Speech Processing (MISP) challenge aims to
extend the application of signal processing technology in specific scenarios by
promoting the research into wake-up words, speaker diarization, speech
recognition, and other technologies. The MISP2022 challenge has two tracks: 1)
audio-visual speaker diarization (AVSD), aiming to solve ``who spoken when''
using both audio and visual data; 2) a novel audio-visual diarization and
recognition (AVDR) task that focuses on addressing ``who spoken what when''
with audio-visual speaker diarization results. Both tracks focus on the Chinese
language, and use far-field audio and video in real home-tv scenarios: 2-6
people communicating each other with TV noise in the background. This paper
introduces the dataset, track settings, and baselines of the MISP2022
challenge. Our analyses of experiments and examples indicate the good
performance of AVDR baseline system, and the potential difficulties in this
challenge due to, e.g., the far-field video quality, the presence of TV noise
in the background, and the indistinguishable speakers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, to be published in ICASSP2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Asymmetric Invertible Network for Compression-Aware Image Rescaling <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02353v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02353v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhai Yang, Mengxi Guo, Shijie Zhao, Junlin Li, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-resolution (HR) images are usually downscaled to low-resolution (LR)
ones for better display and afterward upscaled back to the original size to
recover details. Recent work in image rescaling formulates downscaling and
upscaling as a unified task and learns a bijective mapping between HR and LR
via invertible networks. However, in real-world applications (e.g., social
media), most images are compressed for transmission. Lossy compression will
lead to irreversible information loss on LR images, hence damaging the inverse
upscaling procedure and degrading the reconstruction accuracy. In this paper,
we propose the Self-Asymmetric Invertible Network (SAIN) for compression-aware
image rescaling. To tackle the distribution shift, we first develop an
end-to-end asymmetric framework with two separate bijective mappings for
high-quality and compressed LR images, respectively. Then, based on empirical
analysis of this framework, we model the distribution of the lost information
(including downscaling and compression) using isotropic Gaussian mixtures and
propose the Enhanced Invertible Block to derive high-quality/compressed LR
images in one forward pass. Besides, we design a set of losses to regularize
the learned LR images and enhance the invertibility. Extensive experiments
demonstrate the consistent improvements of SAIN across various image rescaling
datasets in terms of both quantitative and qualitative evaluation under
standard image compression formats (i.e., JPEG and WebP).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023. Code is available at
  https://github.com/yang-jin-hai/SAIN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMCosine: Multi-Modal Cosine Loss Towards Balanced Audio-Visual
  Fine-Grained Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05338v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05338v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruize Xu, Ruoxuan Feng, Shi-Xiong Zhang, Di Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual learning helps to comprehensively understand the world by fusing
practical information from multiple modalities. However, recent studies show
that the imbalanced optimization of uni-modal encoders in a joint-learning
model is a bottleneck to enhancing the model's performance. We further find
that the up-to-date imbalance-mitigating methods fail on some audio-visual
fine-grained tasks, which have a higher demand for distinguishable feature
distribution. Fueled by the success of cosine loss that builds hyperspherical
feature spaces and achieves lower intra-class angular variability, this paper
proposes Multi-Modal Cosine loss, MMCosine. It performs a modality-wise $L_2$
normalization to features and weights towards balanced and better multi-modal
fine-grained learning. We demonstrate that our method can alleviate the
imbalanced optimization from the perspective of weight norm and fully exploit
the discriminability of the cosine metric. Extensive experiments prove the
effectiveness of our method and the versatility with advanced multi-modal
fusion strategies and up-to-date imbalance-mitigating methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AMPose: Alternatively Mixed Global-Local Attention Model for 3D Human
  Pose Estimation <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.04216v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.04216v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongxin Lin, Yunwei Chiu, Peiyuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The graph convolutional networks (GCNs) have been applied to model the
physically connected and non-local relations among human joints for 3D human
pose estimation (HPE). In addition, the purely Transformer-based models
recently show promising results in video-based 3D HPE. However, the
single-frame method still needs to model the physically connected relations
among joints because the feature representations transformed only by global
relations via the Transformer neglect information on the human skeleton. To
deal with this problem, we propose a novel method in which the Transformer
encoder and GCN blocks are alternately stacked, namely AMPose, to combine the
global and physically connected relations among joints towards HPE. In the
AMPose, the Transformer encoder is applied to connect each joint with all the
other joints, while GCNs are applied to capture information on physically
connected relations. The effectiveness of our proposed method is evaluated on
the Human3.6M dataset. Our model also shows better generalization ability by
testing on the MPI-INF-3DHP dataset. Code can be retrieved at
https://github.com/erikervalid/AMPose.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2023 Accepted Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging <span class="highlight-title">Pre-train</span>ed AudioLDM for Text to Sound Generation: A
  Benchmark Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03857v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03857v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Yuan, Haohe Liu, Jinhua Liang, Xubo Liu, Mark D. Plumbley, Wenwu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have recently achieved breakthroughs in sound generation
with text prompts. Despite their promising performance, current text-to-sound
generation models face issues on small-scale datasets (e.g., overfitting),
significantly limiting their performance. In this paper, we investigate the use
of pre-trained AudioLDM, the state-of-the-art model for text-to-audio
generation, as the backbone for sound generation. Our study demonstrates the
advantages of using pre-trained models for text-to-sound generation, especially
in data-scarcity scenarios. In addition, experiments show that different
training strategies (e.g., training conditions) may affect the performance of
AudioLDM on datasets of different scales. To facilitate future studies, we also
evaluate various text-to-sound generation systems on several frequently used
datasets under the same evaluation protocols, which allow fair comparisons and
benchmarking of these methods on the common ground.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EUSIPCO 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LDMIC: Learning-based Distributed Multi-view Image Coding <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinjie Zhang, Jiawei Shao, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view image compression plays a critical role in 3D-related
applications. Existing methods adopt a predictive coding architecture, which
requires joint encoding to compress the corresponding disparity as well as
residual information. This demands collaboration among cameras and enforces the
epipolar geometric constraint between different views, which makes it
challenging to deploy these methods in distributed camera systems with randomly
overlapping fields of view. Meanwhile, distributed source coding theory
indicates that efficient data compression of correlated sources can be achieved
by independent encoding and joint decoding, which motivates us to design a
learning-based distributed multi-view image coding (LDMIC) framework. With
independent encoders, LDMIC introduces a simple yet effective joint context
transfer module based on the cross-attention mechanism at the decoder to
effectively capture the global inter-view correlations, which is insensitive to
the geometric relationships between images. Experimental results show that
LDMIC significantly outperforms both traditional and learning-based MIC methods
while enjoying fast encoding speed. Code will be released at
https://github.com/Xinjie-Q/LDMIC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-10T00:00:00Z">2023-03-10</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiNet: A Novel Multi-Scenario & Multi-Task Learning Approach with
  Hierarchical Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Zhou, Xianshuai Cao, Wenhao Li, Kun Zhang, Chuan Luo, Qian Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-scenario & multi-task learning has been widely applied to many
recommendation systems in industrial applications, wherein an effective and
practical approach is to carry out multi-scenario transfer learning on the
basis of the Mixture-of-Expert (MoE) architecture. However, the MoE-based
method, which aims to project all information in the same feature space, cannot
effectively deal with the complex relationships inherent among various
scenarios and tasks, resulting in unsatisfactory performance. To tackle the
problem, we propose a Hierarchical information extraction Network (HiNet) for
multi-scenario and multi-task recommendation, which achieves hierarchical
extraction based on coarse-to-fine knowledge transfer scheme. The multiple
extraction layers of the hierarchical network enable the model to enhance the
capability of transferring valuable information across scenarios while
preserving specific features of scenarios and tasks. Furthermore, a novel
scenario-aware attentive network module is proposed to model correlations
between scenarios explicitly. Comprehensive experiments conducted on real-world
industrial datasets from Meituan Meishi platform demonstrate that HiNet
achieves a new state-of-the-art performance and significantly outperforms
existing solutions. HiNet is currently fully deployed in two scenarios and has
achieved 2.87% and 1.75% order quantity gain respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gradient Coordination for Quantifying and Maximizing Knowledge
  Transference in Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanhua Yang, Jianxin Zhao, Shaoguo Liu, Liang Wang, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task learning (MTL) has been widely applied in online advertising and
recommender systems. To address the negative transfer issue, recent studies
have proposed optimization methods that thoroughly focus on the gradient
alignment of directions or magnitudes. However, since prior study has proven
that both general and specific knowledge exist in the limited shared capacity,
overemphasizing on gradient alignment may crowd out task-specific knowledge,
and vice versa. In this paper, we propose a transference-driven approach CoGrad
that adaptively maximizes knowledge transference via Coordinated Gradient
modification. We explicitly quantify the transference as loss reduction from
one task to another, and then derive an auxiliary gradient from optimizing it.
We perform the optimization by incorporating this gradient into original task
gradients, making the model automatically maximize inter-task transfer and
minimize individual losses. Thus, CoGrad can harmonize between general and
specific knowledge to boost overall performance. Besides, we introduce an
efficient approximation of the Hessian matrix, making CoGrad computationally
efficient and simple to implement. Both offline and online experiments verify
that CoGrad significantly outperforms previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-supervised Adversarial Learning for Complementary Item
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koby Bibas, Oren Sar Shalom, Dietmar Jannach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complementary item recommendations are a ubiquitous feature of modern
e-commerce sites. Such recommendations are highly effective when they are based
on collaborative signals like co-purchase statistics. In certain online
marketplaces, however, e.g., on online auction sites, constantly new items are
added to the catalog. In such cases, complementary item recommendations are
often based on item side-information due to a lack of interaction data. In this
work, we propose a novel approach that can leverage both item side-information
and labeled complementary item pairs to generate effective complementary
recommendations for cold items, i.e., for items for which no co-purchase
statistics yet exist. Given that complementary items typically have to be of a
different category than the seed item, we technically maintain a latent space
for each item category. Simultaneously, we learn to project distributed item
representations into these category spaces to determine suitable
recommendations. The main learning process in our architecture utilizes labeled
pairs of complementary items. In addition, we adopt ideas from Cycle Generative
Adversarial Networks (CycleGAN) to leverage available item information even in
case no labeled data exists for a given item and category. Experiments on three
e-commerce datasets show that our method is highly effective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Web Conference 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pacos: Modeling Users' Interpretable and Context-Dependent Choices in
  Preference Reversals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingming Li, H. Vicky Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Choice problems refer to selecting the best choices from several items, and
learning users' preferences in choice problems is of great significance in
understanding the decision making mechanisms and providing personalized
services. Existing works typically assume that people evaluate items
independently. In practice, however, users' preferences depend on the market in
which items are placed, which is known as context effects; and the order of
users' preferences for two items may even be reversed, which is referred to
preference reversals. In this work, we identify three factors contributing to
context effects: users' adaptive weights, the inter-item comparison, and
display positions. We propose a context-dependent preference model named Pacos
as a unified framework for addressing three factors simultaneously, and
consider two design methods including an additive method with high
interpretability and an ANN-based method with high accuracy. We study the
conditions for preference reversals to occur and provide an theoretical proof
of the effectiveness of Pacos in addressing preference reversals. Experimental
results show that the proposed method has better performance than prior works
in predicting users' choices, and has great interpretability to help understand
the cause of preference reversals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Event-based News Narrative Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Keith Norambuena, Tanushree Mitra, Chris North
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Narratives are fundamental to our understanding of the world, providing us
with a natural structure for knowledge representation over time. Computational
narrative extraction is a subfield of artificial intelligence that makes heavy
use of information retrieval and natural language processing techniques.
Despite the importance of computational narrative extraction, relatively little
scholarly work exists on synthesizing previous research and strategizing future
research in the area. In particular, this article focuses on extracting news
narratives from an event-centric perspective. Extracting narratives from news
data has multiple applications in understanding the evolving information
landscape. This survey presents an extensive study of research in the area of
event-based news narrative extraction. In particular, we screened over 900
articles that yielded 54 relevant articles. These articles are synthesized and
organized by representation model, extraction criteria, and evaluation
approaches. Based on the reviewed studies, we identify recent trends, open
challenges, and potential research lines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 3 figures, to be published in the journal ACM CSUR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Task Recommendations with Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziru Liu, Jiejie Tian, Qingpeng Cai, Xiangyu Zhao, Jingtong Gao, Shuchang Liu, Dayou Chen, Tonghao He, Dong Zheng, Peng Jiang, Kun Gai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Multi-task Learning (MTL) has yielded immense success in
Recommender System (RS) applications. However, current MTL-based recommendation
models tend to disregard the session-wise patterns of user-item interactions
because they are predominantly constructed based on item-wise datasets.
Moreover, balancing multiple objectives has always been a challenge in this
field, which is typically avoided via linear estimations in existing works. To
address these issues, in this paper, we propose a Reinforcement Learning (RL)
enhanced MTL framework, namely RMTL, to combine the losses of different
recommendation tasks using dynamic weights. To be specific, the RMTL structure
can address the two aforementioned issues by (i) constructing an MTL
environment from session-wise interactions and (ii) training multi-task
actor-critic network structure, which is compatible with most existing
MTL-based recommendation models, and (iii) optimizing and fine-tuning the MTL
loss function using the weights generated by critic networks. Experiments on
two real-world public datasets demonstrate the effectiveness of RMTL with a
higher AUC against state-of-the-art MTL-based recommendation models.
Additionally, we evaluate and validate RMTL's compatibility and transferability
across various MTL models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TheWebConf2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Achievable Rates and Low-Complexity Encoding of Posterior Matching for
  the BSC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amaael Antonini, Rita Gimelshein, Richard Wesel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Horstein, Burnashev, Shayevitz and Feder, Naghshvar et al. and others have
studied sequential transmission of a K-bit message over the binary symmetric
channel (BSC) with full, noiseless feedback using posterior matching. Yang et
al. provide an improved lower bound on the achievable rate using martingale
analysis that relies on the small-enough difference (SED) partitioning
introduced by Naghshvar et al. SED requires a relatively complex encoder and
decoder. To reduce complexity, this paper replaces SED with relaxed constraints
that admit the small enough absolute difference (SEAD) partitioning rule. The
main analytical results show that achievable-rate bounds higher than those
found by Yang et al. are possible even under the new constraints, which are
less restrictive than SED. The new analysis does not use martingale theory for
the confirmation phase and applies a surrogate channel technique to tighten the
results. An initial systematic transmission further increases the achievable
rate bound. The simplified encoder associated with SEAD has a complexity below
order O(K^2) and allows simulations for message sizes of at least 1000 bits.
For example, simulations achieve 99% of of the channel's 0.50-bit capacity with
an average block size of 200 bits for a target codeword error rate of 10^(-3).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper consists of 26 pages and contains 6 figures. An earlier
  version of the algorithm included in this paper was published at the 2020
  IEEE International Symposium on Information Theory (ISIT), (DOI:
  10.1109/ISIT44484.2020.9174232)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AUTODIAL: Efficient Asynchronous Task-Oriented Dialogue Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prajjwal Bhargava, Pooyan Amini, Shahin Shayandeh, Chinnadhurai Sankar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large dialogue models become commonplace in practice, the problems
surrounding high compute requirements for training, inference and larger memory
footprint still persists. In this work, we present AUTODIAL, a multi-task
dialogue model that addresses the challenges of deploying dialogue model.
AUTODIAL utilizes parallel decoders to perform tasks such as dialogue act
prediction, domain prediction, intent prediction, and dialogue state tracking.
Using classification decoders over generative decoders allows AUTODIAL to
significantly reduce memory footprint and achieve faster inference times
compared to existing generative approach namely SimpleTOD. We demonstrate that
AUTODIAL provides 3-6x speedups during inference while having 11x fewer
parameters on three dialogue tasks compared to SimpleTOD. Our results show that
extending current dialogue models to have parallel decoders can be a viable
alternative for deploying them in resource-constrained environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Query Focused Summaries without Fine-tuning the
  <span class="highlight-title">Transformer</span>-based <span class="highlight-title">Pre-train</span>ed Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deen Abdullah, Shamanth Nayak, Gandharv Suri, Yllias Chali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning the Natural Language Processing (NLP) models for each new data
set requires higher computational time associated with increased carbon
footprint and cost. However, fine-tuning helps the pre-trained models adapt to
the latest data sets; what if we avoid the fine-tuning steps and attempt to
generate summaries using just the pre-trained models to reduce computational
time and cost. In this paper, we tried to omit the fine-tuning steps and
investigate whether the Marginal Maximum Relevance (MMR)-based approach can
help the pre-trained models to obtain query-focused summaries directly from a
new data set that was not used to pre-train the models. First, we used topic
modelling on Wikipedia Current Events Portal (WCEP) and Debatepedia datasets to
generate queries for summarization tasks. Then, using MMR, we ranked the
sentences of the documents according to the queries. Next, we passed the ranked
sentences to seven transformer-based pre-trained models to perform the
summarization tasks. Finally, we used the MMR approach again to select the
query relevant sentences from the generated summaries of individual pre-trained
models and constructed the final summary. As indicated by the experimental
results, our MMR-based approach successfully ranked and selected the most
relevant sentences as summaries and showed better performance than the
individual pre-trained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert
  (MoE) Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyang Huang, Newsha Ardalani, Anna Sun, Liu Ke, Hsien-Hsin S. Lee, Anjali Sridhar, Shruti Bhosale, Carole-Jean Wu, Benjamin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) models have recently gained steam in achieving the
state-of-the-art performance in a wide range of tasks in computer vision and
natural language processing. They effectively expand the model capacity while
incurring a minimal increase in computation cost during training. However,
deploying such models for inference is difficult due to their large model size
and complex communication pattern. In this work, we provide a characterization
of two MoE workloads, namely Language Modeling (LM) and Machine Translation
(MT) and identify their sources of inefficiencies at deployment.
  We propose three optimization techniques to mitigate sources of
inefficiencies, namely (1) Dynamic gating, (2) Expert Buffering, and (3) Expert
load balancing. We show that dynamic gating improves execution time by
1.25-4$\times$ for LM, 2-5$\times$ for MT Encoder and 1.09-1.5$\times$ for MT
Decoder. It also reduces memory usage by up to 1.36$\times$ for LM and up to
1.1$\times$ for MT. We further propose Expert Buffering, a new caching
mechanism that only keeps hot, active experts in GPU memory while buffering the
rest in CPU memory. This reduces static memory allocation by 1.47$\times$. We
finally propose a load balancing methodology that provides additional
robustness to the workload. The code will be open-sourced upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rewarding Chatbots for Real-World Engagement with Millions of Users 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Irvine, Douglas Boubert, Vyas Raina, Adian Liusie, Vineet Mudupalli, Aliaksei Korshuk, Zongyi Liu, Fritz Cremer, Valentin Assassi, Christie-Carol Beauchamp, Xiaoding Lu, Thomas Rialan, William Beauchamp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of pretrained large language models has led to the deployment
of a range of social chatbots for chitchat. Although these chatbots demonstrate
language ability and fluency, they are not guaranteed to be engaging and can
struggle to retain users. This work investigates the development of social
chatbots that prioritize user engagement to enhance retention, specifically
examining the use of human feedback to efficiently develop highly engaging
chatbots. The proposed approach uses automatic pseudo-labels collected from
user interactions to train a reward model that can be used to reject
low-scoring sample responses generated by the chatbot model at inference time.
Intuitive evaluation metrics, such as mean conversation length (MCL), are
introduced as proxies to measure the level of engagement of deployed chatbots.
A/B testing on groups of 10,000 new daily chatbot users on the Chai Research
platform shows that this approach increases the MCL by up to 70%, which
translates to a more than 30% increase in user retention for a GPT-J 6B model.
Future work aims to use the reward model to realise a data fly-wheel, where the
latest user conversations can be used to alternately fine-tune the language
model and the reward model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Susceptibility to Influence of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lewis D Griffin, Bennett Kleinberg, Maximilian Mozes, Kimberly T Mai, Maria Vau, Matthew Caldwell, Augustine Marvor-Parker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two studies tested the hypothesis that a Large Language Model (LLM) can be
used to model psychological change following exposure to influential input. The
first study tested a generic mode of influence - the Illusory Truth Effect
(ITE) - where earlier exposure to a statement (through, for example, rating its
interest) boosts a later truthfulness test rating. Data was collected from 1000
human participants using an online experiment, and 1000 simulated participants
using engineered prompts and LLM completion. 64 ratings per participant were
collected, using all exposure-test combinations of the attributes: truth,
interest, sentiment and importance. The results for human participants
reconfirmed the ITE, and demonstrated an absence of effect for attributes other
than truth, and when the same attribute is used for exposure and test. The same
pattern of effects was found for LLM-simulated participants. The second study
concerns a specific mode of influence - populist framing of news to increase
its persuasion and political mobilization. Data from LLM-simulated participants
was collected and compared to previously published data from a 15-country
experiment on 7286 human participants. Several effects previously demonstrated
from the human study were replicated by the simulated study, including effects
that surprised the authors of the human study by contradicting their
theoretical expectations (anti-immigrant framing of news decreases its
persuasion and mobilization); but some significant relationships found in human
data (modulation of the effectiveness of populist framing according to relative
deprivation of the participant) were not present in the LLM data. Together the
two studies support the view that LLMs have potential to act as models of the
effect of influence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 6 figures, 7 tables, 53 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is In-hospital Meta-information Useful for Abstractive Discharge Summary
  Generation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenichiro Ando, Mamoru Komachi, Takashi Okumura, Hiromasa Horiguchi, Yuji Matsumoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During the patient's hospitalization, the physician must record daily
observations of the patient and summarize them into a brief document called
"discharge summary" when the patient is discharged. Automated generation of
discharge summary can greatly relieve the physicians' burden, and has been
addressed recently in the research community. Most previous studies of
discharge summary generation using the sequence-to-sequence architecture focus
on only inpatient notes for input. However, electric health records (EHR) also
have rich structured metadata (e.g., hospital, physician, disease, length of
stay, etc.) that might be useful. This paper investigates the effectiveness of
medical meta-information for summarization tasks. We obtain four types of
meta-information from the EHR systems and encode each meta-information into a
sequence-to-sequence model. Using Japanese EHRs, meta-information encoded
models increased ROUGE-1 by up to 4.45 points and BERTScore by 3.77 points over
the vanilla Longformer. Also, we found that the encoded meta-information
improves the precisions of its related terms in the outputs. Our results showed
the benefit of the use of medical meta-information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Knowledge Distillation from RNN-T Models With Noisy Training
  Labels Using Full-Sum Loss <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05958v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05958v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Zeineldeen, Kartik Audhkhasi, Murali Karthick Baskar, Bhuvana Ramabhadran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work studies knowledge distillation (KD) and addresses its constraints
for recurrent neural network transducer (RNN-T) models. In hard distillation, a
teacher model transcribes large amounts of unlabelled speech to train a student
model. Soft distillation is another popular KD method that distills the output
logits of the teacher model. Due to the nature of RNN-T alignments, applying
soft distillation between RNN-T architectures having different posterior
distributions is challenging. In addition, bad teachers having high
word-error-rate (WER) reduce the efficacy of KD. We investigate how to
effectively distill knowledge from variable quality ASR teachers, which has not
been studied before to the best of our knowledge. We show that a sequence-level
KD, full-sum distillation, outperforms other distillation methods for RNN-T
models, especially for bad teachers. We also propose a variant of full-sum
distillation that distills the sequence discriminative knowledge of the teacher
leading to further improvement in WER. We conduct experiments on public
datasets namely SpeechStew and LibriSpeech, and on in-house production data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Algorithmic Ghost in the Research Shell: Large Language Models and
  Academic Knowledge Creation in Management Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nigel Williams, Stanislav Ivanov, Dimitrios Buhalis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper looks at the role of large language models in academic knowledge
creation based on a scoping review (2018 to January 2023) of how researchers
have previously used the language model GPT to assist in the performance of
academic knowledge creation tasks beyond data analysis. These tasks include
writing, editing, reviewing, dataset creation and curation, which have been
difficult to perform using earlier ML tools. Based on a synthesis of these
papers, this study identifies pathways for a future academic research landscape
that incorporates wider usage of large language models based on the current
modes of adoption in published articles as a Co-Writer, Research Assistant and
Respondent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Creation and evaluation of timelines for longitudinal user posts <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Hills, Adam Tsakalidis, Federico Nanni, Ioannis Zachos, Maria Liakata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is increasing interest to work with user generated content in social
media, especially textual posts over time. Currently there is no consistent way
of segmenting user posts into timelines in a meaningful way that improves the
quality and cost of manual annotation. Here we propose a set of methods for
segmenting longitudinal user posts into timelines likely to contain interesting
moments of change in a user's behaviour, based on their online posting
activity. We also propose a novel framework for evaluating timelines and show
its applicability in the context of two different social media datasets.
Finally, we present a discussion of the linguistic content of highly ranked
timelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EACL 2023 (main, long); camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An algebraic approach to translating Japanese 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Boboc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We use Lambek's pregroups and the framework of compositional distributional
models of language ("DisCoCat") to study translations from Japanese to English
as pairs of functors. Adding decorations to pregroups we show how to handle
word order changes between languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, multiple diagrams and glosses</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An <span class="highlight-title">Overview</span> on Language Models: Recent Developments and Outlook 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengwei Wei, Yun-Cheng Wang, Bin Wang, C. -C. Jay Kuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language modeling studies the probability distributions over strings of
texts. It is one of the most fundamental tasks in natural language processing
(NLP). It has been widely used in text generation, speech recognition, machine
translation, etc. Conventional language models (CLMs) aim to predict the
probability of linguistic sequences in a causal manner. In contrast,
pre-trained language models (PLMs) cover broader concepts and can be used in
both causal sequential modeling and fine-tuning for downstream applications.
PLMs have their own training paradigms (usually self-supervised) and serve as
foundation models in modern NLP systems. This overview paper provides an
introduction to both CLMs and PLMs from five aspects, i.e., linguistic units,
structures, training methods, evaluation methods, and applications.
Furthermore, we discuss the relationship between CLMs and PLMs and shed light
on the future directions of language modeling in the pre-trained era.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIXPGD: Hybrid Adversarial Training for Speech Recognition Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aminul Huq, Weiyi Zhang, Xiaolin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic speech recognition (ASR) systems based on deep neural networks are
weak against adversarial perturbations. We propose mixPGD adversarial training
method to improve the robustness of the model for ASR systems. In standard
adversarial training, adversarial samples are generated by leveraging
supervised or unsupervised methods. We merge the capabilities of both
supervised and unsupervised approaches in our method to generate new
adversarial samples which aid in improving model robustness. Extensive
experiments and comparison across various state-of-the-art defense methods and
adversarial attacks have been performed to show that mixPGD gains 4.1% WER of
better performance than previous best performing models under white-box
adversarial attack setting. We tested our proposed defense method against both
white-box and transfer based black-box attack settings to ensure that our
defense strategy is robust against various types of attacks. Empirical results
on several adversarial attacks validate the effectiveness of our proposed
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MuLTI: Efficient Video-and-Language Understanding with MultiWay-Sampler
  and Multiple Choice Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Xu, Bo Liu, Yunkuo Chen, Mengli Cheng, Xing Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-and-language understanding has a variety of applications in the
industry, such as video question answering, text-video retrieval and
multi-label classification. Existing video-and-language understanding methods
generally adopt heavy multi-modal encoders and feature fusion modules, which
consume large amounts of GPU memory. Especially, they have difficulty dealing
with dense video frames or long text that are prevalent in industrial
applications. In this paper, we propose MuLTI, a highly accurate and
memory-efficient video-and-language understanding model that achieves efficient
and effective feature fusion through feature sampling and attention modules.
Therefore, MuLTI can handle longer sequences with limited GPU memory. Then, we
introduce an attention-based adapter to the encoders, which finetunes the
shallow features to improve the model's performance with low GPU memory
consumption. Finally, to further improve the model's performance, we introduce
a new pretraining task named Multiple Choice Modeling to bridge the task gap
between pretraining and downstream tasks and enhance the model's ability to
align the video and the text. Benefiting from the efficient feature fusion
module, the attention-based adapter and the new pretraining task, MuLTI
achieves state-of-the-art performance on multiple datasets. Implementation and
pretrained models will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logic Against Bias: Textual Entailment Mitigates Stereotypical Sentence
  Reasoning <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyin Luo, James Glass
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to their similarity-based learning objectives, pretrained sentence
encoders often internalize stereotypical assumptions that reflect the social
biases that exist within their training corpora. In this paper, we describe
several kinds of stereotypes concerning different communities that are present
in popular sentence representation models, including pretrained next sentence
prediction and contrastive sentence representation models. We compare such
models to textual entailment models that learn language logic for a variety of
downstream language understanding tasks. By comparing strong pretrained models
based on text similarity with textual entailment learning, we conclude that the
explicit logic learning with textual entailment can significantly reduce bias
and improve the recognition of social communities, without an explicit
de-biasing process
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Event-based News Narrative Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Keith Norambuena, Tanushree Mitra, Chris North
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Narratives are fundamental to our understanding of the world, providing us
with a natural structure for knowledge representation over time. Computational
narrative extraction is a subfield of artificial intelligence that makes heavy
use of information retrieval and natural language processing techniques.
Despite the importance of computational narrative extraction, relatively little
scholarly work exists on synthesizing previous research and strategizing future
research in the area. In particular, this article focuses on extracting news
narratives from an event-centric perspective. Extracting narratives from news
data has multiple applications in understanding the evolving information
landscape. This survey presents an extensive study of research in the area of
event-based news narrative extraction. In particular, we screened over 900
articles that yielded 54 relevant articles. These articles are synthesized and
organized by representation model, extraction criteria, and evaluation
approaches. Based on the reviewed studies, we identify recent trends, open
challenges, and potential research lines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 3 figures, to be published in the journal ACM CSUR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning the Legibility of Visual Text Perturbations <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dev Seth, Rickard Stureborg, Danish Pruthi, Bhuwan Dhingra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many adversarial attacks in NLP perturb inputs to produce visually similar
strings ('ergo' $\rightarrow$ '$\epsilon$rgo') which are legible to humans but
degrade model performance. Although preserving legibility is a necessary
condition for text perturbation, little work has been done to systematically
characterize it; instead, legibility is typically loosely enforced via
intuitions around the nature and extent of perturbations. Particularly, it is
unclear to what extent can inputs be perturbed while preserving legibility, or
how to quantify the legibility of a perturbed string. In this work, we address
this gap by learning models that predict the legibility of a perturbed string,
and rank candidate perturbations based on their legibility. To do so, we
collect and release LEGIT, a human-annotated dataset comprising the legibility
of visually perturbed text. Using this dataset, we build both text- and
vision-based models which achieve up to $0.91$ F1 score in predicting whether
an input is legible, and an accuracy of $0.86$ in predicting which of two given
perturbations is more legible. Additionally, we discover that legible
perturbations from the LEGIT dataset are more effective at lowering the
performance of NLP models than best-known attack strategies, suggesting that
current models may be vulnerable to a broad range of perturbations beyond what
is captured by existing visual attacks. Data, code, and models are available at
https://github.com/dvsth/learning-legibility-2023.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures. Accepted at EACL 2023 (main, long)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Arabic aspect sentiment polarity classification using <span class="highlight-title">BERT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.13290v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.13290v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed M. Abdelgwad, Taysir Hassan A Soliman, Ahmed I. Taloba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspect-based sentiment analysis(ABSA) is a textual analysis methodology that
defines the polarity of opinions on certain aspects related to specific
targets. The majority of research on ABSA is in English, with a small amount of
work available in Arabic. Most previous Arabic research has relied on deep
learning models that depend primarily on context-independent word embeddings
(e.g.word2vec), where each word has a fixed representation independent of its
context. This article explores the modeling capabilities of contextual
embeddings from pre-trained language models, such as BERT, and making use of
sentence pair input on Arabic aspect sentiment polarity classification task. In
particular, we develop a simple but effective BERT-based neural baseline to
handle this task. Our BERT architecture with a simple linear classification
layer surpassed the state-of-the-art works, according to the experimental
results on three different Arabic datasets. Achieving an accuracy of 89.51% on
the Arabic hotel reviews dataset, 73% on the Human annotated book reviews
dataset, and 85.73% on the Arabic news dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span>-3-driven pedagogical agents for training children's curious
  question-asking skills 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14228v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14228v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rania Abdelghani, Yen-Hsiang Wang, Xingdi Yuan, Tong Wang, Pauline Lucas, Hélène Sauzéon, Pierre-Yves Oudeyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to train children's ability to ask curiosity-driven questions,
previous research has explored designing specific exercises relying on
providing semantic and linguistic cues to help formulate such questions. But
despite showing pedagogical efficiency, this method is still limited as it
relies on generating the said cues by hand, which can be a very costly process.
In this context, we propose to leverage advances in the natural language
processing field (NLP) and investigate the efficiency of using a large language
model (LLM) for automating the production of the pedagogical content of a
curious question-asking (QA) training. We study generating the said content
using the "prompt-based" method that consists of explaining the task to the LLM
in natural text. We evaluate the output using human experts annotations and
comparisons with hand-generated content. Results suggested indeed the relevance
and usefulness of this content. We also conduct a field study in primary school
(75 children aged 9-10), where we evaluate children's QA performance when
having this training. We compare 3 types of content : 1) hand-generated content
that proposes "closed" cues leading to predefined questions; 2) GPT-3-generated
content that proposes the same type of cues; 3) GPT-3-generated content that
proposes "open" cues leading to several possible questions. We see a similar QA
performance between the two "closed" trainings (showing the scalability of the
approach using GPT-3), and a better one for participants with the "open"
training. These results suggest the efficiency of using LLMs to support
children in generating more curious questions, using a natural language
prompting approach that affords usability by teachers and other users not
specialists of AI techniques. Furthermore, results also show that open-ended
content may be more suitable for training curious question-asking skills.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models Are Human-Level <span class="highlight-title">Prompt</span> Engineers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By conditioning on natural language instructions, large language models
(LLMs) have displayed impressive capabilities as general-purpose computers.
However, task performance depends significantly on the quality of the prompt
used to steer the model, and most effective prompts have been handcrafted by
humans. Inspired by classical program synthesis and the human approach to
prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic
instruction generation and selection. In our method, we treat the instruction
as the "program," optimized by searching over a pool of instruction candidates
proposed by an LLM in order to maximize a chosen score function. To evaluate
the quality of the selected instruction, we evaluate the zero-shot performance
of another LLM following the selected instruction. Experiments on 24 NLP tasks
show that our automatically generated instructions outperform the prior LLM
baseline by a large margin and achieve better or comparable performance to the
instructions generated by human annotators on 19/24 tasks. We conduct extensive
qualitative and quantitative analyses to explore the performance of APE. We
show that APE-engineered prompts can be applied to steer models toward
truthfulness and/or informativeness, as well as to improve few-shot learning
performance by simply prepending them to standard in-context learning prompts.
Please check out our webpage at
https://sites.google.com/view/automatic-prompt-engineer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Kind Introduction to Lexical and Grammatical Aspect, with a <span class="highlight-title">Survey</span> of
  Computational Approaches <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.09012v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.09012v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Annemarie Friedrich, Nianwen Xue, Alexis Palmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspectual meaning refers to how the internal temporal structure of situations
is presented. This includes whether a situation is described as a state or as
an event, whether the situation is finished or ongoing, and whether it is
viewed as a whole or with a focus on a particular phase. This survey gives an
overview of computational approaches to modeling lexical and grammatical aspect
along with intuitive explanations of the necessary linguistic concepts and
terminology. In particular, we describe the concepts of stativity, telicity,
habituality, perfective and imperfective, as well as influential inventories of
eventuality and situation types. We argue that because aspect is a crucial
component of semantics, especially when it comes to reporting the temporal
structure of situations in a precise way, future NLP approaches need to be able
to handle and evaluate it systematically in order to achieve human-level
language understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EACL 2023, camera ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Adaptive Named Entity Recognition by Retrieving Unstructured
  Knowledge <span class="chip">EACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07523v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07523v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kosuke Nishida, Naoki Yoshinaga, Kyosuke Nishida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although named entity recognition (NER) helps us to extract domain-specific
entities from text (e.g., artists in the music domain), it is costly to create
a large amount of training data or a structured knowledge base to perform
accurate NER in the target domain. Here, we propose self-adaptive NER, which
retrieves external knowledge from unstructured text to learn the usages of
entities that have not been learned well. To retrieve useful knowledge for NER,
we design an effective two-stage model that retrieves unstructured knowledge
using uncertain entities as queries. Our model predicts the entities in the
input and then finds those of which the prediction is not confident. Then, it
retrieves knowledge by using these uncertain entities as queries and
concatenates the retrieved text to the original input to revise the prediction.
Experiments on CrossNER datasets demonstrated that our model outperforms strong
baselines by 2.35 points in F1 metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL2023 (long)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topic Modelling of Swedish Newspaper Articles about Coronavirus: a Case
  Study using Latent Dirichlet Allocation Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.03029v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.03029v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernadeta Griciūtė, Lifeng Han, Hao Li, Goran Nenadic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic Modelling (TM) is from the research branches of natural language
understanding (NLU) and natural language processing (NLP) that is to facilitate
insightful analysis from large documents and datasets, such as a summarisation
of main topics and the topic changes. This kind of discovery is getting more
popular in real-life applications due to its impact on big data analytics. In
this study, from the social-media and healthcare domain, we apply popular
Latent Dirichlet Allocation (LDA) methods to model the topic changes in Swedish
newspaper articles about Coronavirus. We describe the corpus we created
including 6515 articles, methods applied, and statistics on topic changes over
approximately 1 year and two months period of time from 17th January 2020 to
13th March 2021. We hope this work can be an asset for grounding applications
of topic modelling and can be inspiring for similar case studies in an era with
pandemics, to support socio-economic impact research as well as clinical and
healthcare analytics. Our data and source code are openly available at
https://github. com/poethan/Swed_Covid_TM Keywords: Latent Dirichlet Allocation
(LDA); Topic Modelling; Coronavirus; Pandemics; Natural Language Understanding;
BERT-topic
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fillers in Spoken Language Understanding: Computational and
  Psycholinguistic Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10761v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10761v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanvi Dinkar, Chloé Clavel, Ioana Vasilescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Disfluencies (i.e. interruptions in the regular flow of speech), are
ubiquitous to spoken discourse. Fillers ("uh", "um") are disfluencies that
occur the most frequently compared to other kinds of disfluencies. Yet, to the
best of our knowledge, there isn't a resource that brings together the research
perspectives influencing Spoken Language Understanding (SLU) on these speech
events. This aim of this article is to survey a breadth of perspectives in a
holistic way; i.e. from considering underlying (psycho)linguistic theory, to
their annotation and consideration in Automatic Speech Recognition (ASR) and
SLU systems, to lastly, their study from a generation standpoint. This article
aims to present the perspectives in an approachable way to the SLU and
Conversational AI community, and discuss moving forward, what we believe are
the trends and challenges in each area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in TAL Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Word-Graph2vec: An efficient word embedding approach on word
  co-occurrence graph using random walk sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.04312v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.04312v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenting Li, Yuanzhe Cai, Jiahong Xue, Xi Zhang, Huacan Chen, Zeyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word embedding has become ubiquitous and is widely used in various text
mining and natural language processing (NLP) tasks, such as information
retrieval, semantic analysis, and machine translation, among many others.
Unfortunately, it is prohibitively expensive to train the word embedding in a
relatively large corpus. We propose a graph-based word embedding algorithm,
called Word-Graph2vec, which converts the large corpus into a word
co-occurrence graph, then takes the word sequence samples from this graph by
randomly traveling and trains the word embedding on this sampling corpus in the
end. We posit that because of the stable vocabulary, relative idioms, and fixed
expressions in English, the size and density of the word co-occurrence graph
change slightly with the increase in the training corpus. So that
Word-Graph2vec has stable runtime on the large scale data set, and its
performance advantage becomes more and more obvious with the growth of the
training corpus. Extensive experiments conducted on real-world datasets show
that the proposed algorithm outperforms traditional Skip-Gram by four-five
times in terms of efficiency, while the error generated by the random walk
sampling is small.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">BERT</span>-Deep CNN: State-of-the-Art for Sentiment Analysis of COVID-19
  Tweets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09733v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09733v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javad Hassannataj Joloudari, Sadiq Hussain, Mohammad Ali Nematollahi, Rouhollah Bagheri, Fatemeh Fazl, Roohallah Alizadehsani, Reza Lashgari, Ashis Talukder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The free flow of information has been accelerated by the rapid development of
social media technology. There has been a significant social and psychological
impact on the population due to the outbreak of Coronavirus disease (COVID-19).
The COVID-19 pandemic is one of the current events being discussed on social
media platforms. In order to safeguard societies from this pandemic, studying
people's emotions on social media is crucial. As a result of their particular
characteristics, sentiment analysis of texts like tweets remains challenging.
Sentiment analysis is a powerful text analysis tool. It automatically detects
and analyzes opinions and emotions from unstructured data. Texts from a wide
range of sources are examined by a sentiment analysis tool, which extracts
meaning from them, including emails, surveys, reviews, social media posts, and
web articles. To evaluate sentiments, natural language processing (NLP) and
machine learning techniques are used, which assign weights to entities, topics,
themes, and categories in sentences or phrases. Machine learning tools learn
how to detect sentiment without human intervention by examining examples of
emotions in text. In a pandemic situation, analyzing social media texts to
uncover sentimental trends can be very helpful in gaining a better
understanding of society's needs and predicting future trends. We intend to
study society's perception of the COVID-19 pandemic through social media using
state-of-the-art BERT and Deep CNN models. The superiority of BERT models over
other deep models in sentiment analysis is evident and can be concluded from
the comparison of the various research studies mentioned in this article.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach
  for Speech Emotion Recognition <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Ye, Xin-cheng Wen, Yujie Wei, Yong Xu, Kunhong Liu, Hongming Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech emotion recognition (SER) plays a vital role in improving the
interactions between humans and machines by inferring human emotion and
affective states from speech signals. Whereas recent works primarily focus on
mining spatiotemporal information from hand-crafted features, we explore how to
model the temporal patterns of speech emotions from dynamic temporal scales.
Towards that goal, we introduce a novel temporal emotional modeling approach
for SER, termed Temporal-aware bI-direction Multi-scale Network (TIM-Net),
which learns multi-scale contextual affective representations from various time
scales. Specifically, TIM-Net first employs temporal-aware blocks to learn
temporal affective representation, then integrates complementary information
from the past and the future to enrich contextual representations, and finally,
fuses multiple time scale features for better adaptation to the emotional
variation. Extensive experimental results on six benchmark SER datasets
demonstrate the superior performance of TIM-Net, gaining 2.34% and 2.61%
improvements of the average UAR and WAR over the second-best on each corpus.
The source code is available at https://github.com/Jiaxin-Ye/TIM-Net_SER.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoSyn: Detecting Implicit Hate Speech in Online Conversations Using a
  Context Synergized Hyperbolic Network <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03387v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03387v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sreyan Ghosh, Manan Suri, Purva Chiniya, Utkarsh Tyagi, Sonal Kumar, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The tremendous growth of social media users interacting in online
conversations has also led to significant growth in hate speech. Most of the
prior works focus on detecting explicit hate speech, which is overt and
leverages hateful phrases, with very little work focusing on detecting hate
speech that is implicit or denotes hatred through indirect or coded language.
In this paper, we present CoSyn, a user- and conversational-context synergized
network for detecting implicit hate speech in online conversation trees. CoSyn
first models the user's personal historical and social context using a novel
hyperbolic Fourier attention mechanism and hyperbolic graph convolution
network. Next, we jointly model the user's personal context and the
conversational context using a novel context interaction mechanism in the
hyperbolic space that clearly captures the interplay between the two and makes
independent assessments on the amounts of information to be retrieved from both
contexts. CoSyn performs all operations in the hyperbolic space to account for
the scale-free dynamics of social media. We demonstrate the effectiveness of
CoSyn both qualitatively and quantitatively on an open-source hate speech
dataset with Twitter conversations and show that CoSyn outperforms all our
baselines in detecting implicit hate speech with absolute improvements in the
range of 8.15% - 19.50%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at IJCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReAct: Synergizing Reasoning and Acting in Language Models <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.03629v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.03629v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have demonstrated impressive capabilities
across tasks in language understanding and interactive decision making, their
abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.
action plan generation) have primarily been studied as separate topics. In this
paper, we explore the use of LLMs to generate both reasoning traces and
task-specific actions in an interleaved manner, allowing for greater synergy
between the two: reasoning traces help the model induce, track, and update
action plans as well as handle exceptions, while actions allow it to interface
with external sources, such as knowledge bases or environments, to gather
additional information. We apply our approach, named ReAct, to a diverse set of
language and decision making tasks and demonstrate its effectiveness over
state-of-the-art baselines, as well as improved human interpretability and
trustworthiness over methods without reasoning or acting components.
Concretely, on question answering (HotpotQA) and fact verification (Fever),
ReAct overcomes issues of hallucination and error propagation prevalent in
chain-of-thought reasoning by interacting with a simple Wikipedia API, and
generates human-like task-solving trajectories that are more interpretable than
baselines without reasoning traces. On two interactive decision making
benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and
reinforcement learning methods by an absolute success rate of 34% and 10%
respectively, while being prompted with only one or two in-context examples.
Project site with code: https://react-lm.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v3 is the ICLR camera ready version with some typos fixed. Project
  site with code: https://react-lm.github.io</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QVRF: A Quantization-error-aware Variable Rate Framework for Learned
  Image Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kedeng Tong, Yaojun Wu, Yue Li, Kai Zhang, Li Zhang, Xin Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learned image compression has exhibited promising compression performance,
but variable bitrates over a wide range remain a challenge. State-of-the-art
variable rate methods compromise the loss of model performance and require
numerous additional parameters. In this paper, we present a
Quantization-error-aware Variable Rate Framework (QVRF) that utilizes a
univariate quantization regulator a to achieve wide-range variable rates within
a single model. Specifically, QVRF defines a quantization regulator vector
coupled with predefined Lagrange multipliers to control quantization error of
all latent representation for discrete variable rates. Additionally, the
reparameterization method makes QVRF compatible with a round quantizer.
Exhaustive experiments demonstrate that existing fixed-rate VAE-based methods
equipped with QVRF can achieve wide-range continuous variable rates within a
single model without significant performance degradation. Furthermore, QVRF
outperforms contemporary variable-rate methods in rate-distortion performance
with minimal additional parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MuLTI: Efficient Video-and-Language Understanding with MultiWay-Sampler
  and Multiple Choice Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Xu, Bo Liu, Yunkuo Chen, Mengli Cheng, Xing Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-and-language understanding has a variety of applications in the
industry, such as video question answering, text-video retrieval and
multi-label classification. Existing video-and-language understanding methods
generally adopt heavy multi-modal encoders and feature fusion modules, which
consume large amounts of GPU memory. Especially, they have difficulty dealing
with dense video frames or long text that are prevalent in industrial
applications. In this paper, we propose MuLTI, a highly accurate and
memory-efficient video-and-language understanding model that achieves efficient
and effective feature fusion through feature sampling and attention modules.
Therefore, MuLTI can handle longer sequences with limited GPU memory. Then, we
introduce an attention-based adapter to the encoders, which finetunes the
shallow features to improve the model's performance with low GPU memory
consumption. Finally, to further improve the model's performance, we introduce
a new pretraining task named Multiple Choice Modeling to bridge the task gap
between pretraining and downstream tasks and enhance the model's ability to
align the video and the text. Benefiting from the efficient feature fusion
module, the attention-based adapter and the new pretraining task, MuLTI
achieves state-of-the-art performance on multiple datasets. Implementation and
pretrained models will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Video Compression with Diverse Contexts <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14402v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14402v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Li, Bin Li, Yan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For any video codecs, the coding efficiency highly relies on whether the
current signal to be encoded can find the relevant contexts from the previous
reconstructed signals. Traditional codec has verified more contexts bring
substantial coding gain, but in a time-consuming manner. However, for the
emerging neural video codec (NVC), its contexts are still limited, leading to
low compression ratio. To boost NVC, this paper proposes increasing the context
diversity in both temporal and spatial dimensions. First, we guide the model to
learn hierarchical quality patterns across frames, which enriches long-term and
yet high-quality temporal contexts. Furthermore, to tap the potential of
optical flow-based coding framework, we introduce a group-based offset
diversity where the cross-group interaction is proposed for better context
mining. In addition, this paper also adopts a quadtree-based partition to
increase spatial context diversity when encoding the latent representation in
parallel. Experiments show that our codec obtains 23.5% bitrate saving over
previous SOTA NVC. Better yet, our codec has surpassed the under-developing
next generation traditional codec/ECM in both RGB and YUV420 colorspaces, in
terms of PSNR. The codes are at https://github.com/microsoft/DCVC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023. Codes are at https://github.com/microsoft/DCVC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Audio-Visual Masked Autoencoder <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07839v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07839v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Gong, Andrew Rouditchenko, Alexander H. Liu, David Harwath, Leonid Karlinsky, Hilde Kuehne, James Glass
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we first extend the recent Masked Auto-Encoder (MAE) model
from a single modality to audio-visual multi-modalities. Subsequently, we
propose the Contrastive Audio-Visual Masked Auto-Encoder (CAV-MAE) by combining
contrastive learning and masked data modeling, two major self-supervised
learning frameworks, to learn a joint and coordinated audio-visual
representation. Our experiments show that the contrastive audio-visual
correspondence learning objective not only enables the model to perform
audio-visual retrieval tasks, but also helps the model learn a better joint
representation. As a result, our fully self-supervised pretrained CAV-MAE
achieves a new SOTA accuracy of 65.9% on VGGSound, and is comparable with the
previous best supervised pretrained model on AudioSet in the audio-visual event
classification task. Code and pretrained models are at
https://github.com/yuangongnd/cav-mae.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2023 as a notable top 25% paper. Code and pretrained
  models are at https://github.com/yuangongnd/cav-mae</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-09T00:00:00Z">2023-03-09</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalization analysis of an unfolding network for analysis-based
  Compressed Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vicky Kouni, Yannis Panagakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unfolding networks have shown promising results in the Compressed Sensing
(CS) field. Yet, the investigation of their generalization ability is still in
its infancy. In this paper, we perform generalization analysis of a
state-of-the-art ADMM-based unfolding network, which jointly learns a decoder
for CS and a sparsifying redundant analysis operator. To this end, we first
impose a structural constraint on the learnable sparsifier, which parametrizes
the network's hypothesis class. For the latter, we estimate its Rademacher
complexity. With this estimate in hand, we deliver generalization error bounds
for the examined network. Finally, the validity of our theory is assessed and
numerical comparisons to a state-of-the-art unfolding network are made, on
synthetic and real-world datasets. Our experimental results demonstrate that
our proposed framework complies with our theoretical findings and outperforms
the baseline, consistently for all datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Robustness of Conversational Recommender Systems by
  Adversarial Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Montazeralghaem, James Allan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational recommender systems (CRSs) are improving rapidly, according to
the standard recommendation accuracy metrics. However, it is essential to make
sure that these systems are robust in interacting with users including regular
and malicious users who want to attack the system by feeding the system
modified input data. In this paper, we propose an adversarial evaluation scheme
including four scenarios in two categories and automatically generate
adversarial examples to evaluate the robustness of these systems in the face of
different input data. By executing these adversarial examples we can compare
the ability of different conversational recommender systems to satisfy the
user's preferences. We evaluate three CRSs by the proposed adversarial examples
on two datasets. Our results show that none of these systems are robust and
reliable to the adversarial examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modelling Projection Bias in Intertemporal Choices: A Prospect Theory
  Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingming Li, H. Vicky Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Users often face bundle promotions when purchasing, where they have to select
between two options: buy the single item at full price, or buy the bundle at a
discount. In this scenario, users' preferences are usually influenced by the
projection bias, that is, users often believe that their future preferences are
similar to their current preferences, causing them to make irrational and
short-sighted decisions. It is of great significance to analyze the effect of
the projection bias on users' preferences, and this study may help understand
users' decision-making process and provide bundling and pricing strategies for
sellers. Prior works typically use a linear bias model for qualitative
analysis, and they cannot quantitatively calculate users' nonlinear and
personalized bias. In this work, we propose Pobe, a projection bias-embedded
preference model to accurately predict users' choices. The proposed Pobe
introduces the prospect theory to analyze users' irrational decisions, and
utilizes the weight function to handle users' nonlinear and personalized bias.
Based on the proposed Pobe, we also study the impact of items' correlations or
discount prices on users' choices, and provide four bundling strategies.
Experimental results show that the proposed method can achieve better
performance than prior works, especially when only small data is available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Algorithmic neutrality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milo Phillips-Brown
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bias infects the algorithms that wield increasing control over our lives.
Predictive policing systems overestimate crime in communities of color; hiring
algorithms dock qualified female candidates; and facial recognition software
struggles to recognize dark-skinned faces. Algorithmic bias has received
significant attention. Algorithmic neutrality, in contrast, has been largely
neglected. Algorithmic neutrality is my topic. I take up three questions. What
is algorithmic neutrality? Is algorithmic neutrality possible? When we have an
eye to algorithmic neutrality, what can we learn about algorithmic bias? To
answer these questions in concrete terms, I work with a case study: search
engines. Drawing on work about neutrality in science, I say that a search
engine is neutral only if certain values, like political ideologies or the
financial interests of the search engine operator, play no role in how the
search engine ranks pages. Search neutrality, I argue, is impossible. Its
impossibility seems to threaten the significance of search bias: if no search
engine is neutral, then every search engine is biased. To defuse this threat, I
distinguish two forms of bias, failing-on-its-own-terms bias and other-values
bias. This distinction allows us to make sense of search bias, and capture its
normative complexion, despite the impossibility of neutrality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Recommendation Systems with User Personality Inferred from
  Product <span class="highlight-title">Review</span>s <span class="chip">WSDM'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Lu, Min-Yen Kan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personality is a psychological factor that reflects people's preferences,
which in turn influences their decision-making. We hypothesize that accurate
modeling of users' personalities improves recommendation systems' performance.
However, acquiring such personality profiles is both sensitive and expensive.
We address this problem by introducing a novel method to automatically extract
personality profiles from public product review text. We then design and assess
three context-aware recommendation architectures that leverage the profiles to
test our hypothesis.
  Experiments on our two newly contributed personality datasets --
Amazon-beauty and Amazon-music -- validate our hypothesis, showing performance
boosts of 3--28%.Our analysis uncovers that varying personality types
contribute differently to recommendation performance: open and extroverted
personalities are most helpful in music recommendation, while a conscientious
personality is most helpful in beauty product recommendation. The dataset is
available at https://github.com/XinyuanLu00/IRS-WSDM2023-personality-dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IRS@WSDM'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Complex QA and language models hybrid architectures, <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09051v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09051v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, Elisabeth Murisasco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper reviews the state-of-the-art of hybrid language models
architectures and strategies for "complex" question-answering (QA, CQA, CPS).
Large Language Models (LLM) are good at leveraging public data on standard
problems but once you want to tackle more specific complex questions or
problems you may need specific architecture, knowledge, skills, methods,
sensitive data protection, explainability, human approval and versatile
feedback... We identify key elements augmenting LLM to solve complex questions
or problems. We extend findings from the robust community edited research
papers BIG, BLOOM and HELM which open source, benchmark and analyze limits and
challenges of LLM in terms of tasks complexity and strict evaluation on
accuracy (e.g. fairness, robustness, toxicity, ...). Recent projects like
ChatGPT and GALACTICA have allowed non-specialists to grasp the great potential
as well as the equally strong limitations of language models in complex QA.
Hybridizing these models with different components could allow to overcome
these different limits and go much further. We discuss some challenges
associated with complex QA, including domain adaptation, decomposition and
efficient multi-step QA, long form and non-factoid QA, safety and
multi-sensitivity data protection, multimodal search, hallucinations,
explainability and truthfulness, temproal reasoning. Therefore, we analyze
current solutions and promising research trends, using elements such as: hybrid
LLM architectures, active human reinforcement learning supervised with AI,
prompting adaptation, neuro-symbolic and structured knowledge grounding,
program synthesis, iterated decomposition and others.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Next Basket Recommendation Reality Check 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.14233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.14233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Li, Sami Jullien, Mozhdeh Ariannezhad, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of a next basket recommendation (NBR) system is to recommend items
for the next basket for a user, based on the sequence of their prior baskets.
Recently, a number of methods with complex modules have been proposed that
claim state-of-the-art performance. They rarely look into the predicted basket
and just provide intuitive reasons for the observed improvements, e.g., better
representation, capturing intentions or relations, etc. We provide a novel
angle on the evaluation of next basket recommendation methods, centered on the
distinction between repetition and exploration: the next basket is typically
composed of previously consumed items (i.e., repeat items) and new items (i.e,
explore items). We propose a set of metrics that measure the repeat/explore
ratio and performance of NBR models. Using these new metrics, we analyze
state-of-the-art NBR models. The results of our analysis help to clarify the
extent of the actual progress achieved by existing NBR methods as well as the
underlying reasons for the improvements. Overall, our work sheds light on the
evaluation problem of NBR and provides useful insights into the model design
for this task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to ACM TOIS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Federated Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehua Sun, Yonghui Xu, Yong Liu, Wei He, Lanju Kong, Fangzhao Wu, Yali Jiang, Lizhen Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning has recently been applied to recommendation systems to
protect user privacy. In federated learning settings, recommendation systems
can train recommendation models only collecting the intermediate parameters
instead of the real user data, which greatly enhances the user privacy. Beside,
federated recommendation systems enable to collaborate with other data
platforms to improve recommended model performance while meeting the regulation
and privacy constraints. However, federated recommendation systems faces many
new challenges such as privacy, security, heterogeneity and communication
costs. While significant research has been conducted in these areas, gaps in
the surveying literature still exist. In this survey, we-(1) summarize some
common privacy mechanisms used in federated recommendation systems and discuss
the advantages and limitations of each mechanism; (2) review some robust
aggregation strategies and several novel attacks against security; (3)
summarize some approaches to address heterogeneity and communication costs
problems; (4)introduce some open source platforms that can be used to build
federated recommendation systems; (5) present some prospective research
directions in the future. This survey can guide researchers and practitioners
understand the research progress in these areas.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open World Classification with Adaptive Negative Samples <span class="chip">EMNLP 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Bai, Guoyin Wang, Jiwei Li, Sunghyun Park, Sungjin Lee, Puyang Xu, Ricardo Henao, Lawrence Carin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open world classification is a task in natural language processing with key
practical relevance and impact. Since the open or {\em unknown} category data
only manifests in the inference phase, finding a model with a suitable decision
boundary accommodating for the identification of known classes and
discrimination of the open category is challenging. The performance of existing
models is limited by the lack of effective open category data during the
training stage or the lack of a good mechanism to learn appropriate decision
boundaries. We propose an approach based on \underline{a}daptive
\underline{n}egative \underline{s}amples (ANS) designed to generate effective
synthetic open category samples in the training stage and without requiring any
prior knowledge or external datasets. Empirically, we find a significant
advantage in using auxiliary one-versus-rest binary classifiers, which
effectively utilize the generated negative samples and avoid the complex
threshold-seeking stage in previous works. Extensive experiments on three
benchmark datasets show that ANS achieves significant improvements over
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2021 (Main Track, Long Paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Planning with Large Language Models for Code Generation <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B. Tenenbaum, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing large language model-based code generation pipelines typically use
beam search or sampling algorithms during the decoding process. Although the
programs they generate achieve high token-matching-based scores, they often
fail to compile or generate incorrect outputs. The main reason is that
conventional Transformer decoding algorithms may not be the best choice for
code generation. In this work, we propose a novel Transformer decoding
algorithm, Planning-Guided Transformer Decoding (PG-TD), that uses a planning
algorithm to do lookahead search and guide the Transformer to generate better
programs. Specifically, instead of simply optimizing the likelihood of the
generated sequences, the Transformer makes use of a planner to generate
candidate programs and test them on public test cases. The Transformer can
therefore make more informed decisions and generate tokens that will eventually
lead to higher-quality programs. We also design a mechanism that shares
information between the Transformer and the planner to make our algorithm
computationally efficient. We empirically evaluate our framework with several
large language models as backbones on public coding challenge benchmarks,
showing that 1) it can generate programs that consistently achieve higher
performance compared with competing baseline methods; 2) it enables
controllable code generation, such as concise codes and highly-commented codes
by optimizing modified objective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023. Project page:https://codeaimcts.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalisation within bounds: A risk taxonomy and policy framework for
  the alignment of large language models with personalised feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah Rose Kirk, Bertie Vidgen, Paul Röttger, Scott A. Hale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are used to generate content for a wide range of
tasks, and are set to reach a growing audience in coming years due to
integration in product interfaces like ChatGPT or search engines like Bing.
This intensifies the need to ensure that models are aligned with human
preferences and do not produce unsafe, inaccurate or toxic outputs. While
alignment techniques like reinforcement learning with human feedback (RLHF) and
red-teaming can mitigate some safety concerns and improve model capabilities,
it is unlikely that an aggregate fine-tuning process can adequately represent
the full range of users' preferences and values. Different people may
legitimately disagree on their preferences for language and conversational
norms, as well as on values or ideologies which guide their communication.
Personalising LLMs through micro-level preference learning processes may result
in models that are better aligned with each user. However, there are several
normative challenges in defining the bounds of a societally-acceptable and safe
degree of personalisation. In this paper, we ask how, and in what ways, LLMs
should be personalised. First, we review literature on current paradigms for
aligning LLMs with human feedback, and identify issues including (i) a lack of
clarity regarding what alignment means; (ii) a tendency of technology providers
to prescribe definitions of inherently subjective preferences and values; and
(iii) a 'tyranny of the crowdworker', exacerbated by a lack of documentation in
who we are really aligning to. Second, we present a taxonomy of benefits and
risks associated with personalised LLMs, for individuals and society at large.
Finally, we propose a three-tiered policy framework that allows users to
experience the benefits of personalised alignment, while restraining unsafe and
undesirable LLM-behaviours within (supra-)national and organisational bounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Robustness of Text Vectorizers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rémi Catellier, Samuel Vaiter, Damien Garreau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental issue in natural language processing is the robustness of the
models with respect to changes in the input. One critical step in this process
is the embedding of documents, which transforms sequences of words or tokens
into vector representations. Our work formally proves that popular embedding
schemes, such as concatenation, TF-IDF, and Paragraph Vector (a.k.a. doc2vec),
exhibit robustness in the H\"older or Lipschitz sense with respect to the
Hamming distance. We provide quantitative bounds for these schemes and
demonstrate how the constants involved are affected by the length of the
document. These findings are exemplified through a series of numerical
examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing Chat<span class="highlight-title">GPT</span> Through Students' Eyes: An Analysis of TikTok Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna-Carolina Haensch, Sarah Ball, Markus Herklotz, Frauke Kreuter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advanced large language models like ChatGPT have gained considerable
attention recently, including among students. However, while the debate on
ChatGPT in academia is making waves, more understanding is needed among
lecturers and teachers on how students use and perceive ChatGPT. To address
this gap, we analyzed the content on ChatGPT available on TikTok in February
2023. TikTok is a rapidly growing social media platform popular among
individuals under 30. Specifically, we analyzed the content of the 100 most
popular videos in English tagged with #chatgpt, which collectively garnered
over 250 million views. Most of the videos we studied promoted the use of
ChatGPT for tasks like writing essays or code. In addition, many videos
discussed AI detectors, with a focus on how other tools can help to transform
ChatGPT output to fool these detectors. This also mirrors the discussion among
educators on how to treat ChatGPT as lecturers and teachers in teaching and
grading. What is, however, missing from the analyzed clips on TikTok are videos
that discuss ChatGPT producing content that is nonsensical or unfaithful to the
training data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Replacement as a Self-supervision for Fine-grained Vision-language
  <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisai Zhang, Qingcai Chen, Zhijian Chen, Yunpeng Han, Zhonghua Li, Zhao Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-grained supervision based on object annotations has been widely used for
vision and language pre-training (VLP). However, in real-world application
scenarios, aligned multi-modal data is usually in the image-caption format,
which only provides coarse-grained supervision. It is cost-expensive to collect
object annotations and build object annotation pre-extractor for different
scenarios. In this paper, we propose a fine-grained self-supervision signal
without object annotations from a replacement perspective. First, we propose a
homonym sentence rewriting (HSR) algorithm to provide token-level supervision.
The algorithm replaces a verb/noun/adjective/quantifier word of the caption
with its homonyms from WordNet. Correspondingly, we propose a replacement
vision-language modeling (RVLM) framework to exploit the token-level
supervision. Two replaced modeling tasks, i.e., replaced language contrastive
(RLC) and replaced language modeling (RLM), are proposed to learn the
fine-grained alignment. Extensive experiments on several downstream tasks
demonstrate the superior performance of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MixSpeech: Cross-Modality Self-Learning with Audio-Visual Stream Mixup
  for Visual Speech Translation and Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xize Cheng, Linjun Li, Tao Jin, Rongjie Huang, Wang Lin, Zehan Wang, Huangdai Liu, Ye Wang, Aoxiong Yin, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-media communications facilitate global interaction among people.
However, despite researchers exploring cross-lingual translation techniques
such as machine translation and audio speech translation to overcome language
barriers, there is still a shortage of cross-lingual studies on visual speech.
This lack of research is mainly due to the absence of datasets containing
visual speech and translated text pairs. In this paper, we present
\textbf{AVMuST-TED}, the first dataset for \textbf{A}udio-\textbf{V}isual
\textbf{Mu}ltilingual \textbf{S}peech \textbf{T}ranslation, derived from
\textbf{TED} talks. Nonetheless, visual speech is not as distinguishable as
audio speech, making it difficult to develop a mapping from source speech
phonemes to the target language text. To address this issue, we propose
MixSpeech, a cross-modality self-learning framework that utilizes audio speech
to regularize the training of visual speech tasks. To further minimize the
cross-modality gap and its impact on knowledge transfer, we suggest adopting
mixed speech, which is created by interpolating audio and visual streams, along
with a curriculum learning strategy to adjust the mixing ratio as needed.
MixSpeech enhances speech translation in noisy environments, improving BLEU
scores for four languages on AVMuST-TED by +1.4 to +4.2. Moreover, it achieves
state-of-the-art performance in lip reading on CMLR (11.1\%), LRS2 (25.5\%),
and LRS3 (28.0\%).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/Exgc/AVMuST-TED</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Stashing Quantization for Efficient <span class="highlight-title">Transformer</span> Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guo Yang, Daniel Lo, Robert Mullins, Yiren Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated impressive performance on a
range of Natural Language Processing (NLP) tasks. Unfortunately, the immense
amount of computations and memory accesses required for LLM training makes them
prohibitively expensive in terms of hardware cost, and thus challenging to
deploy in use cases such as on-device learning. In this paper, motivated by the
observation that LLM training is memory-bound, we propose a novel dynamic
quantization strategy, termed Dynamic Stashing Quantization (DSQ), that puts a
special focus on reducing the memory operations, but also enjoys the other
benefits of low precision training, such as the reduced arithmetic cost. We
conduct a thorough study on two translation tasks (trained-from-scratch) and
three classification tasks (fine-tuning). DSQ reduces the amount of arithmetic
operations by $20.95\times$ and the number of DRAM operations by $2.55\times$
on IWSLT17 compared to the standard 16-bit fixed-point, which is widely used in
on-device learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEAM: An Integrated Activation-Coupled Model of Sentence Processing and
  Eye Movements in Reading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian M. Rabe, Dario Paape, Daniela Mertzen, Shravan Vasishth, Ralf Engbert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models of eye-movement control during reading, developed largely within
psychology, usually focus on visual, attentional, and motor processes but
neglect post-lexical language processing; by contrast, models of sentence
comprehension processes, developed largely within psycholinguistics, generally
focus only on post-lexical language processes. We present a model that combines
these two research threads, by integrating eye-movement control and sentence
processing. Developing such an integrated model is extremely challenging and
computationally demanding, but such an integration is an important step toward
complete mathematical models of natural language comprehension in reading. We
combine the SWIFT model of eye-movement control (Engbert et al., Psychological
Review, 112, 2005, pp. 777-813) with key components of the Lewis and Vasishth
sentence processing model (Lewis and Vasishth, Cognitive Science, 29, 2005, pp.
375-419). This integration becomes possible, for the first time, due in part to
recent advances in successful parameter identification in dynamical models,
which allows us to investigate profile log-likelihoods for individual model
parameters. We present a fully implemented proof-of-concept model demonstrating
how such an integrated model can be achieved; our approach includes Bayesian
model inference with Markov Chain Monte Carlo (MCMC) sampling as a key
computational tool. The integrated model, SEAM, can successfully reproduce eye
movement patterns that arise due to similarity-based interference in reading.
To our knowledge, this is the first-ever integration of a complete process
model of eye-movement control with linguistic dependency completion processes
in sentence comprehension. In future work, this proof of concept model will
need to be evaluated using a comprehensive set of benchmark data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometry of Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Loe Feijs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we present a fresh perspective on language, combining ideas
from various sources, but mixed in a new synthesis. As in the minimalist
program, the question is whether we can formulate an elegant formalism, a
universal grammar or a mechanism which explains significant aspects of the
human faculty of language, which in turn can be considered a natural
disposition for the evolution and deployment of the diverse human languages. We
describe such a mechanism, which differs from existing logical and grammatical
approaches by its geometric nature. Our main contribution is to explore the
assumption that sentence recognition takes place by forming chains of tokens
representing words, followed by matching these chains with pre-existing chains
representing grammatical word orders. The aligned chains of tokens give rise to
two- and three-dimensional complexes. The resulting model gives an alternative
presentation for subtle rules, traditionally formalized using categorial
grammar.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 24 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $π$-augmented pregroups and applications to linguistics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Boboc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We enrich pregroups with a mapping which allows us to locally apply precyclic
permutations to designated substrings. We prove a normalisation theorem for
such algebraic structures and briefly formalise some known applications of
pregroups to the analysis of clitic pronouns in certain natural languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can a Frozen <span class="highlight-title">Pretrain</span>ed Language Model be used for Zero-shot Neural
  Retrieval on Entity-centric Questions? <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasuto Hoshi, Daisuke Miyashita, Yasuhiro Morioka, Youyang Ng, Osamu Torii, Jun Deguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural document retrievers, including dense passage retrieval (DPR), have
outperformed classical lexical-matching retrievers, such as BM25, when
fine-tuned and tested on specific question-answering datasets. However, it has
been shown that the existing dense retrievers do not generalize well not only
out of domain but even in domain such as Wikipedia, especially when a named
entity in a question is a dominant clue for retrieval. In this paper, we
propose an approach toward in-domain generalization using the embeddings
generated by the frozen language model trained with the entities in the domain.
By not fine-tuning, we explore the possibility that the rich knowledge
contained in a pretrained language model can be used for retrieval tasks. The
proposed method outperforms conventional DPRs on entity-centric questions in
Wikipedia domain and achieves almost comparable performance to BM25 and
state-of-the-art SPAR model. We also show that the contextualized keys lead to
strong improvements compared to BM25 when the entity names consist of common
words. Our results demonstrate the feasibility of the zero-shot retrieval
method for entity-centric questions of Wikipedia domain, where DPR has
struggled to perform.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Workshop on Knowledge Augmented Methods for Natural
  Language Processing, in conjunction with AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ESCL: Equivariant Self-Contrastive Learning for Sentence Representations <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Liu, Yixuan Liu, Xue Han, Chao Deng, Junlan Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous contrastive learning methods for sentence representations often
focus on insensitive transformations to produce positive pairs, but neglect the
role of sensitive transformations that are harmful to semantic representations.
Therefore, we propose an Equivariant Self-Contrastive Learning (ESCL) method to
make full use of sensitive transformations, which encourages the learned
representations to be sensitive to certain types of transformations with an
additional equivariant learning task. Meanwhile, in order to improve
practicability and generality, ESCL simplifies the implementations of
traditional equivariant contrastive methods to share model parameters from the
perspective of multi-task learning. We evaluate our ESCL on semantic textual
similarity tasks. The proposed method achieves better results while using fewer
learning parameters compared to previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Video Retrieval by Adaptive Margin <span class="chip">SIGIR 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng He, Qi Wang, Zhifan Feng, Wenbin Jiang, Yajuan Lv, Yong zhu, Xiao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video retrieval is becoming increasingly important owing to the rapid
emergence of videos on the Internet. The dominant paradigm for video retrieval
learns video-text representations by pushing the distance between the
similarity of positive pairs and that of negative pairs apart from a fixed
margin. However, negative pairs used for training are sampled randomly, which
indicates that the semantics between negative pairs may be related or even
equivalent, while most methods still enforce dissimilar representations to
decrease their similarity. This phenomenon leads to inaccurate supervision and
poor performance in learning video-text representations.
  While most video retrieval methods overlook that phenomenon, we propose an
adaptive margin changed with the distance between positive and negative pairs
to solve the aforementioned issue. First, we design the calculation framework
of the adaptive margin, including the method of distance measurement and the
function between the distance and the margin. Then, we explore a novel
implementation called "Cross-Modal Generalized Self-Distillation" (CMGSD),
which can be built on the top of most video retrieval models with few
modifications. Notably, CMGSD adds few computational overheads at train time
and adds no computational overhead at test time. Experimental results on three
widely used datasets demonstrate that the proposed method can yield
significantly better performance than the corresponding backbone model, and it
outperforms state-of-the-art methods by a large margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Multi-View Fusion Mechanism For Chinese Relation Extraction <span class="chip">PAKDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Yang, Bin Ji, Shasha Li, Jun Ma, Long Peng, Jie Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, many studies incorporate external knowledge into character-level
feature based models to improve the performance of Chinese relation extraction.
However, these methods tend to ignore the internal information of the Chinese
character and cannot filter out the noisy information of external knowledge. To
address these issues, we propose a mixture-of-view-experts framework (MoVE) to
dynamically learn multi-view features for Chinese relation extraction. With
both the internal and external knowledge of Chinese characters, our framework
can better capture the semantic information of Chinese characters. To
demonstrate the effectiveness of the proposed framework, we conduct extensive
experiments on three real-world datasets in distinct domains. Experimental
results show consistent and significant superiority and robustness of our
proposed framework. Our code and dataset will be released at:
https://gitee.com/tmg-nudt/multi-view-of-expert-for-chineserelation-extraction
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by PAKDD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting the relevance of traditional genres: a network analysis of
  fiction readers' preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taom Sakal, Stephen Proulx
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate how well traditional fiction genres like Fantasy, Thriller,
and Literature represent readers' preferences. Using user data from Goodreads
we construct a book network where two books are strongly linked if the same
people tend to read or enjoy them both. We then partition this network into
communities of similar books and assign each a list of subjects from The Open
Library to serve as a proxy for traditional genres. Our analysis reveals that
the network communities correspond to existing combinations of traditional
genres, but that the exact communities differ depending on whether we consider
books that people read or books that people enjoy.
  In addition, we apply principal component analysis to the data and find that
the variance in the book communities is best explained by two factors: the
maturity/childishness and realism/fantastical nature of the books. We propose
using this maturity-realism plane as a coarse classification tool for stories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Supplementary materials at https://github.com/taomsakal/book-networks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for
  Document Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiabang He, Lei Wang, Yi Hu, Ning Liu, Hui Liu, Xing Xu, Heng Tao Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), such as GPT-3 and ChatGPT, have demonstrated
remarkable results in various natural language processing (NLP) tasks with
in-context learning, which involves inference based on a few demonstration
examples. Despite their successes in NLP tasks, no investigation has been
conducted to assess the ability of LLMs to perform document information
extraction (DIE) using in-context learning. Applying LLMs to DIE poses two
challenges: the modality and task gap. To this end, we propose a simple but
effective in-context learning framework called ICL-D3IE, which enables LLMs to
perform DIE with different types of demonstration examples. Specifically, we
extract the most difficult and distinct segments from hard training documents
as hard demonstrations for benefiting all test instances. We design
demonstrations describing relationships that enable LLMs to understand
positional relationships. We introduce formatting demonstrations for easy
answer extraction. Additionally, the framework improves diverse demonstrations
by updating them iteratively. Our experiments on three widely used benchmark
datasets demonstrate that the ICL-D3IE framework enables GPT-3/ChatGPT to
achieve superior performance when compared to previous pre-trained methods
fine-tuned with full training in both the in-distribution (ID) setting and in
the out-of-distribution (OOD) setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Language agnostic WER Standardization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satarupa Guha, Rahul Ambavat, Ankur Gupta, Manish Gupta, Rupeshkumar Mehta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word error rate (WER) is a standard metric for the evaluation of Automated
Speech Recognition (ASR) systems. However, WER fails to provide a fair
evaluation of human perceived quality in presence of spelling variations,
abbreviations, or compound words arising out of agglutination. Multiple
spelling variations might be acceptable based on locale/geography, alternative
abbreviations, borrowed words, and transliteration of code-mixed words from a
foreign language to the target language script. Similarly, in case of
agglutination, often times the agglutinated, as well as the split forms, are
acceptable. Previous work handled this problem by using manually identified
normalization pairs and applying them to both the transcription and the
hypothesis before computing WER. In this paper, we propose an automatic WER
normalization system consisting of two modules: spelling normalization and
segmentation normalization. The proposed system is unsupervised and language
agnostic, and therefore scalable. Experiments with ASR on 35K utterances across
four languages yielded an average WER reduction of 13.28%. Human judgements of
these automatically identified normalization pairs show that our WER-normalized
evaluation is highly consistent with the perceived quality of ASR output.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Stage Coarse-to-Fine Contrastive Learning for Conversation Intent
  Induction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caiyuan Chu, Ya Li, Yifan Liu, Jia-Chen Gu, Quan Liu, Yongxin Ge, Guoping Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intent recognition is critical for task-oriented dialogue systems. However,
for emerging domains and new services, it is difficult to accurately identify
the key intent of a conversation due to time-consuming data annotation and
comparatively poor model transferability. Therefore, the automatic induction of
dialogue intention is very important for intelligent dialogue systems. This
paper presents our solution to Track 2 of Intent Induction from Conversations
for Task-Oriented Dialogue at the Eleventh Dialogue System Technology Challenge
(DSTC11). The essence of intention clustering lies in distinguishing the
representation of different dialogue utterances. The key to automatic intention
induction is that, for any given set of new data, the sentence representation
obtained by the model can be well distinguished from different labels.
Therefore, we propose a multi-stage coarse-to-fine contrastive learning model
training scheme including unsupervised contrastive learning pre-training,
supervised contrastive learning pre-training, and fine-tuning with joint
contrastive learning and clustering to obtain a better dialogue utterance
representation model for the clustering task. In the released DSTC11 Track 2
evaluation results, our proposed system ranked first on both of the two
subtasks of this Track.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ranked 1st on Track 2 at DSTC 11, Accepted by DSTC 11 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BeamAttack: Generating High-quality Textual Adversarial Examples through
  Beam Search and Mixed Semantic Spaces <span class="chip">PAKDD2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai Zhu, Qingyang Zhao, Yuren Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language processing models based on neural networks are vulnerable to
adversarial examples. These adversarial examples are imperceptible to human
readers but can mislead models to make the wrong predictions. In a black-box
setting, attacker can fool the model without knowing model's parameters and
architecture. Previous works on word-level attacks widely use single semantic
space and greedy search as a search strategy. However, these methods fail to
balance the attack success rate, quality of adversarial examples and time
consumption. In this paper, we propose BeamAttack, a textual attack algorithm
that makes use of mixed semantic spaces and improved beam search to craft
high-quality adversarial examples. Extensive experiments demonstrate that
BeamAttack can improve attack success rate while saving numerous queries and
time, e.g., improving at most 7\% attack success rate than greedy search when
attacking the examples from MR dataset. Compared with heuristic search,
BeamAttack can save at most 85\% model queries and achieve a competitive attack
success rate. The adversarial examples crafted by BeamAttack are highly
transferable and can effectively improve model's robustness during adversarial
training. Code is available at
https://github.com/zhuhai-ustc/beamattack/tree/master
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PAKDD2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Let's Get Personal: Personal Questions Improve SocialBot Performance in
  the Alexa Prize 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin K. Bowden, Marilyn Walker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been an increased focus on creating conversational open-domain
dialogue systems in the spoken dialogue community. Unlike traditional dialogue
systems, these conversational systems cannot assume any specific information
need or domain restrictions, i.e., the only inherent goal is to converse with
the user on an unknown set of topics. While massive improvements in Natural
Language Understanding (NLU) and the growth of available knowledge resources
can partially support a robust conversation, these conversations generally lack
the rapport between two humans that know each other. We developed a robust
open-domain conversational system, Athena, that real Amazon Echo users access
and evaluate at scale in the context of the Alexa Prize competition. We
experiment with methods intended to increase intimacy between Athena and the
user by heuristically developing a rule-based user model that personalizes both
the current and subsequent conversations and evaluating specific personal
opinion question strategies in A/B studies. Our results show a statistically
significant positive impact on perceived conversation quality and length when
employing these strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Won Best Paper at IWSDS '23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Feasibility of Chat<span class="highlight-title">GPT</span> for Event Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03836v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03836v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Gao, Huan Zhao, Changlong Yu, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event extraction is a fundamental task in natural language processing that
involves identifying and extracting information about events mentioned in text.
However, it is a challenging task due to the lack of annotated data, which is
expensive and time-consuming to obtain. The emergence of large language models
(LLMs) such as ChatGPT provides an opportunity to solve language tasks with
simple prompts without the need for task-specific datasets and fine-tuning.
While ChatGPT has demonstrated impressive results in tasks like machine
translation, text summarization, and question answering, it presents challenges
when used for complex tasks like event extraction. Unlike other tasks, event
extraction requires the model to be provided with a complex set of instructions
defining all event types and their schemas. To explore the feasibility of
ChatGPT for event extraction and the challenges it poses, we conducted a series
of experiments. Our results show that ChatGPT has, on average, only 51.04% of
the performance of a task-specific model such as EEQA in long-tail and complex
scenarios. Our usability testing experiments indicate that ChatGPT is not
robust enough, and continuous refinement of the prompt does not lead to stable
performance improvements, which can result in a poor user experience. Besides,
ChatGPT is highly sensitive to different prompt styles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Complex QA and language models hybrid architectures, <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09051v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09051v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, Elisabeth Murisasco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper reviews the state-of-the-art of hybrid language models
architectures and strategies for "complex" question-answering (QA, CQA, CPS).
Large Language Models (LLM) are good at leveraging public data on standard
problems but once you want to tackle more specific complex questions or
problems you may need specific architecture, knowledge, skills, methods,
sensitive data protection, explainability, human approval and versatile
feedback... We identify key elements augmenting LLM to solve complex questions
or problems. We extend findings from the robust community edited research
papers BIG, BLOOM and HELM which open source, benchmark and analyze limits and
challenges of LLM in terms of tasks complexity and strict evaluation on
accuracy (e.g. fairness, robustness, toxicity, ...). Recent projects like
ChatGPT and GALACTICA have allowed non-specialists to grasp the great potential
as well as the equally strong limitations of language models in complex QA.
Hybridizing these models with different components could allow to overcome
these different limits and go much further. We discuss some challenges
associated with complex QA, including domain adaptation, decomposition and
efficient multi-step QA, long form and non-factoid QA, safety and
multi-sensitivity data protection, multimodal search, hallucinations,
explainability and truthfulness, temproal reasoning. Therefore, we analyze
current solutions and promising research trends, using elements such as: hybrid
LLM architectures, active human reinforcement learning supervised with AI,
prompting adaptation, neuro-symbolic and structured knowledge grounding,
program synthesis, iterated decomposition and others.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ELODIN: Naming Concepts in Embedding Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04001v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04001v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Mello, Filipe Calegario, Geber Ramalho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advancements, the field of text-to-image synthesis still
suffers from lack of fine-grained control. Using only text, it remains
challenging to deal with issues such as concept coherence and concept
contamination. We propose a method to enhance control by generating specific
concepts that can be reused throughout multiple images, effectively expanding
natural language with new words that can be combined much like a painter's
palette. Unlike previous contributions, our method does not copy visuals from
input data and can generate concepts through text alone. We perform a set of
comparisons that finds our method to be a significant improvement over
text-only prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added quantitative data, fixed formatting issues</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linearly Mapping from Image to Text Space <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.15162v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.15162v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Merullo, Louis Castricato, Carsten Eickhoff, Ellie Pavlick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The extent to which text-only language models (LMs) learn to represent
features of the non-linguistic world is an open question. Prior work has shown
that pretrained LMs can be taught to caption images when a vision model's
parameters are optimized to encode images in the language space. We test a
stronger hypothesis: that the conceptual representations learned by frozen
text-only models and vision-only models are similar enough that this can be
achieved with a linear map. We show that the image representations from vision
models can be transferred as continuous prompts to frozen LMs by training only
a single linear projection. Using these to prompt the LM achieves competitive
performance on captioning and visual question answering tasks compared to
models that tune both the image encoder and text decoder (such as the MAGMA
model). We compare three image encoders with increasing amounts of linguistic
supervision seen during pretraining: BEIT (no linguistic information),
NF-ResNET (lexical category information), and CLIP (full natural language
descriptions). We find that all three encoders perform equally well at
transferring visual property information to the language model (e.g., whether
an animal is large or small), but that image encoders pretrained with
linguistic supervision more saliently encode category information (e.g.,
distinguishing hippo vs. elephant) and thus perform significantly better on
benchmark language-and-vision tasks. Our results indicate that LMs encode
conceptual information structurally similarly to vision-based models, even
those that are solely trained on images. Code is available here:
https://github.com/jmerullo/limber
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedKLIP: Medical Knowledge Enhanced Language-Image <span class="highlight-title">Pre-Train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.02228v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.02228v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider enhancing medical visual-language pre-training
(VLP) with domain-specific knowledge, by exploiting the paired image-text
reports from the radiological daily practice. In particular, we make the
following contributions: First, unlike existing works that directly process the
raw reports, we adopt a novel triplet extraction module to extract the
medical-related information, avoiding unnecessary complexity from language
grammar and enhancing the supervision signals; Second, we propose a novel
triplet encoding module with entity translation by querying a knowledge base,
to exploit the rich domain knowledge in medical field, and implicitly build
relationships between medical entities in the language embedding space; Third,
we propose to use a Transformer-based fusion model for spatially aligning the
entity description with visual signals at the image patch level, enabling the
ability for medical diagnosis; Fourth, we conduct thorough experiments to
validate the effectiveness of our architecture, and benchmark on numerous
public benchmarks, e.g., ChestX-ray14, RSNA Pneumonia, SIIM-ACR Pneumothorax,
COVIDx CXR-2, COVID Rural, and EdemaSeverity. In both zero-shot and fine-tuning
settings, our model has demonstrated strong performance compared with the
former methods on disease classification and grounding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weight Averaging: A Simple Yet Effective Method to Overcome Catastrophic
  Forgetting in Automatic Speech Recognition <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.15282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.15282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Vander Eeckt, Hugo Van hamme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting a trained Automatic Speech Recognition (ASR) model to new tasks
results in catastrophic forgetting of old tasks, limiting the model's ability
to learn continually and to be extended to new speakers, dialects, languages,
etc. Focusing on End-to-End ASR, in this paper, we propose a simple yet
effective method to overcome catastrophic forgetting: weight averaging. By
simply taking the average of the previous and the adapted model, our method
achieves high performance on both the old and new tasks. It can be further
improved by introducing a knowledge distillation loss during the adaptation. We
illustrate the effectiveness of our method on both monolingual and multilingual
ASR. In both cases, our method strongly outperforms all baselines, even in its
simplest form.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2023. 5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning for Monolingual End-to-End Automatic Speech
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09427v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09427v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Vander Eeckt, Hugo Van hamme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting Automatic Speech Recognition (ASR) models to new domains results in
a deterioration of performance on the original domain(s), a phenomenon called
Catastrophic Forgetting (CF). Even monolingual ASR models cannot be extended to
new accents, dialects, topics, etc. without suffering from CF, making them
unable to be continually enhanced without storing all past data. Fortunately,
Continual Learning (CL) methods, which aim to enable continual adaptation while
overcoming CF, can be used. In this paper, we implement an extensive number of
CL methods for End-to-End ASR and test and compare their ability to extend a
monolingual Hybrid CTC-Transformer model across four new tasks. We find that
the best performing CL method closes the gap between the fine-tuned model
(lower bound) and the model trained jointly on all tasks (upper bound) by more
than 40%, while requiring access to only 0.6% of the original data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at EUSIPCO 2022. 5 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Paraphrasing Techniques for Maritime QA system 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10854v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10854v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemeh Shiri, Terry Yue Zhuo, Zhuang Li, Van Nguyen, Shirui Pan, Weiqing Wang, Reza Haffari, Yuan-Fang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been an increasing interest in incorporating Artificial
Intelligence (AI) into Defence and military systems to complement and augment
human intelligence and capabilities. However, much work still needs to be done
toward achieving an effective human-machine partnership. This work is aimed at
enhancing human-machine communications by developing a capability for
automatically translating human natural language into a machine-understandable
language (e.g., SQL queries). Techniques toward achieving this goal typically
involve building a semantic parser trained on a very large amount of
high-quality manually-annotated data. However, in many real-world Defence
scenarios, it is not feasible to obtain such a large amount of training data.
To the best of our knowledge, there are few works trying to explore the
possibility of training a semantic parser with limited manually-paraphrased
data, in other words, zero-shot. In this paper, we investigate how to exploit
paraphrasing methods for the automated generation of large-scale training
datasets (in the form of paraphrased utterances and their corresponding logical
forms in SQL format) and present our experimental results using real-world data
in the maritime domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages. The first three authors contribute equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ViLPAct: A Benchmark for Compositional Generalization on Multimodal
  Human Activities <span class="chip">EACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.05556v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.05556v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Terry Yue Zhuo, Yaqing Liao, Yuecheng Lei, Lizhen Qu, Gerard de Melo, Xiaojun Chang, Yazhou Ren, Zenglin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ViLPAct, a novel vision-language benchmark for human activity
planning. It is designed for a task where embodied AI agents can reason and
forecast future actions of humans based on video clips about their initial
activities and intents in text. The dataset consists of 2.9k videos from
\charades extended with intents via crowdsourcing, a multi-choice question test
set, and four strong baselines. One of the baselines implements a neurosymbolic
approach based on a multi-modal knowledge base (MKB), while the other ones are
deep generative models adapted from recent state-of-the-art (SOTA) methods.
According to our extensive experiments, the key challenges are compositional
generalization and effective use of information from both modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EACL2023 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Robustness of <span class="highlight-title">Prompt</span>-based Semantic Parsing with Large <span class="highlight-title">Pre-train</span>ed
  Language Model: An Empirical Study on Codex <span class="chip">EACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12868v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12868v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Terry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh Shiri, Weiqing Wang, Gholamreza Haffari, Yuan-Fang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic parsing is a technique aimed at constructing a structured
representation of the meaning of a natural-language question. Recent
advancements in few-shot language models trained on code have demonstrated
superior performance in generating these representations compared to
traditional unimodal language models, which are trained on downstream tasks.
Despite these advancements, existing fine-tuned neural semantic parsers are
susceptible to adversarial attacks on natural-language inputs. While it has
been established that the robustness of smaller semantic parsers can be
enhanced through adversarial training, this approach is not feasible for large
language models in real-world scenarios, as it requires both substantial
computational resources and expensive human annotation on in-domain semantic
parsing data. This paper presents the first empirical study on the adversarial
robustness of a large prompt-based language model of code, \codex. Our results
demonstrate that the state-of-the-art (SOTA) code-language models are
vulnerable to carefully crafted adversarial examples. To address this
challenge, we propose methods for improving robustness without the need for
significant amounts of labeled data or heavy computational resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EACL2023 (main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Learning for Event Extraction with Memory-based Loss Prediction
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.03073v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.03073v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shirong Shen, Zhen Li, Guilin Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event extraction (EE) plays an important role in many industrial application
scenarios, and high-quality EE methods require a large amount of manual
annotation data to train supervised learning models. However, the cost of
obtaining annotation data is very high, especially for annotation of domain
events, which requires the participation of experts from corresponding domain.
So we introduce active learning (AL) technology to reduce the cost of event
annotation. But the existing AL methods have two main problems, which make them
not well used for event extraction. Firstly, the existing pool-based selection
strategies have limitations in terms of computational cost and sample validity.
Secondly, the existing evaluation of sample importance lacks the use of local
sample information. In this paper, we present a novel deep AL method for EE. We
propose a batch-based selection strategy and a Memory-Based Loss Prediction
model (MBLP) to select unlabeled samples efficiently. During the selection
process, we use an internal-external sample loss ranking method to evaluate the
sample importance by using local information. Finally, we propose a delayed
training strategy to train the MBLP model. Extensive experiments are performed
on three domain datasets, and our method outperforms other state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Challenges in Explanation Quality Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07126v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07126v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hendrik Schuff, Heike Adel, Peng Qi, Ngoc Thang Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While much research focused on producing explanations, it is still unclear
how the produced explanations' quality can be evaluated in a meaningful way.
Today's predominant approach is to quantify explanations using proxy scores
which compare explanations to (human-annotated) gold explanations. This
approach assumes that explanations which reach higher proxy scores will also
provide a greater benefit to human users. In this paper, we present problems of
this approach. Concretely, we (i) formulate desired characteristics of
explanation quality, (ii) describe how current evaluation practices violate
them, and (iii) support our argumentation with initial evidence from a
crowdsourcing case study in which we investigate the explanation quality of
state-of-the-art explainable question answering systems. We find that proxy
scores correlate poorly with human quality ratings and, additionally, become
less expressive the more often they are used (i.e. following Goodhart's law).
Finally, we propose guidelines to enable a meaningful evaluation of
explanations to drive the development of systems that provide tangible benefits
to human users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Challenging Benchmark for Low-Resource Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03840v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03840v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yudong Wang, Chang Ma, Qingxiu Dong, Lingpeng Kong, Jingjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With promising yet saturated results in high-resource settings, low-resource
datasets have gradually become popular benchmarks for evaluating the learning
ability of advanced neural networks (e.g., BigBench, superGLUE). Some models
even surpass humans according to benchmark test results. However, we find that
there exists a set of hard examples in low-resource settings that challenge
neural networks but are not well evaluated, which causes over-estimated
performance. We first give a theoretical analysis on which factors bring the
difficulty of low-resource learning. It then motivate us to propose a
challenging benchmark hardBench to better evaluate the learning ability, which
covers 11 datasets, including 3 computer vision (CV) datasets and 8 natural
language process (NLP) datasets. Experiments on a wide range of models show
that neural networks, even pre-trained language models, have sharp performance
drops on our benchmark, demonstrating the effectiveness on evaluating the
weaknesses of neural networks. On NLP tasks, we surprisingly find that despite
better results on traditional low-resource benchmarks, pre-trained networks,
does not show performance improvements on our benchmarks. These results
demonstrate that there are still a large robustness gap between existing models
and human-level performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flat Multi-modal Interaction <span class="highlight-title">Transformer</span> for Named Entity Recognition <span class="chip">COLING 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.11039v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.11039v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Lu, Dixiang Zhang, Pingjian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal named entity recognition (MNER) aims at identifying entity spans
and recognizing their categories in social media posts with the aid of images.
However, in dominant MNER approaches, the interaction of different modalities
is usually carried out through the alternation of self-attention and
cross-attention or over-reliance on the gating machine, which results in
imprecise and biased correspondence between fine-grained semantic units of text
and image. To address this issue, we propose a Flat Multi-modal Interaction
Transformer (FMIT) for MNER. Specifically, we first utilize noun phrases in
sentences and general domain words to obtain visual cues. Then, we transform
the fine-grained semantic representation of the vision and text into a unified
lattice structure and design a novel relative position encoding to match
different modalities in Transformer. Meanwhile, we propose to leverage entity
boundary detection as an auxiliary task to alleviate visual bias. Experiments
show that our methods achieve the new state-of-the-art performance on two
benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by COLING 2022, oral paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Risks of Stealing the Decoding Algorithms of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Naseh, Kalpesh Krishna, Mohit Iyyer, Amir Houmansadr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key component of generating text from modern language models (LM) is the
selection and tuning of decoding algorithms. These algorithms determine how to
generate text from the internal probability distribution generated by the LM.
The process of choosing a decoding algorithm and tuning its hyperparameters
takes significant time, manual effort, and computation, and it also requires
extensive human evaluation. Therefore, the identity and hyperparameters of such
decoding algorithms are considered to be extremely valuable to their owners. In
this work, we show, for the first time, that an adversary with typical API
access to an LM can steal the type and hyperparameters of its decoding
algorithms at very low monetary costs. Our attack is effective against popular
LMs used in text generation APIs, including GPT-2 and GPT-3. We demonstrate the
feasibility of stealing such information with only a few dollars, e.g.,
$\$0.8$, $\$1$, $\$4$, and $\$40$ for the four versions of GPT-3.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mask-guided <span class="highlight-title">BERT</span> for Few Shot Text Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10447v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10447v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxiong Liao, Zhengliang Liu, Haixing Dai, Zihao Wu, Yiyang Zhang, Xiaoke Huang, Yuzhong Chen, Xi Jiang, Wei Liu, Dajiang Zhu, Tianming Liu, Sheng Li, Xiang Li, Hongmin Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based language models have achieved significant success in
various domains. However, the data-intensive nature of the transformer
architecture requires much labeled data, which is challenging in low-resource
scenarios (i.e., few-shot learning (FSL)). The main challenge of FSL is the
difficulty of training robust models on small amounts of samples, which
frequently leads to overfitting. Here we present Mask-BERT, a simple and
modular framework to help BERT-based architectures tackle FSL. The proposed
approach fundamentally differs from existing FSL strategies such as prompt
tuning and meta-learning. The core idea is to selectively apply masks on text
inputs and filter out irrelevant information, which guides the model to focus
on discriminative tokens that influence prediction results. In addition, to
make the text representations from different categories more separable and the
text representations from the same category more compact, we introduce a
contrastive learning loss function. Experimental results on public-domain
benchmark datasets demonstrate the effectiveness of Mask-BERT.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Few-Shot Learning for Talking Face System with TTS Data
  Augmentation <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Chen, Ziyang Ma, Tao Liu, Xu Tan, Qu Lu, Xie Chen, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-driven talking face has attracted broad interest from academia and
industry recently. However, data acquisition and labeling in audio-driven
talking face are labor-intensive and costly. The lack of data resource results
in poor synthesis effect. To alleviate this issue, we propose to use TTS
(Text-To-Speech) for data augmentation to improve few-shot ability of the
talking face system. The misalignment problem brought by the TTS audio is
solved with the introduction of soft-DTW, which is first adopted in the talking
face task. Moreover, features extracted by HuBERT are explored to utilize
underlying information of audio, and found to be superior over other features.
The proposed method achieves 17%, 14%, 38% dominance on MSE score, DTW score
and user study preference repectively over the baseline model, which shows the
effectiveness of improving few-shot learning for talking face system with TTS
augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages. Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Robust Image-in-Audio Deep Steganography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaume Ros Alonso, Margarita Geleta, Jordi Pons, Xavier Giro-i-Nieto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of steganography has experienced a surge of interest due to the
recent advancements in AI-powered techniques, particularly in the context of
multimodal setups that enable the concealment of signals within signals of a
different nature. The primary objectives of all steganographic methods are to
achieve perceptual transparency, robustness, and large embedding capacity -
which often present conflicting goals that classical methods have struggled to
reconcile. This paper extends and enhances an existing image-in-audio deep
steganography method by focusing on improving its robustness. The proposed
enhancements include modifications to the loss function, utilization of the
Short-Time Fourier Transform (STFT), introduction of redundancy in the encoding
process for error correction, and buffering of additional information in the
pixel subconvolution operation. The results demonstrate that our approach
outperforms the existing method in terms of robustness and perceptual
transparency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BIRD-PCC: Bi-directional Range Image-based Deep LiDAR Point Cloud
  Compression <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Sheng Liu, Jia-Fong Yeh, Hao Hsu, Hung-Ting Su, Ming-Sui Lee, Winston H. Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The large amount of data collected by LiDAR sensors brings the issue of LiDAR
point cloud compression (PCC). Previous works on LiDAR PCC have used range
image representations and followed the predictive coding paradigm to create a
basic prototype of a coding framework. However, their prediction methods give
an inaccurate result due to the negligence of invalid pixels in range images
and the omission of future frames in the time step. Moreover, their handcrafted
design of residual coding methods could not fully exploit spatial redundancy.
To remedy this, we propose a coding framework BIRD-PCC. Our prediction module
is aware of the coordinates of invalid pixels in range images and takes a
bidirectional scheme. Also, we introduce a deep-learned residual coding module
that can further exploit spatial redundancy within a residual frame.
Experiments conducted on SemanticKITTI and KITTI-360 datasets show that
BIRD-PCC outperforms other methods in most bitrate conditions and generalizes
well to unseen environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-08T00:00:00Z">2023-03-08</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastFill: Efficient Compatible Model Update 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Jaeckle, Fartash Faghri, Ali Farhadi, Oncel Tuzel, Hadi Pouransari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many retrieval systems the original high dimensional data (e.g., images)
is mapped to a lower dimensional feature through a learned embedding model. The
task of retrieving the most similar data from a gallery set to a given query
data is performed through a similarity comparison on features. When the
embedding model is updated, it might produce features that are not
comparable/compatible with features already in the gallery computed with the
old model. Subsequently, all features in the gallery need to be re-computed
using the new embedding model -- a computationally expensive process called
backfilling. Recently, compatible representation learning methods have been
proposed to avoid backfilling. Despite their relative success, there is an
inherent trade-off between the new model performance and its compatibility with
the old model. In this work, we introduce FastFill: a compatible model update
process using feature alignment and policy based partial backfilling to
promptly elevate retrieval performance. We show that previous backfilling
strategies suffer from decreased performance and demonstrate the importance of
both the training objective and the ordering in online partial backfilling. We
propose a new training method for feature alignment between old and new
embedding models using uncertainty estimation. Compared to previous works, we
obtain significantly improved backfilling results on a variety of datasets: mAP
on ImageNet (+4.4\%), Places-365 (+2.7\%), and VGG-Face2 (+1.3\%). Further, we
demonstrate that when updating a biased model with FastFill, the minority
subgroup accuracy gap promptly vanishes with a small fraction of partial
backfilling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in The Eleventh International Conference on Learning
  Representations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Prompt</span> Log Analysis of Text-to-Image Generation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Xie, Zhaoying Pan, Jinge Ma, Jie Luo, Qiaozhu Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in diffusion models have unleashed the astonishing
capabilities of text-to-image generation systems to synthesize high-quality
images that are faithful to a given reference text, known as a "prompt." These
systems, once released to the public, have immediately received tons of
attention from researchers, creators, and common users. Despite the plenty of
efforts to improve the underneath generative models, there is limited work on
understanding the information needs of the real users of these systems, e.g.,
by investigating the prompts the users input at scale. In this paper, we take
the initiative to conduct a comprehensive analysis of large-scale prompt logs
collected from multiple text-to-image generation systems. Our work is analogous
to analyzing the query log of Web search engines, a line of work that has made
critical contributions to the glory of the Web search industry and research. We
analyze over two million user-input prompts submitted to three popular
text-to-image systems at scale. Compared to Web search queries, text-to-image
prompts are significantly longer, often organized into unique structures, and
present different categories of information needs. Users tend to make more
edits within creation sessions, showing remarkable exploratory patterns. Our
findings provide concrete implications on how to improve text-to-image
generation systems for creation purposes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kernel-CF: Collaborative filtering done right with social network
  analysis and kernel smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative filtering is the simplest but oldest machine learning algorithm
in the field of recommender systems. In spite of its long history, it remains a
discussion topic in research venues. Usually people use users/items whose
similarity scores with the target customer greater than 0 to compute the
algorithms. However, this might not be the optimal solution after careful
scrutiny. In this paper, we transform the recommender system input data into a
2-D social network, and apply kernel smoothing to compute preferences for
unknown values in the user item rating matrix. We unifies the theoretical
framework of recommender system and non-parametric statistics and provides an
algorithmic procedure with optimal parameter selection method to achieve the
goal.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Class Cardinality Comparison as a Fermi Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrestha Ghosh, Simon Razniewski, Gerhard Weikum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Questions on class cardinality comparisons are quite tricky to answer and
come with its own challenges. They require some kind of reasoning since web
documents and knowledge bases, indispensable sources of information, rarely
store direct answers to questions, such as, ``Are there more astronauts or
Physics Nobel Laureates?'' We tackle questions on class cardinality comparison
by tapping into three sources for absolute cardinalities as well as the
cardinalities of orthogonal subgroups of the classes. We propose novel
techniques for aggregating signals with partial coverage for more reliable
estimates and evaluate them on a dataset of 4005 class pairs, achieving an
accuracy of 83.7%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Web Conference 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unbiased Learning to Rank with Biased Continuous Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Ren, Hongyan Tang, Siwen Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is a well-known challenge to learn an unbiased ranker with biased
feedback. Unbiased learning-to-rank(LTR) algorithms, which are verified to
model the relative relevance accurately based on noisy feedback, are appealing
candidates and have already been applied in many applications with single
categorical labels, such as user click signals. Nevertheless, the existing
unbiased LTR methods cannot properly handle continuous feedback, which are
essential for many industrial applications, such as content recommender
systems.
  To provide personalized high-quality recommendation results, recommender
systems need model both categorical and continuous biased feedback, such as
click and dwell time. Accordingly, we design a novel unbiased LTR algorithm to
tackle the challenges, which innovatively models position bias in the pairwise
fashion and introduces the pairwise trust bias to separate the position bias,
trust bias, and user relevance explicitly and can work for both continuous and
categorical feedback. Experiment results on public benchmark datasets and
internal live traffic of a large-scale recommender system at Tencent News show
superior results for continuous labels and also competitive performance for
categorical labels of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages. arXiv admin note: substantial text overlap with
  arXiv:2111.12929</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Context Pattern Generation for Entity Set Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08087v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08087v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghui Li, Shulin Huang, Xinwei Zhang, Qingyu Zhou, Yangning Li, Ruiyang Liu, Yunbo Cao, Hai-Tao Zheng, Ying Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity Set Expansion (ESE) is a valuable task that aims to find entities of
the target semantic class described by given seed entities. Various Natural
Language Processing (NLP) and Information Retrieval (IR) downstream
applications have benefited from ESE due to its ability to discover knowledge.
Although existing corpus-based ESE methods have achieved great progress, they
still rely on corpora with high-quality entity information annotated, because
most of them need to obtain the context patterns through the position of the
entity in a sentence. Therefore, the quality of the given corpora and their
entity annotation has become the bottleneck that limits the performance of such
methods. To overcome this dilemma and make the ESE models free from the
dependence on entity annotation, our work aims to explore a new ESE paradigm,
namely corpus-independent ESE. Specifically, we devise a context pattern
generation module that utilizes autoregressive language models (e.g., GPT-2) to
automatically generate high-quality context patterns for entities. In addition,
we propose the GAPA, a novel ESE framework that leverages the aforementioned
GenerAted PAtterns to expand target entities. Extensive experiments and
detailed analyses on three widely used datasets demonstrate the effectiveness
of our method. All the codes of our experiments are available at
https://github.com/geekjuruo/GAPA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GLCC: A General Framework for Graph-Level Clustering <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.11879v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.11879v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Ju, Yiyang Gu, Binqi Chen, Gongbo Sun, Yifang Qin, Xingyuming Liu, Xiao Luo, Ming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the problem of graph-level clustering, which is a novel
yet challenging task. This problem is critical in a variety of real-world
applications such as protein clustering and genome analysis in bioinformatics.
Recent years have witnessed the success of deep clustering coupled with graph
neural networks (GNNs). However, existing methods focus on clustering among
nodes given a single graph, while exploring clustering on multiple graphs is
still under-explored. In this paper, we propose a general graph-level
clustering framework named Graph-Level Contrastive Clustering (GLCC) given
multiple graphs. Specifically, GLCC first constructs an adaptive affinity graph
to explore instance- and cluster-level contrastive learning (CL).
Instance-level CL leverages graph Laplacian based contrastive loss to learn
clustering-friendly representations while cluster-level CL captures
discriminative cluster representations incorporating neighbor information of
each sample. Moreover, we utilize neighbor-aware pseudo-labels to reward the
optimization of representation learning. The two steps can be alternatively
trained to collaborate and benefit each other. Experiments on a range of
well-known datasets demonstrate the superiority of our proposed GLCC over
competitive baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Proceedings of the AAAI Conference on Artificial
  Intelligence (AAAI 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RETEXO: Scalable Neural Network Training over Distributed Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aashish Kolluri, Sarthak Choudhary, Bryan Hooi, Prateek Saxena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks offer a promising approach to supervised learning over
graph data. Graph data, especially when it is privacy-sensitive or too large to
train on centrally, is often stored partitioned across disparate processing
units (clients) which want to minimize the communication costs during
collaborative training. The fully-distributed setup takes such partitioning to
its extreme, wherein features of only a single node and its adjacent edges are
kept locally with one client processor. Existing GNNs are not architected for
training in such setups and incur prohibitive costs therein. We propose RETEXO,
a novel transformation of existing GNNs that improves the communication
efficiency during training in the fully-distributed setup. We experimentally
confirm that RETEXO offers up to 6 orders of magnitude better communication
efficiency even when training shallow GNNs, with a minimal trade-off in
accuracy for supervised node classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Line Graph Contrastive Learning for Link Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.13795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.13795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehua Zhang, Shilin Sun, Guixiang Ma, Caiming Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Link prediction tasks focus on predicting possible future connections. Most
existing researches measure the likelihood of links by different similarity
scores on node pairs and predict links between nodes. However, the
similarity-based approaches have some challenges in information loss on nodes
and generalization ability on similarity indexes. To address the above issues,
we propose a Line Graph Contrastive Learning(LGCL) method to obtain rich
information with multiple perspectives. LGCL obtains a subgraph view by h-hop
subgraph sampling with target node pairs. After transforming the sampled
subgraph into a line graph, the link prediction task is converted into a node
classification task, which graph convolution progress can learn edge embeddings
from graphs more effectively. Then we design a novel cross-scale contrastive
learning framework on the line graph and the subgraph to maximize the mutual
information of them, so that fuses the structure and feature information. The
experimental results demonstrate that the proposed LGCL outperforms the
state-of-the-art methods and has better performance on generalization and
robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FaceChat: An Emotion-Aware Face-to-face Dialogue Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deema Alnuhait, Qingyang Wu, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While current dialogue systems like ChatGPT have made significant
advancements in text-based interactions, they often overlook the potential of
other modalities in enhancing the overall user experience. We present FaceChat,
a web-based dialogue framework that enables emotionally-sensitive and
face-to-face conversations. By seamlessly integrating cutting-edge technologies
in natural language processing, computer vision, and speech processing,
FaceChat delivers a highly immersive and engaging user experience. FaceChat
framework has a wide range of potential applications, including counseling,
emotional support, and personalized customer service. The system is designed to
be simple and flexible as a platform for future researchers to advance the
field of multimodal dialogue systems. The code is publicly available at
https://github.com/qywu/FaceChat.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lexical Complexity Prediction: An <span class="highlight-title">Overview</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai North, Marcos Zampieri, Matthew Shardlow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The occurrence of unknown words in texts significantly hinders reading
comprehension. To improve accessibility for specific target populations,
computational modelling has been applied to identify complex words in texts and
substitute them for simpler alternatives. In this paper, we present an overview
of computational approaches to lexical complexity prediction focusing on the
work carried out on English data. We survey relevant approaches to this problem
which include traditional machine learning classifiers (e.g. SVMs, logistic
regression) and deep neural networks as well as a variety of features, such as
those inspired by literature in psycholinguistics as well as word frequency,
word length, and many others. Furthermore, we introduce readers to past
competitions and available datasets created on this topic. Finally, we include
brief sections on applications of lexical complexity prediction, such as
readability and text simplification, together with related studies on languages
other than English.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Casual Conversations v2 <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bilal Porgali, Vítor Albiero, Jordan Ryda, Cristian Canton Ferrer, Caner Hazirbas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a new large consent-driven dataset aimed at assisting
in the evaluation of algorithmic bias and robustness of computer vision and
audio speech models in regards to 11 attributes that are self-provided or
labeled by trained annotators. The dataset includes 26,467 videos of 5,567
unique paid participants, with an average of almost 5 videos per person,
recorded in Brazil, India, Indonesia, Mexico, Vietnam, Philippines, and the
USA, representing diverse demographic characteristics. The participants agreed
for their data to be used in assessing fairness of AI models and provided
self-reported age, gender, language/dialect, disability status, physical
adornments, physical attributes and geo-location information, while trained
annotators labeled apparent skin tone using the Fitzpatrick Skin Type and Monk
Skin Tone scales, and voice timbre. Annotators also labeled for different
recording setups and per-second activity annotations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ disco: a toolkit for Distributional Control of Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Germán Kruszewski, Jos Rozen, Marc Dymetman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models and other generative models have revolutionized
NLP and beyond. However, these models tend to reproduce undesirable biases
present in their training data. Also, they may overlook patterns that are
important but challenging to capture. To address these limitations, researchers
have introduced distributional control techniques. These techniques, not
limited to language, allow controlling the prevalence (i.e., expectations) of
any features of interest in the model's outputs. Despite their potential, the
widespread adoption of these techniques has been hindered by the difficulty in
adapting complex, disconnected code. Here, we present disco, an open-source
Python library that brings these techniques to the broader public.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comprehensive Event Representations using Event Knowledge Graphs and
  Natural Language Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tin Kuculo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has utilised knowledge-aware approaches to natural language
understanding, question answering, recommendation systems, and other tasks.
These approaches rely on well-constructed and large-scale knowledge graphs that
can be useful for many downstream applications and empower knowledge-aware
models with commonsense reasoning. Such knowledge graphs are constructed
through knowledge acquisition tasks such as relation extraction and knowledge
graph completion. This work seeks to utilise and build on the growing body of
work that uses findings from the field of natural language processing (NLP) to
extract knowledge from text and build knowledge graphs. The focus of this
research project is on how we can use transformer-based approaches to extract
and contextualise event information, matching it to existing ontologies, to
build a comprehensive knowledge of graph-based event representations.
Specifically, sub-event extraction is used as a way of creating sub-event-aware
event representations. These event representations are then further enriched
through fine-grained location extraction and contextualised through the
alignment of historically relevant quotes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is the author's version of the work. It is posted here for your
  personal use. Not for redistribution. The definitive Version of Record was
  published in Companion Proceedings of the Web Conference 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extending the <span class="highlight-title">Pre-Train</span>ing of BLOOM for Improved Support of Traditional
  Chinese: Models, Methods and Results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Ennen, Po-Chun Hsu, Chan-Jan Hsu, Chang-Le Liu, Yen-Chen Wu, Yin-Hsiang Liao, Chin-Tung Lin, Da-Shan Shiu, Wei-Yun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present the multilingual language model BLOOM-zh that
features enhanced support for Traditional Chinese. BLOOM-zh has its origins in
the open-source BLOOM models presented by BigScience in 2022. Starting from
released models, we extended the pre-training of BLOOM by additional 7.4
billion tokens in Traditional Chinese and English covering a variety of domains
such as news articles, books, encyclopedias, educational materials as well as
spoken language. In order to show the properties of BLOOM-zh, both existing and
newly created benchmark scenarios are used for evaluating the performance.
BLOOM-zh outperforms its predecessor on most Traditional Chinese benchmarks
while maintaining its English capability. We release all our models to the
research community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-contained Beta-with-Spikes Approximation for Inference Under a
  Wright-Fisher Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Guerrero Montero, Richard A. Blythe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We construct a reliable estimation of evolutionary parameters within the
Wright-Fisher model, which describes changes in allele frequencies due to
selection and genetic drift, from time-series data. Such data exists for
biological populations, for example via artificial evolution experiments, and
for the cultural evolution of behavior, such as linguistic corpora that
document historical usage of different words with similar meanings. Our method
of analysis builds on a Beta-with-Spikes approximation to the distribution of
allele frequencies predicted by the Wright-Fisher model. We introduce a
self-contained scheme for estimating the parameters in the approximation, and
demonstrate its robustness with synthetic data, especially in the
strong-selection and near-extinction regimes where previous approaches fail. We
further apply to allele frequency data for baker's yeast (Saccharomyces
cerevisiae), finding a significant signal of selection in cases where
independent evidence supports such a conclusion. We further demonstrate the
possibility of detecting time-points at which evolutionary parameters change in
the context of a historical spelling reform in the Spanish language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cost-Effective Hyperparameter Optimization for Large Language Model
  Generation Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Wang, Susan Xueqing Liu, Ahmed H. Awadallah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) like GPT-3 have sparked significant interest in
their generative capabilities, leading to the development of various commercial
applications. The high cost of using the models drives application builders to
maximize the value of generation under a limited inference budget. This paper
presents a study of optimizing inference hyperparameters like the number of
responses, temperature and max tokens, which significantly affects the
utility/cost of text generation. We design a framework named EcoOptiGen which
leverages economical hyperparameter optimization and cost-based pruning.
Experiments with the latest GPT-3.5 models on a variety of tasks verify its
effectiveness. EcoOptiGen is implemented in the FLAML library:
https://github.com/microsoft/FLAML, and we provide one example of using it at:
https://microsoft.github.io/FLAML/docs/Examples/Integrate%20-%20OpenAI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extrapolative Controlled Sequence Generation via Iterative Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishakh Padmakumar, Richard Yuanzhe Pang, He He, Ankur P. Parikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of extrapolative controlled generation, i.e., generating
sequences with attribute values beyond the range seen in training. This task is
of significant importance in automated design, especially drug discovery, where
the goal is to design novel proteins that are \textit{better} (e.g., more
stable) than existing sequences. Thus, by definition, the target sequences and
their attribute values are out of the training distribution, posing challenges
to existing methods that aim to directly generate the target sequence. Instead,
in this work, we propose Iterative Controlled Extrapolation (ICE) which
iteratively makes local edits to a sequence to enable extrapolation. We train
the model on synthetically generated sequence pairs that demonstrate small
improvement in the attribute value. Results on one natural language task
(sentiment analysis) and two protein engineering tasks (ACE2 stability and AAV
fitness) show that ICE considerably outperforms state-of-the-art approaches
despite its simplicity. Our code and models are available at:
https://github.com/vishakhpk/iter-extrapolation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Models of symbol emergence in communication: a conceptual <span class="highlight-title">review</span> and a
  guide for avoiding local minima 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Zubek, Tomasz Korbak, Joanna Rączaszek-Leonardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational simulations are a popular method for testing hypotheses about
the emergence of communication. This kind of research is performed in a variety
of traditions including language evolution, developmental psychology, cognitive
science, machine learning, robotics, etc. The motivations for the models are
different, but the operationalizations and methods used are often similar. We
identify the assumptions and explanatory targets of several most representative
models and summarise the known results. We claim that some of the assumptions
-- such as portraying meaning in terms of mapping, focusing on the descriptive
function of communication, modelling signals with amodal tokens -- may hinder
the success of modelling. Relaxing these assumptions and foregrounding the
interactions of embodied and situated agents allows one to systematise the
multiplicity of pressures under which symbolic systems evolve. In line with
this perspective, we sketch the road towards modelling the emergence of
meaningful symbolic communication, where symbols are simultaneously grounded in
action and perception and form an abstract system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Detection of Industry Sectors in Legal Articles Using Machine
  Learning Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Yang, Stella Hadjiantoni, Yunfei Long, Ruta Petraityte, Berthold Lausen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to automatically identify industry sector coverage in articles on
legal developments, or any kind of news articles for that matter, can bring
plentiful of benefits both to the readers and the content creators themselves.
By having articles tagged based on industry coverage, readers from all around
the world would be able to get to legal news that are specific to their region
and professional industry. Simultaneously, writers would benefit from
understanding which industries potentially lack coverage or which industries
readers are currently mostly interested in and thus, they would focus their
writing efforts towards more inclusive and relevant legal news coverage. In
this paper, a Machine Learning-powered industry analysis approach which
combined Natural Language Processing (NLP) with Statistical and Machine
Learning (ML) techniques was investigated. A dataset consisting of over 1,700
annotated legal articles was created for the identification of six industry
sectors. Text and legal based features were extracted from the text. Both
traditional ML methods (e.g. gradient boosting machine algorithms, and
decision-tree based algorithms) and deep neural network (e.g. transformer
models) were applied for performance comparison of predictive models. The
system achieved promising results with area under the receiver operating
characteristic curve scores above 0.90 and F-scores above 0.81 with respect to
the six industry sectors. The experimental results show that the suggested
automated industry analysis which employs ML techniques allows the processing
of large collections of text data in an easy, efficient, and scalable way.
Traditional ML methods perform better than deep neural networks when only a
small and domain-specific training data is available for the study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 5 figures, 3 tables. Paper was presented at 'Classification
  and Data Science in the Digital Age', 17th conference of the International
  Federation of Classification Societies (IFCS2022), Porto, Portugal,
  https://ifcs2022.fep.up.pt/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Student's t-Distribution: On Measuring the Inter-Rater Reliability When
  the Observations are Scarce 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Serge Gladkoff, Lifeng Han, Goran Nenadic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In natural language processing (NLP) we always rely on human judgement as the
golden quality evaluation method. However, there has been an ongoing debate on
how to better evaluate inter-rater reliability (IRR) levels for certain
evaluation tasks, such as translation quality evaluation (TQE), especially when
the data samples (observations) are very scarce. In this work, we first
introduce the study on how to estimate the confidence interval for the
measurement value when only one data (evaluation) point is available. Then,
this leads to our example with two human-generated observational scores, for
which, we introduce ``Student's \textit{t}-Distribution'' method and explain
how to use it to measure the IRR score using only these two data points, as
well as the confidence intervals (CIs) of the quality evaluation. We give
quantitative analysis on how the evaluation confidence can be greatly improved
by introducing more observations, even if only one extra observation. We
encourage researchers to report their IRR scores in all possible means, e.g.
using Student's \textit{t}-Distribution method whenever possible; thus making
the NLP evaluation more meaningful, transparent, and trustworthy. This
\textit{t}-Distribution method can be also used outside of NLP fields to
measure IRR level for trustworthy evaluation of experimental investigations,
whenever the observational data is scarce.
  Keywords: Inter-Rater Reliability (IRR); Scarce Observations; Confidence
Intervals (CIs); Natural Language Processing (NLP); Translation Quality
Evaluation (TQE); Student's \textit{t}-Distribution
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Check Your Facts and Try Again: Improving Large Language Models with
  External Knowledge and Automated Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.12813v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.12813v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, Jianfeng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), such as ChatGPT, are able to generate
human-like, fluent responses for many downstream tasks, e.g., task-oriented
dialog and question answering. However, applying LLMs to real-world,
mission-critical applications remains challenging mainly due to their tendency
to generate hallucinations and their inability to use external knowledge. This
paper proposes a LLM-Augmenter system, which augments a black-box LLM with a
set of plug-and-play modules. Our system makes the LLM generate responses
grounded in external knowledge, e.g., stored in task-specific databases. It
also iteratively revises LLM prompts to improve model responses using feedback
generated by utility functions, e.g., the factuality score of a LLM-generated
response. The effectiveness of LLM-Augmenter is empirically validated on two
types of scenarios, task-oriented dialog and open-domain question answering.
LLM-Augmenter significantly reduces ChatGPT's hallucinations without
sacrificing the fluency and informativeness of its responses. We make the
source code and models publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ kogito: A Commonsense Knowledge Inference Toolkit <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08451v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08451v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mete Ismayilzada, Antoine Bosselut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present kogito, an open-source tool for generating
commonsense inferences about situations described in text. kogito provides an
intuitive and extensible interface to interact with natural language generation
models that can be used for hypothesizing commonsense knowledge inference from
a textual input. In particular, kogito offers several features for targeted,
multi-granularity knowledge generation. These include a standardized API for
training and evaluating knowledge models, and generating and filtering
inferences from them. We also include helper functions for converting natural
language texts into a format ingestible by knowledge models - intermediate
pipeline stages such as knowledge head extraction from text, heuristic and
model-based knowledge head-relation matching, and an ability to define and use
custom knowledge relations. We make the code for kogito available at
https://github.com/epfl-nlp/kogito along with thorough documentation at
https://kogito.readthedocs.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL 2023 Camera ready, 9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A data science and machine learning approach to continuous analysis of
  Shakespeare's plays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06024v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06024v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles Swisher, Lior Shamir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The availability of quantitative methods that can analyze text has provided
new ways of examining literature in a manner that was not available in the
pre-information era. Here we apply comprehensive machine learning analysis to
the work of William Shakespeare. The analysis shows clear change in style of
writing over time, with the most significant changes in the sentence length,
frequency of adjectives and adverbs, and the sentiments expressed in the text.
Applying machine learning to make a stylometric prediction of the year of the
play shows a Pearson correlation of 0.71 between the actual and predicted year,
indicating that Shakespeare's writing style as reflected by the quantitative
measurements changed over time. Additionally, it shows that the stylometrics of
some of the plays is more similar to plays written either before or after the
year they were written. For instance, Romeo and Juliet is dated 1596, but is
more similar in stylometrics to plays written by Shakespeare after 1600. The
source code for the analysis is available for free download.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal of Data Mining and Digital Humanities, accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lila: A Unified Benchmark for Mathematical Reasoning <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.17517v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.17517v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, Ashwin Kalyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical reasoning skills are essential for general-purpose intelligent
systems to perform tasks from grocery shopping to climate modeling. Towards
evaluating and improving AI systems in this domain, we propose LILA, a unified
mathematical reasoning benchmark consisting of 23 diverse tasks along four
dimensions: (i) mathematical abilities e.g., arithmetic, calculus (ii) language
format e.g., question-answering, fill-in-the-blanks (iii) language diversity
e.g., no language, simple language (iv) external knowledge e.g., commonsense,
physics. We construct our benchmark by extending 20 datasets benchmark by
collecting task instructions and solutions in the form of Python programs,
thereby obtaining explainable solutions in addition to the correct answer. We
additionally introduce two evaluation datasets to measure out-of-distribution
performance and robustness to language perturbation. Finally, we introduce
BHASKARA, a general-purpose mathematical reasoning model trained on LILA.
Importantly, we find that multi-tasking leads to significant improvements
(average relative improvement of 21.83% F1 score vs. single-task models), while
the best performing model only obtains 60.40%, indicating the room for
improvement in general mathematical reasoning and understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Context Pattern Generation for Entity Set Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08087v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08087v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghui Li, Shulin Huang, Xinwei Zhang, Qingyu Zhou, Yangning Li, Ruiyang Liu, Yunbo Cao, Hai-Tao Zheng, Ying Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity Set Expansion (ESE) is a valuable task that aims to find entities of
the target semantic class described by given seed entities. Various Natural
Language Processing (NLP) and Information Retrieval (IR) downstream
applications have benefited from ESE due to its ability to discover knowledge.
Although existing corpus-based ESE methods have achieved great progress, they
still rely on corpora with high-quality entity information annotated, because
most of them need to obtain the context patterns through the position of the
entity in a sentence. Therefore, the quality of the given corpora and their
entity annotation has become the bottleneck that limits the performance of such
methods. To overcome this dilemma and make the ESE models free from the
dependence on entity annotation, our work aims to explore a new ESE paradigm,
namely corpus-independent ESE. Specifically, we devise a context pattern
generation module that utilizes autoregressive language models (e.g., GPT-2) to
automatically generate high-quality context patterns for entities. In addition,
we propose the GAPA, a novel ESE framework that leverages the aforementioned
GenerAted PAtterns to expand target entities. Extensive experiments and
detailed analyses on three widely used datasets demonstrate the effectiveness
of our method. All the codes of our experiments are available at
https://github.com/geekjuruo/GAPA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weakly Supervised Concept Map Generation through Task-Guided Graph
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.15720v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.15720v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaying Lu, Xiangjue Dong, Carl Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed the rapid development of concept map generation
techniques due to their advantages in providing well-structured summarization
of knowledge from free texts. Traditional unsupervised methods do not generate
task-oriented concept maps, whereas deep generative models require large
amounts of training data. In this work, we present GT-D2G (Graph
Translation-based Document To Graph), an automatic concept map generation
framework that leverages generalized NLP pipelines to derive semantic-rich
initial graphs, and translates them into more concise structures under the weak
supervision of downstream task labels. The concept maps generated by GT-D2G can
provide interpretable summarization of structured knowledge for the input
texts, which are demonstrated through human evaluation and case studies on
three real-world corpora. Further experiments on the downstream task of
document classification show that GT-D2G beats other concept map generation
methods. Moreover, we specifically validate the labeling efficiency of GT-D2G
in the label-efficient learning setting and the flexibility of generated graph
sizes in controlled hyper-parameter studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TKDE. All code and data available at
  https://github.com/lujiaying/GT-doc2graph</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Adaptation of <span class="highlight-title">Transformer</span>-Based Models using Unlabeled Data for
  Relevance and Polarity Classification of German Customer Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Idrissi-Yaghir, Henning Schäfer, Nadja Bauer, Christoph M. Friedrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding customer feedback is becoming a necessity for companies to
identify problems and improve their products and services. Text classification
and sentiment analysis can play a major role in analyzing this data by using a
variety of machine and deep learning approaches. In this work, different
transformer-based models are utilized to explore how efficient these models are
when working with a German customer feedback dataset. In addition, these
pre-trained models are further analyzed to determine if adapting them to a
specific domain using unlabeled data can yield better results than
off-the-shelf pre-trained models. To evaluate the models, two downstream tasks
from the GermEval 2017 are considered. The experimental results show that
transformer-based models can reach significant improvements compared to a
fastText baseline and outperform the published scores and previous models. For
the subtask Relevance Classification, the best models achieve a micro-averaged
$F1$-Score of 96.1 % on the first test set and 95.9 % on the second one, and a
score of 85.1 % and 85.3 % for the subtask Polarity Classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Complete</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Inferential Reproducibility of Machine Learning Research <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.04054v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.04054v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Hagmann, Philipp Meier, Stefan Riezler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliability of machine learning evaluation -- the consistency of observed
evaluation scores across replicated model training runs -- is affected by
several sources of nondeterminism which can be regarded as measurement noise.
Current tendencies to remove noise in order to enforce reproducibility of
research results neglect inherent nondeterminism at the implementation level
and disregard crucial interaction effects between algorithmic noise factors and
data properties. This limits the scope of conclusions that can be drawn from
such experiments. Instead of removing noise, we propose to incorporate several
sources of variance, including their interaction with data properties, into an
analysis of significance and reliability of machine learning evaluation, with
the aim to draw inferences beyond particular instances of trained models. We
show how to use linear mixed effects models (LMEMs) to analyze performance
evaluation scores, and to conduct statistical inference with a generalized
likelihood ratio test (GLRT). This allows us to incorporate arbitrary sources
of noise like meta-parameter variations into statistical significance testing,
and to assess performance differences conditional on data properties.
Furthermore, a variance component analysis (VCA) enables the analysis of the
contribution of noise sources to overall variance and the computation of a
reliability coefficient by the ratio of substantial to total variance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2023 (see https://openreview.net/pdf?id=li4GQCQWkv)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grounding Language with Visual Affordances over Unstructured Data <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01911v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01911v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oier Mees, Jessica Borja-Diaz, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown that Large Language Models (LLMs) can be applied to
ground natural language to a wide variety of robot skills. However, in
practice, learning multi-task, language-conditioned robotic skills typically
requires large-scale data collection and frequent human intervention to reset
the environment or help correcting the current policies. In this work, we
propose a novel approach to efficiently learn general-purpose
language-conditioned robot skills from unstructured, offline and reset-free
data in the real world by exploiting a self-supervised visuo-lingual affordance
model, which requires annotating as little as 1% of the total data with
language. We evaluate our method in extensive experiments both in simulated and
real-world robotic tasks, achieving state-of-the-art performance on the
challenging CALVIN benchmark and learning over 25 distinct visuomotor
manipulation tasks with a single policy in the real world. We find that when
paired with LLMs to break down abstract natural language instructions into
subgoals via few-shot prompting, our method is capable of completing
long-horizon, multi-tier tasks in the real world, while requiring an order of
magnitude less data than previous approaches. Code and videos are available at
http://hulc2.cs.uni-freiburg.de
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2023 IEEE International Conference on Robotics and
  Automation (ICRA). Project website: http://hulc2.cs.uni-freiburg.de</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-MEC Cooperation Based VR Video Transmission and Cache using
  K-Shortest Paths Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwen Xia, Luyao Chen, Yong Tang, Ting Yang, Wenyong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent network architectures, multi-MEC cooperative caching has been
introduced to reduce the transmission latency of VR videos, in which MEC
servers' computing and caching capability are utilized to optimize the
transmission process. However, many solutions that use the computing capability
of MEC servers ignore the additional arithmetic power consumed by the codec
process, thus making them infeasible. Besides, the minimum cache unit is
usually the entire VR video, which makes caching inefficient.
  To address these challenges, we split VR videos into tile files for caching
based on the current popular network architecture and provide a reliable
transmission mechanism and an effective caching strategy. Since the number of
different tile files N is too large, the current cooperative caching algorithms
do not cope with such large-scale input data. We further analyze the problem
and propose an optimized k-shortest paths (OKSP) algorithm with an upper bound
time complexity of O((K * M + N) * M * logN)), and suitable for shortest paths
with restricted number of edges, where K is the total number of tiles that all
M MEC servers can cache in the collaboration domain. And we prove the OKSP
algorithm can compute the caching scheme with the lowest average latency in any
case, which means the solution given is the exact solution. The simulation
results show that the OKSP algorithm has excellent speed for solving
large-scale data and consistently outperforms other caching algorithms in the
experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CaDM: Codec-aware Diffusion Modeling for Neural-enhanced Video Streaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihua Zhou, Ruibin Li, Song Guo, Peiran Dong, Yi Liu, Jingcai Guo, Zhenda Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed the dramatic growth of Internet video traffic,
where the video bitstreams are often compressed and delivered in low quality to
fit the streamer's uplink bandwidth. To alleviate the quality degradation, it
comes the rise of Neural-enhanced Video Streaming (NVS), which shows great
prospects for recovering low-quality videos by mostly deploying neural
super-resolution (SR) on the media server. Despite its benefit, we reveal that
current mainstream works with SR enhancement have not achieved the desired
rate-distortion trade-off between bitrate saving and quality restoration, due
to: (1) overemphasizing the enhancement on the decoder side while omitting the
co-design of encoder, (2) limited generative capacity to recover high-fidelity
perceptual details, and (3) optimizing the compression-and-restoration pipeline
from the resolution perspective solely, without considering color bit-depth.
Aiming at overcoming these limitations, we are the first to conduct an
encoder-decoder (i.e., codec) synergy by leveraging the inherent
visual-generative property of diffusion models. Specifically, we present the
Codec-aware Diffusion Modeling (CaDM), a novel NVS paradigm to significantly
reduce streaming delivery bitrates while holding pretty higher restoration
capacity over existing methods. First, CaDM improves the encoder's compression
efficiency by simultaneously reducing resolution and color bit-depth of video
frames. Second, CaDM empowers the decoder with high-quality enhancement by
making the denoising diffusion restoration aware of encoder's resolution-color
conditions. Evaluation on public cloud services with OpenMMLab benchmarks shows
that CaDM effectively saves up to 5.12 - 21.44 times bitrates based on common
video standards and achieves much better recovery quality (e.g., FID of 0.61)
over state-of-the-art neural-enhancing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video Question Answering Using CLIP-Guided Visual-Text Attention <span class="chip">ICIP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhong Ye, Weikai Kong, Chenglin Yao, Jianfeng Ren, Xudong Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-modal learning of video and text plays a key role in Video Question
Answering (VideoQA). In this paper, we propose a visual-text attention
mechanism to utilize the Contrastive Language-Image Pre-training (CLIP) trained
on lots of general domain language-image pairs to guide the cross-modal
learning for VideoQA. Specifically, we first extract video features using a
TimeSformer and text features using a BERT from the target application domain,
and utilize CLIP to extract a pair of visual-text features from the
general-knowledge domain through the domain-specific learning. We then propose
a Cross-domain Learning to extract the attention information between visual and
linguistic features across the target domain and general domain. The set of
CLIP-guided visual-text features are integrated to predict the answer. The
proposed method is evaluated on MSVD-QA and MSRVTT-QA datasets, and outperforms
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the 2023 IEEE International Conference on Image
  Processing (ICIP 2023)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-07T00:00:00Z">2023-03-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clustering large 3D volumes: A sampling-based approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Lang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many applications of X-ray computed tomography, an unsupervised
segmentation of the reconstructed 3D volumes forms an important step in the
image processing chain for further investigation of the digitized object.
Therefore, the goal is to train a clustering algorithm on the volume, which
produces a voxelwise classification by assigning a cluster index to each voxel.
However, clustering methods, e.g., K-Means, typically have an asymptotic
polynomial runtime with respect to the dataset size, and thus, these techniques
are rarely applicable to large volumes. In this work, we introduce a novel
clustering technique based on random sampling, which allows for the voxelwise
classification of arbitrarily large volumes. The presented method conducts
efficient linear passes over the data to extract a representative random sample
of a fixed size on which the classifier can be trained. Then, a final linear
pass performs the segmentation and assigns a cluster index to each individual
voxel. Quantitative and qualitative evaluations show that excellent results can
be achieved even with a very small sample size. Consequently, the unsupervised
segmentation by means of clustering becomes feasible for arbitrarily large
volumes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatically Summarizing Evidence from Clinical Trials: A Prototype
  Highlighting Current Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjana Ramprasad, Denis Jered McInerney, Iain J. Marshal, Byron C. Wallace
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present TrialsSummarizer, a system that aims to automatically summarize
evidence presented in the set of randomized controlled trials most relevant to
a given query. Building on prior work, the system retrieves trial publications
matching a query specifying a combination of condition, intervention(s), and
outcome(s), and ranks these according to sample size and estimated study
quality. The top-k such studies are passed through a neural multi-document
summarization system, yielding a synopsis of these trials. We consider two
architectures: A standard sequence-to-sequence model based on BART and a
multi-headed architecture intended to provide greater transparency to
end-users. Both models produce fluent and relevant summaries of evidence
retrieved for queries, but their tendency to introduce unsupported statements
render them inappropriate for use in this domain at present. The proposed
architecture may help users verify outputs allowing users to trace generated
tokens back to inputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Privacy Preserving System for Movie Recommendations using Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Neumann, Andreas Lutz, Karsten Müller, Wojciech Samek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have become ubiquitous in the past years. They solve the
tyranny of choice problem faced by many users, and are employed by many online
businesses to drive engagement and sales. Besides other criticisms, like
creating filter bubbles within social networks, recommender systems are often
reproved for collecting considerable amounts of personal data. However, to
personalize recommendations, personal information is fundamentally required. A
recent distributed learning scheme called federated learning has made it
possible to learn from personal user data without its central collection.
Accordingly, we present a complete recommender system for movie
recommendations, which provides privacy and thus trustworthiness on two levels:
First, it is trained using federated learning and thus is, by its very nature,
privacy-preserving, while still enabling individual users to benefit from
global insights. And second, a novel federated learning scheme, FedQ, is
employed, which not only addresses the problem of non-i.i.d. and small local
datasets, but also prevents input data reconstruction attacks by aggregating
client models early. To reduce the communication overhead, compression is
applied, which significantly reduces the exchanged neural network updates to a
fraction of their original data. We conjecture that it may also improve data
privacy through its lossy quantization stage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the ACM TORS Special Issue on Trustworthy Recommender
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AugTriever: Unsupervised Dense Retrieval by Scalable Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Meng, Ye Liu, Semih Yavuz, Divyansh Agarwal, Lifu Tu, Ning Yu, Jianguo Zhang, Meghana Bhat, Yingbo Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense retrievers have made significant strides in text retrieval and
open-domain question answering, even though most achievements were made
possible only with large amounts of human supervision. In this work, we aim to
develop unsupervised methods by proposing two methods that create pseudo
query-document pairs and train dense retrieval models in an annotation-free and
scalable manner: query extraction and transferred query generation. The former
method produces pseudo queries by selecting salient spans from the original
document. The latter utilizes generation models trained for other NLP tasks
(e.g., summarization) to produce pseudo queries. Extensive experiments show
that models trained with the proposed augmentation methods can perform
comparably well (or better) to multiple strong baselines. Combining those
strategies leads to further improvements, achieving the state-of-the-art
performance of unsupervised dense retrieval on both BEIR and ODQA datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unifying Remote Sensing Image Retrieval and Classification with Robust
  Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.13392v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.13392v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitri Gominski, Valérie Gouet-Brunet, Liming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in high resolution remote sensing image analysis are currently
hampered by the difficulty of gathering enough annotated data for training deep
learning methods, giving rise to a variety of small datasets and associated
dataset-specific methods. Moreover, typical tasks such as classification and
retrieval lack a systematic evaluation on standard benchmarks and training
datasets, which make it hard to identify durable and generalizable scientific
contributions. We aim at unifying remote sensing image retrieval and
classification with a new large-scale training and testing dataset, SF300,
including both vertical and oblique aerial images and made available to the
research community, and an associated fine-tuning method. We additionally
propose a new adversarial fine-tuning method for global descriptors. We show
that our framework systematically achieves a boost of retrieval and
classification performance on nine different datasets compared to an ImageNet
pretrained baseline, with currently no other method to compare to.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Performance margin with the proposed method is not statistically
  significant. Please refer to http://alegoria.ign.fr/en/SF300_dataset if you
  are interested in the dataset</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LambdaKG: A Library for <span class="highlight-title">Pre-train</span>ed Language Model-Based Knowledge Graph
  Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00305v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00305v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Xie, Zhoubo Li, Xiaohan Wang, Yuqi Zhu, Ningyu Zhang, Jintian Zhang, Siyuan Cheng, Bozhong Tian, Shumin Deng, Feiyu Xiong, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graphs (KGs) often have two characteristics: heterogeneous graph
structure and text-rich entity/relation information. Text-based KG embeddings
can represent entities by encoding descriptions with pre-trained language
models, but no open-sourced library is specifically designed for KGs with PLMs
at present. In this paper, we present LambdaKG, a library for KGE that equips
with many pre-trained language models (e.g., BERT, BART, T5, GPT-3), and
supports various tasks (e.g., knowledge graph completion, question answering,
recommendation, and knowledge probing). LambdaKG is publicly open-sourced at
https://github.com/zjunlp/PromptKG/tree/main/lambdaKG, with a demo video at
http://deepke.zjukg.cn/lambdakg.mp4 and long-term maintenance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress and the project website is
  https://zjunlp.github.io/project/promptkg/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAMDR: A Model Agnostic Learning Method for Multi-Domain Recommendation <span class="chip">ICDE 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.12524v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.12524v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linhao Luo, Yumeng Li, Buyu Gao, Shuai Tang, Sinan Wang, Jiancheng Li, Tanchao Zhu, Jiancai Liu, Zhao Li, Shirui Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale e-commercial platforms in the real-world usually contain various
recommendation scenarios (domains) to meet demands of diverse customer groups.
Multi-Domain Recommendation (MDR), which aims to jointly improve
recommendations on all domains and easily scales to thousands of domains, has
attracted increasing attention from practitioners and researchers. Existing MDR
methods usually employ a shared structure and several specific components to
respectively leverage reusable features and domain-specific information.
However, data distribution differs across domains, making it challenging to
develop a general model that can be applied to all circumstances. Additionally,
during training, shared parameters often suffer from the domain conflict while
specific parameters are inclined to overfitting on data sparsity domains. we
first present a scalable MDR platform served in Taobao that enables to provide
services for thousands of domains without specialists involved. To address the
problems of MDR methods, we propose a novel model agnostic learning framework,
namely MAMDR, for the multi-domain recommendation. Specifically, we first
propose a Domain Negotiation (DN) strategy to alleviate the conflict between
domains. Then, we develop a Domain Regularization (DR) to improve the
generalizability of specific parameters by learning from other domains. We
integrate these components into a unified framework and present MAMDR, which
can be applied to any model structure to perform multi-domain recommendation.
Finally, we present a large-scale implementation of MAMDR in the Taobao
application and construct various public MDR benchmark datasets which can be
used for following studies. Extensive experiments on both benchmark datasets
and industry datasets demonstrate the effectiveness and generalizability of
MAMDR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by ICDE 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FSVVD: A <span class="highlight-title">Dataset</span> of Full Scene Volumetric Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyuan Hu, Yili Jin, Haowen Yang, Junhua Liu, Fangxin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed a rapid development of immersive multimedia which
bridges the gap between the real world and virtual space. Volumetric videos, as
an emerging representative 3D video paradigm that empowers extended reality,
stand out to provide unprecedented immersive and interactive video watching
experience. Despite the tremendous potential, the research towards 3D
volumetric video is still in its infancy, relying on sufficient and complete
datasets for further exploration. However, existing related volumetric video
datasets mostly only include a single object, lacking details about the scene
and the interaction between them. In this paper, we focus on the current most
widely used data format, point cloud, and for the first time release a
full-scene volumetric video dataset that includes multiple people and their
daily activities interacting with the external environments. Comprehensive
dataset description and analysis are conducted, with potential usage of this
dataset. The dataset and additional tools can be accessed via the following
website: https://cuhksz-inml.github.io/full_scene_volumetric_video_dataset/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by MMSys'23 Open Dataset and Software Track, A preliminary
  version. The dataset and additional tools can be accessed via
  https://cuhksz-inml.github.io/full_scene_volumetric_video_dataset/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approach to Learning Generalized Audio Representation Through Batch
  Embedding Covariance Regularization and Constant-Q Transforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankit Shah, Shuyi Chen, Kejun Zhou, Yue Chen, Bhiksha Raj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General-purpose embedding is highly desirable for few-shot even zero-shot
learning in many application scenarios, including audio tasks. In order to
understand representations better, we conducted a thorough error analysis and
visualization of HEAR 2021 submission results. Inspired by the analysis, this
work experiments with different front-end audio preprocessing methods,
including Constant-Q Transform (CQT) and Short-time Fourier transform (STFT),
and proposes a Batch Embedding Covariance Regularization (BECR) term to uncover
a more holistic simulation of the frequency information received by the human
auditory system. We tested the models on the suite of HEAR 2021 tasks, which
encompass a broad category of tasks. Preliminary results show (1) the proposed
BECR can incur a more dispersed embedding on the test set, (2) BECR improves
the PaSST model without extra computation complexity, and (3) STFT
preprocessing outperforms CQT in all tasks we tested.
Github:https://github.com/ankitshah009/general_audio_embedding_hear_2021
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report, 10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compose & Embellish: Well-Structured Piano Performance Generation via A
  Two-Stage Approach <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.08212v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.08212v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shih-Lun Wu, Yi-Hsuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Even with strong sequence models like Transformers, generating expressive
piano performances with long-range musical structures remains challenging.
Meanwhile, methods to compose well-structured melodies or lead sheets (melody +
chords), i.e., simpler forms of music, gained more success. Observing the
above, we devise a two-stage Transformer-based framework that Composes a lead
sheet first, and then Embellishes it with accompaniment and expressive touches.
Such a factorization also enables pretraining on non-piano data. Our objective
and subjective experiments show that Compose & Embellish shrinks the gap in
structureness between a current state of the art and real performances by half,
and improves other musical aspects such as richness and coherence as well.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning for Predictive Analytics in Reversible Steganography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.06924v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.06924v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ching-Chun Chang, Xu Wang, Sisheng Chen, Isao Echizen, Victor Sanchez, Chang-Tsun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning is regarded as a promising solution for reversible
steganography. There is an accelerating trend of representing a reversible
steo-system by monolithic neural networks, which bypass intermediate operations
in traditional pipelines of reversible steganography. This end-to-end paradigm,
however, suffers from imperfect reversibility. By contrast, the modular
paradigm that incorporates neural networks into modules of traditional
pipelines can stably guarantee reversibility with mathematical explainability.
Prediction-error modulation is a well-established reversible steganography
pipeline for digital images. It consists of a predictive analytics module and a
reversible coding module. Given that reversibility is governed independently by
the coding module, we narrow our focus to the incorporation of neural networks
into the analytics module, which serves the purpose of predicting pixel
intensities and a pivotal role in determining capacity and imperceptibility.
The objective of this study is to evaluate the impacts of different training
configurations upon predictive accuracy of neural networks and provide
practical insights. In particular, we investigate how different initialisation
strategies for input images may affect the learning process and how different
training strategies for dual-layer prediction respond to the problem of
distributional shift. Furthermore, we compare steganographic performance of
various model architectures with different loss functions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Neural Networks for Reversible Steganography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.02478v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.02478v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ching-Chun Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep learning have led to a paradigm shift in the field of
reversible steganography. A fundamental pillar of reversible steganography is
predictive modelling which can be realised via deep neural networks. However,
non-trivial errors exist in inferences about some out-of-distribution and noisy
data. In view of this issue, we propose to consider uncertainty in predictive
models based upon a theoretical framework of Bayesian deep learning, thereby
creating an adaptive steganographic system. Most modern deep-learning models
are regarded as deterministic because they only offer predictions while failing
to provide uncertainty measurement. Bayesian neural networks bring a
probabilistic perspective to deep learning and can be regarded as self-aware
intelligent machinery; that is, a machine that knows its own limitations. To
quantify uncertainty, we apply Bayesian statistics to model the predictive
distribution and approximate it through Monte Carlo sampling with stochastic
forward passes. We further show that predictive uncertainty can be disentangled
into aleatoric and epistemic uncertainties and these quantities can be learnt
unsupervised. Experimental results demonstrate an improvement delivered by
Bayesian uncertainty analysis upon steganographic rate-distortion performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the predictability in reversible steganography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.02518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.02518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ching-Chun Chang, Xu Wang, Sisheng Chen, Hitoshi Kiya, Isao Echizen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial neural networks have advanced the frontiers of reversible
steganography. The core strength of neural networks is the ability to render
accurate predictions for a bewildering variety of data. Residual modulation is
recognised as the most advanced reversible steganographic algorithm for digital
images. The pivot of this algorithm is predictive analytics in which pixel
intensities are predicted given some pixel-wise contextual information. This
task can be perceived as a low-level vision problem and hence neural networks
for addressing a similar class of problems can be deployed. On top of the prior
art, this paper investigates predictability of pixel intensities based on
supervised and unsupervised learning frameworks. Predictability analysis
enables adaptive data embedding, which in turn leads to a better trade-off
between capacity and imperceptibility. While conventional methods estimate
predictability by the statistics of local image patterns, learning-based
frameworks consider further the degree to which correct predictions can be made
by a designated predictor. Not only should the image patterns be taken into
account but also the predictor in use. Experimental results show that
steganographic performance can be significantly improved by incorporating the
learning-based predictability analysers into a reversible steganographic
system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Video Quality Assessment on User Generated Contents from
  Aesthetic and Technical Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.04894v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.04894v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, Weisi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid increase in user-generated-content (UGC) videos calls for the
development of effective video quality assessment (VQA) algorithms. However,
the objective of the UGC-VQA problem is still ambiguous and can be viewed from
two perspectives: the technical perspective, measuring the perception of
distortions; and the aesthetic perspective, which relates to preference and
recommendation on contents. To understand how these two perspectives affect
overall subjective opinions in UGC-VQA, we conduct a large-scale subjective
study to collect human quality opinions on overall quality of videos as well as
perceptions from aesthetic and technical perspectives. The collected
Disentangled Video Quality Database (DIVIDE-3k) confirms that human quality
opinions on UGC videos are universally and inevitably affected by both
aesthetic and technical perspectives. In light of this, we propose the
Disentangled Objective Video Quality Evaluator (DOVER) to learn the quality of
UGC videos based on the two perspectives. The DOVER proves state-of-the-art
performance in UGC-VQA under very high efficiency. With perspective opinions in
DIVIDE-3k, we further propose DOVER++, the first approach to provide reliable
clear-cut quality evaluations from a single aesthetic or technical perspective.
Code at https://github.com/VQAssessment/DOVER.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Action-<span class="highlight-title">GPT</span>: Leveraging Large-scale Language Models for Improved and
  Generalized Action Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15603v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15603v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Shashank Kalakonda, Shubh Maheshwari, Ravi Kiran Sarvadevabhatla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Action-GPT, a plug-and-play framework for incorporating Large
Language Models (LLMs) into text-based action generation models. Action phrases
in current motion capture datasets contain minimal and to-the-point
information. By carefully crafting prompts for LLMs, we generate richer and
fine-grained descriptions of the action. We show that utilizing these detailed
descriptions instead of the original action phrases leads to better alignment
of text and motion spaces. We introduce a generic approach compatible with
stochastic (e.g. VAE-based) and deterministic (e.g. MotionCLIP) text-to-motion
models. In addition, the approach enables multiple text descriptions to be
utilized. Our experiments show (i) noticeable qualitative and quantitative
improvement in the quality of synthesized motions, (ii) benefits of utilizing
multiple LLM-generated descriptions, (iii) suitability of the prompt function,
and (iv) zero-shot generation capabilities of the proposed approach. Project
page: https://actiongpt.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code, pretrained models and sample videos will be made available at
  \url{https://actiongpt.github.io}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Confidence-based Event-centric Online Video Question Answering on a
  Newly Constructed ATBS <span class="highlight-title">Dataset</span> <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03105v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03105v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weikai Kong, Shuhong Ye, Chenglin Yao, Jianfeng Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks facilitate video question answering (VideoQA), but the
real-world applications on video streams such as CCTV and live cast place
higher demands on the solver. To address the challenges of VideoQA on long
videos of unknown length, we define a new set of problems called Online
Open-ended Video Question Answering (O^2VQA). It requires an online
state-updating mechanism for the solver to decide if the collected information
is sufficient to conclude an answer. We then propose a Confidence-based
Event-centric Online Video Question Answering (CEO-VQA) model to solve this
problem. Furthermore, a dataset called Answer Target in Background Stream
(ATBS) is constructed to evaluate this newly developed online VideoQA
application. Compared to the baseline VideoQA method that watches the whole
video, the experimental results show that the proposed method achieves a
significant performance gain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 2023 IEEE International Conference on
  Acoustics, Speech, and Signal Processing (ICASSP 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time-frequency Network for Robust Speaker Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02673v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02673v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiguo Li, Tianzi Zhang, Xiaobin Liu, Lirong Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The wide deployment of speech-based biometric systems usually demands
high-performance speaker recognition algorithms. However, most of the prior
works for speaker recognition either process the speech in the frequency domain
or time domain, which may produce suboptimal results because both time and
frequency domains are important for speaker recognition. In this paper, we
attempt to analyze the speech signal in both time and frequency domains and
propose the time-frequency network~(TFN) for speaker recognition by extracting
and fusing the features in the two domains. Based on the recent advance of deep
neural networks, we propose a convolution neural network to encode the raw
speech waveform and the frequency spectrum into domain-specific features, which
are then fused and transformed into a classification feature space for speaker
recognition. Experimental results on the publicly available datasets TIMIT and
LibriSpeech show that our framework is effective to combine the information in
the two domains and performs better than the state-of-the-art methods for
speaker recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-06T00:00:00Z">2023-03-06</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Auditing Methodologies Can Impact Our Understanding of YouTube's
  Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarmad Chandio, Daniyal Pirwani Dar, Rishab Nithyanand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data generated by audits of social media websites have formed the basis of
our understanding of the biases presented in algorithmic content recommendation
systems. As legislators around the world are beginning to consider regulating
the algorithmic systems that drive online platforms, it is critical to ensure
the correctness of these inferred biases. However, as we will show in this
paper, doing so is a challenging task for a variety of reasons related to the
complexity of configuration parameters associated with the audits that gather
data from a specific platform.
  Focusing specifically on YouTube, we show that conducting audits to make
inferences about YouTube's recommendation systems is more methodologically
challenging than one might expect. There are many methodological decisions that
need to be considered in order to obtain scientifically valid results, and each
of these decisions incur costs. For example, should an auditor use (expensive
to obtain) logged-in YouTube accounts while gathering recommendations from the
algorithm to obtain more accurate inferences? We explore the impact of this and
many other decisions and make some startling discoveries about the
methodological choices that impact YouTube's recommendations. Taken all
together, our research suggests auditing configuration compromises that YouTube
auditors and researchers can use to reduce audit overhead, both economically
and computationally, without sacrificing accuracy of their inferences.
Similarly, we also identify several configuration parameters that have a
significant impact on the accuracy of measured inferences and should be
carefully considered.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implementation of a noisy hyperlink removal system: A semantic and
  relatedness approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazem Taghandiki, Elnaz Rezaei Ehsan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the volume of data on the web grows, the web structure graph, which is a
graph representation of the web, continues to evolve. The structure of this
graph has gradually shifted from content-based to non-content-based.
Furthermore, spam data, such as noisy hyperlinks, in the web structure graph
adversely affect the speed and efficiency of information retrieval and link
mining algorithms. Previous works in this area have focused on removing noisy
hyperlinks using structural and string approaches. However, these approaches
may incorrectly remove useful links or be unable to detect noisy hyperlinks in
certain circumstances. In this paper, a data collection of hyperlinks is
initially constructed using an interactive crawler. The semantic and
relatedness structure of the hyperlinks is then studied through semantic web
approaches and tools such as the DBpedia ontology. Finally, the removal process
of noisy hyperlinks is carried out using a reasoner on the DBpedia ontology.
Our experiments demonstrate the accuracy and ability of semantic web
technologies to remove noisy hyperlinks
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AmQA: Amharic Question Answering <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tilahun Abedissa, Ricardo Usbeck, Yaregal Assabie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question Answering (QA) returns concise answers or answer lists from natural
language text given a context document. Many resources go into curating QA
datasets to advance robust models' development. There is a surge of QA datasets
for languages like English, however, this is not true for Amharic. Amharic, the
official language of Ethiopia, is the second most spoken Semitic language in
the world. There is no published or publicly available Amharic QA dataset.
Hence, to foster the research in Amharic QA, we present the first Amharic QA
(AmQA) dataset. We crowdsourced 2628 question-answer pairs over 378 Wikipedia
articles. Additionally, we run an XLMR Large-based baseline model to spark
open-domain QA research interest. The best-performing baseline achieves an
F-score of 69.58 and 71.74 in reader-retriever QA and reading comprehension
settings respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LongEval-Retrieval: French-English Dynamic Test Collection for
  Continuous Web Search Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Petra Galuščáková, Romain Deveaud, Gabriela Gonzalez-Saez, Philippe Mulhem, Lorraine Goeuriot, Florina Piroi, Martin Popel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LongEval-Retrieval is a Web document retrieval benchmark that focuses on
continuous retrieval evaluation. This test collection is intended to be used to
study the temporal persistence of Information Retrieval systems and will be
used as the test collection in the Longitudinal Evaluation of Model Performance
Track (LongEval) at CLEF 2023. This benchmark simulates an evolving information
system environment - such as the one a Web search engine operates in - where
the document collection, the query distribution, and relevance all move
continuously, while following the Cranfield paradigm for offline evaluation. To
do that, we introduce the concept of a dynamic test collection that is composed
of successive sub-collections each representing the state of an information
system at a given time step. In LongEval-Retrieval, each sub-collection
contains a set of queries, documents, and soft relevance assessments built from
click models. The data comes from Qwant, a privacy-preserving Web search engine
that primarily focuses on the French market. LongEval-Retrieval also provides a
'mirror' collection: it is initially constructed in the French language to
benefit from the majority of Qwant's traffic, before being translated to
English. This paper presents the creation process of LongEval-Retrieval and
provides baseline runs and analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MABNet: Master Assistant Buddy Network with Hybrid Learning for Image
  Retrieval <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Agarwal, Gyanendra Das, Saksham Aggarwal, Alexander Horsch, Dilip K. Prasad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image retrieval has garnered growing interest in recent times. The current
approaches are either supervised or self-supervised. These methods do not
exploit the benefits of hybrid learning using both supervision and
self-supervision. We present a novel Master Assistant Buddy Network (MABNet)
for image retrieval which incorporates both learning mechanisms. MABNet
consists of master and assistant blocks, both learning independently through
supervision and collectively via self-supervision. The master guides the
assistant by providing its knowledge base as a reference for self-supervision
and the assistant reports its knowledge back to the master by weight transfer.
We perform extensive experiments on public datasets with and without
post-processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy-Preserving Fair Item Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Ao Sun, Sikha Pentyala, Martine De Cock, Golnoosh Farnadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Users worldwide access massive amounts of curated data in the form of
rankings on a daily basis. The societal impact of this ease of access has been
studied and work has been done to propose and enforce various notions of
fairness in rankings. Current computational methods for fair item ranking rely
on disclosing user data to a centralized server, which gives rise to privacy
concerns for the users. This work is the first to advance research at the
conjunction of producer (item) fairness and consumer (user) privacy in rankings
by exploring the incorporation of privacy-preserving techniques; specifically,
differential privacy and secure multi-party computation. Our work extends the
equity of amortized attention ranking mechanism to be privacy-preserving, and
we evaluate its effects with respect to privacy, fairness, and ranking quality.
Our results using real-world datasets show that we are able to effectively
preserve the privacy of users and mitigate unfairness of items without making
additional sacrifices to the quality of rankings in comparison to the ranking
mechanism in the clear.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Feedback Attention Framework via Boundary-Aware Auxiliary and
  Progressive Semantic Optimization for Salient Object Detection in Optical
  Remote Sensing Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02867v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02867v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dejun Feng, Hongyu Chen, Suning Liu, Xingyu Shen, Ziyang Liao, Yakun Xie, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Salient object detection in optical remote sensing image (ORSI-SOD) has
gradually attracted attention thanks to the development of deep learning (DL)
and salient object detection in natural scene image (NSI-SOD). However, NSI and
ORSI are different in many aspects, such as large coverage, complex background,
and large differences in target types and scales. Therefore, a new dedicated
method is needed for ORSI-SOD. In addition, existing methods do not pay
sufficient attention to the boundary of the object, and the completeness of the
final saliency map still needs improvement. To address these issues, we propose
a novel method called Dual Feedback Attention Framework via Boundary-Aware
Auxiliary and Progressive Semantic Optimization (DFA-BASO). First, Boundary
Protection Calibration (BPC) module is proposed to reduce the loss of edge
position information during forward propagation and suppress noise in low-level
features. Second, a Dual Feature Feedback Complementary (DFFC) module is
proposed based on BPC module. It aggregates boundary-semantic dual features and
provides effective feedback to coordinate features across different layers.
Finally, a Strong Semantic Feedback Refinement (SSFR) module is proposed to
obtain more complete saliency maps. This module further refines feature
representation and eliminates feature differences through a unique feedback
mechanism. Extensive experiments on two public datasets show that DFA-BASO
outperforms 15 state-of-the-art methods. Furthermore, this paper strongly
demonstrates the true contribution of DFA-BASO to ORSI-SOD by in-depth analysis
of the visualization figure. All codes can be found at
https://github.com/YUHsss/DFA-BASO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Incremental Update for Neural Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyan Zhang, Sunghun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender Systems (RS) aim to provide personalized suggestions of items for
users against consumer over-choice. Although extensive research has been
conducted to address different aspects and challenges of RS, there still exists
a gap between academic research and industrial applications. Specifically, most
of the existing models still work in an offline manner, in which the
recommender is trained on a large static training set and evaluated on a very
restrictive testing set in a one-time process. RS will stay unchanged until the
next batch retrain is performed. We frame such RS as Batch Update Recommender
Systems (BURS). In reality, they have to face the challenges where RS are
expected to be instantly updated with new data streaming in, and generate
updated recommendations for current user activities based on the newly arrived
data. We frame such RS as Incremental Update Recommender Systems (IURS).
  In this article, we offer a systematic survey of incremental update for
neural recommender systems. We begin the survey by introducing key concepts and
formulating the task of IURS. We then illustrate the challenges in IURS
compared with traditional BURS. Afterwards, we detail the introduction of
existing literature and evaluation issues. We conclude the survey by outlining
some prominent open research issues in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Recommend Using Non-Uniform Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.11248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.11248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanning Chen, Mohsen Bayati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning user preferences for products based on their past purchases or
reviews is at the cornerstone of modern recommendation engines. One
complication in this learning task is that some users are more likely to
purchase products or review them, and some products are more likely to be
purchased or reviewed by the users. This non-uniform pattern degrades the power
of many existing recommendation algorithms, as they assume that the observed
data are sampled uniformly at random among user-product pairs. In addition,
existing literature on modeling non-uniformity either assume user interests are
independent of the products, or lack theoretical understanding. In this paper,
we first model the user-product preferences as a partially observed matrix with
non-uniform observation pattern. Next, building on the literature about
low-rank matrix estimation, we introduce a new weighted trace-norm penalized
regression to predict unobserved values of the matrix. We then prove an upper
bound for the prediction error of our proposed approach. Our upper bound is a
function of a number of parameters that are based on a certain weight matrix
that depends on the joint distribution of users and products. Utilizing this
observation, we introduce a new optimization problem to select a weight matrix
that minimizes the upper bound on the prediction error. The final product is a
new estimator, NU-Recommend, that outperforms existing methods in both
synthetic and real datasets. Our approach aims at accurate predictions for all
users while prioritizing fairness. To achieve this, we employ a bias-variance
tradeoff mechanism that ensures good overall prediction performance without
compromising the predictive accuracy for less active users.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neighborhood Contrastive <span class="highlight-title">Transformer</span> for Change Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunbin Tu, Liang Li, Li Su, Ke Lu, Qingming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Change captioning is to describe the semantic change between a pair of
similar images in natural language. It is more challenging than general image
captioning, because it requires capturing fine-grained change information while
being immune to irrelevant viewpoint changes, and solving syntax ambiguity in
change descriptions. In this paper, we propose a neighborhood contrastive
transformer to improve the model's perceiving ability for various changes under
different scenes and cognition ability for complex syntax structure.
Concretely, we first design a neighboring feature aggregating to integrate
neighboring context into each feature, which helps quickly locate the
inconspicuous changes under the guidance of conspicuous referents. Then, we
devise a common feature distilling to compare two images at neighborhood level
and extract common properties from each image, so as to learn effective
contrastive information between them. Finally, we introduce the explicit
dependencies between words to calibrate the transformer decoder, which helps
better understand complex syntax structure during training. Extensive
experimental results demonstrate that the proposed method achieves the
state-of-the-art performance on three public datasets with different change
scenarios. The code is available at https://github.com/tuyunbin/NCT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TMM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IPA-CLIP: Integrating Phonetic Priors into Vision and Language
  <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chihaya Matsuhira, Marc A. Kastner, Takahiro Komamizu, Takatsugu Hirayama, Keisuke Doman, Yasutomo Kawanishi, Ichiro Ide
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, large-scale Vision and Language (V\&L) pretraining has become the
standard backbone of many multimedia systems. While it has shown remarkable
performance even in unseen situations, it often performs in ways not intuitive
to humans. Particularly, they usually do not consider the pronunciation of the
input, which humans would utilize to understand language, especially when it
comes to unknown words. Thus, this paper inserts phonetic prior into
Contrastive Language-Image Pretraining (CLIP), one of the V\&L pretrained
models, to make it consider the pronunciation similarity among its
pronunciation inputs. To achieve this, we first propose a phoneme embedding
that utilizes the phoneme relationships provided by the International Phonetic
Alphabet (IPA) chart as a phonetic prior. Next, by distilling the frozen CLIP
text encoder, we train a pronunciation encoder employing the IPA-based
embedding. The proposed model named IPA-CLIP comprises this pronunciation
encoder and the original CLIP encoders (image and text). Quantitative
evaluation reveals that the phoneme distribution on the embedding space
represents phonetic relationships more accurately when using the proposed
phoneme embedding. Furthermore, in some multimodal retrieval tasks, we confirm
that the proposed pronunciation encoder enhances the performance of the text
encoder and that the pronunciation encoder handles nonsense words in a more
phonetic manner than the text encoder. Finally, qualitative evaluation verifies
the correlation between the pronunciation encoder and human perception
regarding pronunciation similarity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures, 5 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Butterfly: Multiple Reference Frames Feature Propagation Mechanism for
  Neural Video Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Wang, Haihang Ruan, Fei Xiong, Jiayu Yang, Litian Li, Ronggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using more reference frames can significantly improve the compression
efficiency in neural video compression. However, in low-latency scenarios, most
existing neural video compression frameworks usually use the previous one frame
as reference. Or a few frameworks which use the previous multiple frames as
reference only adopt a simple multi-reference frames propagation mechanism. In
this paper, we present a more reasonable multi-reference frames propagation
mechanism for neural video compression, called butterfly multi-reference frame
propagation mechanism (Butterfly), which allows a more effective feature fusion
of multi-reference frames. By this, we can generate more accurate temporal
context conditional prior for Contextual Coding Module. Besides, when the
number of decoded frames does not meet the required number of reference frames,
we duplicate the nearest reference frame to achieve the requirement, which is
better than duplicating the furthest one. Experiment results show that our
method can significantly outperform the previous state-of-the-art (SOTA), and
our neural codec can achieve -7.6% bitrate save on HEVC Class D dataset when
compares with our base single-reference frame model with the same compression
configuration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by DCC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MultiViz: Towards Visualizing and Understanding Multimodal Models <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.00056v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.00056v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Pu Liang, Yiwei Lyu, Gunjan Chhablani, Nihal Jain, Zihao Deng, Xingbo Wang, Louis-Philippe Morency, Ruslan Salakhutdinov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The promise of multimodal models for real-world applications has inspired
research in visualizing and understanding their internal mechanics with the end
goal of empowering stakeholders to visualize model behavior, perform model
debugging, and promote trust in machine learning models. However, modern
multimodal models are typically black-box neural networks, which makes it
challenging to understand their internal mechanics. How can we visualize the
internal modeling of multimodal interactions in these models? Our paper aims to
fill this gap by proposing MultiViz, a method for analyzing the behavior of
multimodal models by scaffolding the problem of interpretability into 4 stages:
(1) unimodal importance: how each modality contributes towards downstream
modeling and prediction, (2) cross-modal interactions: how different modalities
relate with each other, (3) multimodal representations: how unimodal and
cross-modal interactions are represented in decision-level features, and (4)
multimodal prediction: how decision-level features are composed to make a
prediction. MultiViz is designed to operate on diverse modalities, models,
tasks, and research areas. Through experiments on 8 trained models across 6
real-world tasks, we show that the complementary stages in MultiViz together
enable users to (1) simulate model predictions, (2) assign interpretable
concepts to features, (3) perform error analysis on model misclassifications,
and (4) use insights from error analysis to debug models. MultiViz is publicly
available, will be regularly updated with new interpretation tools and metrics,
and welcomes inputs from the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023. Code available at: https://github.com/pliang279/MultiViz</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perfectly Secure Steganography Using Minimum Entropy Coupling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.14889v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.14889v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Schroeder de Witt, Samuel Sokota, J. Zico Kolter, Jakob Foerster, Martin Strohmeier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Steganography is the practice of encoding secret information into innocuous
content in such a manner that an adversarial third party would not realize that
there is hidden meaning. While this problem has classically been studied in
security literature, recent advances in generative models have led to a shared
interest among security and machine learning researchers in developing scalable
steganography techniques. In this work, we show that a steganography procedure
is perfectly secure under \citet{cachin_perfect}'s information theoretic-model
of steganography if and only if it is induced by a coupling. Furthermore, we
show that, among perfectly secure procedures, a procedure is maximally
efficient if and only if it is induced by a minimum entropy coupling. These
insights yield what are, to the best of our knowledge, the first steganography
algorithms to achieve perfect security guarantees with non-trivial efficiency;
additionally, these algorithms are highly scalable. To provide empirical
validation, we compare a minimum entropy coupling-based approach to three
modern baselines -- arithmetic coding, Meteor, and adaptive dynamic grouping --
using GPT-2 and WaveRNN as communication channels. We find that the minimum
entropy coupling-based approach yields superior encoding efficiency, despite
its stronger security constraints. In aggregate, these results suggest that it
may be natural to view information-theoretic steganography through the lens of
minimum entropy coupling.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2023-03-14T05:24:37.853132989Z">
            2023-03-14 05:24:37 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
